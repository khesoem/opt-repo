owner,repo,commit_hash,commit_url,commit_message
spring-projects,spring-boot,93113a415f1516b75a21822c4912e7946f8868ae,https://github.com/spring-projects/spring-boot/commit/93113a415f1516b75a21822c4912e7946f8868ae,Improve performance by not checking all indexed elements  Update `IndexedElementsBinder` so that bind operations are faster at the expense of not checking that all elements have been bound. The updated code now uses a window of 10 elements and assumes that if no elements are missing from that window then exhaustive checking is not required.  Closes gh-44867
spring-projects,spring-boot,9c25b69c06badb57ba0bf31ee387bbb23a2a2ca2,https://github.com/spring-projects/spring-boot/commit/9c25b69c06badb57ba0bf31ee387bbb23a2a2ca2,Improve performance of MapBinder by calculating items only once  Closes gh-44868
spring-projects,spring-boot,859d074764989a09911eeb3cba474ffcf1b26fac,https://github.com/spring-projects/spring-boot/commit/859d074764989a09911eeb3cba474ffcf1b26fac,Replace streams on hot paths with for loops  Replace a few usages of stream with simple for loops. Although this doesn't seem to make much difference to performance  it does help when profiling applications since it reduces the stack depth.
spring-projects,spring-boot,ae6908e4d87b0dd9077f53294afb65a87d6fd187,https://github.com/spring-projects/spring-boot/commit/ae6908e4d87b0dd9077f53294afb65a87d6fd187,Cache property mappings in ConfigurationPropertyName  Relocate `SystemEnvironmentPropertyMapper` methods into `ConfigurationPropertyName` so that they can be cached to improve performance  Closes gh-44858
spring-projects,spring-boot,81dee5413723796308372d073de6e76d90ee0a40,https://github.com/spring-projects/spring-boot/commit/81dee5413723796308372d073de6e76d90ee0a40,Improve ConfigurationPropertyName equals/hashCode performance  Update `ConfigurationPropertyName` to improve performance of the `equals(...)` and `hashCode()` methods by making the following changes:  - Move element hashCode logic to Element and cache the results - Exit the equals method early if hashcodes don't match - Exit the equals method early if toString() values match  Closes gh-44857
spring-projects,spring-boot,9890872a9aa5a7e2dfed223cf41f12f0a6b8914b,https://github.com/spring-projects/spring-boot/commit/9890872a9aa5a7e2dfed223cf41f12f0a6b8914b,Improve performance of ConcurrentReferenceCachingMetadataReaderFactory  Update `ConcurrentReferenceCachingMetadataReaderFactory` with cache by class name.  Closes gh-42949
spring-projects,spring-boot,c693b2bd8cfa7c10b2791aea58f26d55be548c33,https://github.com/spring-projects/spring-boot/commit/c693b2bd8cfa7c10b2791aea58f26d55be548c33,Add support for webjars-locator-lite  This is a follow-up to spring-projects/spring-framework#27619 This commit adds support for "org.webjars:webjars-locator-lite" for enabling the statis resources chain.  As of this commit  support for "org.webjars:webjars-locator-core" is deprecated for obvious performance reasons.  Closes gh-40146
elastic,elasticsearch,3af0568137b53a1ebc430f36a675b3c2024fe428,https://github.com/elastic/elasticsearch/commit/3af0568137b53a1ebc430f36a675b3c2024fe428,Speed up read dimension fields in TS (#128283)  When reading dimension fields in the TS command  we can skip reading values while the `tsid` remains unchanged to improve performance. I benchmarked this change with the following query:  ``` POST /_query { "query": "TS metrics-hostmetricsreceiver.otel-default | WHERE @timestamp >= \"2025-05-08T18:00:08.001Z\" | STATS cpu = avg(rate(`metrics.process.cpu.time`)) BY host.name  BUCKET(@timestamp  5 minute)" } ```  The total query time was reduced from 51ms to 39ms  with processing time in the time-series source operator reduced from 26ms to 17ms.
elastic,elasticsearch,3551494b9acc37bcb7506aade144e6e677da2fb5,https://github.com/elastic/elasticsearch/commit/3551494b9acc37bcb7506aade144e6e677da2fb5,ESQL: `text ==` and `text !=` pushdown  (#127355)  Reenables `text ==` pushdown and adds support for `text !=` pushdown.  It does so by making `TranslationAware#translatable` return something we can turn into a tri-valued function. It has these values: * `YES` * `NO` * `RECHECK`  `YES` means the `Expression` is entirely pushable into Lucene. They will be pushed into Lucene and removed from the plan.  `NO` means the `Expression` can't be pushed to Lucene at all and will stay in the plan.  `RECHECK` mean the `Expression` can push a query that makes *candidate* matches but must be rechecked. Documents that don't match the query won't match the expression  but documents that match the query might not match the expression. These are pushed to Lucene *and* left in the plan.  This is required because `txt != "b"` can build a *candidate* query against the `txt.keyword` subfield but it can't be sure of the match without loading the `_source` - which we do in the compute engine.  I haven't plugged rally into this  but here's some basic performance tests: ``` Before: not text eq {"took":460 "documents_found":1000000} text eq {"took":432 "documents_found":1000000}  After: text eq {"took":5 "documents_found":1} not text eq {"took":351 "documents_found":800000} ```  This comes from: ``` rm -f /tmp/bulk* for a in {1..1000}; do echo '{"index":{}}' >> /tmp/bulk echo '{"text":"text '$(printf $(($a % 5)))'"}' >> /tmp/bulk done ls -l /tmp/bulk*  passwd="redacted" curl -sk -uelastic:$passwd -HContent-Type:application/json -XDELETE https://localhost:9200/test curl -sk -uelastic:$passwd -HContent-Type:application/json -XPUT https://localhost:9200/test -d'{ "settings": { "index.codec": "best_compression"  "index.refresh_interval": -1 }  "mappings": { "properties": { "many": { "enabled": false } } } }' for a in {1..1000}; do printf %04d: $a curl -sk -uelastic:$passwd -HContent-Type:application/json -XPOST https://localhost:9200/test/_bulk?pretty --data-binary @/tmp/bulk | grep errors done curl -sk -uelastic:$passwd -HContent-Type:application/json -XPOST https://localhost:9200/test/_forcemerge?max_num_segments=1 curl -sk -uelastic:$passwd -HContent-Type:application/json -XPOST https://localhost:9200/test/_refresh echo curl -sk -uelastic:$passwd https://localhost:9200/_cat/indices?v  text_eq() { echo -n "    text eq " curl -sk -uelastic:$passwd -HContent-Type:application/json -XPOST 'https://localhost:9200/_query?pretty' -d'{ "query": "FROM test | WHERE text == \"text 1\" | STATS COUNT(*)"  "pragma": { "data_partitioning": "shard" } }' | jq -c '{took  documents_found}' }  not_text_eq() { echo -n "not text eq " curl -sk -uelastic:$passwd -HContent-Type:application/json -XPOST 'https://localhost:9200/_query?pretty' -d'{ "query": "FROM test | WHERE NOT text == \"text 1\" | STATS COUNT(*)"  "pragma": { "data_partitioning": "shard" } }' | jq -c '{took  documents_found}' }   for a in {1..100}; do text_eq not_text_eq done ```
elastic,elasticsearch,d65f34d173bf419df55cf355639be32c929992a5,https://github.com/elastic/elasticsearch/commit/d65f34d173bf419df55cf355639be32c929992a5,Push down field extraction to time-series source (#127445)  This change pushes down field extractions to the time-series source operator  providing these advantages:  - Avoids building `DocVector` and its forward/backward maps.  - Leverages the `DocValues` cache (i.e.  blocks that are already decompressed/decoded) when loading values  which can be lost when reading blocks with the `ValuesSourceReaderOperator`.  - Eliminates the need to rebuild blocks with backward mappings after reading values.  The following query against the TSDB track previously took 19 seconds but was reduced to 13 seconds with this change:  ``` TS tsdb | STATS sum(rate(kubernetes.container.memory.pagefaults)) by bucket(@timestamp  5minute) ```  Note that with this change:  ``` TS tsdb | STATS sum(rate(kubernetes.container.memory.pagefaults)) by bucket(@timestamp  5minute) ``` now performs as well as:  ``` FROM tsdb | STATS sum(last_over_time(kubernetes.container.memory.pagefaults)) by bucket(@timestamp  5minute) ```  when using the shard level data partitioning. This means the performance of the TS command is comparable to the `FROM` command  except that it does not yet support segment-level or doc-level concurrency. I will try to add support for segment-level concurrency  as document-level partitioning is not useful when iterating over documents in order.
elastic,elasticsearch,d39f4725773c3cc4f15bc15e20abf456d20e194f,https://github.com/elastic/elasticsearch/commit/d39f4725773c3cc4f15bc15e20abf456d20e194f,Make can_match code a little easier to reuse (#126588)  Step 1 to refactoring this with reuse in a per-datanode fashion for batched execution. Some obvious cleanup essentially making this a utility  removing one weird indirection and reducing the use of the actual instance of `CanMatchPreFilterSearchPhase` (this also results in a real performance gain from moving work for sorting shards etc. off of the transport_workers and closer to where its result is used).  This should by relatively trivial to review and allows for a simple follow up that extracts the ability to run an individual round in isolation as well as running the coordinator rewrite phase separately.
elastic,elasticsearch,0c90de5ed56e5d4ac3e1ee4b3e085655d7af6251,https://github.com/elastic/elasticsearch/commit/0c90de5ed56e5d4ac3e1ee4b3e085655d7af6251,Fork time-series source to allow field extractions (#127375)  This change prepares for pushing down field extractions to the time-series source for performance reasons. It is a non-issue  as the actual change will occur in a follow-up.
elastic,elasticsearch,21813604b4e2f1328877b5dc3bb24f87597e2836,https://github.com/elastic/elasticsearch/commit/21813604b4e2f1328877b5dc3bb24f87597e2836,Skip listing MPUs if TTL set to -1 (#127166)  Recent versions of MinIO will sometimes leak multi-part uploads under concurrent load  leaving them in the `ListMultipartUploads` output even though they cannot be aborted. Today this causes repository analysis to fail since compare-and-exchange operations will not even start if there are any pre-existing uploads. This commit makes it possible to skip this pre-flight check (and accept the performance consequences) by adjusting the relevant settings.  Workaround for minio/minio#21189 Closes #122670
elastic,elasticsearch,4f506d47a5c2b3148ec7bca851593708c6adb730,https://github.com/elastic/elasticsearch/commit/4f506d47a5c2b3148ec7bca851593708c6adb730,Optimize time-series source operator (#127095)  This query against the TSDB track took 50 seconds and was reduced to 19 seconds with this changes.  ``` TS tsdb | STATS sum(rate(kubernetes.container.memory.pagefaults)) by bucket(@timestamp  5minute) ```  This change introduces several optimizations to improve the performance of the time-series source operator:  - Split the leaf queue into two: one for `_tsid` and another for `@timestamp`. This avoids repeatedly comparing large `_tsid` values while iterating over a single `_tsid`.  - Track the number of emitted documents per segment and use this data to build forward and backward document maps  reducing the need for expensive sorts.  - Use ordinal blocks to avoid duplicating the same `_tsid` multiple times
elastic,elasticsearch,173904924a1024110498d1ba2ded1bd02c82a0ae,https://github.com/elastic/elasticsearch/commit/173904924a1024110498d1ba2ded1bd02c82a0ae,Add avg_over_time (#126572)  This change adds the `avg_over_time` aggregation for time series indices. Similar to other time series aggregations  we need to translate `avg_over_time` into regular aggregations. There are two options for this translation:  1. Translate `avg_over_time` to `EVAL div(sum_over_time  count_over_time)`  then translate `sum_over_time` and `count_over_time` to `sum` and `count`. 2. Translate `avg_over_time` directly to `avg`  and then to `div(sum  count)`.  This PR chooses the latter approach. Below is an example:  ``` TS k8s | STATS sum(avg_over_time(memory_usage)) BY host  bucket(@timestamp  1minute) ```  translates to:  ``` TS k8s | STATS avg_memory_usage = avg(memory_usage)  host_values=VALUES(host) BY _tsid  time_bucket=bucket(@timestamp  1minute) | STATS sum(avg_memory_usage) BY host_values  time_bucket ```  and then:  ``` TS k8s | STATS sum_memory_usage = sum(memory_usage)  count_memory_usage = count(memory_usage)  host_values=VALUES(host) BY _tsid  time_bucket=bucket(@timestamp  1minute) | EVAL avg_memory_usage = sum_memory_usage / count_memory_usage | STATS sum(avg_memory_usage) BY host_values  time_bucket ```  Since we need to substitute `AVG` with `SUM` and `COUNT` after translation  we need to call `SubstituteSurrogates` twice in `LogicalPlanOptimizer`. If there is a performance impact  we can move this rule to `TranslateTimeSeriesAggregate`.
elastic,elasticsearch,7e1e45eaa46dc5034bbcdd2d9e6d3ef0999ba3b7,https://github.com/elastic/elasticsearch/commit/7e1e45eaa46dc5034bbcdd2d9e6d3ef0999ba3b7,ESQL: Speed up TO_IP (#126338)  Speed up the TO_IP method by converting directly from utf-8 encoded strings to the ip encoding. Previously we did: ``` utf-8 -> String -> INetAddress -> ip encoding ```  In a step towards solving #125460 this creates three IP parsing functions  one the rejects leading zeros  one that interprets leading zeros as decimal numbers  and one the interprets leading zeros as octal numbers. IPs have historically been parsed in all three of those ways.  This plugs the "rejects leading zeros" parser into `TO_IP` because that's the behavior it had before.  Here is the performance: ``` Benchmark               Score    Error  Units leadingZerosAreDecimal  14.007 ± 0.093  ns/op leadingZerosAreOctal    15.020 ± 0.373  ns/op leadingZerosRejected    14.176 ± 3.861  ns/op original                32.950 ± 1.062  ns/op ```  So this is roughly 45% faster than what we had.
elastic,elasticsearch,b01438a95f1a6e1518a2700ec9f147292fafcb60,https://github.com/elastic/elasticsearch/commit/b01438a95f1a6e1518a2700ec9f147292fafcb60,Re-enable parallel collection for field sorted top hits (#125916)  With #123610 we disabled parallel collection for field and script sorted top hits  aligning its behaviour with that of top level search. This was mainly to work around a bug in script sorting that did not support inter-segment concurrency.  The bug with script sort has been fixed with #123757 and concurrency re-enabled for it.  While sort by field is not optimized for search concurrency  top hits benefits from it and disabling concurrency for sort by field in top hits has caused performance regressions in our nightly benchmarks.  This commit re-enables concurrency for top hits with sort by field is used. This introduces back a discrepancy between top level search and top hits  in that concurrency is applied for top hits despite sort by field normally disables it. The key difference is the context where sorting is applied  and the fact that concurrency is disabled only for performance reasons on top level searches and not for functional reasons.
elastic,elasticsearch,fd2cc975418f16926bff08115c79d89c89c17114,https://github.com/elastic/elasticsearch/commit/fd2cc975418f16926bff08115c79d89c89c17114,Introduce batched query execution and data-node side reduce (#121885)   This change moves the query phase a single roundtrip per node just like can_match or field_caps work already. A a result of executing multiple shard queries from a single request we can also partially reduce each node's query results on the data node side before responding to the coordinating node.  As a result this change significantly reduces the impact of network latencies on the end-to-end query performance  reduces the amount of work done (memory and cpu) on the coordinating node and the network traffic by factors of up to the number of shards per data node!  Benchmarking shows up to orders of magnitude improvements in heap and network traffic dimensions in querying across a larger number of shards.
elastic,elasticsearch,632b9e79bd5c5ae441b6bc30046859026e4fb3f7,https://github.com/elastic/elasticsearch/commit/632b9e79bd5c5ae441b6bc30046859026e4fb3f7,Load FieldInfos from store if not yet initialised through a refresh on IndexShard (#125650)  Load field caps from store if they haven't been initialised through a refresh yet. Keep the plain reads to not mess with performance characteristics too much on the good path but protect against confusing races when loading field infos now (that probably should have been ordered stores in the first place but this was safe due to other locks/volatiles on the refresh path).  Closes #125483
elastic,elasticsearch,c5e76847ad9b6900b1cc4dffa44e485bb8bd1e5b,https://github.com/elastic/elasticsearch/commit/c5e76847ad9b6900b1cc4dffa44e485bb8bd1e5b,ESQL: Keep ordinals in conversion functions (#125357)  Make the conversion functions that process `BytesRef`s into `BytesRefs` keep the `OrdinalBytesRefVector`s when processing. Let's use `TO_LOWER` as an example. First  the performance numbers: ``` (operation)  Mode   Score   Error ->  Score    Error Units to_lower  30.662 ± 6.163 -> 30.048 ±  0.479 ns/op to_lower_ords  30.773 ± 0.370 ->  0.025 ±  0.001 ns/op to_upper  33.552 ± 0.529 -> 35.775 ±  1.799 ns/op to_upper_ords  35.791 ± 0.658 ->  0.027 ±  0.001 ns/op ``` The test has a 8192 positions containing alternating `foo` and `bar`. Running `TO_LOWER` via ordinals is super duper faster. No longer `O(positions)` and now `O(unique_values)`.  Let's paint some pictures! `OrdinalBytesRefVector` is a lookup table. Like this: ``` +-------+----------+ | bytes | ordinals | | ----- | -------- | |  FOO  | 0        | |  BAR  | 1        | |  BAZ  | 2        | +-------+ 1        | | 1        | | 0        | +----------+ ```  That lookup table is one block. When you read it you look up the `ordinal` and match it to the `bytes`. Previously `TO_LOWER` would process each value one at a time and make: ``` bytes ----- foo bar baz bar bar foo ```  So it'd run `TO_LOWER` once per `ordinal` and it'd make an ordinal non-lookup table. With this change `TO_LOWER` will now make: ``` +-------+----------+ | bytes | ordinals | | ----- | -------- | |  foo  | 0        | |  bar  | 1        | |  baz  | 2        | +-------+ 1        | | 1        | | 0        | +----------+ ``` We don't even have to copy the `ordinals` - we can reuse those from the input and just bump the reference count. That's why this goes from `O(positions)` to `O(unique_values)`.
elastic,elasticsearch,fc4d8d65e5e0a17e38baee2cb50a74d77f351e8b,https://github.com/elastic/elasticsearch/commit/fc4d8d65e5e0a17e38baee2cb50a74d77f351e8b,ESQL: Enable visualizing a query profile (#124361)  To understand query performance  we often peruse the output of `_query`-requests run with `"profile": true`.  This is difficult when the query runs in a large cluster with many nodes and shards  or in case of CCQ.  This adds an option to visualize a query using Chromium's/Chrome's builtin `about:tracing` - or  for even better visuals and querying the different drivers via SQL  perfetto (c.f. https://ui.perfetto.dev/).  To use  save the JSON output of a query run with `"profile": true` to a file  like `output.json` and then invoke the following Gradle task:  ``` ./gradlew x-pack:plugin:esql:tools:parseProfile --args='~/output.json ~/parsed_profile.json' ```  Either open `about:tracing` in Chromium/Chrome ![image](https://github.com/user-attachments/assets/75e17ddf-f032-4aa1-bf3e-61b985b4e0b6) Or head over to https://ui.perfetto.dev (build locally in case of potentially sensitive data in the profille): ![image](https://github.com/user-attachments/assets/b3372b7d-fbec-45aa-a68c-b24e62a8c704)  Every slice is a driver  the colors indicating the ratio of cpu time over total time. - In Perfetto  essentials like duration  cpu duration  timestamp and a few others can be queried via SQL - this allows e.g. querying for all drivers that spent more than 50% of their time waiting and other fun things. ![image](https://github.com/user-attachments/assets/4a0ab2ce-3585-4953-b2eb-71991777b3fa)  - Details about a driver  esp. which operators it ran  are available when clicking the driver's slice. ![image](https://github.com/user-attachments/assets/e1c0b30d-0a31-468c-9ff4-27ca452716fc)
elastic,elasticsearch,ce3a778fa140c9931dc79686b1a5d616f267e32d,https://github.com/elastic/elasticsearch/commit/ce3a778fa140c9931dc79686b1a5d616f267e32d,Improve downsample performance by buffering docids and do bulk processing. (#124477)
elastic,elasticsearch,def4c890bcb438197bb2b13aa72db42d283ae1a0,https://github.com/elastic/elasticsearch/commit/def4c890bcb438197bb2b13aa72db42d283ae1a0,Fix concurrency issue in ScriptSortBuilder (#123757)  Inter-segment concurrency is disabled whenever sort by field  included script sorting  is used in a search request.  The reason why sort by field does not use concurrency is that there are some performance implications  given that the hit queue in Lucene is build per slice and the different search threads don't share information about the documents they have already visited etc.  The reason why script sort has concurrency disabled is that the script sorting implementation is not thread safe. This commit addresses such concurrency issue and re-enables search concurrency for search requests that use script sorting. In addition  missing tests are added to cover for sort scripts that rely on _score being available and top_hits aggregation with a scripted sort clause.
elastic,elasticsearch,79a1626160602fbdc11fd691e2119125415834cd,https://github.com/elastic/elasticsearch/commit/79a1626160602fbdc11fd691e2119125415834cd,Speed up block serialization (#124394)  Currently  we use NamedWriteable for serializing blocks. While convenient  it incurs a noticeable performance penalty when pages contain thousands of blocks. Since block types are small and already centered in ElementType  we can safely switch from NamedWriteable to typed code. For example  the NamedWriteable alone of a small page with 10K fields would be 180KB  whereas the new method reduces it to 10KB. Below are the serialization improvements with FROM idx | LIMIT 10000 where the target index has 10K fields:  - write_exchange_response executed 173 times took: 73.2ms -> 26.7ms - read_exchange_response executed 173 times took: 49.4ms -> 25.8ms
elastic,elasticsearch,333e252aee006fe6a7578495088c659132fbbb3a,https://github.com/elastic/elasticsearch/commit/333e252aee006fe6a7578495088c659132fbbb3a,Avoid over collecting in Limit or Lucene Operator (#123296)  Currently  we rely on signal propagation for early termination. For example  FROM index | LIMIT 10 can be executed by multiple Drivers: several Drivers to read document IDs and extract fields  and the final Driver to select at most 10 rows. In this scenario  each Lucene Driver can independently collect up to 10 rows until the final Driver has enough rows and signals them to stop collecting. In most cases  this model works fine  but when extracting fields from indices in the warm/cold tier  it can impact performance. This change introduces a Limiter used between LimitOperator and LuceneSourceOperator to avoid over-collecting. We will also need a follow-up to ensure that we do not over-collect between multiple stages of query execution.
elastic,elasticsearch,4d2b8dc4f2e908821dfb34d4ffc14244fce83c41,https://github.com/elastic/elasticsearch/commit/4d2b8dc4f2e908821dfb34d4ffc14244fce83c41,Fix early termination in LuceneSourceOperator (#123197)  The LuceneSourceOperator is supposed to terminate when it reaches the limit; unfortunately  we don't have a test to cover this. Due to this bug  we continue scanning all segments  even though we discard the results as the limit was reached. This can cause performance issues for simple queries like FROM .. | LIMIT 10  when Lucene indices are on the warm or cold tier. I will submit a follow-up PR to ensure we only collect up to the limit across multiple drivers.
elastic,elasticsearch,780cac5a6daf8b6d57a191733589d27cafd65634,https://github.com/elastic/elasticsearch/commit/780cac5a6daf8b6d57a191733589d27cafd65634,Enable a sparse doc values index for `@timestamp` in LogsDB (#122161)  This PR extends the work done in #121751 by enabling a sparse doc values index for the @timestamp field in LogsDB.  Similar to the previous PR  the setting index.mapping.use_doc_values_skipper will override the index mapping parameter when all of the following conditions are met:  * The index mode is LogsDB. * The field name is @timestamp. * Index sorting is configured on @timestamp (regardless of whether it is a primary sort field or not). * Doc values are enabled.  This ensures that only one index structure is defined on the @timestamp field: * If the conditions above are met  the inverted index is replaced with a sparse doc values index. * This prevents both the inverted index and sparse doc values index from being enabled together  reducing unnecessary storage overhead.  This change aligns with our goal of optimizing LogsDB for storage efficiency while possibly maintaining reasonable query latency performance. It will enable us to run benchmarks and evaluate the impact of sparse indexing on the @timestamp field as well.
elastic,elasticsearch,77f8558d0ba4c6f5b7966b481eb4651b02d03647,https://github.com/elastic/elasticsearch/commit/77f8558d0ba4c6f5b7966b481eb4651b02d03647,ESQL: Add description to status and profile (#121783)  This adds a `task_description` field to `profile` output and task `status`. This looks like: ``` ... "profile" : { "drivers" : [ { "task_description" : "final"  "start_millis" : 1738768795349  "stop_millis" : 1738768795405  ... "task_description" : "node_reduce"  "start_millis" : 1738768795392  "stop_millis" : 1738768795406  ... "task_description" : "data"  "start_millis" : 1738768795391  "stop_millis" : 1738768795404  ... ```  Previously you had to look at the signature of the operators in the driver to figure out what the driver is *doing*. You had to know enough about how ESQL works to guess. Now you can look at this description to see what the server *thinks* it is doing. No more manual classification.  This will be useful when debugging failures and performance regressions because it is much easier to use `jq` to group on it: ``` | jq '.profile[] | group_by(.task_description)[]' ```
elastic,elasticsearch,6a526755de4b560e6c4d9a211fb783723b1f2807,https://github.com/elastic/elasticsearch/commit/6a526755de4b560e6c4d9a211fb783723b1f2807,Use synthetic recovery source by default if synthetic source is enabled (#119110)  We experimented with using synthetic source for recovery and observed quite positive impact on indexing throughput by means of our nightly Rally benchmarks. As a result  here we enable it by default when synthetic source is used. To be more precise  if `index.mapping.source.mode` setting is `synthetic` we enable recovery source by means of synthetic source.  Moreover  enabling synthetic source recovery is done behind a feature flag. That would allow us to enable it in snapshot builds which in turn will allow us to see performance results in Rally nightly benchmarks.
elastic,elasticsearch,e1c6c3f9b2516574267000e33563d90c75e9d673,https://github.com/elastic/elasticsearch/commit/e1c6c3f9b2516574267000e33563d90c75e9d673,Configurable limit on concurrent shard closing (#121267)  Today we limit the number of shards concurrently closed by the `IndicesClusterStateService`  but this limit is currently a function of the CPU count of the node. On nodes with plentiful CPU but poor IO performance we may want to restrict this limit further. This commit exposes the throttling limit as a setting.
elastic,elasticsearch,d5bccfeca49cdc041bcfbfbb3cd037c05f701a6c,https://github.com/elastic/elasticsearch/commit/d5bccfeca49cdc041bcfbfbb3cd037c05f701a6c,Drier and faster SumAggregator and AvgAggregator (#120436)  Dried up (and moved to the much faster inline logic) for the summation here for both implementations. Obviously this could have been done even drier but it didn't seem like that was possible without a performance hit (we really don't want to sub-class the leaf-collector I think). Benchmarks suggest this variant is ~10% faster than the previous iteration of `SumAggregator` (probably from making the grow method smaller) and a bigger than that improvement for the `AvgAggregator`.
elastic,elasticsearch,7b8f545e19cb370e72281b33bdfeb0cc74fe2f2e,https://github.com/elastic/elasticsearch/commit/7b8f545e19cb370e72281b33bdfeb0cc74fe2f2e,Fix realtime get of nested fields with synthetic source (#119575)  Today  for get-from-translog operations  we only need to reindex the root document into an in-memory Lucene  as the _source is stored in the root document and is sufficient. However  synthesizing the source for nested fields requires both the root document and its child documents. This causes realtime-get operations (as well as update and update-by-query operations) to miss nested fields.  Another issue is that the translog operation is reindexed lazily during get-from-translog operations. As a result  two realtime-get operations can return slightly different outputs: one reading from the translog and the other from Lucene.  This change resolves both issues. However  addressing the second issue can degrade the performance of realtime-get and update operations. If slight inconsistencies are acceptable  the translog operation should be reindexed lazily instead.  Closes #119553
elastic,elasticsearch,7750cf5b94e05bf7ee1c5c5ce796d62b04407d88,https://github.com/elastic/elasticsearch/commit/7750cf5b94e05bf7ee1c5c5ce796d62b04407d88,Remove default auto_expand_replicas of lookup indices (#120073)  This change disables auto_expand_replicas on lookup indices to enhance the lookup join user experience. Users can  however  enable this setting at any time to optimize performance.
elastic,elasticsearch,c0553d472152f273961d144b3bf085b94a0cde9f,https://github.com/elastic/elasticsearch/commit/c0553d472152f273961d144b3bf085b94a0cde9f,Remove ChunkedToXContentBuilder  (#119310)  Reverts the introduction of the ChunkedToXContentBuilder to fix the various performance regressions it introduced and the theoretical impossibility of fixing its performance to rival that of the iterator based solution. With the exception of a few minor adjustments that came out of changes already made on top of the builder migration this simply returns to the previous implementations (and some of the stuff in that code could be done better with the utilities available now). I also verified that this solves the performance issues that we've been running into with the builder.  closes #118647  This reverts commit 918a9cc35ada3a348f0bd4ed24e7ab6f836d468e This reverts commit 8c378754 This reverts commit 11c2eb29 This reverts commit c3115156
elastic,elasticsearch,a5c57ba966cfd088b8d79fd51fe3fb35163b22a2,https://github.com/elastic/elasticsearch/commit/a5c57ba966cfd088b8d79fd51fe3fb35163b22a2,Adjust random_score default field to _seq_no field (#118671)  In an effort to improve performance and continue to provide unique seeded scores for documents in the same index  we are switching from _id to _seq_no.  Requiring a field that is "unique" for a field and to help with random scores is burdensome for the user. So  we should default to a unique field (per index) when the user provides a seed.  Using `_seq_no` should be better as:  - We don't have to grab stored fields values - Bytes used are generally smaller  Additionally this removes the deprecation warning.  Marking as "breaking" as it does change the scores & behavior  but the API provide is the same.
elastic,elasticsearch,8bbc6b314149163be3fa26d7f9066cc79f68a866,https://github.com/elastic/elasticsearch/commit/8bbc6b314149163be3fa26d7f9066cc79f68a866,Suppress the for-loop warnings since it is a conscious performance choice. (#118530)
elastic,elasticsearch,34b7e60f7589f0fd01b04a5cf28a088a254ad295,https://github.com/elastic/elasticsearch/commit/34b7e60f7589f0fd01b04a5cf28a088a254ad295,Re-add ResolvedExpression wrapper (#118174)  This PR reapplies #114592 along with an update to remove the performance regression introduced with the original change.
elastic,elasticsearch,eb0a21efd8946df04a3f6f0457bb0a1b73b50bf0,https://github.com/elastic/elasticsearch/commit/eb0a21efd8946df04a3f6f0457bb0a1b73b50bf0,Speedup OsStats initialization (#118141)  Similar to other OS/FS type stats we can optimize here. Found this as a slowdown when profiling tests in a loop during test fixing. This helps node startup and maybe more importantly test performance. No need to initialize the stats eagerly when we can just get them as we load them the first time.
elastic,elasticsearch,79ce6e38728a7710f01f18d9769cd6941c2312f6,https://github.com/elastic/elasticsearch/commit/79ce6e38728a7710f01f18d9769cd6941c2312f6,Improve performance of H3.h3ToGeoBoundary (#117812)  There are two clear code paths depending if a h3 bin belongs to even resolutions (class II) or uneven resolutions (class III). especializing the code paths for each type leads to an improvement in performance.
elastic,elasticsearch,e90eb7ab0df06239a69a1945ca6ef5effc065433,https://github.com/elastic/elasticsearch/commit/e90eb7ab0df06239a69a1945ca6ef5effc065433,Improve halfbyte transposition performance  marginally improving bbq performance (#117350)  The transposition of the bits in half-byte queries for BBQ is pretty convoluted and slow. This commit greatly simplifies & improves performance for this small part of bbq queries and indexing.  Here are the results of a small JMH benchmark for this particular function.  ``` TransposeBinBenchmark.transposeBinNew     1024  thrpt    5  857.779 ± 44.031  ops/ms TransposeBinBenchmark.transposeBinOrig    1024  thrpt    5   94.950 ±  2.898  ops/ms ```  While this is a huge improvement for this small function  the impact at query and index time is only marginal. But  the code simplification itself is enough to warrant this change in my opinion.
elastic,elasticsearch,7369c0818df0166ee18d50f5a1d9be0ba0bc005b,https://github.com/elastic/elasticsearch/commit/7369c0818df0166ee18d50f5a1d9be0ba0bc005b,Add new multi_dense_vector field for brute-force search (#116275)  This adds a new `multi_dense_vector` field that focuses on the maxSim usecase provided by Col[BERT|Pali].  Indexing vectors in HNSW as it stands makes no sense. Performance wise or for cost. However  we should totally support rescoring and brute-force search over vectors with maxSim.  This is step one of many. Behind a feature flag  this adds support for indexing any number of vectors of the same dimension.  Supports bit/byte/float.  Scripting support will be a follow up.  Marking as non-issue as its behind a flag and unusable currently.
elastic,elasticsearch,e304c1d5c1dfd20c9b7ea4da3bf0560c0c82c1e9,https://github.com/elastic/elasticsearch/commit/e304c1d5c1dfd20c9b7ea4da3bf0560c0c82c1e9,ESQL: Speed up grouping by bytes (#114021)  This speeds up grouping by bytes valued fields (keyword  text  ip  and wildcard) when the input is an ordinal block: ``` bytes_refs 22.213 ± 0.322 -> 19.848 ± 0.205 ns/op (*maybe* real  maybe noise. still good) ordinal didn't exist   ->  2.988 ± 0.011 ns/op ``` I see this as 20ns -> 3ns  an 85% speed up. We never hard the ordinals branch before so I'm expecting the same performance there - about 20ns per op.  This also speeds up grouping by a pair of byte valued fields: ``` two_bytes_refs 83.112 ± 42.348  -> 46.521 ± 0.386 ns/op two_ordinals 83.531 ± 23.473  ->  8.617 ± 0.105 ns/op ``` The speed up is much better when the fields are ordinals because hashing bytes is comparatively slow.  I believe the ordinals case is quite common. I've run into it in quite a few profiles.
elastic,elasticsearch,14f0b4840464816a149e0bc0bab0961040c40780,https://github.com/elastic/elasticsearch/commit/14f0b4840464816a149e0bc0bab0961040c40780,Improve performance of LongObjectPagedHashMap#removeAndAdd and ObjectObjectPagedHashMap#removeAndAdd (#114280)
elastic,elasticsearch,e129822f11befad0558773d8626e5286715dbff8,https://github.com/elastic/elasticsearch/commit/e129822f11befad0558773d8626e5286715dbff8,Improve performance of Int3Hash#removeAndAdd (#114383)
elastic,elasticsearch,58cc37922c159714c5c384a44cdf444686021e51,https://github.com/elastic/elasticsearch/commit/58cc37922c159714c5c384a44cdf444686021e51,Improve performance of LongLongHash#removeAndAdd (#114230)  remove some unnecessary manipulation of the keys in the method removeAndAdd.
elastic,elasticsearch,c4731aaf08949174e00a29fffc3ada326b0df14d,https://github.com/elastic/elasticsearch/commit/c4731aaf08949174e00a29fffc3ada326b0df14d,Improve performance of LongHash#removeAndAdd (#114199)  Remove unnecessary remove and later add of the key.
elastic,elasticsearch,052dbb4dacca29c3abf96e1b1579c569a2ec7095,https://github.com/elastic/elasticsearch/commit/052dbb4dacca29c3abf96e1b1579c569a2ec7095,Optimize error handling after lazy rollovers (#111572)  This commit improves the performance of the error-handling process after a lazy rollover or an index creation failed.
elastic,elasticsearch,5c91edda9f9fd2e0dd044e8b6f47a5b0465d4e95,https://github.com/elastic/elasticsearch/commit/5c91edda9f9fd2e0dd044e8b6f47a5b0465d4e95,ESQL: Speed up CASE for some parameters (#112295)  This speeds up the `CASE` function when it has two or three arguments and both of the arguments are constants or fields. This works because `CASE` is lazy so it can avoid warnings in cases like ``` CASE(foo != 0  2 / foo  1) ```  And  in the case where the function is *very* slow  it can avoid the computations.  But if the lhs  and rhs of the `CASE` are constant then there isn't any work to avoid.  The performance improvment is pretty substantial: ``` (operation)  Before   Error   After    Error  Units case_1_lazy  97.422 ± 1.048  101.571 ± 0.737  ns/op case_1_eager  79.312 ± 1.190    4.601 ± 0.049  ns/op ```  The top line is a `CASE` that has to be lazy - it shouldn't change. The 4 nanos change here is noise. The eager version improves by about 94%.
elastic,elasticsearch,d9e0cbeb59638cb7476942b4cb4b9ea857a6703e,https://github.com/elastic/elasticsearch/commit/d9e0cbeb59638cb7476942b4cb4b9ea857a6703e,Small performance improvement in h3 library (#113385)  Changing some FDIV's into FMUL's leads to performance improvements
elastic,elasticsearch,90e343cfef0c3bbe120c5fe629652b49190d6fa5,https://github.com/elastic/elasticsearch/commit/90e343cfef0c3bbe120c5fe629652b49190d6fa5,Use ChannelFutureListener in Netty code to reduce capturing lambdas (#112967)  Mainly motivated by simplifying the reference chains for Netty buffers and have easier to analyze heap dumps in some spots but also a small performance win in and of itself.
elastic,elasticsearch,d7cc4074175f9a91b1a455b1d31bdfb050215aa2,https://github.com/elastic/elasticsearch/commit/d7cc4074175f9a91b1a455b1d31bdfb050215aa2,ESQL: Compute support for filtering ungrouped aggs (#112717)  Adds support to the compute engine for filtering which positions are processed by ungrouping aggs. This should allow syntax like:  ``` | STATS success = COUNT(*) WHERE 200 <= response_code AND response_code < 300  redirect = COUNT(*) WHERE 300 <= response_code AND response_code < 400  client_err = COUNT(*) WHERE 400 <= response_code AND response_code < 500  server_err = COUNT(*) WHERE 500 <= response_code AND response_code < 600  total_count = COUNT(*) ```  We could translate the WHERE expression into an `ExpressionEvaluator` and run it  then plug it into the filtering support added in this PR.  The actual filtering is done by creating a `FilteredAggregatorFunction` which wraps a regular `AggregatorFunction` first executing the filter against the incoming `Page` and then passing the resulting mask to the `AggregatorFunction`. We've then added a `mask` to `AggregatorFunction#process` which each aggregation function must use for filtering.  We keep the unfiltered behavior by sending a constant block with `true` in it. Each agg detects this and takes an "unfiltered" path  preserving the original performance.  Importantly  when you don't turn this on it doesn't effect performance:  ``` (blockType)  (grouping)   (op)  Score    Error -> Score    Error  Units vector_longs        none  count  0.007 ±  0.001 -> 0.007 ±  0.001  ns/op vector_longs        none    min  0.123 ±  0.004 -> 0.128 ±  0.005  ns/op vector_longs       longs  count  4.311 ±  0.192 -> 4.218 ±  0.053  ns/op vector_longs       longs    min  5.476 ±  0.077 -> 5.451 ±  0.074  ns/op ```
elastic,elasticsearch,01fb50142e9c8ad0a2de13a2f0311c9c50604b57,https://github.com/elastic/elasticsearch/commit/01fb50142e9c8ad0a2de13a2f0311c9c50604b57,Speedup HealthNodeTaskExecutor (#112558)  The introduction of this class introduced a significant regression in cluster state update performance and increased test execution times visibly. The `clusterHasFeature` check is very expensive  lets do it laster and do the effectively free checks first.
elastic,elasticsearch,2a9e47458bb394cd9d08c5a3d8b08156f23b4f66,https://github.com/elastic/elasticsearch/commit/2a9e47458bb394cd9d08c5a3d8b08156f23b4f66,Rework fix for stale data in synthetic source to improve performance (#112480)
elastic,elasticsearch,306491aa9dc80176ddde58902caa7b2d87f0e178,https://github.com/elastic/elasticsearch/commit/306491aa9dc80176ddde58902caa7b2d87f0e178,Fix a few toString implementations+usages that affect test performance (#112380)  No need to precompute the toString for `ActionListener` and `Releasable`  that's quite expensive at times. Also string concat is way faster than formating these days  so use that in the transport channels. Lastly  short-circuit some obvious spots in network address serialization and remove code that duplicates the JDK (remove the IPV4 specific forbidden API because it makes no sense  but still needed to disable the check to make the build green because of the exclude on the parent class).
elastic,elasticsearch,c05f7e9c81169f710be92ec7c913118aa0982902,https://github.com/elastic/elasticsearch/commit/c05f7e9c81169f710be92ec7c913118aa0982902,ESQL: Add way for `Block` to `keepMask` (#112160)  This adds a `Block#keepMask(BooleanVector)` method that will make a new block  keeping all of the values where the vector is `true` and `null`ing all of the velues where the vector is false.  This will be useful for implementing partial aggregation application like `| STATS MAX(a WHERE b > 1)  MIN(j WHERE b > 2) BY bar`. Or however the syntax ends up being. We already skip `null` group keys and we can evaluate the `b > 2` bits to a mask pretty easily. It should also be useful in optimizing `CASE(a > 2  foo)` - but only when the RHS of the CASE is `null` and the LHS is a constant or constant-like.  This is something that's very optimize-able. I haven't really optimized it in this PR  but it should be possible to speed this up a ton and remove a lot of copying. Here's where the benchmarks start: ``` (dataTypeAndBlockKind)  Mode  Cnt  Score   Error  Units int/array  avgt    7  3.705 ± 0.153  ns/op int/vector  avgt    7  3.234 ± 0.078  ns/op ```  That's about the same speed as reading the block. In a few of these cases I expect we can get them to constant performance rather than per-record performance.
elastic,elasticsearch,a02dc7165c75f12701f8d47a2bdefe5283735267,https://github.com/elastic/elasticsearch/commit/a02dc7165c75f12701f8d47a2bdefe5283735267,Improve performance of grok pattern cycle detection (#111947)
elastic,elasticsearch,17339198d8ef563661f6189853130ea968cb76fe,https://github.com/elastic/elasticsearch/commit/17339198d8ef563661f6189853130ea968cb76fe,Allow legacy_* index.codec options to be configured. (#111867)  For escape hatch reasons when zstd unexpectedly worse performance.
elastic,elasticsearch,70dfb5216bab5f17cdbed1e0153e0df13d0ca6de,https://github.com/elastic/elasticsearch/commit/70dfb5216bab5f17cdbed1e0153e0df13d0ca6de,Speedup InternalEngine setup (#111801)  We can speed up the setup of the InternalEngine quite a bit  mostly to help test performance by not re-reading the system properties over and over and saving deserializing the latest commit info redundantly in the constructor.
elastic,elasticsearch,9fbdfcf650fe817a39b3e9a9ada1a42970fdeb0c,https://github.com/elastic/elasticsearch/commit/9fbdfcf650fe817a39b3e9a9ada1a42970fdeb0c,Fix unnecessary mustache template evaluation (#110986)  Addresses the performance issue in the date ingest processor where Mustache template evaluation is unnecessarily applied inside a loop. The timezone and locale templates are now evaluated once before the loop  improving efficiency.  closes #110191 --------- Co-authored-by: Joe Gallo <joegallo@gmail.com>
elastic,elasticsearch,5b8a62960764bdcc3cb2b9204ad8cffb12bd95a3,https://github.com/elastic/elasticsearch/commit/5b8a62960764bdcc3cb2b9204ad8cffb12bd95a3,Deduplicate FieldInfo attributes and field names (#110561)  We can use a similar strategy to what worked with mappers+settings and reuse the string deduplicator to deal with a large chunk (more than 70% from heap dumps we've seen in production)  of the `FieldInfo` duplication overhead without any Lucene changes. There's generally only a very limited number of attribute maps out there and the "dedup up to 100" logic in here deals with all scenarios I have observed in the wild thus far. As a side effect of deduplicating the field name and always working with an interned string now  I would expect the performance of field caps filtering for empty fields to improve measurably.
elastic,elasticsearch,67da6ba645afc5a6c9bf5478ce3e92a7d8cb7d08,https://github.com/elastic/elasticsearch/commit/67da6ba645afc5a6c9bf5478ce3e92a7d8cb7d08,Deduplicate FieldInfo attributes and field names (#110561)  We can use a similar strategy to what worked with mappers+settings and reuse the string deduplicator to deal with a large chunk (more than 70% from heap dumps we've seen in production)  of the `FieldInfo` duplication overhead without any Lucene changes. There's generally only a very limited number of attribute maps out there and the "dedup up to 100" logic in here deals with all scenarios I have observed in the wild thus far. As a side effect of deduplicating the field name and always working with an interned string now  I would expect the performance of field caps filtering for empty fields to improve measurably.
elastic,elasticsearch,5409aa7dcf1de1db3938dc30c942935c4b959149,https://github.com/elastic/elasticsearch/commit/5409aa7dcf1de1db3938dc30c942935c4b959149,Support mixed aggregates in METRICS (#110206)  This pull request supports mixed aggregates in the METRICS command. Non-rate aggregates will be rewritten as a pair of `to_partial` and `from_partial` aggregates:  - The `to_partial` aggregates will be executed in the first pass and always produce an intermediate output regardless of the aggregate mode.  - The `from_partial` aggregates will be executed in the second pass and always receive the intermediate output produced by `to_partial`.  Example:  **METRICS k8s max(rate(request))  max(memory_used)** becomes:  ``` METRICS k8s | STATS rate(request)  $p1=to_partial(max(memory_used)) BY _tsid | STATS max(`rate(request)`)  `max(memory_used)` = from_partial($p1  max($_)) ```  **METRICS k8s max(rate(request))  avg(memory_used) BY host** becomes:  ``` METRICS k8s | STATS rate(request)  $p1=to_partial(sum(memory_used))  $p2=to_partial(count(memory_used))  values(host) BY _tsid | STATS max(`rate(request)`)  $sum=from_partial($p1  sum($_))  $count=from_partial($p2  count($_)) BY host=`values(host)` | EVAL `avg(memory_used)` = $sum / $count | KEEP `max(rate(request))`  `avg(memory_used)`  host ```  **METRICS k8s min(memory_used)  sum(rate(request)) BY pod  bucket(@timestamp  5m)** becomes:  ``` METRICS k8s | EVAL `bucket(@timestamp  5m)` = datetrunc(@timestamp  '5m') | STATS rate(request)  $p1=to_partial(min(memory_used))  VALUES(pod) BY _tsid  `bucket(@timestamp  5m)` | STATS sum(`rate(request)`)  `min(memory_used)` = from_partial($p1  min($)) BY pod=`VALUES(pod)`  `bucket(@timestamp  5m)` | KEEP `min(memory_used)`  `sum(rate(request))`  pod  `bucket(@timestamp  5m)` ``` ---- I also took a different approach for this. The alternative is to extend the runtime to support scatter/gather via exchange. We could have two pipelines: one aggregate grouped by _tsid (and time bucket)  and another grouped by the user-specified keys. These pipelines expand to fill necessary blocks so that they have the same output. However  this requires replicating most of the aggregate rules for dual aggregates.  Hence  I opted for the approach in this PR  which doesn't change anything with non-metrics  making it safer. However  the dual aggregates should have better performance and use less memory than the approach in this PR.  Relates #109979
elastic,elasticsearch,42e1a1be0114a4bbb5908e0843c218e1c95934ac,https://github.com/elastic/elasticsearch/commit/42e1a1be0114a4bbb5908e0843c218e1c95934ac,Remove time_interval from time series source operator (#109982)  This change removes the time_bucket from the time-series source operator. This should simplify the planner. Running a separate date trunc eval specified by bucket should yield the same output and performance.  Spin-off from #109979
elastic,elasticsearch,d1e3c0afc419a73bdb7b6305e0663f81e80cec2e,https://github.com/elastic/elasticsearch/commit/d1e3c0afc419a73bdb7b6305e0663f81e80cec2e,ESQL: Union Types Support (#107545)  * Union Types Support  The second prototype replaced MultiTypeField.Unresolved with MultiTypeField  but this clashed with existing behaviour around mapping unused MultiTypeFields to `unsupported` and `null`  so this new attempt simply adds new fields  resulting in more than one field with the same name. We still need to store this new field in EsRelation  so that physical planner can insert it into FieldExtractExec  so this is quite similar to the second protototype.  The following query works in this third prototype:  ``` multiIndexIpString FROM sample_data* METADATA _index | EVAL client_ip = TO_IP(client_ip) | KEEP _index  @timestamp  client_ip  event_duration  message | SORT _index ASC  @timestamp DESC ```  As with the previous prototyep  we no longer need an aggregation to force the conversion function onto the data node  as the 'real' conversion is now done at field extraction time using the converter function previously saved in the EsRelation and replanned into the EsQueryExec.  Support row-stride-reader for LoadFromMany  Add missing ESQL version after rebase on main  Fixed missing block release  Simplify UnresolvedUnionTypes  Support other commands  notably WHERE  Update docs/changelog/107545.yaml  Fix changelog  Removed unused code  Slight code reduction in analyser of union types  Removed unused interface method  Fix bug in copying blocks (array overrun)  Convert MultiTypeEsField.UnresolvedField back to InvalidMappedField  This is to ensure older behaviour still works.  Simplify InvalidMappedField support  Rather than complex code to recreate InvalidMappedField from MultiTypeEsField.UnresolvedField  we rely on the fact that this is the parent class anyway  so we can resolve this during plan serialization/deserialization anyway. Much simpler  Simplify InvalidMappedField support further  Combining InvalidMappedField and MultiTypeEsField.UnresolvedField into one class simplifies plan serialization even further.  InvalidMappedField is used slightly differently in QL  We need to separate the aggregatable used in the original really-invalid mapped field from the aggregatable used if the field can indeed be used as a union-type in ES|QL.  Updated version limitation after 8.14 branch  Try debug CI failures in multi-node clusters  Support type conversion in rowstride reader on single leaf  Disable union_types from CsvTests  Keep track of per-shard converters for LoadFromMany  Simplify block loader convert function  Code cleanup  Added unit test for ValuesSourceReaderOperator including field type conversions at block loading  Added test for @timestamp and fixed related bug  It turns out that most  but not all  DataType values have the same esType as typeName  and @timestamp is one that does not  using `date` for esType and `datetime` for typename. Our EsqlIndexResolver was recording multi-type fields with `esType`  while later the actual type conversion was using an evaluator that relied on DataTypes.typeFromName(typeName). So we fixed the EsqlIndexResolver to rather use typeName.  Added more tests  with three indices combined and two type conversions  Disable lucene-pushdown on union-type fields  Since the union-type rewriter replaced conversion functions with new FieldAttributes  these were passing the check for being possible to push-down  which was incorrect. Now we prevent that.  Set union-type aggregatable flag to false always  This simplifies the push-down check.  Fixed tests after rebase on main  Add unit tests for union-types (same field  different type)  Remove generic warnings  Test code cleanup and clarifying comments  Remove -IT_tests_only in favor of CsvTests assumeFalse  Improved comment  Code review updates  Code review updates  Remove changes to ql/EsRelation  And it turned out the latest version of union type no longer needed these changes anyway  and was using the new EsRelation in the ESQL module without these changes.  Port InvalidMappedField to ESQL  Note  this extends the QL version of InvalidMappedField  so is not a complete port. This is necessary because of the intertwining of QL IndexResolver and EsqlIndexResolver. Once those classes are disentangled  we can completely break InvalidMappedField from QL and make it a forbidden type.  Fix capabilities line after rebase on main  Revert QL FieldAttribute and extend with ESQL FieldAttribute  So as to remove any edits to QL code  we extend FieldAttribute in the ESQL code with the changes required  since is simply to include the `field` in the hascode and equals methods.  Revert "Revert QL FieldAttribute and extend with ESQL FieldAttribute"  This reverts commit 168c6c75436e26b83e083cd3de8e18062e116bc9.  Switch UNION_TYPES from EsqlFeatures to EsqlCapabilities  Make hashcode and equals aligned  And removed unused method from earlier union-types work where we kept the NodeId during re-writing (which we no longer do).  Replace required_feature with required_capability after rebase  Switch union_types capability back to feature  because capabilities do not work in mixed clusters  Revert "Switch union_types capability back to feature  because capabilities do not work in mixed clusters"  This reverts commit 56d58bedf756dbad703c07bf4cdb991d4341c1ae.  Added test for multiple columns from same fields  Both IP and Date are tested  Fix bug with incorrectly resolving invalid types  And added more tests  Fixed bug with multiple fields of same name  This fix simply removes the original field already at the EsRelation level  which covers all test cases but has the side effect of having the final field no-longer be unsupported/null when the alias does not overwrite the field with the same name. This is not exactly the correct semantic intent. The original field name should be unsupported/null unless the user explicitly overwrote the name with `field=TO_TYPE(field)`  which effectively deletes the old field anyway.  Fixed bug with multiple conversions of the same field  This also fixes the issue with the previous fix that incorrectly reported the converted type for the original field.  More tests with multiple fields and KEEP/DROP combinations  Replace skip with capabilities in YML tests  Fixed missing ql->esql import change afer merging main  Merged two InvalidMappedField classes  After the QL code was ported to esql.core  we can now make the edits directly in InvalidMappedField instead of having one extend the other.  Move FieldAttribute edits from QL to ESQL  ESQL: Prepare analyzer for LOOKUP (#109045)  This extracts two fairly uncontroversial changes that were in the main LOOKUP PR into a smaller change that's easier to review.  ESQL: Move serialization for EsField (#109222)  This moves the serialization logic for `EsField` into the `EsField` subclasses to better align with the way rest of Elasticsearch works. It also switches them from ESQL's home grown `writeNamed` thing to `NamedWriteable`. These are wire compatible with one another.  ESQL: Move serialization of `Attribute` (#109267)  This moves the serialization of `Attribute` classes used in ESQL into the classes themselves to better line up with the rest of Elasticsearch.  ES|QL: add MV_APPEND function (#107001)  Adding `MV_APPEND(value1  value2)` function  that appends two values creating a single multi-value. If one or both the inputs are multi-values  the result is the concatenation of all the values  eg.  ``` MV_APPEND([a  b]  [c  d]) -> [a  b  c  d] ```  ~I think for this specific case it makes sense to consider `null` values as empty arrays  so that~ ~MV_APPEND(value  null) -> value~ ~It is pretty uncommon for ESQL (all the other functions  apart from `COALESCE`  short-circuit to `null` when one of the values is null)  so let's discuss this behavior.~  [EDIT] considering the feedback from Andrei  I changed this logic and made it consistent with the other functions: now if one of the parameters is null  the function returns null  [ES|QL] Convert string to datetime when the other size of an arithmetic operator is date_period or time_duration (#108455)  * convert string to datetime when the other side of binary operator is temporal amount  ESQL: Move `NamedExpression` serialization (#109380)  This moves the serialization for the remaining `NamedExpression` subclass into the class itself  and switches all direct serialization of `NamedExpression`s to `readNamedWriteable` and friends. All other `NamedExpression` subclasses extend from `Attribute` who's serialization was moved ealier. They are already registered under the "category class" for `Attribute`. This also registers them as `NamedExpression`s.  ESQL: Implement LOOKUP  an "inline" enrich (#107987)  This adds support for `LOOKUP`  a command that implements a sort of inline `ENRICH`  using data that is passed in the request:  ``` $ curl -uelastic:password -HContent-Type:application/json -XPOST \ 'localhost:9200/_query?error_trace&pretty&format=txt' \ -d'{ "query": "ROW a=1::LONG | LOOKUP t ON a"  "tables": { "t": { "a:long":     [    1      4      2]  "v1:integer": [   10     11     12]  "v2:keyword": ["cat"  "dog"  "wow"] } }  "version": "2024.04.01" }' v1       |      v2       |       a ---------------+---------------+--------------- 10             |cat            |1 ```  This required these PRs: * #107624 * #107634 * #107701 * #107762 *  Closes #107306  parent 32ac5ba755dd5c24364a210f1097ae093fdcbd75 author Craig Taverner <craig@amanzi.com> 1717779549 +0200 committer Craig Taverner <craig@amanzi.com> 1718115775 +0200  Fixed compile error after merging in main  Fixed strange merge issues from main  Remove version from ES|QL test queries after merging main  Fixed union-types on nested fields  Switch to Luigi's solution  and expand nested tests  Cleanup after rebase  * Added more tests from code review  Note that one test  `multiIndexIpStringStatsInline` is muted due to failing with the error:  UnresolvedException: Invalid call to dataType on an unresolved object ?client_ip  * Make CsvTests consistent with integration tests for capabilities  The integration tests do not fail the tests if the capability does not even exist on cluster nodes  instead the tests are ignored. The same behaviour should happen with CsvTests for consistency.  * Return assumeThat to assertThat  but change order  This way we don't have to add more features to the test framework in this PR  but we would probably want a mute feature (like a `skip` line).  * Move serialization of MultiTypeEsField to NamedWritable approach  Since the sub-fields are AbstractConvertFunction expressions  and Expression is not yet fully supported as a category class for NamedWritable  we need a few slight tweaks to this  notably registering this explicitly in the EsqlPlugin  as well as calling PlanStreamInput.readExpression() instead of StreamInput.readNamedWritable(Expression.class). These can be removed later once Expression is fully supported as a category class.  * Remove attempt to mute two failed tests  We used required_capability to mute the tests  but this caused issues with CsvTests which also uses this as a spelling mistake checker for typing the capability name wrong  so we tried to use muted-tests.yml  but that only mutes tests in specific run configurations (ie. we need to mute each and every IT class separately).  So now we just remove the tests entirely. We left a comment in the muted-tests.yml file for future reference about how to mute csv-spec tests.  * Fix rather massive issue with performance of testConcurrentSerialization  Recreating the config on every test was very expensive.  * Code review by Nik  ---------  Co-authored-by: Elastic Machine <elasticmachine@users.noreply.github.com>
elastic,elasticsearch,0ae5aa35b7e5d1b4835d81a1d95dd1475b9c6a1f,https://github.com/elastic/elasticsearch/commit/0ae5aa35b7e5d1b4835d81a1d95dd1475b9c6a1f,Optimize BytesReference related code field access patterns (#109782)  Cache object fields (even when final  see https://openjdk.org/jeps/8132243) to generate smaller byte code as well as more optimized compiled code for this performance critical code.
elastic,elasticsearch,b99b5d5f253bc0f91783aa52f79d23443a8205ce,https://github.com/elastic/elasticsearch/commit/b99b5d5f253bc0f91783aa52f79d23443a8205ce,Remove unused seek-tracking plugin (#109600)  This was used for some performance investigations but is not currently needed  and would need updating in order to complete #100878. Instead  this commit removes it.
TheAlgorithms,Java,df0c997e4bce827246ee9d93ec3b1fe3c55a4332,https://github.com/TheAlgorithms/Java/commit/df0c997e4bce827246ee9d93ec3b1fe3c55a4332,General performance improvement (#6078)
spring-projects,spring-framework,dbd47ff4f9a7cf241eda414ca7be6af9db55aae6,https://github.com/spring-projects/spring-framework/commit/dbd47ff4f9a7cf241eda414ca7be6af9db55aae6,Implement additional micro performance optimizations  See gh-34717
spring-projects,spring-framework,0f2308e85f327600653b0f3f8aaf4e0e3131b4aa,https://github.com/spring-projects/spring-framework/commit/0f2308e85f327600653b0f3f8aaf4e0e3131b4aa,Implement micro performance optimizations  - ClassUtils.isAssignable(): Avoid Map lookup when the type is not a primitive.  - AnnotationsScanner: Perform low cost array length check before String comparisons.  - BeanFactoryUtils: Use char comparison instead of String comparison. The bean factory prefix is '&'  so we can use a char comparison instead of more heavyweight String.startsWith("&").  - AbstractBeanFactory.getMergedBeanDefinition(): Perform the low cost check first. Map lookup  while cheap  is still more expensive than instanceof.  Closes gh-34717  Signed-off-by: Olivier Bourgain <olivierbourgain02@gmail.com>
spring-projects,spring-framework,466ac6b703787663e300614033c9fb922b140175,https://github.com/spring-projects/spring-framework/commit/466ac6b703787663e300614033c9fb922b140175,Improve SimpleKey hashing function  Prior to this commit  `SimpleKey` would be used in Spring Framework's caching support and its `hashCode` value would be used to efficiently store this key in data structures.  While the current hashcode strategy works  the resulting values don't spread well enough when input keys are sequential (which is often the case). This can have negative performance impacts  depending on the data structures used by the cache implementation.  This commit improves the `hashCode` function with a mixer to better spread the hash values. This is using the mixer function from the MurMur3 hash algorithm.  Closes gh-34483
spring-projects,spring-framework,3b65506c13f18535e2fb055fd790bf45efb13de5,https://github.com/spring-projects/spring-framework/commit/3b65506c13f18535e2fb055fd790bf45efb13de5,Use ByteBuffer support in ServletHttpHandlerAdapter  As of Servlet 6.1  the `ServletInputStream` and `ServletOutputStream` offer read and write variants based on `ByteBuffer` instead of byte arrays. This can improve performance and avoid memory copy for I/O calls.  This was already partially supported for some servers like Tomcat through specific adapters. This commit moves this support to the standard `ServletHttpHandlerAdapter` and makes it available for all Servlet 6.1+ containers.  Closes gh-33748
spring-projects,spring-framework,1c69a3c521d3ad87c77ad067389e1217f0acb188,https://github.com/spring-projects/spring-framework/commit/1c69a3c521d3ad87c77ad067389e1217f0acb188,Fix `PathMatchingResourcePatternResolver` manifest classpath discovery  Update `PathMatchingResourcePatternResolver` so that in addition to searching the `java.class.path` system property for classpath enties  it also searches the `MANIFEST.MF` files from within those jars.  Prior to this commit  the `addClassPathManifestEntries()` method expected that the JVM had added `Class-Path` manifest entries to the `java.class.path` system property  however  this did not always happen.  The updated code now performs a deep search by loading `MANIFEST.MF` files from jars discovered from the system property. To deal with potential performance issue  loaded results are also now cached.  The updated code has been tested with Spring Boot 3.3 jars extracted using `java -Djarmode=tools`.  See gh-33705
NationalSecurityAgency,ghidra,e386550016c10db36052c6d394a4fd5cd25befcd,https://github.com/NationalSecurityAgency/ghidra/commit/e386550016c10db36052c6d394a4fd5cd25befcd,Merge remote-tracking branch 'origin/GP-2941_dev747368_definedstringstable_performance--SQUASHED' into Ghidra_11.4 (Closes #5726  Closes #8134  Closes #3498)
NationalSecurityAgency,ghidra,c396867209edf0de7dbeabed14da0764492386ab,https://github.com/NationalSecurityAgency/ghidra/commit/c396867209edf0de7dbeabed14da0764492386ab,GP-4512 Constant propagation and stack analysis performance changes
NationalSecurityAgency,ghidra,6fa543c2e2698773ebc0b537abf9ef47be817577,https://github.com/NationalSecurityAgency/ghidra/commit/6fa543c2e2698773ebc0b537abf9ef47be817577,GP-5477 - Decompiler - Fixed performance when using many global highlighters; updated the highlight service to allow for function-specific highlighting
NationalSecurityAgency,ghidra,86e77bd9efce139844830f9ec8a884b16c3dc83a,https://github.com/NationalSecurityAgency/ghidra/commit/86e77bd9efce139844830f9ec8a884b16c3dc83a,Merge remote-tracking branch 'origin/GP-4949_ghidra1_StructureEditorPerformance--SQUASHED' into Ghidra_11.2 (Closes #6936  Closes #6504)
NationalSecurityAgency,ghidra,ef724708df27c042509d8324d1a2a2f5c568bd04,https://github.com/NationalSecurityAgency/ghidra/commit/ef724708df27c042509d8324d1a2a2f5c568bd04,GP-4949 Added Structure.setLength method and made structure editor performance improvements and various bug fixes.
google,guava,75da92419a7d414bd3de23c89f2ddfd83754767d,https://github.com/google/guava/commit/75da92419a7d414bd3de23c89f2ddfd83754767d,Use `AtomicReferenceFieldUpdater` instead of `VarHandle` when `guava-android` runs under a JVM.  For much more discussion  see the code comments  especially in the backport copy of `AbstractFutureState`. (For a bit more on performance  see https://shipilev.net/blog/2015/faster-atomic-fu/  including its notes that `Unsafe` is not necessarily faster than `AtomicReferenceFieldUpdater`.)  Now that we no longer use `VarHandle` under Android  we can remove some Proguard rules for it: - We no longer need the `-dontwarn` rule for `VarHandleAtomicHelper`  since that type no longer exists in `guava-android`. - We no longer need the `-assumevalues` rule for `mightBeAndroid`  since it served only to strip the `VarHandle` code (to hide it from optimizers that run on the optimized code). And all else being equal  we'd rather _not_ have that rule _just in case_ someone is running `guava-android` through an optimizer and using it under the JVM (in which case we'd like to follow the JVM code path so that we don't try to use `Unsafe`). (OK  maybe it would be nice to keep the rule just so that Android doesn't have to perform a check of the `java.runtime.name` system property once during `AbstractFutureState` initialization. If anyone finds this to be an issue  please let us know.)  I've also updated the tests to better reflect which ones we run only under a JVM  not in an Android emulator. (I should really have done that back in cl/742859752.)  Fixes https://github.com/google/guava/issues/7769  RELNOTES=`util.concurrent`: Removed our `VarHandle` code from `guava-android`. While the code was never used at runtime under Android  it was causing [problems under the Android Gradle Plugin](https://github.com/google/guava/issues/7769) with a `minSdkVersion` below 26. To continue to avoid `sun.misc.Unsafe` under the JVM  `guava-android` will now always use `AtomicReferenceFieldUpdater` when run there. PiperOrigin-RevId: 746800729
google,guava,96d0d4b48f1b6981798c42ff2ed0820a2ddb81fa,https://github.com/google/guava/commit/96d0d4b48f1b6981798c42ff2ed0820a2ddb81fa,Rename fields to make them harder to use by accident.  Nearly all access should be performed through the various methods (e.g.  `casListeners(...)`  `listeners()`). In fact  we could be principled and perform _all_ access through those methods (except of course for within the implementations of those methods themselves). I haven't done that here just out of fear that it will somehow affect performance or cause stack overflows.  Accidental usage has been something that I've historically worried about in  e.g.  `AbstractFuture.set(V value)`  whose parameter has had the same name as a field. That _particular_ example matters less at the moment because the field recently became `private` to a new superclass  `AbstractFutureState`  and so it's not accessible in the subclass `AbstractFuture`.  But it's going to matter again: I'm likely to make the fields package-private as part of [work to migrate `guava-android` off `Unsafe`](https://github.com/google/guava/issues/7742). Currently  the fields can be `private` because we call `MethodHandles.lookup()` from within `AbsractFutureState`. (Yes  it would initially seem that [we shouldn't have to](https://github.com/google/guava/blob/c7363f7fb40698bb5f99d198cc45884f38642f86/guava/src/com/google/common/util/concurrent/AbstractFutureState.java#L612-L632)  but we do.) But that requires `AbstractFutureState` to refer to `MethodHandles.Lookup` in one of its method signatures\[*\]  and that makes old versions of Android upset. To avoid that  I will make `value` package-private  at which point I won't need the `MethodHandles.Lookup` reference in `AbstractFutureState` itself.  And when I tried making `value` package-private  _one test_ started to fail. It should have taken me less time to figure out  but I eventually discovered that the problem was that [the test refers to "`value`" inside an `AbstractFuture` subclass](https://github.com/google/guava/blob/c7363f7fb40698bb5f99d198cc45884f38642f86/guava-tests/test/com/google/common/util/concurrent/AbstractFutureTest.java#L78). Previously  this referred to the local variable `value` from the test method; with my change  it was instead referring to the `value` field.  (I wouldn't have to have gone down the road of making the field non-`private` in the first place [if not for Java 8 compatibility](https://github.com/google/guava/issues/6614#issuecomment-2548660841).... Still  as discussed above  this rename could protect against problems _within_ the file  too  and such problems could arise even if the field were to remain `private`.)  \[*\] Or maybe I could declare the method as returning `Object` instead of `MethodHandles.Lookup`  and the caller could cast it back? But we found during [our `SequencedCollection` work](https://github.com/google/guava/issues/6903) that Android (and I think maybe the JVM  but I can't find my record of this) can produce a `VerifyError` in some cases in which _implementation_ code refers to an unknown type  I think specifically when it needs to check whether a `return someThingOfTypeFoo` from that method is compatible with the return type `Bar` of the method. We _might_ be able to work around that by performing an explicit  "redundant" cast to `Object`  but I'm not even sure how to get javac to do that  and it feels very fragile  especially in the presence of optimization/minification tools.  RELNOTES=n/a PiperOrigin-RevId: 741607075
google,guava,6ce7830421a57decf7441b5a68b2544424caef9a,https://github.com/google/guava/commit/6ce7830421a57decf7441b5a68b2544424caef9a,Fix a minor weirdness in the implementation of `ByteSource.contentEquals(ByteSource)` and move the part of its implementation that compares two `InputStream`s to a package-private method in `ByteStreams` while I'm at it.  The minor weirdness was this: when the same number of bytes was read into both buffers _but_ the number of bytes read was less than the length of the buffers  we would then use `Arrays.equals` to compare the _full_ contents of both buffers even though the reads only partially filled them.  This was not a correctness issue because it was always the case that if we got to that point  all previous reads into the buffers must have filled both buffers with the _same_ bytes  so the bytes in the section of the buffers that was not filled in the most recent read were guaranteed to be the same.  This was also not much of a performance issue since it could only occur once per `contentEquals` call  when the end of both `InputStream`s had been reached (the buffers are always completely filled otherwise).  That said  it's more obviously correct (and saves a tiny bit of work) to ensure that we only compare the bytes that were actually read in the most recent read operations.  RELNOTES=n/a PiperOrigin-RevId: 738928305
google,guava,7719744040d05004bd7051fc33a782fa78b01634,https://github.com/google/guava/commit/7719744040d05004bd7051fc33a782fa78b01634,Make `Atomic*FieldUpdater` fields `static` for better performance.  Compare cl/713006636.  (Also  better document the similar code in `AbstractFuture`.)  Note that I also evaluated performance with `VarHandle`  and I found it no better. (Maybe we did a similar experiment with `Unsafe` way back when and came to a similar conclusion?)  Note that that's all based on _JVM_ performance (and on benchmarks that are not necessarily great). It's possible that Android it worth a further look someday. But our only option there _today_ might be `Unsafe`  and new usages of `Unsafe` would be moving backward.  RELNOTES=n/a PiperOrigin-RevId: 724013501
google,guava,400af25292096746ed3f6164f0ff88209acbb19f,https://github.com/google/guava/commit/400af25292096746ed3f6164f0ff88209acbb19f,Make `AtomicReferenceFieldUpdater` fields `static` for [better performance](https://shipilev.net/blog/2015/faster-atomic-fu/#:~:text=thrown%20out%20of%20the%20window).  This may eliminate the reason for [an `Unsafe`-based implementation](https://github.com/google/guava/issues/6806) even under Java 8  but we will ideally confirm that with benchmarks before ripping that implementation out. (There's also Android: `AtomicReferenceFieldUpdater` is available there  but we should look into performance  including under older versions  especially with AFAIK no plans to remove `Unsafe` there.)  (Also  make a few other tiny edits to the backport copy so that it matches the mainline copy more closely.)  RELNOTES=n/a PiperOrigin-RevId: 713006636
google,guava,1a300f6b2f7ba03ae9bc3620a80c4d4589c65b69,https://github.com/google/guava/commit/1a300f6b2f7ba03ae9bc3620a80c4d4589c65b69,Make `AbstractFuture` use `VarHandle` when available.  For now  this is only for the JRE flavor  not for Android.  Our not entirely great benchmarks suggest that this may actually improve performance slightly over the current `Unsafe`-based implementation. This matches my sense that [alternatives to `Unsafe` have gotten faster](https://github.com/google/guava/issues/6806#issuecomment-2518256341).  There are plenty of other [places in Guava that we use `Unsafe`](https://github.com/google/guava/issues/6806)  but this is a start.  Also: I'm going to consider this CL to "fix" https://github.com/google/guava/issues/6549  even though: - We already started requiring newer versions of Java to build our _tests_ in cl/705512728. (This CL is the first to require a newer JDK to build _prod_ code  again only to _build_ it  not to _run_ it.) - We already started requiring newer versions of Java to build our _GWT_ module in cl/711487270. - This CL requires only Java 9  not Java 11. - None of the changes so far should require people who _build our Maven project_ to do anything (aside from GWT users)  since our build automatically downloads a new JDK to use for javac since cl/655647768. RELNOTES=n/a PiperOrigin-RevId: 711733182
google,guava,a4a7f6bd00ca1acd2efcb81e493720569ba58424,https://github.com/google/guava/commit/a4a7f6bd00ca1acd2efcb81e493720569ba58424,Recommend the JDK `compareUnsigned` methods over our equivalents.  This is a followup to cl/655152611. As noted in the description of that CL  I didn't migrate the _implementations_ because of GWT+J2CL constraints. Those constraints bind very few users  so I don't think we need to acknowledge them in the Javadoc.  (If we start to think that an implementation migration might actually pay off in performance improvements  we could arrange for it by setting up GWT+J2CL supersource.)  Also  note that the methods are [available under Android even without opting in to library desugaring](https://r8.googlesource.com/r8/+/5c88be7bffa502cddc989f80beffc0dd5402a057/src/main/java/com/android/tools/r8/ir/desugar/BackportedMethodRewriter.java#961).  RELNOTES=n/a PiperOrigin-RevId: 662217380
dbeaver,dbeaver,c476c0293fdb0b22463c29e81c14172641719dd4,https://github.com/dbeaver/dbeaver/commit/c476c0293fdb0b22463c29e81c14172641719dd4,#35466 Launch performance fix
dbeaver,dbeaver,d59815e792f55e79d6c5dae4cde64c05b0b6e49a,https://github.com/dbeaver/dbeaver/commit/d59815e792f55e79d6c5dae4cde64c05b0b6e49a,dbeaver/dbeaver#36752 Hints cache refresh performance (#36792)  * dbeaver/dbeaver#36752 Hints cache refresh performance  * dbeaver/dbeaver#36752 Find/replace performance  ---------  Co-authored-by: Matvey16 <82543000+Matvey16@users.noreply.github.com>
dbeaver,dbeaver,0d350529df9f4e5f919fcf49aaecd5b1da6bb55d,https://github.com/dbeaver/dbeaver/commit/0d350529df9f4e5f919fcf49aaecd5b1da6bb55d,dbeaver/pro#3410 Init-performance improvement. Read only 1 setting. (#35800)
dbeaver,dbeaver,7bd668a9bf63d24df939013c12e401f0042e13f8,https://github.com/dbeaver/dbeaver/commit/7bd668a9bf63d24df939013c12e401f0042e13f8,dbeaver/pro#3198 Refactor `DBUtils#getAttributeValue` (#35413)  * dbeaver/pro#3198 Refactor `DBUtils#getAttributeValue`  * dbeaver/pro#3198 Remove unused code  * dbeaver/pro#3198 Properly show first deepest collection element  * dbeaver/pro#3198 Slightly clean up the code  * dbeaver/pro#3300 Use indices instead of deque  * dbeaver/pro#3198 Improve logging  * dbeaver/pro#3198 Improve formatting for collections  * dbeaver/pro#3198 Expanders labels  * dbeaver/pro#3198 Value update redesign  * dbeaver/pro#3198 Grid complex value edit fixes. UI and performance improvements  * dbeaver/pro#3198 Changes reset fix  * dbeaver/pro#3198 Grid complex value edit fixes  * dbeaver/pro#3198 Grid complex value edit fixes  * dbeaver/pro#3198 Grid complex value edit fixes  * dbeaver/pro#3198 Expand/collapse icons  * dbeaver/pro#3198 Code cleanup  * dbeaver/pro#3198 Fix connection invalidate during data save  * dbeaver/pro#3237 Boolean values toggle and render fix  * dbeaver/pro#3237 Record mode fixes. Keep elements in nested rows.  * dbeaver/pro#3237 Record mode fixes. Keep elements in nested rows.  * dbeaver/pro#3237 Record mode fixes  * dbeaver/pro#3237 Record mode fixes + edit fixes  * dbeaver/pro#3237 Record mode resize fix  * dbeaver/pro#3237 Record mode editor fix  * dbeaver/pro#3237 Nested records edit fix (FQN generation)  * dbeaver/pro#3237 UI enhancement + default value handler error improvement  * dbeaver/pro#3237 Clickhouse arrays handle fix  ---------  Co-authored-by: Serge Rider <serge@jkiss.org> Co-authored-by: serge-rider <serge@dbeaver.com> Co-authored-by: Diana <31996417+uslss@users.noreply.github.com>
dbeaver,dbeaver,fdc170b372156c8033efc06f4ef61f9d2e5cdcad,https://github.com/dbeaver/dbeaver/commit/fdc170b372156c8033efc06f4ef61f9d2e5cdcad,App shutdown performance improvement (do not init task manager)
dbeaver,dbeaver,be88eb0a52fa98f1d332abe41c058be4c462e0c4,https://github.com/dbeaver/dbeaver/commit/be88eb0a52fa98f1d332abe41c058be4c462e0c4,Project discovery performance improvement
dbeaver,dbeaver,0ec82bd5e82cbb7d3f93cd2d9ca665fc21bc539e,https://github.com/dbeaver/dbeaver/commit/0ec82bd5e82cbb7d3f93cd2d9ca665fc21bc539e,Minor performance fixes
dbeaver,dbeaver,ba5cfa2a1924d92f8f4cfa3ed1d4e89d9e081adb,https://github.com/dbeaver/dbeaver/commit/ba5cfa2a1924d92f8f4cfa3ed1d4e89d9e081adb,dbeaver/pro#3135 Oracle session query performance + connection page fix (#35084)  * dbeaver/pro#3135 Oracle session query performance + connection page fix  * dbeaver/pro#3135 Oracle session query fix
dbeaver,dbeaver,19a4432c2a7dcc1ee86de3ebf46275718740fa6c,https://github.com/dbeaver/dbeaver/commit/19a4432c2a7dcc1ee86de3ebf46275718740fa6c,3085 sql completion performance (#34826)  * #34450 Workaround for word-wrap (lazy activation)  * #34450 Workaround for word-wrap (lazy activation)  * #34450 Workaround for word-wrap (lazy activation)  * #34450 Workaround for word-wrap (lazy activation)  * #3085 SQL completion performance: cache tables  ---------  Co-authored-by: Elizabeth <e1izabeth.k@outlook.com>
dbeaver,dbeaver,e4e88b152ec67e7f3f4b1140f83775f56000a2ac,https://github.com/dbeaver/dbeaver/commit/e4e88b152ec67e7f3f4b1140f83775f56000a2ac,#29986 [CUBRID] Create and edit View using UI (#34335)  * #29986 Create and edit View using UI  * #29986 Improve code optimizations for better performance  * #29986 Change value to constant variable  ---------  Co-authored-by: Dziyana <31996417+uslss@users.noreply.github.com>
dbeaver,dbeaver,fbc607b78a6d5d54801ec6b2126fb3e81e524e73,https://github.com/dbeaver/dbeaver/commit/fbc607b78a6d5d54801ec6b2126fb3e81e524e73,DB2 metadata loading performance (load index columns in a single select)
termux,termux-app,f80b46487df539c7e9214550002f461e5c66131c,https://github.com/termux/termux-app/commit/f80b46487df539c7e9214550002f461e5c66131c,Fixed: Improve handling of empty ';' SGR sequences  Currently the Termux terminal emulator prints "HI" in red with:  ```sh printf "\e[31;m HI \e[0m" ```  This is not how other terminals (tested on xterm  gnome-terminal  alacritty and the mac built in terminal) handle it  since they parse ""\e[31;m" as "\e[31;0m"  where the "0" resets the colors.  This change aligns with other terminals  as well as improves performance by avoiding allocating a new int[] array for each byte processed by `parseArg()`  and most importantly simplifies things by removing the `mIsCSIStart` and `mLastCSIArg` state  preparing for supporting ':' separated sub parameters such as used in https://sw.kovidgoyal.net/kitty/underlines/
apache,dubbo,d6b11cf466919a8888f1d3a2481cd9cef0a828bd,https://github.com/apache/dubbo/commit/d6b11cf466919a8888f1d3a2481cd9cef0a828bd,[ISSUE #15377] The value of grpc-timeout may not comply with the specification (#15378)  * perf: Optimize the grpc-timeout generation method  * fix: Fix format errors
apache,dubbo,e2987318b2a22fbd7d9976b7e47a5bc2c31ebac2,https://github.com/apache/dubbo/commit/e2987318b2a22fbd7d9976b7e47a5bc2c31ebac2,perf: Remove duplicate checks (#15327)
apache,dubbo,3771339964ae52c21dac6fb34d7988314dde7e46,https://github.com/apache/dubbo/commit/3771339964ae52c21dac6fb34d7988314dde7e46,Optimize parseCharset method for best performance. (#15203)
apache,dubbo,51f4f7454e70c2acd31355763a5186f9d6f257f9,https://github.com/apache/dubbo/commit/51f4f7454e70c2acd31355763a5186f9d6f257f9,Performance tuning (#14604)  Co-authored-by: Albumen Kevin <jhq0812@gmail.com>
apache,dubbo,f90314ab334cf287bbe5c9eb8bf23a3673dac074,https://github.com/apache/dubbo/commit/f90314ab334cf287bbe5c9eb8bf23a3673dac074,A couple of performance and logging issue fix (#14564)
halo-dev,halo,eb969122ff19fa6deb31a9120ca070dfa3c09596,https://github.com/halo-dev/halo/commit/eb969122ff19fa6deb31a9120ca070dfa3c09596,perf: add caching for extension getter to enhance performance (#7102)  #### What type of PR is this? /kind improvement /area core /milestone 2.20.x  #### What this PR does / why we need it: 为扩展获取增加缓存以提高网站整体性能  在此之前，每个请求都要经过很多过滤器，而一些过滤器会获取扩展因此导致频繁查询扩展和扩展点定义拖慢了速度  **测试情况**  初始化一个全新环境，安装并启用以下插件和主题 - 已激活主题: [Earth 1.11.0](https://github.com/halo-dev/theme-earth) - 已启动插件: - [SEO 工具集 1.0.1](https://github.com/f2ccloud/plugin-seo-tools) - [OAuth2 认证 1.5.0](https://github.com/halo-sigs/plugin-oauth2) - [Trailing Slash 1.0.0](https://github.com/halo-sigs/plugin-trailing-slash) - [评论组件 2.5.1](https://github.com/halo-dev/plugin-comment-widget) - [KaTeX 2.1.0](https://github.com/halo-sigs/plugin-katex) - [应用市场 1.9.0](https://www.halo.run/store/apps/app-VYJbF)  通过 Apache Benchmark (ab) 进行 1w 次请求并发 100 个，测试访问首页，得到以下测试结果：  核心指标对比  |指标|改进前|改进后|提升情况| |---|---|---|---| |**总耗时 (Time taken)**|27.030 秒|25.718 秒|减少约 **4.9%**| |**每秒请求数 (RPS)**|369.96 req/sec|388.83 req/sec|提升约 **5.1%**| |**单请求平均耗时**|270.298 ms|257.181 ms|减少约 **4.9%**| |**传输速率 (Transfer Rate)**|6346.44 KB/s|6670.12 KB/s|提升约 **5.1%**|  综合分析 - 性能提升主要体现在：请求处理时间（Processing）、等待时间（Waiting）以及每秒请求数（RPS）均有 约5% 左右的提升。 - 传输效率更高：通过更快的处理时间，传输速率提高了 5.1%。 - 长尾请求优化显著：最大响应时间减少了约 14.9%，意味着极端情况下的性能更优。  #### Does this PR introduce a user-facing change?  ```release-note 为扩展获取增加缓存使网站整体性能提升 5% 以上 ```
halo-dev,halo,2b4d1ab8d81ea1ce1767da526fec4144d40d4265,https://github.com/halo-dev/halo/commit/2b4d1ab8d81ea1ce1767da526fec4144d40d4265,perf: add caching for system configuration fetcher to enhance performance (#7100)  #### What type of PR is this? /kind improvement /area core /milestone 2.20.x  #### What this PR does / why we need it: 为系统配置获取增加缓存以提高路由和主题模板渲染的速度  #### Special notes for your reviewer: 1. 系统能正确初始化 2. 测试修改系统配置后 http://localhost:8090/actuator/globalinfo 和主题端 `${site}` 是否都是新的 3. 更改了文章路由规则后能正确调整到新的规则  #### Does this PR introduce a user-facing change?  ```release-note 为系统配置的获取增加缓存以提高路由和主题模板渲染的速度 ```
halo-dev,halo,25c54d792e46030fe57c6044f3338e799b37771b,https://github.com/halo-dev/halo/commit/25c54d792e46030fe57c6044f3338e799b37771b,perf: replace concatMap to flatMapSequential to improve parallelism and efficiency (#6706)  #### What type of PR is this? /kind improvement /area core /milestone 2.20.x  #### What this PR does / why we need it: 将 concatMap 替换为 flatMapSequential 以提高并行度和执行效率  可以看一下这个场景示例来模拟像文章列表 API 的数据组装 假如每个步骤的执行时间是 1s 有 4 个步骤 同时 Flux 发出 4 条数据:  ```java @Test void test() { var startMs = System.currentTimeMillis();  var monoA = Mono.fromSupplier( () -> { sleep(); return "A"; })        .subscribeOn(Schedulers.boundedElastic());  var monoB = Mono.fromSupplier( () -> { sleep(); return "B"; })        .subscribeOn(Schedulers.boundedElastic());  var monoC = Mono.fromSupplier( () -> { sleep(); return "C"; })        .subscribeOn(Schedulers.boundedElastic());  var monoD = Mono.fromSupplier( () -> { sleep(); return "D"; })        .subscribeOn(Schedulers.boundedElastic());  var convert = Mono.when(monoA  monoB  monoC  monoD);  Flux.just("1"  "2"  "3"  "4") // concatMap(convert::thenReturn) .flatMapSequential(convert::thenReturn) .collectList() .block();  System.out.println("Time: " + (System.currentTimeMillis() - startMs)); }  private static void sleep() { try { Thread.sleep(1000); } catch (InterruptedException e) { throw new RuntimeException(e); } } ``` **结果:** 1. 如果每个步骤没有加  subscribeOn 且使用 concatMap 耗时: 16362 ms 2. 每个步骤使用 subscribeOn 且使用 concatMap 耗时: 4174 ms 3. 每个步骤使用 subscribeOn 且使用 flatMapSequential 耗时: 1185 ms  #### Does this PR introduce a user-facing change? ```release-note 提升页面访问速度 ```
halo-dev,halo,c10862d6fe9b4dd978ba8cf80907a54f35606ec7,https://github.com/halo-dev/halo/commit/c10862d6fe9b4dd978ba8cf80907a54f35606ec7,refactor: index mechanism to enhance overall performance (#6039)  #### What type of PR is this? /kind improvement /area core /milestone 2.17.x  #### What this PR does / why we need it: 重构索引机制的查询和排序以提升整体性能  **how to test it?** 使用 postgre 数据库，初始化 Halo ，然后执行以下脚本创建 30w 文章数据进行测试: <details> <summary>点击展开查看 SQL</summary>  ```sql DO $$ DECLARE i integer; postNameIndex integer; snapshotName varchar; totalRecords integer; BEGIN postNameIndex := 1; totalRecords := 300000;  FOR i IN 1..3 LOOP INSERT INTO "public"."extensions" ("name"  "data"  "version") VALUES ( '/registry/content.halo.run/categories/category-'||i  convert_to( jsonb_build_object( 'spec'  jsonb_build_object( 'displayName'  '分类-'||i  'slug'  'category-'||i  'description'  '测试分类'  'cover'  ''  'template'  ''  'priority'  0  'children'  '[]'::jsonb )  'status'  jsonb_build_object( 'permalink'  '/categories/category-'||i  'postCount'  totalRecords  'visiblePostCount'  totalRecords )  'apiVersion'  'content.halo.run/v1alpha1'  'kind'  'Category'  'metadata'  jsonb_build_object( 'finalizers'  jsonb_build_array('category-protection')  'name'  'category-' || i  'annotations'  jsonb_build_object( 'content.halo.run/permalink-pattern'  'categories' )  'version'  0  'creationTimestamp'  '2024-06-12T03:56:40.315592Z' ) )::text  'UTF8')  0 ); END LOOP;   FOR i IN 1..3 LOOP INSERT INTO "public"."extensions" ("name"  "data"  "version") VALUES ( '/registry/content.halo.run/tags/tag-' || i  convert_to( jsonb_build_object( 'spec'  jsonb_build_object( 'displayName'  'Halo tag ' || i  'slug'  'tag-'||i  'color'  '#ffffff'  'cover'  '' )  'status'  jsonb_build_object( 'permalink'  '/tags/tag-' || i  'visiblePostCount'  totalRecords  'postCount'  totalRecords  'observedVersion'  0 )  'apiVersion'  'content.halo.run/v1alpha1'  'kind'  'Tag'  'metadata'  jsonb_build_object( 'finalizers'  jsonb_build_array('tag-protection')  'name'  'tag-'||i  'annotations'  jsonb_build_object( 'content.halo.run/permalink-pattern'  'tags' )  'version'  0  'creationTimestamp'  '2024-06-12T03:56:40.406407Z' ) )::text  'UTF8')  0); END LOOP;  FOR i IN postNameIndex..totalRecords LOOP -- Generate snapshotName snapshotName := 'snapshot-' || i;  -- Insert post data INSERT INTO "public"."extensions" ("name"  "data"  "version") VALUES ( '/registry/content.halo.run/posts/post-' || postNameIndex  convert_to( jsonb_build_object( 'spec'  jsonb_build_object( 'title'  'title-' || postNameIndex  'slug'  'slug-' || postNameIndex  'releaseSnapshot'  snapshotName  'headSnapshot'  snapshotName  'baseSnapshot'  snapshotName  'owner'  'admin'  'template'  ''  'cover'  ''  'deleted'  false  'publish'  true  'pinned'  false  'allowComment'  true  'visible'  'PUBLIC'  'priority'  0  'excerpt'  jsonb_build_object( 'autoGenerate'  true  'raw'  '' )  'categories'  ARRAY['category-kEvDb'  'category-XcRVk'  'category-adca']  'tags'  ARRAY['tag-RtKos'  'tag-vEsTR'  'tag-UBKCc']  'htmlMetas'  '[]'::jsonb )  'status'  jsonb_build_object( 'phase'  'PUBLISHED'  'conditions'  ARRAY[ jsonb_build_object( 'type'  'PUBLISHED'  'status'  'TRUE'  'lastTransitionTime'  '2024-06-11T10:16:15.617748Z'  'message'  'Post published successfully.'  'reason'  'Published' )  jsonb_build_object( 'type'  'DRAFT'  'status'  'TRUE'  'lastTransitionTime'  '2024-06-11T10:16:15.457668Z'  'message'  'Drafted post successfully.'  'reason'  'DraftedSuccessfully' ) ]  'permalink'  '/archives/slug-' || postNameIndex  'excerpt'  '如果你看到了这一篇文章，那么证明你已经安装成功了，感谢使用 Halo 进行创作，希望能够使用愉快。'  'inProgress'  false  'contributors'  ARRAY['admin']  'lastModifyTime'  '2024-06-11T10:16:15.421467Z'  'observedVersion'  0 )  'apiVersion'  'content.halo.run/v1alpha1'  'kind'  'Post'  'metadata'  jsonb_build_object( 'finalizers'  ARRAY['post-protection']  'name'  'post-' || postNameIndex  'labels'  jsonb_build_object( 'content.halo.run/published'  'true'  'content.halo.run/deleted'  'false'  'content.halo.run/owner'  'admin'  'content.halo.run/visible'  'PUBLIC'  'content.halo.run/archive-year'  '2024'  'content.halo.run/archive-month'  '06'  'content.halo.run/archive-day'  '11' )  'annotations'  jsonb_build_object( 'content.halo.run/permalink-pattern'  '/archives/{slug}'  'content.halo.run/last-released-snapshot'  snapshotName  'checksum/config'  '73e40d4115f5a7d1e74fcc9228861c53d2ef60468e1e606e367b01efef339309' )  'version'  0  'creationTimestamp'  '2024-06-11T05:51:46.059292Z' ) )::text  'UTF8')  1 );  -- Insert content data INSERT INTO "public"."extensions" ("name"  "data"  "version") VALUES ( '/registry/content.halo.run/snapshots/' || snapshotName  convert_to( jsonb_build_object( 'spec'  jsonb_build_object( 'subjectRef'  jsonb_build_object( 'group'  'content.halo.run'  'version'  'v1alpha1'  'kind'  'Post'  'name'  'post-' || postNameIndex )  'rawType'  'HTML'  'rawPatch'  '<p style=\"\">测试内容</p>'  'contentPatch'  '<p style=\"\">测试内容</p>'  'lastModifyTime'  '2024-06-11T06:01:25.748755Z'  'owner'  'admin'  'contributors'  ARRAY['admin'] )  'apiVersion'  'content.halo.run/v1alpha1'  'kind'  'Snapshot'  'metadata'  jsonb_build_object( 'name'  snapshotName  'annotations'  jsonb_build_object( 'content.halo.run/keep-raw'  'true' )  'creationTimestamp'  '2024-06-11T06:01:25.748925Z' ) )::text  'UTF8')  1 );  postNameIndex := postNameIndex + 1; END LOOP; END $$; ```  </details>  使用以下 API 查询文章 ``` curl 'http://localhost:8090/apis/api.console.halo.run/v1alpha1/posts?page=1&size=20&labelSelector=content.halo.run%2Fdeleted%3Dfalse&labelSelector=content.halo.run%2Fpublished%3Dtrue&fieldSelector=spec.categories%3Dcategory-1&fieldSelector=spec.tags%3Dc33ceabb-d8f1-4711-8991-bb8f5c92ad7c&fieldSelector=status.contributors%3Dadmin&fieldSelector=spec.visible%3DPUBLIC' \ --header 'Authorization: Basic YWRtaW46YWRtaW4=' ``` Before:  ![SCR-20240612-o20](https://github.com/halo-dev/halo/assets/38999863/fc27a265-6571-4361-a707-a683ea040837) After:  ![SCR-20240612-q1c](https://github.com/halo-dev/halo/assets/38999863/c0a241b8-5ed4-4973-8dfc-c260ffccd727)  #### Does this PR introduce a user-facing change? ```release-note 重构索引机制的查询和排序使整体性能提升 50% 以上 ```
airbnb,lottie-android,5deb2deeb3507a39fca4ee17d29e45113302b93e,https://github.com/airbnb/lottie-android/commit/5deb2deeb3507a39fca4ee17d29e45113302b93e,Drop shadow overhaul: improve correctness and performance (#2548)  ## High-level summary  This PR introduces a large change to how drop shadows are rendered  introducing an `applyShadowsToLayers` flag which  by analogy to `applyOpacitiesToLayers`  allows layers to be treated as a whole for the purposes of drop shadows  improving the accuracy and bringing lottie-android in line with other renderers (lottie-web and lottie-ios).  Several different codepaths for different hardware/software combinations are introduced to ensure the fastest rendering available  even on legacy devices.  The calculation of shadow direction with respect to transforms is improved so that the output matches lottie-web and lottie-ios.  Image layers now cast shadows correctly thanks to a workaround to device-specific issues when combining `Paint.setShadowLayer()` and bitmap rendering.  Even in non-`applyShadowsToLayers` mode  correctness is improved by allowing the shadow-to-be-applied to propagate in a similar way as alpha. This allows some amount of visual fidelity to be recovered for animations or environments where enabling `applyShadowsToLayers` is not possible.  A number of issues that caused incorrect rendering in some other cases have been fixed.  ## Background  ### Drop shadows in Lottie  Lottie specifies drop shadows as a tuple of (angle  distance  radius  color  alpha)  with each element being animatable.  The consensus behavior for the rendering of a layer with a drop shadow  which seems to be mostly respected in lottie-web and lottie-ios  seems to be:  1. Evaluate the values at the current frame for angle (`theta`)  distance (`d`)  radius (`r`)  color with alpha (`C`). 2. Apply the layer transform and render the layer normally to a surface `So` (original layer). 3. Copy `So` to new surface `Ss` (shadow). 4. Apply a gaussian blur of radius `r' = c * r` to `Ss`  where `c` is some platform-specific constant intended to normalize blur implementations between platforms. (Ours is 0.33  lottie-web's is 0.25; see https://github.com/airbnb/lottie-android/pull/2541). 5. Tint `Ss` with the color and combine the alpha by applying the following for each pixel `P`: `P.rgb = C.rgb * P.a; P.a = C.a * P.a`. 6. Now the shadow is ready on `Ss`  and needs to be drawn into its final position. 7. Convert from polar coordinates `theta` and `d` into `dx` and `dy`  with the 0 position at 12 o'clock: `dx = d * cos(theta - pi/2); dy = d*sin(theta - pi/2)`. 8. Draw `Ss` onto `Si` (intermediate surface) with a translation of `(dx  dy)`. 9. Draw `So` (original layer) onto `Si` with identity transform. 10. Compose `Si` into the framebuffer using the layer's alpha and blend mode.  Some non-obvious consequences of the definition above: - The angle  distance  and radius are relative to the layer post-transform  not pre-transform. That is  rotating the layer (via its transform) still keeps the same screen-space direction of the shadow  and scaling the layer (via its transform) still keeps the same screen-space shadow blur radius. - The drop shadow is not based on any derived outline  so a layer's drop shadow can be seen through its non-fully-opaque pixels. At the same time  reducing the alpha of a pixel in a layer reduces its alpha in the drop shadow. - A layer's shadow and the layer do not blend on top of each other on the final canvas in case the layer has a blend mode or alpha. Instead  the shadow and the layer are alpha-blended with each other  and the result is then composited onto the canvas. - In case the layer has a normal blend mode  this is equivalent to alpha-blending the layer's shadow and then the shadow onto the canvas separately.  ### Drop shadows in lottie-android currently  lottie-android's current implementation of drop shadows differs in important ways: 1. **Shadows are applied per-shape.** This means that a case like a shape with both fill and stroke has incorrect shadows  since both the fill and the stroke render a separate shadow on top of each other. 2. **Precomp layer shadows are ignored.** This means that a precomp cannot cause any of its child shapes to cast a shadow. This is a consequence of the current implementation of (1). 3. **Image layers do not render correct shadows ** due to the minefield that is the support matrix (or in Android's case  a more apt name would be a support tensor) of Android's graphics stack - `setShadowLayer()` simply doesn't work for images consistently. (See the last image in https://github.com/airbnb/lottie-android/pull/2523#issue-2428578510.)  ## Contributions of this PR  This PR introduces the following improvements and additions.  1. **Move the drop shadow model from individual content elements to layers ** and add some missing keypath callbacks. This is a prerequisite for handling drop shadows on a layer level. 2. **An `OffscreenLayer` implementation ** which serves as an abstraction that can replace `canvas.saveLayer()` for off-screen rendering and composition onto the final bitmap  but with the important distinction that it can also handle drop shadows  and possibly use hardware-accelerated `RenderNode`s and `RenderEffects` where available. - To use an `OffscreenLayer`  call its `.start()` method with a parent canvas and a `ComposeOp`  and draw on the *returned canvas.* Once finished  call `OffscreenLayer.finish()` to compose everything from the returned canvas to the parent canvas  applying alpha  blend mode  drop shadows  and color filters. - `OffscreenLayer` makes a dynamic decision on what to use for rendering - a no-op  forward to `.saveLayer()`  a HW-accelerated `RenderNode`  or a software bitmap  depending on the requested `ComposeOp` and hardware/SDK support. - The hope is that `OffscreenLayer` becomes a useful abstraction that can be extended to e.g. support hardware blurs  multiple drop shadows  or to support mattes in a hardware-accelerated fashion where possible. 3. **The `applyShadowsToLayers` flag** which  by analogy to `applyOpacityToLayers`  turns on a more accurate mode that implements the drop shadow algorithm described above. - `OffscreenLayer` is used to apply alpha if `applyOpacityToLayers` is enabled  and to apply shadows if `applyShadowsToLayers` is enabled. The cost is paid only once if both alpha and drop shadows are present on a layer. - Not all `saveLayer()` calls in the code have been rewritten to use `OffscreenLayer` - the blast radius is minimized. `OffscreenLayer` is presently used only to apply alpha and drop shadows  and blend mode and color filters are still applied in `BaseLayer` using `saveLayer()` directly. 4. **More accurate shadow transformations.** Previously  the angle and distance were pre-transform  and only the radius was post-transform (contrary to step (2) of the algorithm). We correct this to match other renderers. 5. **More complete shadow handling even when `applyShadowsToLayers` is `false`:** we plumb the shadow through `.draw()` and `drawLayer()` calls similarly to alpha  and this allows us to render per-shape shadows on children of composition layers too. 6. ***Workaround for drop shadows on image layers.** - The workaround relies on `OffscreenLayer` as well  and image layers now render shadows properly in all cases. 7. **Fixes to a few subtle issues** causing incorrect rendering in other cases. (will be marked using PR comments  I might have forgotten some)  ## Open questions  * **Should `applyShadowsToLayers` be `true` by default?** Some codepaths  such as when rendering purely via software  can be slow if shadow-casting layers are exceedingly large. But  the performance is still acceptable  and in the vast majority of cases everything is quite snappy. * **Have I introduced any regressions?** The snapshot tests should answer this. * **How does this perform on older devices?** `applyShadowsToLayers` plus an old device should trigger the purely-software shadow rendering mode. Simulating this in condition manually yields accurate results  and the performance seems surprisingly good  but it's unclear what will happen on a lower-end phone. There's also always the possibility of some device subtlety being missed. I don't have access to an older Android device.  ## Testcases  These files now match between lottie-web and lottie-android:  [drop_shadow_comparator.json](https://github.com/user-attachments/files/16997070/drop_shadow_comparator.json)  [simple_shadow_casters_ll2.json](https://github.com/user-attachments/files/16997084/simple_shadow_casters_ll2.json)  The files from this earlier PR still all render the same: https://github.com/airbnb/lottie-android/pull/2523  with the exception of the fix for image layer bug  which fixes the rendering of the Map icon as mentioned in the comment of that PR.  This file has been used as a perf stress test with many <255 opacity precomps  some stacked inside each other  that must all be blended separately: [precomp_opacity_killer.json](https://github.com/user-attachments/files/16997261/precomp_opacity_killer.json)
netty,netty,7e96030afa2a781f9cd1482b095081dcc014f2fe,https://github.com/netty/netty/commit/7e96030afa2a781f9cd1482b095081dcc014f2fe,Always prefer direct buffers for pooled allocators if not explicit di… (#15232)  …sabled  Motivation:  We should always prefer direct buffers if the ByteBufAllocator implementations are pooled as deallocating these buffers should be rare.  Modifications:  - Add new method to PlatformDependent that can be used to check if default direct buffer usage was explicit disabled - Use this method in our pooled ByteBufAllocator implementations to check if we should use direct buffers or not by default  Result:  Less memory copies and better performance when using the pooled allocators even if Unsafe can not be used.
netty,netty,90dff73a0f698525cbb9492d94331fe6692609cb,https://github.com/netty/netty/commit/90dff73a0f698525cbb9492d94331fe6692609cb,Simplify MQTT remaining length parsing (#15196)  Motivation:  Simplify the MQTT remaining length variable parsing in the MqttDecoder.  Modification:  Extracted the parsing logic to a separate method and refactored it.  Result:  The code becomes clearer and easier to understand. Also  the performance got slightly improved: ``` Benchmark                                                (size)  Mode  Cnt   Score   Error  Units MqttRemainingLengthBench.remainingLengthWithDoWhileLoop       1  avgt    3  31.790 ± 2.197  ns/op MqttRemainingLengthBench.remainingLengthWithDoWhileLoop       2  avgt    3  32.582 ± 1.884  ns/op MqttRemainingLengthBench.remainingLengthWithDoWhileLoop       3  avgt    3  32.656 ± 1.432  ns/op MqttRemainingLengthBench.remainingLengthWithDoWhileLoop       4  avgt    3  32.619 ± 1.795  ns/op MqttRemainingLengthBench.remainingLengthWithForLoop           1  avgt    3  31.516 ± 2.110  ns/op MqttRemainingLengthBench.remainingLengthWithForLoop           2  avgt    3  32.459 ± 2.740  ns/op MqttRemainingLengthBench.remainingLengthWithForLoop           3  avgt    3  32.407 ± 1.630  ns/op MqttRemainingLengthBench.remainingLengthWithForLoop           4  avgt    3  32.238 ± 3.994  ns/op ```  ---------  Co-authored-by: Norman Maurer <norman_maurer@apple.com>
netty,netty,c88cdcd3ca6cf5b0cbd0a77ca79f8a59d562f052,https://github.com/netty/netty/commit/c88cdcd3ca6cf5b0cbd0a77ca79f8a59d562f052,Remove Result instance allocation in MqttDecoder (#15193)  Motivation:  MqttDecoder allocates a number of `Result` class instances to decode almost any MQTT message. This reduces the performance and creates more garbage  making life harder for the garbage collector later.  ![image](https://github.com/user-attachments/assets/c2df372b-dd79-48ba-8cfd-44b0c72cb440)  Modification:  Removed `Result` class instance allocations in MqttDecoder.  Result:  Reduced class instance allocations lead to improved performance and reduced garbage.
netty,netty,eaf38df0eb87c8562e044b512386afd62ffc38d2,https://github.com/netty/netty/commit/eaf38df0eb87c8562e044b512386afd62ffc38d2,MQTT: Improve `isValidPublishTopicName` method performance (#15198)  Motivation:  We can refactor the method to improve performance.  Modifications:  Refactored `MqttCodecUtil.isValidPublishTopicName()` method to boost the performance.  Result:  According to my benchmarks  it is boosted almost twice: ``` Benchmark                           Mode  Cnt   Score   Error  Units TwoIndexOfVSLoopImprove.charAt      avgt    3  13.973 ± 0.137  ns/op TwoIndexOfVSLoopImprove.twoIndexOf  avgt    3  24.519 ± 7.310  ns/op ```  Co-authored-by: Norman Maurer <norman_maurer@apple.com>
netty,netty,31c033a1d95eb4a74002c5f17ea01c967b07add9,https://github.com/netty/netty/commit/31c033a1d95eb4a74002c5f17ea01c967b07add9,Optimize ByteBuf.setCharSequence for adaptive allocator (#15165)  Motivation: The `setCharSequence` method is used a fair bit when encoding HTTP headers. I noticed the AdaptiveByteBuf was sending the call down a path with a more indirections than strictly necessary.  Modification: Implement `setCharSequence` in the `AdaptiveByteBuf` so we skip a level of indirections. This is especially profitable for direct/off-heap buffers  where we see up to a 50% performance improvement in the benchmark. The benefit is greater the longer the string is.  Interestingly  the benchmark also show that it isn't profitable to do this for the `getCharSequence` sibling method.  Result: Direct buffers from the adaptive allocator now have much faster `setCharSequence`  which will help with HTTP header encoding.
netty,netty,4bbe6f71d0d0bbf514b281d920aed276b5156919,https://github.com/netty/netty/commit/4bbe6f71d0d0bbf514b281d920aed276b5156919,Optimize capacity bumping for adaptive ByteBufs (#15080)  Motivation: It is quite common in downstream code to not specify a buffer size when allocating. People instead rely on the ByteBuf ability to increase its size automatically. The ByteBufs allocated by the adaptive allocator also support this  but has thus far required a round-trip through the allocator every time. In contrast  the pooled allocator is reserving the chunk run-size when allocating buffers  and very cheaply allow smaller buffers to bump up to the run-size limit. This is an important performance optimisation in practice. We can do something similar in the adaptive allocator  because we keep statistics about the buffer sizes that we allocate.  Modification: The adaptive allocator now reserves space equal to the 99-percentile buffer size (though with some reasonable heuristic limits for smaller buffers)  when allocating buffers  and allow its ByteBufs to cheaply bump their capacities up to this limit. We already compute this percentile during histogram rotation  so it is simply a matter of storing this value in the magazine  and then using it when reserving space out of chunks.  The collection of buffer size statistics is now also delayed and moved to when the `ByteBuf` instances are reused. This way  we collect statistics about the _final_ buffer size  rather than the requested sizes or the sizes of capacity bumps. This prevents oscillations that would otherwise be caused by undersized buffers producing a lot of statistics as they resize to their final size  and then no statistics being produced for the final buffer sizes if they are correctly predicted at allocation time.  Result: Calls like `ByteBuf.ensureWritable()` will now on average be much cheaper when using the adaptive allocator.  Based on https://github.com/netty/netty/pull/15062
netty,netty,4c1a4bd08d435c44f20cc07e37a45321f520bb18,https://github.com/netty/netty/commit/4c1a4bd08d435c44f20cc07e37a45321f520bb18,IoUring: Allow users to explicit enable RECVSEND_BUNDLE support (#15135)  Motivation:  In the past we did disable support for RECVSEND_BUNDLE completely as we did see issues on our CI. After more debugging we were able to report the issue to the io_uring kernel maintainers as it turned out to be a kernel bug:   https://lore.kernel.org/io-uring/364679fa-8fc3-4bcb-8296-0877f39d6f2c@gmail.com/T/#ma949ad361d376247a16db73e741cb1043e56e6a4  Once there is a kernel which has this bug-fix it is save to use RECVSEND_BUNDLE again and so make use of the extra performance. Because of this we should enable support but disable the feature explicit if not told otherwise. This will allow the end-user to explicit enable it if it is know that there current kernel is not affected anymore.  Modifications:  - Reenable support for RECVSEND_BUNDLE - Disable usage of RECVSEND_BUNDLE by default but let users explicit enable it again via -Dio.netty.iouring.recvsendBundleEnabled=true - Simplify code to use the same code path for RECVSEND_BUNDLE usage and non-usage  Result:  Allow users to explicit opt in for RECVSEND_BUNDLE
netty,netty,7611905d3315c86da8aa4d55469b50f59a9af949,https://github.com/netty/netty/commit/7611905d3315c86da8aa4d55469b50f59a9af949,Fix init order of PlatformDependent0 fields (#15077)  Motivation:  After #14975  `EXPLICIT_NO_UNSAFE_CAUSE` has a dependency to `JAVA_VERSION`  which is initialized after it. When `explicitNoUnsafeCause0()` tries to set `io.netty.noUnsafe` to true by default on Java 24  it fails  because `javaVersion()` always returns 0 at that point.  Also  `isAndroid0()` is called twice  once in `javaVersion0()` and once to initialize `IS_ANDROID`. This is not a bug or a performance issue  but it does have a visible side-effect when debug logging is enabled.  Modification:  The declaration of `EXPLICIT_NO_UNSAFE_CAUSE` is moved after `JAVA_VERSION`  ensuring `javaVersion()` returns the correct version when `explicitNoUnsafeCause0()` calls it.  `IS_ANDROID` is also moved before `JAVA_VERSION`  such that the `isAndroid0()` method is called only once.  Result:  Fixes #14942 correctly
netty,netty,2e9dadb9ae4ffb5a720a680a5195be8193710783,https://github.com/netty/netty/commit/2e9dadb9ae4ffb5a720a680a5195be8193710783,IoUring: Retry buffer ring based ready directly once we receive a ENO… (#14866)  …BUFS.  Motivation:  When we use a buffer ring and receive ENOBUFS we should just let the user know and retry with a buffer ring again. This works as expected as we always refill the buffer ring once we used a buffer out of it. At the moment we will fallback to do a read loop without a buffer ring which slows down things without any benefits.  Modifications:  Just retry directly with a buffer ring.  Result:  Performance improvements when the buffer ring is exhausted in between.  Before:  ``` ./src/tcpkali -m xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx   -c 50 -T 30 127.0.0.1:8081 Destination: [127.0.0.1]:8081 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8081 Ramped up to 50 connections. Total data sent:     130258.4 MiB (136585805824 bytes) Total data received: 130229.7 MiB (136555722112 bytes) Bandwidth per channel: 1456.511⇅ Mbps (182063.8 kBps) Aggregate bandwidth: 36408.753↓  36416.774↑ Mbps Packet rate estimate: 3333377.7↓  3125676.8↑ (12↓  45↑ TCP MSS/op) Test duration: 30.005 s. ```  With this change:  ``` ./src/tcpkali -m xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx   -c 50 -T 30 127.0.0.1:8081 Destination: [127.0.0.1]:8081 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8081 Ramped up to 50 connections. Total data sent:     201195.0 MiB (210968248320 bytes) Total data received: 201185.2 MiB (210958005952 bytes) Bandwidth per channel: 2249.524⇅ Mbps (281190.5 kBps) Aggregate bandwidth: 56236.732↓  56239.462↑ Mbps Packet rate estimate: 5148636.2↓  4827071.8↑ (12↓  45↑ TCP MSS/op) Test duration: 30.01 s. ```
netty,netty,cc934fd06ebdf00ef5c1e1ea9d06c51fb6ddb91f,https://github.com/netty/netty/commit/cc934fd06ebdf00ef5c1e1ea9d06c51fb6ddb91f,Add QueryStringDecoder option to leave '+' alone (#14850) (#14857)  Motivation:  The URI standard RFC 3986 does not specify that query components have their spaces encoded as `+`. It is implied that the encoding is `%20` instead. However  the whatwg HTML standard says explicitly that the query must be encoded using `application/x-www-form-urlencoded` rules  which does use `+` for space. This is also what browsers do.  QueryStringDecoder should offer a way to parse either format.  Modification:  - Modify QueryStringDecoder to use a builder to accommodate the increasing number of flags - Add a `htmlQueryDecoding` flag  enabled by default  to control the `+` decoding  The default value of `htmlQueryDecoding` is appropriate for most use cases  I don't think it should be changed even in netty 5.  Also fixed the benchmark harness for Java 21.  Result:  Query strings encoded purely according to the URI spec can be decoded properly.  I measured the performance of the new builder  and it didn't look any different.  Co-authored-by: Jonas Konrad <me@yawk.at> Co-authored-by: Chris Vest <mr.chrisvest@gmail.com>
netty,netty,a4acd66fee7efc8955ede65fef63012085551ea1,https://github.com/netty/netty/commit/a4acd66fee7efc8955ede65fef63012085551ea1,IoUring: Refill buffer ring more eagerly (#14842)  Motivation:  IoUring: Refill buffer ring more eagerly  Motivation:  We should refill the buffer ring as soon as possible to reduce the possibility of seeing ERRNO_NO_BUFFER  Modifications:  Refill buffer as soon as there is an empty slot.  Result.  Much better performance.  Before:  ``` src/./tcpkali  -m xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx   -c 10 -T 30 127.0.0.1:8081 Destination: [127.0.0.1]:8081 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8081 Ramped up to 10 connections. Total data sent:     200090.4 MiB (209809965056 bytes) Total data received: 200066.2 MiB (209784590336 bytes) Bandwidth per channel: 11185.687⇅ Mbps (1398210.9 kBps) Aggregate bandwidth: 55925.055↓  55931.819↑ Mbps Packet rate estimate: 5120114.0↓  4800666.6↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0094 s. ```  After:  ``` src/./tcpkali  -m xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx   -c 10 -T 30 127.0.0.1:8081 Destination: [127.0.0.1]:8081 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8081 Ramped up to 10 connections. Total data sent:     251409.6 MiB (263622033408 bytes) Total data received: 251353.1 MiB (263562850304 bytes) Bandwidth per channel: 14058.190⇅ Mbps (1757273.7 kBps) Aggregate bandwidth: 70283.059↓  70298.841↑ Mbps Packet rate estimate: 6434619.5↓  6033797.9↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0002 s. ```
netty,netty,0d7320ca4c87bec34db9e2020712e2b93cc3bbf5,https://github.com/netty/netty/commit/0d7320ca4c87bec34db9e2020712e2b93cc3bbf5,Reduce pipeline stack depth and improve performance (#14705)  Motivation: Pipeline calls  such as `ctx.fireChannelRead` and `ctx.write`  currently go through multiple methods. This increases the stack depth in event loop threads  which makes it harder to debug (people have more ceremony mixed in with their code)  and hurts performance (JIT inliner budget gets used up faster).  Furthermore  there are other places in this machinery which can be made faster.  Modification:  **1.** In the `AbstractChannelHandlerContext` all inbound methods  e.g. `fireChannelRead`  have been manually inlined. This means that when a handler calls `ctx.fireChannelRead`  this method call in turn now directly call the `channelRead` method of the _next_ handler in the pipeline. Previously  we had two extra method calls here.  As a consequence of this inlining  we now have to re-compute the target context when we trampoline tasks onto the event-loop. This is presumably rare  and worth the cost. This also means that some code now moves from the executor of the target context  to the executor of the calling context. This can create different behaviors from Netty 4.1  if the pipeline has multiple handlers  is modified by the handlers during the call  and the handlers use child-executors.  **2.** A few outbound methods - `read`  `write`  `writeAndFlush` - are likewise inlined. Their inherent complexity and number of overloads means we can't realistically get them down to a single method call  but we can get them down to two. This is still a nice improvement. The `flush` method is usually not implemented by handlers  so there's no point in inlining that further.  **3.** In every such call  after finding the next context  we have to decide if we can call the handler directly  or need to trampoline onto a different event loop (due to the executor off-loading feature). This means we have to inspect the context and either pick its child executor  or load the channel event loop of the target context  and this latter part (which is the most common case) is a sequence of dependent loads. Dependent loads cause cache misses and CPU pipeline stalls  so to deal with this the `AbstractChannelHandlerContext` now has a `contextExecutor` field  which caches the result of computing the concrete executor. This means our executor is only one dependent (on the channel handler context) load away. The speedup from this is quite noticeable because it's such a common operation.    Results: The `DefaultChannelPipelineBenchmark` on my M1 Pro  running JDK 17  tells an encouraging story:  ``` Before: Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt        Score        Error  Units DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   10  6515502.983 ± 253375.107  ops/s DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   10  7628162.835 ±  40168.895  ops/s  After: Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt        Score        Error  Units DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   50  6899710.275 ± 115305.284  ops/s DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   50  7985208.748 ±  31532.793  ops/s ```
netty,netty,f5bc749b192b64c2bc3c080e24ebb0eb94505aee,https://github.com/netty/netty/commit/f5bc749b192b64c2bc3c080e24ebb0eb94505aee,IOUring: Add supported for provided buffers (#14690)  Motivation:  In scenarios with a large number of inactive connections  reduce memory usage and improve performance.  see https://lwn.net/Articles/815491/  Modification:  Implement IoUringBufferRing and IOSQE_BUFFER_SELECT flag for socket recv:  - Enable the use of IOSQE_BUFFER_SELECT in the socket recv when IoUringBufferRing is supported by the current Linux kernel version and IOUringSocketChannelConfig is configured to enable the feature - IoUringBufferRing elements are not populated immediately; only when submitting recv ops  if there is available space in the current BufferRing  the allocator will be used to allocate a buffer. - If the res in the recv CQE is -ERRNO_NO_BUFFER  mark the bufferRing as temporarily unavailable. The current and subsequent recv operations will fall back to the old recv path. - When a ByteBuf allocated from IoUringBufferRing is released by the user  it will be returned to the IoUringBufferRing  and the buffer will be marked as available for reuse.  Result:  Fixes #14614  ---------  Co-authored-by: Norman Maurer <norman_maurer@apple.com>
netty,netty,cf340037c90a04011c0e9bf45b17f9ff17f49437,https://github.com/netty/netty/commit/cf340037c90a04011c0e9bf45b17f9ff17f49437,Revert "IoUring: Only execute write related completion inline" (#14711)  Motivation:  This change drops perf by 10-15%. Let's revert it for now.  Modifications:  Reverts netty/netty#14704  Result:  Fix performance drop.
netty,netty,f17fb0547575fb0adba3bd2f0ccadf025c1d7d4b,https://github.com/netty/netty/commit/f17fb0547575fb0adba3bd2f0ccadf025c1d7d4b,IoUring: Create ring with IORING_SETUP_SINGLE_ISSUER and IORING_SETUP_DEFER_TASKRUN to reduce overhead (#14699)  Motivation:  We should create our ring with IORING_SETUP_SINGLE_ISSUER and IORING_SETUP_DEFER_TASKRUN to ensure we can make progress as fast as possible. This improves performance quite a bit:  See https://manpages.debian.org/unstable/liburing-dev/io_uring_setup.2.en.html#IORING_SETUP_DEFER_TASKRUN  Modifications:  - Create ring with IORING_SETUP_SINGLE_ISSUER and IORING_SETUP_DEFER_TASKRUN (and also IORING_SETUP_R_DISABLED to be able to select the correct thread)  Result:  Much faster handling of a lot of concurrent connections (while still maintain the same performance with small number).  Before: ``` Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1000 connections. Total data sent:     47317.6 MiB (49616085120 bytes) Total data received: 47430.3 MiB (49734300462 bytes) Bandwidth per channel: 26.492⇅ Mbps (3311.5 kBps) Aggregate bandwidth: 13261.733↓  13230.211↑ Mbps Packet rate estimate: 1137136.1↓  1139280.4↑ (11↓  45↑ TCP MSS/op) Test duration: 30.0017 s. ```  After: ``` Destination: [127.0.0.1]:8088 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1000 connections. Total data sent:     80425.0 MiB (84331771459 bytes) Total data received: 80513.6 MiB (84424659415 bytes) Bandwidth per channel: 44.998⇅ Mbps (5624.7 kBps) Aggregate bandwidth: 22511.278↓  22486.510↑ Mbps Packet rate estimate: 2077374.7↓  1936466.0↑ (11↓  45↑ TCP MSS/op) ```
netty,netty,595a07d432281bbd27e57fd1154477efcd841143,https://github.com/netty/netty/commit/595a07d432281bbd27e57fd1154477efcd841143,IoUring: Submit after completion queue was processed (#14696)  Motivation:  We should try to submit one more time after we did run all completions. Otherwise we might introduce latency that will hurt performance.  Modifications:  Add one more submit call.  Result:  Better troughput.  Before the change: ``` Destination: [127.0.0.1]:8088 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1 connections. Total data sent:     134066.2 MiB (140578586624 bytes) Total data received: 134066.1 MiB (140578537472 bytes) Bandwidth per channel: 74755.196⇅ Mbps (9344399.5 kBps) Aggregate bandwidth: 37377.591↓  37377.605↑ Mbps Packet rate estimate: 3424327.9↓  3208145.5↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0883 s. ```  After this change: ``` Destination: [127.0.0.1]:8088 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1 connections. Total data sent:     250820.2 MiB (263004028928 bytes) Total data received: 250812.7 MiB (262996131840 bytes) Bandwidth per channel: 139956.971⇅ Mbps (17494621.4 kBps) Aggregate bandwidth: 69977.435↓  69979.536↑ Mbps Packet rate estimate: 6406626.3↓  6006391.8↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0664 s. ```
netty,netty,4d868a6da3aeb0b222ac24c8bf5f63a416f74ecf,https://github.com/netty/netty/commit/4d868a6da3aeb0b222ac24c8bf5f63a416f74ecf,IoUring: Force submit and instantly running completions when Channel becomes unwritable (#14693)  Motivation: When the Channel becomes unwritable we need to ensure we can write and release the data as soon as possible. Failing to do so increase the memory usage and also hurts the performance in general and so will reduce throughtput  Modifications:  - Add a new class which is used to drain the CompletionQueue before finally run the completions - When we do a flush() and detect that the Channel became unwritable trigger a submit (io_uring_enter) and directly obtain the completions and run them. This will ensure we can release buffers directly when the socket is writable as the write will be executed inline by io_uring_enter.  Result:  Before the change:  ``` Destination: [127.0.0.1]:8088 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1 connections. Total data sent:     100726.8 MiB (105619652608 bytes) Total data received: 100717.7 MiB (105610199040 bytes) Bandwidth per channel: 56149.221⇅ Mbps (7018652.6 kBps) Aggregate bandwidth: 28073.354↓  28075.867↑ Mbps Packet rate estimate: 2575952.0↓  2409771.0↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0955 s. ```  After this change:  ``` Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1 connections. Total data sent:     134226.4 MiB (140746555392 bytes) Total data received: 134226.5 MiB (140746670080 bytes) Bandwidth per channel: 74841.324⇅ Mbps (9355165.5 kBps) Aggregate bandwidth: 37420.677↓  37420.647↑ Mbps Packet rate estimate: 3427561.0↓  3211839.9↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0896 s. ```  ---------  Co-authored-by: Chris Vest <christianvest_hansen@apple.com>
netty,netty,d25d6667b992cbef2de22ca418e78eaa5d00b75d,https://github.com/netty/netty/commit/d25d6667b992cbef2de22ca418e78eaa5d00b75d,IoUring: Keep on reading data until nothing is left to read on a socket (#14668)  Motivation:  We need to keep on submitting reads until there is nothing left to read from the socket. Otherwise we will suffer from performance problems as we will do more fireChannelReadComplete() calls then required and so hurt batching.  Modifications:  Use our own supplier that ensures we read all data.  Result:  Fix performance problems
netty,netty,5637bbe6d346bb8ee576637f0fed11d1e8524a55,https://github.com/netty/netty/commit/5637bbe6d346bb8ee576637f0fed11d1e8524a55,Make JMH executor threads look like event loop threads (#14444) (#14445)  Motivation: Our buffer allocators make specific optimisations for event loop threads. Thus it makes sense that our benchmark executors look as much like event loops as possible  to trigger those optimisations.  Modification: Make sure that JMH executor threads add themselves to the `ThreadExecutorMap` so that they look like event loop threads.  Result: The various `ByteBufAllocator*Benchmarks` show improved performance because they now go through event-loop optimised code paths.  When needed  it's still possible to disable this behavior by disabling the custom harness executor. That functionality has been part of our custom JMH harness for a long time.  Co-authored-by: Chris Vest <christianvest_hansen@apple.com>
netty,netty,48687be377c1e0b94acddca4ae54eff996c1ee2a,https://github.com/netty/netty/commit/48687be377c1e0b94acddca4ae54eff996c1ee2a,Replace ArrayDeque::pollFirst with ArrayDeque::pollLast for Recycler.… (#14331)  …LocalPool batch. (#14268) (#14292)  Motivation:  Improve performance by replacing ArrayDeque::pollFirst with ArrayDeque::pollLast.  Modifications:  Replaced ArrayDeque::pollFirst with ArrayDeque::pollLast for LocalPool batch.  Result: - The new version shows the best overall performance in benchmarks such as plainNew  recyclerGetAndRecycle  and unguardedProducerConsumer. - Overall  the new version shows a significant reduction in GC time  improving memory efficiency across most tests.  Fixes #14268  Co-authored-by: Obolrom <65775868+Obolrom@users.noreply.github.com>
netty,netty,50e7b2d0e3c25adc9479b85acd0f7ed1571bc045,https://github.com/netty/netty/commit/50e7b2d0e3c25adc9479b85acd0f7ed1571bc045,Switch to AdaptiveByteBufAllocator by default (#14271)  Motivation:  The AdaptiveByteBufAllocator reduce the memory overhead quite a lot while still provide kind of the same performance characteristics. Let's use it as the default allocator  Modifications:  Switch default allocator from PooledByteBufAllocator to AdaptiveByteBufAllocatore. Users can use -Dio.netty.allocator.type=pooled to switch the old default if wanted  Result:  Less memory overhead by default
netty,netty,89c43e464191a8edafacb0ba92b480b884e7d5b8,https://github.com/netty/netty/commit/89c43e464191a8edafacb0ba92b480b884e7d5b8,Add keytool-based self-signed certificate generator (#14198)  Motivation:  On newer Java versions  the JDK-based self-signed certificate generator does not work  so adding bcpkix as a dependency was necessary. This is inconvenient for users that want to use self-signed certs as part of their tests.  Modification:  Add a KeytoolSelfSignedCertGenerator which uses the `keytool` command included in the JDK to generate the key pair and certificate.  Introduce a SelfSignedCertificate.Builder to make the constructor chaos a bit cleaner. Additionally  since the `keytool` generator uses an external RNG  this Builder allows for the in-vm KeyPair generation to happen lazily  improving performance for the keytool path.  The next step I've thought about doing is to move the unnecessary file system saving (SelfSignedCertificate.newSelfSignedCertificate) to be lazy  so that we can avoid writing the key and cert at all in some cases. But that is for another day  it'd require some exception handling changes.  Result:  Users can generate self-signed certificates without additional dependencies.  ---------  Co-authored-by: Norman Maurer <norman_maurer@apple.com>
netty,netty,86b970dfd3e869f3279c100c17b35380e6dfd6a3,https://github.com/netty/netty/commit/86b970dfd3e869f3279c100c17b35380e6dfd6a3, Avoid unnecessary reflective probes on netty initialization (#14107)  Motivation:  https://github.com/netty/netty/pull/14090 added multiple reflective accesses on netty initialization to probe availability of Unsafe methods which may be removed after java 23 (jep 471).  This may have negative performance impact on application startup (particularly with lower end machines) since reflection is expensive before jvm is warmed up. However  these reflective method probes may be potentially useful only after java 23 - when/if method removals actually materialize.  Modification:  Make reflective accesses on netty initialization to probe availability of Unsafe methods only if java version > 23.  Result:  Unnecessary reflective accesses on netty initialization are avoided for java version <= 23.
netty,netty,4565f942690a782f162d6ff4bb5b3167b1b1338d,https://github.com/netty/netty/commit/4565f942690a782f162d6ff4bb5b3167b1b1338d,Optimize wrap buffer cumulation in SslHandler and don't mutate input buffers  (#14086)  Motivation:  SslHandler doesn't support sharing the input buffers since the input buffers get mutated. This issue has been reported as #14069. Fixes have already been made to support the use case using read-only buffers as input. However  this is not useful for existing Netty applications.  I checked the Netty code once again and noticed that it's possible to optimize the current implementation in SslHandler and avoid the buffer mutation.  The default behavior in SslHandler causes very hard to debug problems. In Pulsar  there have been issues open for multiple years that have been caused by the SslHandler's default behavior of mutating the input buffers (apache/pulsar#22601 is one starting point to the issues).  The current implementation of SslHandler will mutate the input buffers by using the input buffer as a cumulation buffer when there's available capacity. This is not optimal at all. This PR contains an improvement where the cumulation buffer is allocated to the final size immediately so that cumulation is more optimal. That's why I think that this will be more optimal in the end and won't cause performance regressions.  After these changes  it should be possible to pass a slice or duplicate of a buffer and the input buffer doesn't get mutated. The input buffer's readIndex will be modified so multiple threads cannot share the same input buffer without a slice or duplicate.  I created a failing test case that shows that a duplicate buffer doesn't prevent input buffer mutation.  ![image](https://github.com/netty/netty/assets/66864/0abec333-6079-40bd-8bc1-ccfd3df6fae1) It's a very surprising behavior of SslHandler that passing a `.retainedDuplicate()` buffer could result in mutations to the original buffer. That's another reason why I think that SslHandler shouldn't ever mutate the inputs.  Modification:  - add `protected ByteBuf composeFirst(ByteBufAllocator allocator  ByteBuf first  int bufferSize)` to AbstractCoalescingBufferQueue to support allocating an optimal cumulation buffer instead of using the input buffer as a cumulation buffer - remove dead code from `compose` where there was handling for CompositeByteBuf - `composeFirst` will be called before `compose` so this case isn't needed.  Result:  Fixes #14069 without the need to set the input buffer as read-only  ---------  Co-authored-by: Norman Maurer <norman_maurer@apple.com>
SeleniumHQ,selenium,6704db0535b1f11ba15b242cb2a6a96f704b4b8f,https://github.com/SeleniumHQ/selenium/commit/6704db0535b1f11ba15b242cb2a6a96f704b4b8f,[java] minor performance improvements and code cleanup (#14054)  * replaced empty string comparison with isEmpty() invoking  * replaced manual array copy with System.arraycopy()  * replaced redundant String.format invoking with printf()  * replaced deprecated setScriptTimeout with scriptTimeout according to instruction "Use scriptTimeout(Duration)"  * replaced iterators with bulk methods invoking  * replaced list creations with List.of()  * there is no need to create mutable lists in tests to only get elements  ---------  Co-authored-by: Puja Jagani <puja.jagani93@gmail.com>
YunaiV,ruoyi-vue-pro,20b4329af65559cd30aef6f8dadc85e3c74694ee,https://github.com/YunaiV/ruoyi-vue-pro/commit/20b4329af65559cd30aef6f8dadc85e3c74694ee,!1350 perf:【INFRA 基础设施】优化一些 todo 提到的问题 Merge pull request !1350 from puhui999/vben5-antd-schema
YunaiV,ruoyi-vue-pro,6982243370fcd5f3234b61a0b6d4dab109b991f3,https://github.com/YunaiV/ruoyi-vue-pro/commit/6982243370fcd5f3234b61a0b6d4dab109b991f3,perf:【INFRA 基础设施】优化一些 todo 提到的问题
YunaiV,ruoyi-vue-pro,b9ffa1820dcaa20b4e8741b4dc5d02d8f03473e0,https://github.com/YunaiV/ruoyi-vue-pro/commit/b9ffa1820dcaa20b4e8741b4dc5d02d8f03473e0,perf:【INFRA 基础设施】优化一些 todo 提到的问题
YunaiV,ruoyi-vue-pro,cd341da674be599301a8fffa3cadc664505c58ad,https://github.com/YunaiV/ruoyi-vue-pro/commit/cd341da674be599301a8fffa3cadc664505c58ad,perf:【INFRA 基础设施】代码生成主子表非 erp 模式时，当子表一对多时更新改为通过 diffList 实现对应的增删改
YunaiV,ruoyi-vue-pro,bc77af09e03039d633aef275a5e729ef1abb5693,https://github.com/YunaiV/ruoyi-vue-pro/commit/bc77af09e03039d633aef275a5e729ef1abb5693,perf:【INFRA 基础设施】代码生成示例 demo 代码格式化
YunaiV,ruoyi-vue-pro,61cfcc283b426dd69e3f479a5600be3c6e1f382b,https://github.com/YunaiV/ruoyi-vue-pro/commit/61cfcc283b426dd69e3f479a5600be3c6e1f382b,perf:【INFRA 基础设施】代码生成配置 delete-batch-enable: false # 是否生成批量删除接口
YunaiV,ruoyi-vue-pro,bb236af6310b852337e501d026d08048fad945bf,https://github.com/YunaiV/ruoyi-vue-pro/commit/bb236af6310b852337e501d026d08048fad945bf,perf: 优化 FileTypeUtils 的 TIKA 创建，提升性能
YunaiV,ruoyi-vue-pro,05bf229a3c91b9e74c11b380703ba5dd42e89fa2,https://github.com/YunaiV/ruoyi-vue-pro/commit/05bf229a3c91b9e74c11b380703ba5dd42e89fa2,perf:【INFRA 基础设施】vben5-antd-schema 主主子表inner代码生成时，可生成批量删除
YunaiV,ruoyi-vue-pro,e10425e049daac247b77b4fc4b193d14298c9e11,https://github.com/YunaiV/ruoyi-vue-pro/commit/e10425e049daac247b77b4fc4b193d14298c9e11,perf:【INFRA 基础设施】vben5-antd-schema 主主子表normal代码生成时，可生成批量删除
YunaiV,ruoyi-vue-pro,ef1e7b312bde4e653da3a7e4b40ef7ff5af04bba,https://github.com/YunaiV/ruoyi-vue-pro/commit/ef1e7b312bde4e653da3a7e4b40ef7ff5af04bba,perf:【INFRA 基础设施】vben5-antd-schema 主主子表erp代码生成时，可生成批量删除
YunaiV,ruoyi-vue-pro,e00bfd02a1008a2a9a832c37b94dec7ac3d0cf2c,https://github.com/YunaiV/ruoyi-vue-pro/commit/e00bfd02a1008a2a9a832c37b94dec7ac3d0cf2c,Merge branch 'master-jdk17' of https://gitee.com/zhijiantianya/ruoyi-vue-pro  # Conflicts: #	pom.xml #	yudao-module-bpm/src/main/java/cn/iocoder/yudao/module/bpm/service/task/BpmProcessInstanceServiceImpl.java #	yudao-module-crm/src/main/java/cn/iocoder/yudao/module/crm/controller/admin/operatelog/CrmOperateLogController.java #	yudao-module-crm/src/main/java/cn/iocoder/yudao/module/crm/controller/admin/statistics/CrmStatisticsPerformanceController.java #	yudao-module-crm/src/main/java/cn/iocoder/yudao/module/crm/controller/admin/statistics/vo/performance/CrmStatisticsPerformanceReqVO.java #	yudao-module-crm/src/main/java/cn/iocoder/yudao/module/crm/service/statistics/CrmStatisticsPerformanceServiceImpl.java #	yudao-module-mall/yudao-module-promotion/pom.xml #	yudao-module-mall/yudao-module-trade/pom.xml #	yudao-module-system/src/main/java/cn/iocoder/yudao/module/system/api/social/SocialUserApi.java #	yudao-module-system/src/main/java/cn/iocoder/yudao/module/system/controller/admin/auth/vo/AuthRegisterReqVO.java #	yudao-module-system/src/main/java/cn/iocoder/yudao/module/system/service/user/AdminUserServiceImpl.java
YunaiV,ruoyi-vue-pro,ce1940e4f36fe9a0bfc9dcc7911c4f695d4ae7af,https://github.com/YunaiV/ruoyi-vue-pro/commit/ce1940e4f36fe9a0bfc9dcc7911c4f695d4ae7af,Merge branch 'master-jdk17' of https://gitee.com/zhijiantianya/ruoyi-vue-pro  # Conflicts: #	yudao-module-crm/yudao-module-crm-biz/src/main/java/cn/iocoder/yudao/module/crm/service/statistics/CrmStatisticsPerformanceServiceImpl.java #	yudao-module-mall/yudao-module-trade-biz/src/main/java/cn/iocoder/yudao/module/trade/controller/app/brokerage/AppBrokerageUserController.java #	yudao-module-mall/yudao-module-trade-biz/src/main/java/cn/iocoder/yudao/module/trade/service/brokerage/BrokerageUserService.java #	yudao-module-mall/yudao-module-trade-biz/src/main/java/cn/iocoder/yudao/module/trade/service/brokerage/BrokerageUserServiceImpl.java #	yudao-module-member/yudao-module-member-biz/src/main/java/cn/iocoder/yudao/module/member/controller/app/social/AppSocialUserController.java #	yudao-module-system/yudao-module-system-api/src/main/java/cn/iocoder/yudao/module/system/api/social/SocialUserApi.java #	yudao-module-system/yudao-module-system-biz/src/main/java/cn/iocoder/yudao/module/system/service/social/SocialClientService.java
apache,kafka,adb76779ed14ef21f61f8dafe9041054e9a32130,https://github.com/apache/kafka/commit/adb76779ed14ef21f61f8dafe9041054e9a32130,KAFKA-19312 Avoiding concurrent execution of onComplete and tryComplete (#19759)  The `onComplete` method in DelayedOperation is guaranteed to run only once  through `forceComplete`  invoked either by `tryComplete` or when operation is expired (`run` method). The invocation of  `tryComplete` is done by attaining `lock` so no concurrent execution of  `tryComplete` happens for same delayed operation. However  there can be  concurrent execution of `tryComplete` and `onComplete` as the  `expiration` thread can trigger a separte run of `onComplete` while  `tryComplete` is still executing. This behaviour is not ideal as there  are parallel runs where 1 threads method execution is wasteful i.e. if  `onComplete` is already invoked by another thread then execution of  `tryComplete` is not required.  I ran some tests and performance is same.  ### After the chages:  ``` --num 10000 --rate 100 --timeout 1000 --pct50 0.5 --pct75 0.75  # latency samples: pct75 = 0  pct50 = 0  min = 0  max = 7 # interval samples: rate = 100.068948  min = 0  max = 129 # enqueue rate (10000 requests): # <elapsed time ms>	<target rate>	<actual rate>	<process cpu time ms>	<G1 Old Generation count> <G1 Young Generation count>	<G1 Old Generation time ms> <G1 Young Generation time ms> 101196	99.809364	99.806376	3240	0 2	0 8 ```  ``` --num 10000 --rate 1000 --timeout 1000 --pct50 0.5 --pct75 0.75  # latency samples: pct75 = 0  pct50 = 0  min = 0  max = 9 # interval samples: rate = 999.371395  min = 0  max = 14 # enqueue rate (10000 requests): # <elapsed time ms>	<target rate>	<actual rate>	<process cpu time ms>	<G1 Old Generation count> <G1 Young Generation count>	<G1 Old Generation time ms> <G1 Young Generation time ms> 11104	989.902990	989.805008	1349	0 2	0 7 ```  ### Before changes:  ``` --num 10000 --rate 100 --timeout 1000 --pct50 0.5 --pct75 0.75  # latency samples: pct75 = 0  pct50 = 0  min = 0  max = 9 # interval samples: rate = 100.020304  min = 0  max = 130 # enqueue rate (10000 requests): # <elapsed time ms>	<target rate>	<actual rate>	<process cpu time ms>	<G1 Old Generation count> <G1 Young Generation count>	<G1 Old Generation time ms> <G1 Young Generation time ms> 102366	98.657274	98.652408	3444	0 2	0 8  --num 10000 --rate 1000 --timeout 1000 --pct50 0.5 --pct75 0.75  # latency samples: pct75 = 0  pct50 = 0  min = 0  max = 8 # interval samples: rate = 997.134236  min = 0  max = 14 # enqueue rate (10000 requests): # <elapsed time ms>	<target rate>	<actual rate>	<process cpu time ms>	<G1 Old Generation count> <G1 Young Generation count>	<G1 Old Generation time ms> <G1 Young Generation time ms> 11218	978.665101	978.665101	1624	0 2	0 7  Reviewers: Jun Rao <junrao@gmail.com>  Andrew Schofield <aschofield@confluent.io>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,eb3714f022c948e9348b7e281c4bd69a38666013,https://github.com/apache/kafka/commit/eb3714f022c948e9348b7e281c4bd69a38666013,KAFKA-19160;KAFKA-19164; Improve performance of fetching stable offsets (#19497)  When fetching stable offsets in the group coordinator  we iterate over all requested partitions. For each partition  we iterate over the group's ongoing transactions to check if there is a pending transactional offset commit for that partition.  This can get slow when there are a large number of partitions and a large number of pending transactions. Instead  maintain a list of pending transactions per partition to speed up lookups.  Reviewers: Shaan  Dongnuo Lyu <dlyu@confluent.io>  Chia-Ping Tsai <chia7712@gmail.com>  David Jaco <djacot@confluent.io>
apache,kafka,b5c468fd7ced52184daa286f00ea7ce8cf760bc0,https://github.com/apache/kafka/commit/b5c468fd7ced52184daa286f00ea7ce8cf760bc0,KAFKA-18115; Fix for loading big files while performing load tests (#18391)  When performing perf tests  we can specify a payload using the "--payloadFile" flag. This file is utilized during the load/performance testing process. This causes the entire file to get loaded into a String and split using the delimiter. However  if the file is large  it may result in  NegativeArraySizeException error.  Moving the file loading logic to Scanner which doesn't have this issue.  Reviewers: José Armando García Sancio <jsancio@apache.org>  Ken Huang <s7133700@gmail.com>  Zhe Guang <zheguang.zhao@alumni.brown.edu>
apache,kafka,ac9520b92201839796ce46cb5852fff08b7d5878,https://github.com/apache/kafka/commit/ac9520b92201839796ce46cb5852fff08b7d5878,KAFKA-19227: Piggybacked share fetch acknowledgements performance issue (#19612)  The PR fixes the issue when ShareAcknowledgements are piggybacked on ShareFetch. The current default configuration in clients sets `batch size` and `max fetch records` as per the `max.poll.records` config  default 500. Which means all records in a single poll will be fetched and acknowledged. Also the default configuration for inflight records in a partition is 200. Which means prior fetch records has to be acknowledged prior fetching another batch from share partition.  The piggybacked share fetch-acknowledgement calls from KafkaApis are async and later the response is combined. If respective share fetch starts waiting in purgatory because all inflight records are currently full  hence when startOffset is moved as part of acknowledgement  then a trigger should happen which should try completing any pending share fetch requests in purgatory. Else the share fetch requests wait in purgatory for timeout though records are available  which dips the share fetch performance.  The regular fetch has a single criteria to land requests in purgatory  which is min bytes criteria  hence any produce in respective topic partition triggers to check any pending fetch requests. But share fetch can wait in purgatory because of multiple reasons: 1) Min bytes 2) Inflight records exhaustion 3) Share partition fetch lock competition. The trigger already happens for 1 and current PR fixes 2. We will investigate further if there should be any handling required for 3.  Reviewers: Abhinav Dixit <adixit@confluent.io>  Andrew Schofield <aschofield@confluent.io>
apache,kafka,434b0d39ae675eb5ac16de81da4a102b32a496a5,https://github.com/apache/kafka/commit/434b0d39ae675eb5ac16de81da4a102b32a496a5,MINOR: use enum map for error counts map (#19314)  Java provides a specialised Map where Enums are the keys  which can provide some performance improvements.  https://docs.oracle.com/javase/8/docs/api/java/util/EnumMap.html  I have updated the Java code where possible to use an EnumMap rather than a HashMap and run the unit tests under the requests directory.  Reviewers: Matthias J. Sax <matthias@confluent.io>  Lianet Magrans <lmagrans@confluent.io>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,31e1a57c41cf9cb600751669dc71bcd9596b45f9,https://github.com/apache/kafka/commit/31e1a57c41cf9cb600751669dc71bcd9596b45f9,KAFKA-18989 Optimize FileRecord#searchForOffsetWithSize (#19214)  The `lastOffset` includes the entire batch header  so we should check `baseOffset` instead.  To optimize this  we need to update the search logic. The previous approach simply checked whether each batch's `lastOffset()` was greater than or equal to the target offset. Once it found the first batch that met this condition  it returned that batch immediately.  Now that we are using `baseOffset()`  we need to handle a special case: if the `targetOffset` falls between the `lastOffset` of the previous batch and the `baseOffset` of the matching batch  we should select the matching batch. The updated logic is structured as follows:  1. First  if baseOffset exactly equals targetOffset  return immediately. 2. If we find the first batch with baseOffset greater than targetOffset - Check if the previous batch contains the target - If there's no previous batch  return the current batch or the previous batch doesn't contain the target  return the current batch 5. After iterating through all batches  check if the last batch contains the target offset.  This code path is not thread-safe  so we need to prevent `EOFException`. To avoid this exception  I am still using an early return. In this scenario  `lastOffset` is still used within the loop  but it should be executed at most once within the loop.  Therefore  in the new implementation  `lastOffset` will be executed at most once. In most cases  this results in an optimization.  Test: Verifying Memory Usage Improvement To evaluate whether this optimization helps  I followed the steps below to monitor memory usage:  1. Start a Standalone Kafka Server ```sh KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)" bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties bin/kafka-server-start.sh config/server.properties ```  2. Use Performance Console Tools to Produce and Consume Records  **Produce Records:** ```sh ./kafka-producer-perf-test.sh \ --topic test-topic \ --num-records 1000000000 \ --record-size 100 \ --throughput -1 \ --producer-props bootstrap.servers=localhost:9092 ``` **Consume Records:** ```sh ./bin/kafka-consumer-perf-test.sh \ --topic test-topic \ --messages 1000000000 \ --bootstrap-server localhost:9092 ``` It can be observed that memory usage has significantly decreased. trunk: ![CleanShot 2025-03-16 at 11 53 31@2x](https://github.com/user-attachments/assets/eec26b1d-38ed-41c8-8c49-e5c68643761b) this PR: ![CleanShot 2025-03-16 at 17 41 56@2x](https://github.com/user-attachments/assets/c8d4c234-18c2-4642-88ae-9f96cf54fccc)  Reviewers: Kirk True <kirk@kirktrue.pro>  TengYao Chi <kitingiao@gmail.com>  David Arthur <mumrah@gmail.com>  Jun Rao <junrao@gmail.com>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,da46cf6e79afbbed1da2bae831e0f70992e85f9b,https://github.com/apache/kafka/commit/da46cf6e79afbbed1da2bae831e0f70992e85f9b,KAFKA-17565 Move MetadataCache interface to metadata module (#18801)  ### Changes  * Move MetadataCache interface to metadata module and change Scala function to Java. * Remove functions `getTopicPartitions`  `getAliveBrokers`  `topicNamesToIds`  `topicIdInfo`  and `getClusterMetadata` from MetadataCache interface  because these functions are only used in test code.  ### Performance  * ReplicaFetcherThreadBenchmark ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.fetcher.ReplicaFetcherThreadBenchmark ``` * trunk ``` Benchmark (partitionCount) Mode Cnt Score Error Units ReplicaFetcherThreadBenchmark.testFetcher 100 avgt 2 4775.490 ns/op ReplicaFetcherThreadBenchmark.testFetcher 500 avgt 2 25730.790 ns/op ReplicaFetcherThreadBenchmark.testFetcher 1000 avgt 2 55334.206 ns/op ReplicaFetcherThreadBenchmark.testFetcher 5000 avgt 2 488427.547 ns/op ``` * branch ``` Benchmark (partitionCount) Mode Cnt Score Error Units ReplicaFetcherThreadBenchmark.testFetcher 100 avgt 2 4825.219 ns/op ReplicaFetcherThreadBenchmark.testFetcher 500 avgt 2 25985.662 ns/op ReplicaFetcherThreadBenchmark.testFetcher 1000 avgt 2 56056.005 ns/op ReplicaFetcherThreadBenchmark.testFetcher 5000 avgt 2 497138.573 ns/op ```  * KRaftMetadataRequestBenchmark ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.metadata.KRaftMetadataRequestBenchmark ``` * trunk ``` Benchmark (partitionCount) (topicCount) Mode Cnt Score Error Units KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 500 avgt 2 884933.558 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 1000 avgt 2 1910054.621 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 5000 avgt 2 21778869.337 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 500 avgt 2 1537550.670 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 1000 avgt 2 3168237.805 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 5000 avgt 2 29699652.466 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 500 avgt 2 3501483.852 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 1000 avgt 2 7405481.182 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 5000 avgt 2 55839670.124 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 500 avgt 2 333.667 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 1000 avgt 2 339.685 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 5000 avgt 2 334.293 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 500 avgt 2 329.899 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 1000 avgt 2 347.537 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 5000 avgt 2 332.781 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 500 avgt 2 327.085 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 1000 avgt 2 325.206 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 5000 avgt 2 316.758 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 500 avgt 2 7.569 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 1000 avgt 2 7.565 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 5000 avgt 2 7.574 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 500 avgt 2 7.568 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 1000 avgt 2 7.557 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 5000 avgt 2 7.585 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 500 avgt 2 7.560 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 1000 avgt 2 7.554 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 5000 avgt 2 7.574 ns/op ``` * branch ``` Benchmark (partitionCount) (topicCount) Mode Cnt Score Error Units KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 500 avgt 2 910337.770 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 1000 avgt 2 1902351.360 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 5000 avgt 2 22215893.338 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 500 avgt 2 1572683.875 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 1000 avgt 2 3188560.081 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 5000 avgt 2 29984751.632 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 500 avgt 2 3413567.549 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 1000 avgt 2 7303174.254 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 5000 avgt 2 54293721.640 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 500 avgt 2 318.335 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 1000 avgt 2 331.386 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 5000 avgt 2 332.944 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 500 avgt 2 340.322 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 1000 avgt 2 330.294 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 5000 avgt 2 342.154 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 500 avgt 2 341.053 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 1000 avgt 2 335.458 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 5000 avgt 2 322.050 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 500 avgt 2 7.538 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 1000 avgt 2 7.548 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 5000 avgt 2 7.545 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 500 avgt 2 7.597 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 1000 avgt 2 7.567 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 5000 avgt 2 7.558 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 500 avgt 2 7.559 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 1000 avgt 2 7.615 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 5000 avgt 2 7.562 ns/op ```  * PartitionMakeFollowerBenchmark ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.partition.PartitionMakeFollowerBenchmark ``` * trunk ``` Benchmark Mode Cnt Score Error Units PartitionMakeFollowerBenchmark.testMakeFollower avgt 2 158.816 ns/op ``` * branch ``` Benchmark Mode Cnt Score Error Units PartitionMakeFollowerBenchmark.testMakeFollower avgt 2 160.533 ns/op ```  * UpdateFollowerFetchStateBenchmark ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.partition.UpdateFollowerFetchStateBenchmark ``` * trunk ``` Benchmark Mode Cnt Score Error Units UpdateFollowerFetchStateBenchmark.updateFollowerFetchStateBench avgt 2 4975.261 ns/op UpdateFollowerFetchStateBenchmark.updateFollowerFetchStateBenchNoChange avgt 2 4880.880 ns/op ``` * branch ``` Benchmark Mode Cnt Score Error Units UpdateFollowerFetchStateBenchmark.updateFollowerFetchStateBench avgt 2 5020.722 ns/op UpdateFollowerFetchStateBenchmark.updateFollowerFetchStateBenchNoChange avgt 2 4878.855 ns/op ```   * CheckpointBench ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.server.CheckpointBench ``` * trunk ``` Benchmark (numPartitions) (numTopics) Mode Cnt Score Error Units CheckpointBench.measureCheckpointHighWatermarks 3 100 thrpt 2 0.997 ops/ms CheckpointBench.measureCheckpointHighWatermarks 3 1000 thrpt 2 0.703 ops/ms CheckpointBench.measureCheckpointHighWatermarks 3 2000 thrpt 2 0.486 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 100 thrpt 2 1.038 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 1000 thrpt 2 0.734 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 2000 thrpt 2 0.637 ops/ms ``` * branch ``` Benchmark (numPartitions) (numTopics) Mode Cnt Score Error Units CheckpointBench.measureCheckpointHighWatermarks 3 100 thrpt 2 0.990 ops/ms CheckpointBench.measureCheckpointHighWatermarks 3 1000 thrpt 2 0.659 ops/ms CheckpointBench.measureCheckpointHighWatermarks 3 2000 thrpt 2 0.508 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 100 thrpt 2 0.923 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 1000 thrpt 2 0.736 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 2000 thrpt 2 0.637 ops/ms ```  * PartitionCreationBench ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.server.PartitionCreationBench ``` * trunk ``` Benchmark (numPartitions) (useTopicIds) Mode Cnt Score Error Units PartitionCreationBench.makeFollower 20 false avgt 2 5.997 ms/op PartitionCreationBench.makeFollower 20 true avgt 2 6.961 ms/op ``` * branch ``` Benchmark (numPartitions) (useTopicIds) Mode Cnt Score Error Units PartitionCreationBench.makeFollower 20 false avgt 2 6.212 ms/op PartitionCreationBench.makeFollower 20 true avgt 2 7.005 ms/op ```  Reviewers: Ismael Juma <ismael@juma.me.uk>  David Arthur <mumrah@gmail.com>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,ab8ef87c7f920894566c25234e9ed2d8f1f9cce2,https://github.com/apache/kafka/commit/ab8ef87c7f920894566c25234e9ed2d8f1f9cce2,KAFKA-18654 [1/2]: Transaction Version 2 performance regression due to early return (#18720)  https://issues.apache.org/jira/browse/KAFKA-18575 solved a critical race condition by returning with CONCURRENT_TRANSACTIONS early when the transaction was still completing. In testing  it was discovered that this early return could cause performance regressions.  Prior to KIP-890 the addpartitions call was a separate call from the producer. There was a previous change https://issues.apache.org/jira/browse/KAFKA-5477 that decreased the retry backoff to 20ms. With KIP-890 and making the call through the produce path  we go back to the default retry backoff which takes longer. Prior to 18575 we introduce a slight delay when sending to the coordinator  so prior to 18575  we are less likely to return quickly and get stuck in this backoff. However  based on results from produce benchmarks  we can still run into the default backoff in some scenarios.  This PR reverts KAFKA-18575  and doesn't return early and wait until the coordinator for checking if a transaction is ongoing. Instead  it will fix the handling with the verification guard so we don't hit the edge condition.  Also cleans up some of the verification text that was unclear.  Reviewers: Jeff Kim <jeff.kim@confluent.io>  Artem Livshits <alivshits@confluent.io>
apache,kafka,b51b31ed9c9cede1ae0ffdf8ca4b471925f72a64,https://github.com/apache/kafka/commit/b51b31ed9c9cede1ae0ffdf8ca4b471925f72a64,KAFKA-18428: Measure share consumers performance (#18415)  Reviewers: Andrew Schofield <aschofield@confluent.io>
apache,kafka,c4840f5e933baaf9c45867940d2f17cc378988f4,https://github.com/apache/kafka/commit/c4840f5e933baaf9c45867940d2f17cc378988f4,KAFKA-16446: Improve controller event duration logging (#15622)  There are times when the controller has a high event processing time  such as during startup  or when creating a topic with many partitions. We can see these processing times in the p99 metric (kafka.controller:type=ControllerEventManager name=EventQueueProcessingTimeMs)  however it's difficult to see exactly which event is causing high processing time.  With DEBUG logs  we see every event along with its processing time. Even with this  it's a bit tedious to find the event with a high processing time.  This PR logs all events which take longer than 2 seconds at ERROR level. This will help identify events that are taking far too long  and which could be disruptive to the operation of the controller. The slow event logging looks like this:  ``` [2024-12-20 15:03:39 754] ERROR [QuorumController id=1] Exceptionally slow controller event createTopics took 5240 ms.  (org.apache.kafka.controller.EventPerformanceMonitor) ```  Also  every 60 seconds  it logs some event time statistics  including average time  maximum time  and the name of the event which took the longest. This periodic message looks like this:  ``` [2024-12-20 15:35:04 798] INFO [QuorumController id=1] In the last 60000 ms period  333 events were completed  which took an average of 12.34 ms each. The slowest event was handleCommit[baseOffset=0]  which took 41.90 ms. (org.apache.kafka.controller.EventPerformanceMonitor) ```  An operator can disable these logs by adding the following to their log4j config:  ``` org.apache.kafka.controller.EventPerformanceMonitor=OFF ```  Reviewers: Colin P. McCabe <cmccabe@apache.org>
apache,kafka,1f26b9607ed58400dffaddb16181bff66a7ffed5,https://github.com/apache/kafka/commit/1f26b9607ed58400dffaddb16181bff66a7ffed5,MINOR: Perf improvement in share state batch combiner. (#18090)  Change from ArrayList to LinkedList following performance analysis  Reviewers: Andrew Schofield <aschofield@confluent.io>
apache,kafka,571f50817c0c3e81a8f767396e485bc23a0731ba,https://github.com/apache/kafka/commit/571f50817c0c3e81a8f767396e485bc23a0731ba,KAFKA-17411: Create local state Standbys on start (#16922)  Instead of waiting until Tasks are assigned to us  we pre-emptively create a StandbyTask for each non-empty Task directory found on-disk.  We do this before starting any StreamThreads  and on our first assignment (after joining the consumer group)  we recycle any of these StandbyTasks that were assigned to us  either as an Active or a Standby.  We can't just use these "initial Standbys" as-is  because they were constructed outside the context of a StreamThread  so we first have to update them with the context (log context  ChangelogReader  and source topics) of the thread that it has been assigned to.  The motivation for this is to (in a later commit) read StateStore offsets for unowned Tasks from the StateStore itself  rather than the .checkpoint file  which we plan to deprecate and remove.  There are a few additional benefits:  Initializing these Tasks on start-up  instead of on-assignment  will reduce the time between a member joining the consumer group and beginning processing. This is especially important when active tasks are being moved over  for example  as part of a rolling restart.  If a Task has corrupt data on-disk  it will be discovered on startup and wiped under EOS. This is preferable to wiping the state after being assigned the Task  because another instance may have non-corrupt data and would not need to restore (as much).  There is a potential performance impact: we open all on-disk Task StateStores  and keep them all open until we have our first assignment. This could require large amounts of memory  in particular when there are a large number of local state stores on-disk.  However  since old local state for Tasks we don't own is automatically cleaned up after a period of time  in practice  we will almost always only be dealing with the state that was last assigned to the local instance.  Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>  Bruno Cadonna <cadonna@apache.org>  Matthias Sax <mjsax@apache.org>
apache,kafka,33147d1089397ea20ba46bd0afb83174b45b506c,https://github.com/apache/kafka/commit/33147d1089397ea20ba46bd0afb83174b45b506c,KAFKA-17863: share consumer max poll records soft limit (#17592)  Make max.poll.records a soft limit so that record batch boundaries can be respected in records returned by ShareConsumer.poll. This gives a significant performance gain because the broker is much more efficient at handling batches which have not been split.  Reviewers: Apoorv Mittal <apoorvmittal10@gmail.com>   Manikumar Reddy <manikumar.reddy@gmail.com>
apache,kafka,017da210999c72789ce3b720af04c1a834b80a5c,https://github.com/apache/kafka/commit/017da210999c72789ce3b720af04c1a834b80a5c,KAFKA-17710; Rework uniform heterogeneous assignor to improve perf (#17385)  Rework the uniform heterogeneous assignor to improve performance  while preserving the high level ideas and structure from the existing implementation: * The assignor works in 3 stages: importing the previous assignment for stickiness  assigning unassigned partitions and iteratively reassigning partitions to improve balance. * Unassigned partitions are assigned to the subscribers with the least number of partitions. This maximizes balance within a single topic. * During the iterative rebalancing phase  partitions are reassigned to their previous owner if it improves balance (stickiness restoration). * During the iterative rebalancing phase  partitions are reassigned to the subscriber with the least number of partitions to improve balance.  A non-exhaustive list of changes is: * The assignment of unassigned partitions and iterative reassignment stages now works through partitions topic by topic. Previously partitions from topics with the same number of partitions per subscriber would be interleaved. Since we iterate topic by topic  we can reuse data about topic subscribers. * Instead of maintaining TreeSets to find the least loaded subscribers  we sort an ArrayList of subscribers once per topic and start filling up subscribers  least loaded first. In testing  this approach was found to be faster than maintaining PriorityQueues. * Implement stickiness restoration by creating a mapping of partitions to previous owner and checking against that mapping  instead of tracking partition movements during iterative reassignment. * Track member partition counts using a plain int array  to avoid overhead from boxing and HashMap lookups. Member partition counts are accessed very frequently and this needs to be fast. As a consequence  we have to number members 0 to M - 1. * Bound the iterative reassignment stage to a fixed number of iterations. Under some uncommon subscription patterns  the iterative reassignment stage converges slowly. In these cases  the iterative reassignment stage terminates without producing an optimally balanced assignment anyway (see javadoc for balanceTopics). * Re-use Maps from the previous assignment where possible  ie. introduce a copy-on-write mechanism while computing the new assignment.  Reviewers: David Jacot <djacot@confluent.io>
apache,kafka,99e1d8fbb30c8c132cd9baec016efb833b6036ec,https://github.com/apache/kafka/commit/99e1d8fbb30c8c132cd9baec016efb833b6036ec,MINOR: Cache topic resolution in TopicIds set (#17285)  Looking up topics in a TopicsImage is relatively slow. Cache the results in TopicIds to improve assignor performance. In benchmarks  we see a noticeable improvement in performance in the heterogeneous case.  Before ``` Benchmark                                       (assignmentType)  (assignorType)  (isRackAware)  (memberCount)  (partitionsToMemberRatio)  (subscriptionType)  (topicCount)  Mode  Cnt    Score   Error  Units ServerSideAssignorBenchmark.doAssignment             INCREMENTAL           RANGE          false          10000                         10         HOMOGENEOUS          1000  avgt    5   36.400 ± 3.004  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL           RANGE          false          10000                         10       HETEROGENEOUS          1000  avgt    5  158.340 ± 0.825  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL         UNIFORM          false          10000                         10         HOMOGENEOUS          1000  avgt    5    1.329 ± 0.041  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL         UNIFORM          false          10000                         10       HETEROGENEOUS          1000  avgt    5  382.901 ± 6.203  ms/op ```  After ``` Benchmark                                       (assignmentType)  (assignorType)  (isRackAware)  (memberCount)  (partitionsToMemberRatio)  (subscriptionType)  (topicCount)  Mode  Cnt    Score   Error  Units ServerSideAssignorBenchmark.doAssignment             INCREMENTAL           RANGE          false          10000                         10         HOMOGENEOUS          1000  avgt    5   36.465 ± 1.954  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL           RANGE          false          10000                         10       HETEROGENEOUS          1000  avgt    5  114.043 ± 1.424  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL         UNIFORM          false          10000                         10         HOMOGENEOUS          1000  avgt    5    1.454 ± 0.019  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL         UNIFORM          false          10000                         10       HETEROGENEOUS          1000  avgt    5  342.840 ± 2.744  ms/op ```  ---  Based heavily on https://github.com/apache/kafka/pull/16527.  Reviewers: David Arthur <mumrah@gmail.com>  David Jacot <djacot@confluent.io>
apache,kafka,f8acfa5257f6ba6c229884b638053d52b1b3e68a,https://github.com/apache/kafka/commit/f8acfa5257f6ba6c229884b638053d52b1b3e68a,KAFKA-17621; Reduce logging verbosity on ConsumerGroupHeartbeat path (#17288)  While running large scale performance tests  we noticed that the logging on the ConsumerGroupHeartbeat path took a significant amount of CPU. It is mainly due to the very large data structures that we print out. I made a pass on those logs and I switched some of them to debug.  Reviewers: Lianet Magrans <lianetmr@gmail.com>
apache,kafka,4a3ab89f95aba294bb536af55548522d946d1ee3,https://github.com/apache/kafka/commit/4a3ab89f95aba294bb536af55548522d946d1ee3,KAFKA-17386 Remove broker-list  threads and num-fetch-threads in ConsumerPerformance (#16983)  Reviewers: Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,f5439864c6a3cf8275c951a5a09a0d500789d869,https://github.com/apache/kafka/commit/f5439864c6a3cf8275c951a5a09a0d500789d869,KAFKA-15406: Add the ForwardingManager metrics from KIP-938 (#16904)  Implement the remaining ForwardingManager metrics from KIP-938: Add more metrics for measuring KRaft performance:  kafka.server:type=ForwardingManager name=QueueTimeMs.p99 kafka.server:type=ForwardingManager name=QueueTimeMs.p999 kafka.server:type=ForwardingManager name=QueueLength kafka.server:type=ForwardingManager name=RemoteTimeMs.p99 kafka.server:type=ForwardingManager name=RemoteTimeMs.p999  Reviewers: Colin P. McCabe <cmccabe@apache.org>
apache,kafka,94f5039350432e25191d708fd8500c6e285f7bd9,https://github.com/apache/kafka/commit/94f5039350432e25191d708fd8500c6e285f7bd9,KAFKA-17378 Fixes for performance testing (#16942)  Reviewers: Apoorv Mittal <apoorvmittal10@gmail.com>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,5a602b2f86935f799e94918c7143820b3d46142f,https://github.com/apache/kafka/commit/5a602b2f86935f799e94918c7143820b3d46142f,KAFKA-17235 system test test_performance_service.py failed (#16789)  related to https://issues.apache.org/jira/browse/KAFKA-17235  The root cause of this issue is a change we introduced in KAFKA-16879  where we modified the PushHttpMetricsReporter constructor to use Time.System [1]. However  Time.System doesn't exist in Kafka versions 0.8.2 and 0.9.  In test_performance_services.py  we have system tests for Kafka versions 0.8.2 and 0.9 [2]. These tests always use the tools JAR from the trunk branch  regardless of the Kafka version being tested [3]  while the client JAR aligns with the Kafka version specified in the test suite [4]. This discrepancy is what causes the issue to arise.  To resolve this issue  we have a few options:  1) Add Time.System to Kafka 0.8.2 and 0.9: This isn't practical  as we no longer maintain these versions. 2) Modify the PushHttpMetricsReporter constructor to use new SystemTime() instead of Time.System: This would contradict the intent of KAFKA-16879  which aims to make SystemTime a singleton. 3) Implement Time in PushHttpMetricsReporter use the time to get current time 4) Remove system tests for Kafka 0.8.2 and 0.9 from test_performance_services.py  Given that we no longer maintain Kafka 0.8.2 and 0.9  and altering the constructor goes against the design goals of KAFKA-16879  option 4 appears to be the most feasible solution. However  I'm not sure whether it's acceptable to remove these old version tests. Maybe someone else has a better solution  "We'll proceed with option 3 since support for versions 0.8 and 0.9 is still required  meaning we can't remove those Kafka versions from the system tests."  Reviewers: Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,3835515feaf7cb5bb7de3c4d63794e79100eb62a,https://github.com/apache/kafka/commit/3835515feaf7cb5bb7de3c4d63794e79100eb62a,KAFKA-16541 Fix potential leader-epoch checkpoint file corruption (#15993)  A patch for KAFKA-15046 got rid of fsync on LeaderEpochFileCache#truncateFromStart/End for performance reason  but it turned out this could cause corrupted leader-epoch checkpoint file on ungraceful OS shutdown  i.e. OS shuts down in the middle when kernel is writing dirty pages back to the device.  To address this problem  this PR makes below changes: (1) Revert LeaderEpochCheckpoint#write to always fsync (2) truncateFromStart/End now call LeaderEpochCheckpoint#write asynchronously on scheduler thread (3) UnifiedLog#maybeCreateLeaderEpochCache now loads epoch entries from checkpoint file only when current cache is absent  Reviewers: Jun Rao <junrao@gmail.com>
apache,kafka,078dd9a311a5e9ddb9d0b49a1164c50dadde3a27,https://github.com/apache/kafka/commit/078dd9a311a5e9ddb9d0b49a1164c50dadde3a27,KAFKA-16821; Member Subscription Spec Interface (#16068)  This patch reworks the `PartitionAssignor` interface to use interfaces instead of POJOs. It mainly introduces the `MemberSubscriptionSpec` interface that represents a member subscription and changes the `GroupSpec` interfaces to expose the subscriptions and the assignments via different methods.  The patch does not change the performance.  before: ``` Benchmark                                     (memberCount)  (partitionsToMemberRatio)  (topicCount)  Mode  Cnt  Score   Error  Units TargetAssignmentBuilderBenchmark.build                10000                         10           100  avgt    5  3.462 ± 0.687  ms/op TargetAssignmentBuilderBenchmark.build                10000                         10          1000  avgt    5  3.626 ± 0.412  ms/op JMH benchmarks done ```  after: ``` Benchmark                                     (memberCount)  (partitionsToMemberRatio)  (topicCount)  Mode  Cnt  Score   Error  Units TargetAssignmentBuilderBenchmark.build                10000                         10           100  avgt    5  3.677 ± 0.683  ms/op TargetAssignmentBuilderBenchmark.build                10000                         10          1000  avgt    5  3.991 ± 0.065  ms/op JMH benchmarks done ```  Reviewers: David Jacot <djacot@confluent.io>
apache,kafka,979f8d9aa3e8840951a151ab83eb006f8e7c1314,https://github.com/apache/kafka/commit/979f8d9aa3e8840951a151ab83eb006f8e7c1314,MINOR: Small refactor in TargetAssignmentBuilder (#16174)  This patch is a small refactoring which mainly aims at avoid to construct a copy of the new target assignment in the TargetAssignmentBuilder because the copy is not used by the caller. The change relies on the exiting tests and it does not really have an impact on performance (e.g. validated with TargetAssignmentBuilderBenchmark).  Reviewers: Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,c8af740bd44dae92bbe68254114c0fd7f7c32345,https://github.com/apache/kafka/commit/c8af740bd44dae92bbe68254114c0fd7f7c32345,Improve producer ID expiration performance (#16075)  Skip using stream when expiring the producer ID. This can improve the performance significantly when the count is high. Before  Benchmark                                        (numProducerIds)  Mode  Cnt      Score       Error  Units ProducerStateManagerBench.testDeleteExpiringIds             10000  avgt    3    101.253 ±    28.031  us/op ProducerStateManagerBench.testDeleteExpiringIds            100000  avgt    3   2297.219 ±  1690.486  us/op ProducerStateManagerBench.testDeleteExpiringIds           1000000  avgt    3  30688.865 ± 16348.768  us/op After  Benchmark                                        (numProducerIds)  Mode  Cnt     Score     Error  Units ProducerStateManagerBench.testDeleteExpiringIds             10000  avgt    3    39.122 ±   1.151  us/op ProducerStateManagerBench.testDeleteExpiringIds            100000  avgt    3   464.363 ±  98.857  us/op ProducerStateManagerBench.testDeleteExpiringIds           1000000  avgt    3  5731.169 ± 674.380  us/op Also  made a change to the JMH testing which excludes the producer ID populating from the testing.  Reviewers: Artem Livshits <alivshits@confluent.io>  Justine Olshan <jolshan@confluent.io>
keycloak,keycloak,24910d9e1c76a1e953936c558ab02a27eb91820e,https://github.com/keycloak/keycloak/commit/24910d9e1c76a1e953936c558ab02a27eb91820e,addresses slow import/export performance by limiting persistence context size (#37926)  * fix: addresses slow import/export performance with more batching  closes: #37991  Signed-off-by: Steve Hawkins <shawkins@redhat.com>  * removing flush/detach manipulation  Signed-off-by: Steve Hawkins <shawkins@redhat.com>  * refining the doc note about using multiple files for larger user counts  Signed-off-by: Steve Hawkins <shawkins@redhat.com>  * adding doc note about useExistingSession method removal  and expanding javadocs  Signed-off-by: Steve Hawkins <shawkins@redhat.com>  ---------  Signed-off-by: Steve Hawkins <shawkins@redhat.com>
keycloak,keycloak,a43b65281d6f3631366b71858cad25676aaf635d,https://github.com/keycloak/keycloak/commit/a43b65281d6f3631366b71858cad25676aaf635d,Search user by id and fallback to username when needed - prevents performance issues when reading policies as users are always stored by id.  Closes #35796  Signed-off-by: Stefan Guilhen <sguilhen@redhat.com>
apache,incubator-seata,b530e30d3da56b89dd01b4ce00a10f4572765b0a,https://github.com/apache/incubator-seata/commit/b530e30d3da56b89dd01b4ce00a10f4572765b0a,optimize: Use shared `EventLoop` for TM and RM clients to reduce thread overhead and improve performance (#7179)
apache,incubator-seata,c1515acbe33193304c814aeadfcf630d19a22710,https://github.com/apache/incubator-seata/commit/c1515acbe33193304c814aeadfcf630d19a22710,optimize: deserialize performance optimize (#6727)
apache,flink,77edff620326d999e793459ab7c86034e943372f,https://github.com/apache/flink/commit/77edff620326d999e793459ab7c86034e943372f,[FLINK-37213][table-runtime] Improve performance of unbounded OVER aggregations  Instead of sorting all of the records based on the row time explicilty use timers to achieve the same thing.  This version vs the previous one register a timer for each record  as opposed to just one timer per key. However since we are using RocksDB for timers  this is a minor problem. In exchange  we: - don't have to iterate over all of the state for each timer - we are firing timers only when needed  vs for each watermark for each key. For example if watermarks are fire every 200ms and for a given key  we have only one record that should be fired 20s into the future  the previous version would be firing a timer for that key for each watermark unnecessarily without doing any work.
apache,flink,94c3b86a368d545a0aa3ff6b5c42f6f8ec3e11de,https://github.com/apache/flink/commit/94c3b86a368d545a0aa3ff6b5c42f6f8ec3e11de,[FLINK-36530][state] Fix S3 performance issue with uncompressed state restore
apache,skywalking,3559e85f36f860382d751dde03ae8fc8ac6e15a1,https://github.com/apache/skywalking/commit/3559e85f36f860382d751dde03ae8fc8ac6e15a1,Improve the performance of OTEL metrics handler. (#12645)  Benchmark for a single node k8s monitoring.  | metrics (avg)            | before | after | | ------------------------ | ------ | ----- | | cpu                      | 19     | 16    | | gc count                 | 16     | 1     | | gc time                  | 38.8   | 5.1   | | otel metrics latency P50 | 125    | 8     | | otel metrics latency P90 | 333.3  | 22.5  | | otel metrics latency P99 | 666.6  | 166.6 |
apache,skywalking,9644f3fda9a7313e5aef9a3b581006710b8369ac,https://github.com/apache/skywalking/commit/9644f3fda9a7313e5aef9a3b581006710b8369ac,BanyanDB: stream sort-by `time` query  use internal time-series rather than `index` to improve the query performance. (#12486)
apache,skywalking,39ee6026f4c1608b56b6686cfcf6da7d798c2588,https://github.com/apache/skywalking/commit/39ee6026f4c1608b56b6686cfcf6da7d798c2588,BanyanDB: Zipkin Module set service as Entity for improving the query performance (#12474)
bazelbuild,bazel,32a816ee37f332ecc39553de528c302d4cb8b2e8,https://github.com/bazelbuild/bazel/commit/32a816ee37f332ecc39553de528c302d4cb8b2e8,Attempt invalidation lookup on all existing deserialized nodes before analysis.  This is a key step towards incremental builds with a remote analysis cache service.  This introduces a new method in `RemoteAnalysisCachingDependenciesProvider` to invalidate `SkyValue`s using their fingerprinted `SkyKey`s. Doing the remote lookup may add more latency on every build before analysis. Current experiments show that the performance may not be a huge concern (sub-second invalidation for O(10000) keys).  This also requires a few changes to the `SkyframeExecutor`  because it needs to persist the deserialized keys across command invocations for invalidation.  PiperOrigin-RevId: 753993489 Change-Id: I06cb244c04e469ba3bcfd226348a7523601e8d6a
bazelbuild,bazel,edc876200bbc51206559e38811221dd41562a641,https://github.com/bazelbuild/bazel/commit/edc876200bbc51206559e38811221dd41562a641,Pass LibraryToLink into createLtoArtifacts  Implement _maybe_do_lto_indexing and create_lto_artifacts_and_lto_indexing_action using LibraryToLink (instead of LegacyLinkerInput).  Works towards removing LegacyLinkerInputs. The change is a no-op.  - libraries_to_link (list[LibraryToLink]) are passed into _maybe_do_lto_indexing. - static libraries_to_link are filtered out (using a new native method to keep the same performance) - _maybe_do_lto_indexing and create_lto_artifacts_and_lto_indexing_action is implemented using static_libraries_to_link. Interface and dynamic libraries play no role here. - libraries (list[LegacyLinkerInput]) is kept to pass into finalize_link_action. It will be removed in followup cl-s.  PiperOrigin-RevId: 751453530 Change-Id: I6a7839ef8213a41636b6e9430a1333de526a4b75
bazelbuild,bazel,03dfb497c16f0ff5a79410fe4f6bb1a154e0335a,https://github.com/bazelbuild/bazel/commit/03dfb497c16f0ff5a79410fe4f6bb1a154e0335a,Plumb the metadata for files referenced by Filesets through ExpandedArtifact.  The benefit is that these won't be considered local files anymore for the purposes of uploading files. In addition to a minor performance improvement  this also allows us to remove the mutation of ActionInputMap from ArtifactValueFileSystem.  That  in turn  will make it possible for ArtifactValueFileSystem to use an InputMetadataProvider instead of an ActionInputMap  which in turn enable putting a Skyframe-based InputMetadataProvider in ActionFileSystem  which will then unlock removing the logic to do Skyframe restarts from ActionFileSystem.  RELNOTES: None. PiperOrigin-RevId: 743648043 Change-Id: Ie61fbbb1f107b106f56d36ab90786411b83e45dc
bazelbuild,bazel,ae39adc27b6cb59f169e2c69867df71edc8a105f,https://github.com/bazelbuild/bazel/commit/ae39adc27b6cb59f169e2c69867df71edc8a105f,Always use an execroot-relative path when a fileset symlink pointing to a generated file is represented as an `ActionInput`.  Some places were using absolute paths while others were using execroot-relative paths  leading to `ActionInputMetadataProvider` having to support metadata lookups both ways.  Note that we still use absolute paths for fileset symlinks pointing to a source file  since there is such a thing as a build with multiple source roots.  An additional performance benefit is that we use the `PathFragment` directly from `FilesetOutputSymlink` instead of prepending the exec root and creating a new string. We've seen severe memory issues from this before  which were mitigated with interning.  PiperOrigin-RevId: 736533456 Change-Id: If3c5797288b3a85687afc5b59368e0bbe57b6a07
bazelbuild,bazel,11eb0c52b8779799608ce7cc598de240807eefcb,https://github.com/bazelbuild/bazel/commit/11eb0c52b8779799608ce7cc598de240807eefcb,Refactor lost input ownership calculation into `ActionRewindStrategy` and simplify it.  Lost input ownership calculation is important for action rewinding - in order to rewind properly  we need to know which special artifacts (runfiles  tree artifact  fileset) that a lost input is a part of.  Historically  lost input ownership calculation was performed by `ActionExecutionFunction` by sharing the same code that handles inputs for regular execution. It accomplished this via an interface `ActionInputMapSink` which was implemented by `ActionInputMap` (for regular execution) and `ActionInputDepOwnerMap` (for lost input ownership).  This solution was chosen to guarantee fidelity between the two. However  it is not necessary to accommodate arbitrary ownership mappings between artifacts - there is a well-defined  finite set of possible owner relationships. By using knowledge of the possible owner relationships  we can simplify and improve the performance of the calculation. In this change  we handle the three types of possible owners as follows:  * Runfiles - iterate over all `RunfilesTree`s in the action's inputs and expand them. * Tree artifact - simply call `hasParent()`/`getParent()` on lost input artifacts. * Fileset (no change from previous) - assume that fileset ownership is tracked by the spawn strategy that threw the original lost inputs exception. A future change will add fileset ownership handling to `ActionRewindStrategy`  making spawn-strategy-side ownership accounting optional.  Notably  we no longer re-flatten and re-process all of the action's inputs  which should improve rewinding performance.  PiperOrigin-RevId: 735759787 Change-Id: I21fbaea40810cc8c6da79c5d8c6d245318be5252
bazelbuild,bazel,3e115b937d7f6eff5c819667c6f6226c8fdba466,https://github.com/bazelbuild/bazel/commit/3e115b937d7f6eff5c819667c6f6226c8fdba466,Fix a severe performance bug for builds with many top-level targets.  This one-line fix speeds up some real-life builds by 90%  saving over 20 minutes!  We only need to remove labels from the weak interner when a package is freshly computed (`directDeps != null`)  not when it is already up-to-date as a top-level key in a skyframe evaluation. Without this guard  the removal code is being superfluously executed (in sequence) for each top-level target's package (without deduplication) twice during `BuildView#update`: once when computing the `labelToTargets` map  and again during `checkTargetEnvironmentRestrictions()`.  PiperOrigin-RevId: 733025147 Change-Id: Ief68c375a5d8444df1869968b4008af4a314b9c9
bazelbuild,bazel,3f36f64df2135cb4be9a0ba917d4d77813772177,https://github.com/bazelbuild/bazel/commit/3f36f64df2135cb4be9a0ba917d4d77813772177,Accept `Duration` in `SpawnMetrics.Builder` methods.  `SpawnMetrics` represents durations using primitive `int` fields for performance reasons. Callers of its `Builder` usually have to convert from `Duration` to `int`. Add `Builder` methods to do this conversion.  PiperOrigin-RevId: 732145629 Change-Id: Ifaaa18450b7fcffe5ab0a52ec4b4d30a8f76ffba
bazelbuild,bazel,70b0afd4e814e49fad93729324519174fe321190,https://github.com/bazelbuild/bazel/commit/70b0afd4e814e49fad93729324519174fe321190,ArgumentProcessor performance tuning:  * Add StarlarkCallable to Starlark.callViaArgumentProcessor() to remove getCallable() in thread.push() call. * Minimize calls between StarlarkFunction.ArgumentProcessor and StarlarkFunction: inline 3 methods from StarlarkFunction and one method from StarlarkFunction.ArgumentProcessor into StarlarkFunction.ArgumentProcessor.call().  PiperOrigin-RevId: 724604311 Change-Id: I18793004800d6fb28d5f5973fd9908f542e0860c
bazelbuild,bazel,ee12906c5d9a48924db6fc3aba36ccd6d5c7f69e,https://github.com/bazelbuild/bazel/commit/ee12906c5d9a48924db6fc3aba36ccd6d5c7f69e,Prevent use of BuiltinFunction.ArgumentProcessor in Eval.evalCall.  This is a temporary fix for a performance regression that the use of BuiltinFunction.ArgumentProcessor caused.  PiperOrigin-RevId: 723413626 Change-Id: I395052c8e05047223dc8b342fbe5c7d6b57369cc
bazelbuild,bazel,e6e8ffaa6dadf45f7b668dc887d7cc81af6a49ff,https://github.com/bazelbuild/bazel/commit/e6e8ffaa6dadf45f7b668dc887d7cc81af6a49ff,Automated rollback of commit 56bf54716094bf6b687366d20b577435213681d5.  *** Reason for rollback ***  performance regression b/392290938  *** Original change description ***  Remove use of fastcall() from Eval.java  PiperOrigin-RevId: 723396402 Change-Id: Icc69adbd41a5148349464544b918d2d7190755e7
bazelbuild,bazel,5f1aa38b2ea824a4b10e83c0baddfcd17fb418a2,https://github.com/bazelbuild/bazel/commit/5f1aa38b2ea824a4b10e83c0baddfcd17fb418a2,Automated rollback of commit b17a37bce15dded96868310616cd3c1c63f36c38.  *** Reason for rollback ***  performance regression b/392290938  *** Original change description ***  Remove fastcall() and related code from BuiltinFunction  PiperOrigin-RevId: 723375619 Change-Id: Ib1dd60269a09472436fe3a03f3fab80402865840
bazelbuild,bazel,936692fb27cb265ff1c832c571a7231157d0b6dd,https://github.com/bazelbuild/bazel/commit/936692fb27cb265ff1c832c571a7231157d0b6dd,Automated rollback of commit 2b928efcd651fe9e42d6d09956365f58b95ce248.  *** Reason for rollback ***  performance regression b/392290938  *** Original change description ***  Remove fastcall() from StarlarkCallable.java and Starlark.java  PiperOrigin-RevId: 723363896 Change-Id: Ic0fd4f5642b106d1e2d3843555f0ebc1fdf7b140
bazelbuild,bazel,0fd3ae4f66e7fb0e1449d1a736693a00b78a3439,https://github.com/bazelbuild/bazel/commit/0fd3ae4f66e7fb0e1449d1a736693a00b78a3439,Delete analyze-profile command  Not quite pulling its weight as a standalone command  and to the best of our knowledge it's not widely used  especially compared to the alternatives documented at https://bazel.build/advanced/performance/json-trace-profile.  PiperOrigin-RevId: 721383891 Change-Id: Ief393c8acdfb40cfe60437798440d95a6366f37b
bazelbuild,bazel,cf701ebeab2565e910453cad8b1b950553596d83,https://github.com/bazelbuild/bazel/commit/cf701ebeab2565e910453cad8b1b950553596d83,Fix performance regression on builds that change a test configuration flag.  Instead of `BuildOptionsScopeFunction` requesting `PrecomputedValue.BASELINE_CONFIGURATION`  request it in `BuildConfigurationKeyProducer` only after we know that we truly need it. This way the dependency is only requested if scopes are being applied.  The performance concern still exists if starlark flag scoping is enabled.  PiperOrigin-RevId: 716785874 Change-Id: Iabd7c11440c70c909f111f1dcafafa3b98610be9
bazelbuild,bazel,33a6b9a8477f70b31441d99758ff3cede9d0c926,https://github.com/bazelbuild/bazel/commit/33a6b9a8477f70b31441d99758ff3cede9d0c926,Fix performance regression in CriticalPathComputer.  Previously  when actions are change pruned  we add their entire transitive closure to critical path graph. This could slow down incremental builds if the action graph is large and only a small portion of it is invalidated.  This CL improves that by only adding actions that are invalidated (including change pruned).  We achieve that by updating Skyframe to emit change pruning event. These event are only emitted for nodes that were invalidated by dirtiness check during incremental builds. So that the CriticalPathComputer now operates on a subset of action graph that only contains dirty actions. This is more correct and performant for incremental builds.  PiperOrigin-RevId: 716146196 Change-Id: I71add4240458b5d4087b1af81a41202f98fee947
bazelbuild,bazel,4e8ae1fd1fb547588ce058478da173348515a1b7,https://github.com/bazelbuild/bazel/commit/4e8ae1fd1fb547588ce058478da173348515a1b7,Improve ConfiguredTargetKeyValueSharingCodec performance.  Use sharing for its BuildConfigurationKey. Inlining it is expensive.  PiperOrigin-RevId: 703309002 Change-Id: Ie12a33f1e131612b068dbbdfd66997ab450f01af
bazelbuild,bazel,a0a72265ea37270c110630f528127e18939f4016,https://github.com/bazelbuild/bazel/commit/a0a72265ea37270c110630f528127e18939f4016,Performance optimization for isSymbolicLink() calls on Windows  Current implementation for WindowsFileSystem.isSymbolicLink() makes unnecessary system calls which significantly affect the performance. Original code goes through the following: - AbstractFileSystemWithCustomStat.isSymbolicLink - WindowsFileSystem.stat - WindowsFileSystem.getIoFile - JavaIoFileSystem.getNioPath - Files.readAttributes - WindowsFileSystem.fileIsSymbolicLink - WindowsFileOperations.getLastChangeTime This implementation skips most of them.  Closes #24047.  PiperOrigin-RevId: 690964117 Change-Id: I2b407d9c69af62e770684d868d04e60d7ce1773e
bazelbuild,bazel,fd67506b0c73eed53089de5df339e6f4e2d810de,https://github.com/bazelbuild/bazel/commit/fd67506b0c73eed53089de5df339e6f4e2d810de,Introduce `FilesetOutputTree`.  This is a mechanical refactoring to replace `ImmutableList<FilesetOutputSymlink>` with an extra abstraction layer called `FilesetOutputTree`.  In this change  `FilesetOutputTree` only contains `ImmutableList<FilesetOutputSymlink>`. In a subsequent change  we will also store a `boolean` indicating whether the fileset has any relative symlinks. If the fileset does not have any relative symlinks (expected to be a common case)  we can make some significant performance optimizations.  PiperOrigin-RevId: 681116734 Change-Id: If49f8acf15122f6287cd426cb36a081952424b84
bazelbuild,bazel,c72af54fcf692ae553071071adda9d29ffad3cdc,https://github.com/bazelbuild/bazel/commit/c72af54fcf692ae553071071adda9d29ffad3cdc,Use a LongAdder instead of an AtomicLong.  Unlikely to matter  but technically has better performance under contention.  PiperOrigin-RevId: 680917850 Change-Id: I1a26f01d9e092427634f725bd2211c272500aecc
bazelbuild,bazel,33aca3fe8803ca55838af5467fa801d03e28ffd8,https://github.com/bazelbuild/bazel/commit/33aca3fe8803ca55838af5467fa801d03e28ffd8,Implement core garbage collection logic for the disk cache.  The garbage collection policy is defined by a maximum target size and a maximum age of individual cache entries  both of which may be simultaneously provided. I/O operations are parallelized to improve performance for large caches or slow filesystems.  PiperOrigin-RevId: 677860078 Change-Id: Ib342ad5e80ef4ef4af237aae243a300d13caaa06
bazelbuild,bazel,9f0a47101ad71c6b9cc018feb99fbb8e987a22c1,https://github.com/bazelbuild/bazel/commit/9f0a47101ad71c6b9cc018feb99fbb8e987a22c1,Performance improvements in `BuildConfigurationKeyMapProducer`.  PiperOrigin-RevId: 673396624 Change-Id: Iafd228f0e626b24da4de0087754c60862e5d27a6
bazelbuild,bazel,6fabb1fc6869a204373e5ee0adde696a659415dd,https://github.com/bazelbuild/bazel/commit/6fabb1fc6869a204373e5ee0adde696a659415dd,No longer eagerly fetch labels in repo rule attributes  Eager fetching of all labels listed in repo rule attributes was introduced as a performance optimization to avoid costly restarts.  Now that restarts are gone by default  this is no longer a benefit as it can cause unnecessary fetches and also create cycles where there wouldn't be any without this behavior (e.g. when two repos write each others labels into a file without resolving them).  Work towards #19055  Closes #23371.  PiperOrigin-RevId: 665744319 Change-Id: Ia27f207793a2da3fb8e37743b328483f9d45192c
bazelbuild,bazel,37df41df54a68e05b0f6c7dafadf1ae745eba968,https://github.com/bazelbuild/bazel/commit/37df41df54a68e05b0f6c7dafadf1ae745eba968,Implements a generic request-batching mechanism.  Purpose: Optimize interactions with services that offer batch request interfaces  significantly reducing overhead compared to individual unary requests. (For example  sending one batched RPC instead of 1000 individual requests)  User Experience: Clients continue to use a simple unary interface  with batching handled seamlessly in the background.  Implementation Overview:  The solution queues requests and distributes them to workers. Each worker asynchronously performs a cycle of:  1. Gathering queued requests. 2. Sending them as a batch to the service. 3. Awaiting the batch response.  Performance Impact: Preliminary testing on a frontier serialization dump operation shows a roughly 50x speed improvement compared to a non-batching approach.  PiperOrigin-RevId: 664822660 Change-Id: I5efa6f33ff1f0704c771f2ba1fc13150ed7ed253
bazelbuild,bazel,ffdf41acfd4d82bdfd3f855b8f4aa3840d918eba,https://github.com/bazelbuild/bazel/commit/ffdf41acfd4d82bdfd3f855b8f4aa3840d918eba,Change rewinding's `ArtifactNestedSetKey` search strategy to improve worst-case performance.  Instead of doing a search for every (lost artifact  `ArtifactNestedSetKey`) pair  do a single "bulk" search looking for all lost artifacts. When there are many lost artifacts  this is much more efficient. Additionally  we can prune visitations of nodes shared by multiple `NestedSet`s.  PiperOrigin-RevId: 663981449 Change-Id: Ic06f65e4f1dc9ca16a9bdd4a9c520b3ecd44be57
bazelbuild,bazel,642b571962ca9e6cf40ce94a8f53b8618f12080a,https://github.com/bazelbuild/bazel/commit/642b571962ca9e6cf40ce94a8f53b8618f12080a,Disable SkyframeStats collection unless specifically requested by flag. Removes the logic to limit on the number of reported types  if you're requesting the stats already you just get all of them.  SkyframeStats generation can be very expensive on large build graphs and can add significantly to latency when completing a build.  Disabling it entirely unless specifically requested mitigates that performance hit  unless the user specifically opts into the additional cost.  PiperOrigin-RevId: 640358760 Change-Id: I8bfa0cfaa5dcb019c541d240687c6d8a9b61aab8
libgdx,libgdx,fa96aafbc6b3205ccfd51ad515d0bc358b787377,https://github.com/libgdx/libgdx/commit/fa96aafbc6b3205ccfd51ad515d0bc358b787377,Add option to use Box2D native ContactFilter for performance optimization (#7578)
libgdx,libgdx,3b2ae2b4a304fcf27d5449584197ec2da78f04c3,https://github.com/libgdx/libgdx/commit/3b2ae2b4a304fcf27d5449584197ec2da78f04c3,Optmize JNI performance of Box2D Body methods (#7579)
libgdx,libgdx,bfe255a2727377b910be20af48d40867c588a8a3,https://github.com/libgdx/libgdx/commit/bfe255a2727377b910be20af48d40867c588a8a3,Optimization for SpriteBatch when running non VertexArray VertexDataModes. (GL30 default) (#7346)  * SpriteBatch optimizations to preupload full indices data for the size of the batch to prevent doing this each frame. Increase the performance of default SpriteBatch in gl30 where this performs worse than gl2 vertex array  * Apply formatter  * Add message for waiting for data  * Changelog  ---------  Co-authored-by: GitHub Action <action@github.com>
jenkinsci,jenkins,f45ba02af86d07caa3928010e384015babc2ccdb,https://github.com/jenkinsci/jenkins/commit/f45ba02af86d07caa3928010e384015babc2ccdb,[JENKINS-33704] Limit scope of `Jenkins#updateComputerList` to improve performance at scale (#10494)  * [JENKINS-33704] Limit scope of `Jenkins#updateComputerList`  Along the same line as #5882  but for computers  Adding/updating/removing a node to the system was causing all nodes retention strategies to be checked. If you have a lot of nodes  this quickly adds up to an extreme amount of checks.  This change makes the calls local  by only affecting the related nodes.  * Clarify variable name  * Fix logic  * Avoid dealing with `null`
Anuken,Mindustry,ab0a47a837628f8bb70802fd7ef6b581005e5113,https://github.com/Anuken/Mindustry/commit/ab0a47a837628f8bb70802fd7ef6b581005e5113,Conveyor progress (#10620)  * sensor progressssssssssssssssssssssssss  * for duct aswell  * better setprop for conveyors  * slight performance improvement i assume  * * import  * check if len is 0
OpenAPITools,openapi-generator,f9f5af5ed99e58a68ee7c55107942d670b962a99,https://github.com/OpenAPITools/openapi-generator/commit/f9f5af5ed99e58a68ee7c55107942d670b962a99,[JAVA][FEIGN] Removing hardcoded HTTP Client which is causing performance issues (#21085)  * [JAVA][FEIGN] Removing hardcoded HTTP Client  Fixing performance issues  * Updating samples for Java Feign performance betterment changes  * added APIClient.java for feign-hc5
redisson,redisson,67f03e214eb0e04b5cdc92d0473ac37240b30dc3,https://github.com/redisson/redisson/commit/67f03e214eb0e04b5cdc92d0473ac37240b30dc3,Improvement - performance optimization for Apache Tomcat Session management
google,gson,d4d6744f8c225709e1adcc38eaae4e968c476125,https://github.com/google/gson/commit/d4d6744f8c225709e1adcc38eaae4e968c476125,Improve `JsonWriter#value(Number)` performance (#2702)  * Improve `JsonWriter#value(Number)` performance  For JDK number types other than `Float` and `Double` there is no need to check if the number string is NaN or Infinity.  * Refactor boolean expression
conductor-oss,conductor,b4e7442d5c8393e19af456ee164c2b2aaebb3566,https://github.com/conductor-oss/conductor/commit/b4e7442d5c8393e19af456ee164c2b2aaebb3566,Address postgresql popmessage query not  limiting update to messages on the specific queue and only non popped messages. Switching to CTE for - better performance at scale - more conistent query planning at scale
yuliskov,SmartTube,274c583d04d44af7868ea0bb00eeec12f94258c5,https://github.com/yuliskov/SmartTube/commit/274c583d04d44af7868ea0bb00eeec12f94258c5,atv channels: performance tweaks
iBotPeaches,Apktool,c41a5272140f51d41c0de015e6f0103498266781,https://github.com/iBotPeaches/Apktool/commit/c41a5272140f51d41c0de015e6f0103498266781,perf: improve doNotCompress lookups (#3872)  We passed doNotCompress to zipDir as a List. The longer the list  the slower the lookups. Convert to a Set for buildApkFile and reuse for all zipDir calls.
apache,rocketmq,ae7179d75e11f469d68be05fbf556fde42c8a795,https://github.com/apache/rocketmq/commit/ae7179d75e11f469d68be05fbf556fde42c8a795,[ISSUE #8765] fix low performance of delay message when enable rocksdb consume queue (#8766)  * #7538 fix wrong cachedMsgSize if msg body is changed in consumer callback * [ISSUE #8765] fix low performance of delay message when enable rocksdb consume queue * remove prefetch
apache,rocketmq,4f5f705f16faeb7d491b25679d1c100f38264bb9,https://github.com/apache/rocketmq/commit/4f5f705f16faeb7d491b25679d1c100f38264bb9,[ISSUE #8780] Implement asynchronous storage of ack/ck messages in pop consume to enhance performance (#8727)  * Pop consume asynchronization  * Pass UTs and ITs  * Pass the checkstyle  * Fix LocalGrpcIT can not pass  * Fix the UT can not pass  * Simplify duplicate methods in EscapeBridge
apache,rocketmq,3aa5d1936f1162afefccdf03fcc55bf0f7ee642c,https://github.com/apache/rocketmq/commit/3aa5d1936f1162afefccdf03fcc55bf0f7ee642c,Adjust the default value of ackMessageThreadPoolNums to 16 to prevent performance bottlenecks during high traffic. (#8337)
elunez,eladmin,31b033afe89356950c2cf19672047455b99a8d1b,https://github.com/elunez/eladmin/commit/31b033afe89356950c2cf19672047455b99a8d1b,perf: 优化用户缓存管理，统一转换为小写 close https://github.com/elunez/eladmin/issues/866
elunez,eladmin,db63c953d49e100b38f65d13b3fc17ead5a831ec,https://github.com/elunez/eladmin/commit/db63c953d49e100b38f65d13b3fc17ead5a831ec,perf: 添加权限检查，优化角色缓存及命名
elunez,eladmin,0a91748fd246e566d09beac1b9593dcd532e17c8,https://github.com/elunez/eladmin/commit/0a91748fd246e566d09beac1b9593dcd532e17c8,perf: 优化Token生成
elunez,eladmin,fb422c6a9a4e9a41d1745ad935b245ca74bdda5f,https://github.com/elunez/eladmin/commit/fb422c6a9a4e9a41d1745ad935b245ca74bdda5f,perf: 优化系统日志参数获取，优化在线用户Token管理，优化SQl日志打印
openjdk,jdk,d9b6e4b13200684b69a161e288b9883ff0d96bec,https://github.com/openjdk/jdk/commit/d9b6e4b13200684b69a161e288b9883ff0d96bec,8352642: Set zipinfo-time=false when constructing zipfs FileSystem in com.sun.tools.javac.file.JavacFileManager$ArchiveContainer for better performance  Reviewed-by: liach  jpai  jlahoda  lancea
openjdk,jdk,84458ec18ce33295636f7b26b8e3ff25ecb349f2,https://github.com/openjdk/jdk/commit/84458ec18ce33295636f7b26b8e3ff25ecb349f2,8353013: java.net.URI.create(String) may have low performance to scan the host/domain name from URI string when the hostname starts with number  Reviewed-by: michaelm  xpeng
openjdk,jdk,5481021ee64fd457279ea7083be0f977c7ce3e3c,https://github.com/openjdk/jdk/commit/5481021ee64fd457279ea7083be0f977c7ce3e3c,8321591: (fs) Improve String -> Path conversion performance (win)  Reviewed-by: alanb
openjdk,jdk,8b0602dbed2f7ced190ec81753defab8a4bc316d,https://github.com/openjdk/jdk/commit/8b0602dbed2f7ced190ec81753defab8a4bc316d,8319447: Improve performance of delayed task handling  Reviewed-by: vklang  alanb
openjdk,jdk,250ff86dc86f73dbf7c944d9b5a792c4bdfeef0d,https://github.com/openjdk/jdk/commit/250ff86dc86f73dbf7c944d9b5a792c4bdfeef0d,8349000: Performance improvement for Currency.isPastCutoverDate(String)  Reviewed-by: naoto  aturbanov
openjdk,jdk,9a60f4457bb56d0f5039a97e6b943e62a8a2c3ee,https://github.com/openjdk/jdk/commit/9a60f4457bb56d0f5039a97e6b943e62a8a2c3ee,8345668: ZoneOffset.ofTotalSeconds performance regression  Reviewed-by: rriggs  aturbanov
openjdk,jdk,06c44dd568d91e1bd68f60fd3e57abcbe97e5dca,https://github.com/openjdk/jdk/commit/06c44dd568d91e1bd68f60fd3e57abcbe97e5dca,8345465: Fix performance regression on x64 after JDK-8345120  Reviewed-by: mcimadamore
openjdk,jdk,5958463cadb04560ec85d9af972255bfe6dcc2f2,https://github.com/openjdk/jdk/commit/5958463cadb04560ec85d9af972255bfe6dcc2f2,8343377: Performance regression in reflective invocation of native methods  Reviewed-by: mchung
openjdk,jdk,d49f21043b84ebcc8b9176de3a84621ca7bca8fb,https://github.com/openjdk/jdk/commit/d49f21043b84ebcc8b9176de3a84621ca7bca8fb,8342040: Further improve entry lookup performance for multi-release JARs  Co-authored-by: Claes Redestad <redestad@openjdk.org> Reviewed-by: redestad
openjdk,jdk,81ff91ef27a6a856ae2c453a9a9b8333b91da3ab,https://github.com/openjdk/jdk/commit/81ff91ef27a6a856ae2c453a9a9b8333b91da3ab,8339531: Improve performance of MemorySegment::mismatch  Reviewed-by: mcimadamore
openjdk,jdk,6be927260a84b1d7542167e526ff41f7dc26cab0,https://github.com/openjdk/jdk/commit/6be927260a84b1d7542167e526ff41f7dc26cab0,8338591: Improve performance of MemorySegment::copy  Reviewed-by: mcimadamore
openjdk,jdk,7a418fc07464fe359a0b45b6d797c65c573770cb,https://github.com/openjdk/jdk/commit/7a418fc07464fe359a0b45b6d797c65c573770cb,8338967: Improve performance for MemorySegment::fill  Reviewed-by: mcimadamore  psandoz
openjdk,jdk,ab8071d28027ecbf5e8984c30b35fa1c2d934de7,https://github.com/openjdk/jdk/commit/ab8071d28027ecbf5e8984c30b35fa1c2d934de7,8338146: Improve Exchanger performance with VirtualThreads  Reviewed-by: alanb
openjdk,jdk,75bea280b9adb6dac9fefafbb3f4b212f100fbb5,https://github.com/openjdk/jdk/commit/75bea280b9adb6dac9fefafbb3f4b212f100fbb5,8333867: SHA3 performance can be improved  Reviewed-by: kvn  valeriep
openjdk,jdk,a941397327972f130e683167a1b429f17603df46,https://github.com/openjdk/jdk/commit/a941397327972f130e683167a1b429f17603df46,8329031: CPUID feature detection for Advanced Performance Extensions (Intel® APX)  Reviewed-by: sviswanathan  kvn
openjdk,jdk,d826127970bd2ae8bf4cacc3c55634dc5af307c4,https://github.com/openjdk/jdk/commit/d826127970bd2ae8bf4cacc3c55634dc5af307c4,8333462: Performance regression of new DecimalFormat() when compare to jdk11  Reviewed-by: liach  naoto  jlu
oracle,graal,66b50d5943e742a7f217710941fd718d5bacff48,https://github.com/oracle/graal/commit/66b50d5943e742a7f217710941fd718d5bacff48,[GR-64745] Adapt JDK-8348638: Performance regression in Math.tanh  PullRequest: graal/20726
oracle,graal,db9fde6c18ace56bbb05b5612096db70ceb5c23f,https://github.com/oracle/graal/commit/db9fde6c18ace56bbb05b5612096db70ceb5c23f,Adapt JDK-8348638: Performance regression in Math.tanh
oracle,graal,72c4ce0bc7574825ddf51df9024fe8ab4e5c3a5d,https://github.com/oracle/graal/commit/72c4ce0bc7574825ddf51df9024fe8ab4e5c3a5d,[GR-64420] Use initial system properties for jvmstat performance counters.  PullRequest: graal/20587
oracle,graal,e3f68dfa84f63a458a6f9bcc67ed95063a30b772,https://github.com/oracle/graal/commit/e3f68dfa84f63a458a6f9bcc67ed95063a30b772,Use initial system properties for jvmstat performance counters.
oracle,graal,0a5253bd7aad2f78489fcf16e7276246fd26a7e7,https://github.com/oracle/graal/commit/0a5253bd7aad2f78489fcf16e7276246fd26a7e7,[JDK-8350376] Adapt JDK-8319447: Improve performance of delayed task handling  PullRequest: graal/20121
oracle,graal,74fbcaf69b17c4da76bcde126abd2596a19d0e30,https://github.com/oracle/graal/commit/74fbcaf69b17c4da76bcde126abd2596a19d0e30,svm: adopt "JDK-8319447: Improve performance of delayed task handling"
oracle,graal,b8693d33f51ca09e60c04a3dcd0e90a12bf6e3b6,https://github.com/oracle/graal/commit/b8693d33f51ca09e60c04a3dcd0e90a12bf6e3b6,[GR-50017] TruffleStrings: internal refactoring for better performance in statically compiled code.  PullRequest: graal/19999
oracle,graal,ff566b6fb87f685ccd3561c75508ba819d29a35b,https://github.com/oracle/graal/commit/ff566b6fb87f685ccd3561c75508ba819d29a35b,[GR-62951] Add Truffle boundary for NFI Panama downcall  * The downcall MethodHandle is in general not safe to PE  looking at compiler graphs. * It was also caught by the Blocklisted methods check. * However performance significantly suffers with just `@TruffleBoundary`  the default. * Performance is the same with `@TruffleBoundary(allowInlining = true)`.
oracle,graal,fb360eb27f2b456e639523e2ac63b3b9a756c29c,https://github.com/oracle/graal/commit/fb360eb27f2b456e639523e2ac63b3b9a756c29c,[GR-60423] Improve FileSystems.isNormalized(Path) performance.  PullRequest: graal/19576
oracle,graal,3632022ed910b7f0d4d6302a4ec277adb52cff2e,https://github.com/oracle/graal/commit/3632022ed910b7f0d4d6302a4ec277adb52cff2e,[GR-60423] Improve FileSystems.isNormalized(Path) performance.
oracle,graal,77d04343a7bbc4c21e6d05c3463ba9ee7fe8fe5f,https://github.com/oracle/graal/commit/77d04343a7bbc4c21e6d05c3463ba9ee7fe8fe5f,Revert PublishWritesNode simplification logic  Benchmark has shown that this does not result in a measurable improvement in performance or memory usage. In the future we could try to a chieve a similar result by postponing lowering of CommitAllocationNodes.
oracle,graal,dc09603757e836cab633f92fa63ff7508ceba341,https://github.com/oracle/graal/commit/dc09603757e836cab633f92fa63ff7508ceba341,[GR-57385] Improve performance of interop invoke member.  PullRequest: graal/18668
oracle,graal,b24d9c7330aa0c0ffb3444dfcd1470c0d48fb9d8,https://github.com/oracle/graal/commit/b24d9c7330aa0c0ffb3444dfcd1470c0d48fb9d8,[GR-56337] Fix performance regressions due to inlining and JavaC.  PullRequest: graal/18503
oracle,graal,dfc4db490068947ee60a85c004a914267e97184b,https://github.com/oracle/graal/commit/dfc4db490068947ee60a85c004a914267e97184b,Fixed a minor performance issue in the serial GC. Various fixes for @Uninterruptible.
apache,shardingsphere,dfe831264086467c31708d06cd3bf165a439312c,https://github.com/apache/shardingsphere/commit/dfe831264086467c31708d06cd3bf165a439312c,Add database type parameter to SQLBindEngine.bind method (#35511)  * Optimize MySQL multi-statements handling  - Extract DatabaseType instance as a private field to improve performance - Update SQLParserEngine initialization to use the pre-loaded DatabaseType - Modify SQLBindEngine invocation to include DatabaseType as a parameter  * Add database type parameter to SQLBindEngine.bind method - Update the SQLBindEngine.bind method call to include the databaseType parameter - This change ensures that the database type is properly passed during SQL statement binding
apache,shardingsphere,df7fe751e0b9419692f003f6860205920b9b7741,https://github.com/apache/shardingsphere/commit/df7fe751e0b9419692f003f6860205920b9b7741,Optimize MySQL multi-statements handling (#35510)  - Extract DatabaseType instance as a private field to improve performance - Update SQLParserEngine initialization to use the pre-loaded DatabaseType - Modify SQLBindEngine invocation to include DatabaseType as a parameter
apache,shardingsphere,8c7c2fce787b1d16679045a7e4f8bad4f1b776d6,https://github.com/apache/shardingsphere/commit/8c7c2fce787b1d16679045a7e4f8bad4f1b776d6,Optimize MySQLComFieldListPacketExecutor with DatabaseType (#35509)  - Add a private final DatabaseType field initialized with "MySQL" - Replace inline DatabaseType retrieval with the new field for better readability and performance
apache,shardingsphere,7eb3176b02d5a786a590c5dcfb3c82454ab9c2eb,https://github.com/apache/shardingsphere/commit/7eb3176b02d5a786a590c5dcfb3c82454ab9c2eb,Add high frequency invocation annotation for tableless route engine (#35415)  - Add @HighFrequencyInvocation annotation to TablelessDataSourceUnicastRouteEngine class - This change helps identify the class as frequently used  potentially for performance monitoring or optimization
apache,shardingsphere,8f5b7a8d12551f2cacf22a6c5279707f3fe07996,https://github.com/apache/shardingsphere/commit/8f5b7a8d12551f2cacf22a6c5279707f3fe07996,Replace ThreadLocalRandom with SecureRandom for better security (#35411)  - Use SecureRandom instead of ThreadLocalRandom for generating random numbers - This change improves the security of the random selection process - The performance impact is negligible for this specific use case
apache,shardingsphere,dd3ee66539a422ea05aeb6aae6237d2c56b2297e,https://github.com/apache/shardingsphere/commit/dd3ee66539a422ea05aeb6aae6237d2c56b2297e,Optimize execution prepare engine for high frequency invocation (#35352)  - Add @HighFrequencyInvocation annotation to relevant classes - Improve data structure usage for better performance - Refactor method names for consistency and clarity
apache,shardingsphere,1c452cbf2646a6ce1213c26a4ebbd49e9570128b,https://github.com/apache/shardingsphere/commit/1c452cbf2646a6ce1213c26a4ebbd49e9570128b,Merge pull request #33361 from strongduanmu/dev-1023  Fix SQL performance issues caused by repeated subquery fetches
apache,shardingsphere,2cfd72981a16e1eaef88cbbe86e0fe4d1e751cff,https://github.com/apache/shardingsphere/commit/2cfd72981a16e1eaef88cbbe86e0fe4d1e751cff,Fix SQL performance issues caused by repeated subquery fetches
dataease,dataease,a959070b47c0ecfbe07c6117a31937953a003170,https://github.com/dataease/dataease/commit/a959070b47c0ecfbe07c6117a31937953a003170,perf: 系统设置-保存系统参数接口逻辑
dataease,dataease,d7c2ec52ca29240e155de5b24dcb545787b310b0,https://github.com/dataease/dataease/commit/d7c2ec52ca29240e155de5b24dcb545787b310b0,perf: 优化菜单查询接口查询速度
dataease,dataease,6567c09acd88470e13b836b44fe5f728e56c5115,https://github.com/dataease/dataease/commit/6567c09acd88470e13b836b44fe5f728e56c5115,perf: 国际化默认语言设置
dataease,dataease,38f4f96901c9272bf74c364b68a73ba8fe49870a,https://github.com/dataease/dataease/commit/38f4f96901c9272bf74c364b68a73ba8fe49870a,perf: 删除存在 SQL 注入风险的代码
dataease,dataease,e13a594d45d58f24247a5663793e3361d53f9ace,https://github.com/dataease/dataease/commit/e13a594d45d58f24247a5663793e3361d53f9ace,perf: 增强url特殊字符攻击检测
dataease,dataease,4e7f8450e5335cbfcade9b2a832cf9d50a4895e8,https://github.com/dataease/dataease/commit/4e7f8450e5335cbfcade9b2a832cf9d50a4895e8,perf: 社区版使用默认密码无法登录
dataease,dataease,6b6376e4bce3610a9f3a84a1aff8b1b30b811e2b,https://github.com/dataease/dataease/commit/6b6376e4bce3610a9f3a84a1aff8b1b30b811e2b,Merge pull request #13953 from dataease/pr@dev-v2@perf_community_language  perf: 社区版语言切换
dataease,dataease,aa39d52a554db503e5ec82d539e8b20466cf34b2,https://github.com/dataease/dataease/commit/aa39d52a554db503e5ec82d539e8b20466cf34b2,perf: 社区版语言切换
dataease,dataease,40404f29c0e9ed96d3b0825afa1a8a43ef8c88b7,https://github.com/dataease/dataease/commit/40404f29c0e9ed96d3b0825afa1a8a43ef8c88b7,Merge pull request #13725 from dataease/pr@dev-v2@perf_auth_weight  perf: 社区版默认资源权重
dataease,dataease,bc777cac252569c78d6dcab3021bfe308fdc6845,https://github.com/dataease/dataease/commit/bc777cac252569c78d6dcab3021bfe308fdc6845,perf: 社区版默认资源权重
dataease,dataease,46c66b2395ec26c18c794989c33d7e97886a7418,https://github.com/dataease/dataease/commit/46c66b2395ec26c18c794989c33d7e97886a7418,Merge pull request #13633 from dataease/pr@dev-v2@perf_del_auto_sync  perf: 删除自动同步游离资源逻辑
dataease,dataease,3b95c6a94510e94f16dd88e328f83c3e3805cf4b,https://github.com/dataease/dataease/commit/3b95c6a94510e94f16dd88e328f83c3e3805cf4b,perf: 删除自动同步游离资源逻辑
dataease,dataease,36c6d84298ea6ef17fed714c4dac1cc9bc356430,https://github.com/dataease/dataease/commit/36c6d84298ea6ef17fed714c4dac1cc9bc356430,perf: typos拼写错误
dataease,dataease,1ca5e8935a3213f42b1015cc8e9c1a85e1b9088a,https://github.com/dataease/dataease/commit/1ca5e8935a3213f42b1015cc8e9c1a85e1b9088a,Merge pull request #13212 from dataease/pr@dev-v2@perf_person_ip  perf: 社区版获取客户端IP信息
dataease,dataease,ed51826bae09b9483b29be969c4522194b41e5c9,https://github.com/dataease/dataease/commit/ed51826bae09b9483b29be969c4522194b41e5c9,perf: 社区版获取客户端IP信息
dataease,dataease,8c7501bada38529e89aee5fceeba42d511425001,https://github.com/dataease/dataease/commit/8c7501bada38529e89aee5fceeba42d511425001,Merge pull request #13038 from dataease/pr@dev-v2@perf_redis_cache  perf: 集群环境redis缓存优化
dataease,dataease,b7368a236ae29aca86072cf6ddb3e179e857e9b9,https://github.com/dataease/dataease/commit/b7368a236ae29aca86072cf6ddb3e179e857e9b9,perf: 集群环境redis缓存优化
dataease,dataease,9d86e9b6494a6853b77493dcf7bd1b0db96269c3,https://github.com/dataease/dataease/commit/9d86e9b6494a6853b77493dcf7bd1b0db96269c3,Merge pull request #12795 from dataease/pr@dev-v2@perf@api_description  perf: api文档描述文案
dataease,dataease,ffbe0e77e4f90d50ead3c882d90fdcd38ada0671,https://github.com/dataease/dataease/commit/ffbe0e77e4f90d50ead3c882d90fdcd38ada0671,perf: api文档描述文案
dataease,dataease,7f116c027168151797f28d8e00682372a1ea836f,https://github.com/dataease/dataease/commit/7f116c027168151797f28d8e00682372a1ea836f,Merge pull request #12550 from dataease/pr@dev-v2@perf_community_token  perf: 优化社区版token机制
dataease,dataease,b3bb62b12362e5a1879a26428f710a4c0c0b44ca,https://github.com/dataease/dataease/commit/b3bb62b12362e5a1879a26428f710a4c0c0b44ca,perf: 优化社区版token机制
dataease,dataease,bec1873704ccc2de12f0143a0cdc02b10b56ceb5,https://github.com/dataease/dataease/commit/bec1873704ccc2de12f0143a0cdc02b10b56ceb5,Merge pull request #12538 from dataease/pr@dev-v2@perf_token  perf: 社区版token机制
dataease,dataease,e755248d59543bcd668ace495f293ff735fa82e9,https://github.com/dataease/dataease/commit/e755248d59543bcd668ace495f293ff735fa82e9,perf: 社区版token机制
dataease,dataease,f16d893120bbfb6c618146cc2f2a5dc8361e66d6,https://github.com/dataease/dataease/commit/f16d893120bbfb6c618146cc2f2a5dc8361e66d6,Merge pull request #12090 from dataease/pr@dev-v2@perf_api  perf: 数据集接口文档参数描述不准确
dataease,dataease,cf9e9d7f8b18ba0f545db9c8b8da9ba08dc32bbd,https://github.com/dataease/dataease/commit/cf9e9d7f8b18ba0f545db9c8b8da9ba08dc32bbd,perf: 数据集接口文档参数描述不准确
dataease,dataease,cea7b58a0c0d2fb70b5b6edbb02b06d4080f3ece,https://github.com/dataease/dataease/commit/cea7b58a0c0d2fb70b5b6edbb02b06d4080f3ece,Merge pull request #12087 from dataease/pr@dev-v2@perf_auth_save_api  perf: 保存权限接口参数文档必填项描述不准确
dataease,dataease,0a35fd1e9a6ff925932a56fe263ede08c2a75595,https://github.com/dataease/dataease/commit/0a35fd1e9a6ff925932a56fe263ede08c2a75595,perf: 保存权限接口参数文档必填项描述不准确
dataease,dataease,3fca8653721b4cc53c06408cc633aca912b29017,https://github.com/dataease/dataease/commit/3fca8653721b4cc53c06408cc633aca912b29017,Merge pull request #10978 from dataease/pr@dev-v2@perf_api_traffic  perf: api限流相关flyway
dataease,dataease,c7038a233ddf633bc43bdcf4d3454b2efc7af969,https://github.com/dataease/dataease/commit/c7038a233ddf633bc43bdcf4d3454b2efc7af969,perf: api限流相关flyway
dataease,dataease,8917c5b2bb49b1046713994807ad606e254c6333,https://github.com/dataease/dataease/commit/8917c5b2bb49b1046713994807ad606e254c6333,Merge pull request #10870 from dataease/pr@dev-v2@perf_delete_retry_task  perf: 项目启动时删除所有重试任务
dataease,dataease,b046d03357a035bf555605cec3dd560edd71eff4,https://github.com/dataease/dataease/commit/b046d03357a035bf555605cec3dd560edd71eff4,perf: 项目启动时删除所有重试任务
dataease,dataease,fd769ddb81c1a32b97ec3eb0cab1a35a756ac4d5,https://github.com/dataease/dataease/commit/fd769ddb81c1a32b97ec3eb0cab1a35a756ac4d5,Merge pull request #10092 from dataease/pr@dev-v2@perf_link_iframe  perf: 优化公共链接iframe嵌入
dataease,dataease,c0d84fe024a753508fa17346f34b1f66128674bd,https://github.com/dataease/dataease/commit/c0d84fe024a753508fa17346f34b1f66128674bd,perf: 优化公共链接iframe嵌入
dataease,dataease,913b99131d1a77001e5d36f93a10353e9d2dd08a,https://github.com/dataease/dataease/commit/913b99131d1a77001e5d36f93a10353e9d2dd08a,Merge pull request #9897 from dataease/pr@dev-v2@perf_div_embedded_cors  perf: div嵌入式跨域设置
dataease,dataease,cae7cd0064a2cebb6d99f6c4487644fb9b6e7cb4,https://github.com/dataease/dataease/commit/cae7cd0064a2cebb6d99f6c4487644fb9b6e7cb4,perf: div嵌入式跨域设置
thingsboard,thingsboard,265e4181b7dc4ae5132fbb23b68bf94c6e80f603,https://github.com/thingsboard/thingsboard/commit/265e4181b7dc4ae5132fbb23b68bf94c6e80f603,improved performance
thingsboard,thingsboard,1499f6fdf43f6ea31015996b62138b7fe04d34db,https://github.com/thingsboard/thingsboard/commit/1499f6fdf43f6ea31015996b62138b7fe04d34db,Merge pull request #11666 from thingsboard/fix/alarms-unassign  Housekeeper: performance improvements for alarms unassigning
thingsboard,thingsboard,dd3936ca666dc9b9ba867b002e553bb5d971c16e,https://github.com/thingsboard/thingsboard/commit/dd3936ca666dc9b9ba867b002e553bb5d971c16e,Performance improvements for processing alarms unassigning task
thingsboard,thingsboard,911232cdd167561b11c6c746d0ef69fdbd3e9002,https://github.com/thingsboard/thingsboard/commit/911232cdd167561b11c6c746d0ef69fdbd3e9002,Merge pull request #11498 from YevhenBondarenko/hotfix/ws-unsubscribe-optimization-3.7.0  DefaultTbLocalSubscriptionService WS single lock refactored by tenantId; TbEntityLocalSubsInfo performance optimizations for remove subscription
thingsboard,thingsboard,bff310b6913ab750fb506884bb29515098177bc0,https://github.com/thingsboard/thingsboard/commit/bff310b6913ab750fb506884bb29515098177bc0,DefaultTbLocalSubscriptionService WS single lock refactored by tenantId; TbEntityLocalSubsInfo performance optimizations for remove subscription
kestra-io,kestra,b0f93e19459c1950a8d3922acef022e81376a88e,https://github.com/kestra-io/kestra/commit/b0f93e19459c1950a8d3922acef022e81376a88e,chore(system): skip sleep in the JdbcQueue when at max poll size  When a queue is at max poll size  this means that it is at full capacity. In this case skip the sleep and process immediatly the next batch of message. This improve latency at high thoughput without adding too much load to the database.  We can even go further by skipping sleep each time the poll returns messages but this would imply database cost so for now we balance performance and database cost by only skipping sleep when at max capacity.
kestra-io,kestra,11a7e68e93e6723e3aa0ece35e83acb3c7ecbb6c,https://github.com/kestra-io/kestra/commit/11a7e68e93e6723e3aa0ece35e83acb3c7ecbb6c,feat(core)!: make tenant id required (#8460)  * feat(core)!: WIP make tenant id required  * feat(core)!: WIP make tenant id required  * test(core)!: WIP fix storage unit test  * build(deps): bump com.google.guava:guava from 33.4.7-jre to 33.4.8-jre  Bumps [com.google.guava:guava](https://github.com/google/guava) from 33.4.7-jre to 33.4.8-jre. - [Release notes](https://github.com/google/guava/releases) - [Commits](https://github.com/google/guava/commits)  --- updated-dependencies: - dependency-name: com.google.guava:guava dependency-version: 33.4.8-jre dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump io.micronaut.platform:micronaut-platform  Bumps [io.micronaut.platform:micronaut-platform](https://github.com/micronaut-projects/micronaut-platform) from 4.8.0 to 4.8.2. - [Release notes](https://github.com/micronaut-projects/micronaut-platform/releases) - [Commits](https://github.com/micronaut-projects/micronaut-platform/compare/v4.8.0...v4.8.2)  --- updated-dependencies: - dependency-name: io.micronaut.platform:micronaut-platform dependency-version: 4.8.2 dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump flyingSaucerVersion from 9.11.6 to 9.12.0  Bumps `flyingSaucerVersion` from 9.11.6 to 9.12.0.  Updates `org.xhtmlrenderer:flying-saucer-core` from 9.11.6 to 9.12.0 - [Release notes](https://github.com/flyingsaucerproject/flyingsaucer/releases) - [Changelog](https://github.com/flyingsaucerproject/flyingsaucer/blob/main/CHANGELOG.md) - [Commits](https://github.com/flyingsaucerproject/flyingsaucer/compare/v9.11.6...v9.12.0)  Updates `org.xhtmlrenderer:flying-saucer-pdf` from 9.11.6 to 9.12.0 - [Release notes](https://github.com/flyingsaucerproject/flyingsaucer/releases) - [Changelog](https://github.com/flyingsaucerproject/flyingsaucer/blob/main/CHANGELOG.md) - [Commits](https://github.com/flyingsaucerproject/flyingsaucer/compare/v9.11.6...v9.12.0)  --- updated-dependencies: - dependency-name: org.xhtmlrenderer:flying-saucer-core dependency-version: 9.12.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: org.xhtmlrenderer:flying-saucer-pdf dependency-version: 9.12.0 dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump software.amazon.awssdk:bom from 2.31.21 to 2.31.25  Bumps software.amazon.awssdk:bom from 2.31.21 to 2.31.25.  --- updated-dependencies: - dependency-name: software.amazon.awssdk:bom dependency-version: 2.31.25 dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump com.github.oshi:oshi-core from 6.8.0 to 6.8.1  Bumps [com.github.oshi:oshi-core](https://github.com/oshi/oshi) from 6.8.0 to 6.8.1. - [Release notes](https://github.com/oshi/oshi/releases) - [Changelog](https://github.com/oshi/oshi/blob/master/CHANGELOG.md) - [Commits](https://github.com/oshi/oshi/compare/oshi-parent-6.8.0...oshi-parent-6.8.1)  --- updated-dependencies: - dependency-name: com.github.oshi:oshi-core dependency-version: 6.8.1 dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump org.opensearch.client:opensearch-java  Bumps [org.opensearch.client:opensearch-java](https://github.com/opensearch-project/opensearch-java) from 2.22.0 to 2.23.0. - [Release notes](https://github.com/opensearch-project/opensearch-java/releases) - [Changelog](https://github.com/opensearch-project/opensearch-java/blob/v2.23.0/CHANGELOG.md) - [Commits](https://github.com/opensearch-project/opensearch-java/compare/v2.22.0...v2.23.0)  --- updated-dependencies: - dependency-name: org.opensearch.client:opensearch-java dependency-version: 2.23.0 dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump co.elastic.logging:logback-ecs-encoder  Bumps [co.elastic.logging:logback-ecs-encoder](https://github.com/elastic/ecs-logging-java) from 1.6.0 to 1.7.0. - [Release notes](https://github.com/elastic/ecs-logging-java/releases) - [Commits](https://github.com/elastic/ecs-logging-java/compare/v1.6.0...v1.7.0)  --- updated-dependencies: - dependency-name: co.elastic.logging:logback-ecs-encoder dependency-version: 1.7.0 dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * tests(system): isolate SchedulerScheduleTest tests with tenantId  * Feat/storage outputs (#8361)  * feat(executions): Store outputs inside the internal storage (1/2)  * feat(executions): Store outputs inside the internal storage (2/2)  * feat(test): allow passing tenantId to tests  ---------  Co-authored-by: Ludovic DEHON <tchiot.ludo@gmail.com>  * chore(deps): regular dependency update (#8484)  Performing a weekly round of dependency updates in the NPM ecosystem to keep everything up to date.  * fix(ui): full view height for single task logs (#8042)  Co-authored-by: Miloš Paunović <paun992@hotmail.com>  * feat: synchronize task edition with editor (#8433)  * fix(controls): adjust bottom position of contorls in multiPanelsEditor (#8465)  * fix(executions): unqueing execution must remove the execution queued  When an execution is queued in the JDBC backend  a record is inserted inside the execution_queued table  we must remove this record when we unqeue an execution.  Fixes #8448  * chore(core): localize to languages other than english (#8485)  Extended localization support by adding translations for multiple languages using English as the base. This enhances accessibility and usability for non-English-speaking users while keeping English as the source reference.  Co-authored-by: GitHub Action <actions@github.com>  * tests(webserver): fix flaky test which could query previous tests tasks  * tests(system): debug flaky error of shouldPauseExecutionByQueryRunningFlows  * tests(system): debug flaky error of shouldPauseExecutionByQueryRunningFlows  * tests(system): try with different task id  * tests(system): bump sleep-short sleep time to 10s  * fix(execution)*: decode and hide nested inputs of type SECRET  Fixes #7964  * refactor(core): pass the dynamic concurrency schema to no code editor (#8488)  There was an issue with passing hard-coded concurrency schema to be rendered in No Code editor  which is now amended and we're passing down the previously fetched one  * chore(deps): update gradle version  * doc(basic.md): add link to configuration for kestra property variables (#8490)  * fix(flows): properly check average duration for dashboard graphs (#8457)  There was a problem on flows view with the main chart not showing proper data until user clicks on duration toggle.  Closes https://github.com/kestra-io/kestra/issues/8435. Closes https://github.com/kestra-io/kestra-ee/issues/3499.  * docs(core-pause): update pauseDuration properties  titles  descriptions (#8495)  * fix(system): restrict the JdbcConcurrencyLimitService to the JDBC runner  * fix(core): fix indexer metric description (#8500)  * Add examples with expression and trimmed values (#6154)  * chore(ui): improvement to drilldown for Default and Custom Charts. (#7885)  * chore(ui): improvement to drilldown for Default and Custom Charts.  * minor tweak  * test: fix the Barchart stories to test drilldown  ---------  Co-authored-by: Bart Ledoux <bledoux@kestra.io>  * fix(ui): restart trigger position for backfill column (#8246)  Co-authored-by: Miloš Paunović <paun992@hotmail.com> Co-authored-by: Barthélémy Ledoux <ledouxb@me.com> Co-authored-by: Bart Ledoux <bledoux@kestra.io>  * feat(plugin): add a way to provide additional type of plugins  Provide a way for plugins to define a new type of plugins. To do that  a plugin must provide both an abstract base class that extends AdditionalPlugin and a set of concret classes. Both the abstract base class and the concrete classes mut be inside the same plugin. This is a limitation that we may work on later by providing  for example  an SPI to add base classes to the application classloader.  * fix(ui): save existing flow after making changes (#8378)  Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  * fix(core): failing DocumentationGeneratorTest.returnDoc()  * chore(core): refactor  component to composition API  structure and with some styling (#8504)  * feat(flows): add validation for use of inputs and outputs with '-' in the name (#8379)  * test(core): fix breaking change in local flow repository (#8517)  Co-authored-by: nKwiatkowski <nkwiatkowski@kestra.io>  * feat(system)!: remove the SQLServer runner  Part-of: https://github.com/kestra-io/kestra-ee/issues/3504  * chore(build): add Postgres stat extension  * feat(plugins): add Langchain4J plugins  * chore(system): add warn log when emit logQueue failed (#8432)  * feat(plugins): add Go Script plugin  * fix(jdbc): add service_id index on service_instance table  * chore(flows): improve the blueprints view within the flow editing panels (#7983)  Changes here consist of removing the tags from blueprint view on Multi Panel flow editor  along with couple of other UI improvements.  Closes https://github.com/kestra-io/kestra/issues/7881.  Co-authored-by: Miloš Paunović <paun992@hotmail.com>  * fix(flows): properly load blueprints in multi panel view (#8524)  While using the new Multi Panel view blueprints were not loading properly due to wrong paramtere being sent to action. now that's sorted.  Closes https://github.com/kestra-io/kestra/issues/8523.  * feat(flows): improve the display of array inputs when running an execution (#7953)  This PR is introducing a change of how the `array` inputs are displayed inside the flow run dialog  to be more user-friendly.  Closes https://github.com/kestra-io/kestra/issues/6947.  ---------  Co-authored-by: Miloš Paunović <paun992@hotmail.com>  * fix(system): change default config values for liveness  Change kestra.server.liveness.interval from 5s to 10s to be less agressive on liveness check. Align other default liveness configs with kafka implementation.  * fix(ui): amend Absolute date filter's looks (#8501)  * chore(core): localize to languages other than english (#8528)  Extended localization support by adding translations for multiple languages using English as the base. This enhances accessibility and usability for non-English-speaking users while keeping English as the source reference.  Co-authored-by: GitHub Action <actions@github.com>  * docs(flow-trigger): add note about no Pebble in conditions  * fix(ui): remove parts of filter using backspace (#8105)  Co-authored-by: Miloš Paunović <paun992@hotmail.com> Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  * fix(ui): open link in markdown in other tab. (#8258)  * fix(ui): open link in markdown in other tab.  * chore(core): restrict attribute to external links.  Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  ---------  Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  * fix(system): load OpenTelemetry lib in the app classloader  Without that  as we have it here  plugins may have class loading issue if they use OpenTelemetry internally (like in the Elasticsearch client).  * feat(ui): Add search in internal docs (#8458)  Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  * fix(triggers): inject default later inside the Scheduler  Today  as they are injected eagerly  they are done even if no trigger exists. This is counter-performant  and in case the flow is an error will log each seconds. Doing it a little later will be better.  * build(deps): bump software.amazon.awssdk:bom from 2.31.25 to 2.31.30  Bumps software.amazon.awssdk:bom from 2.31.25 to 2.31.30.  --- updated-dependencies: - dependency-name: software.amazon.awssdk:bom dependency-version: 2.31.30 dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump org.wiremock:wiremock-jetty12 from 3.12.1 to 3.13.0  Bumps [org.wiremock:wiremock-jetty12](https://github.com/wiremock/wiremock) from 3.12.1 to 3.13.0. - [Release notes](https://github.com/wiremock/wiremock/releases) - [Commits](https://github.com/wiremock/wiremock/compare/3.12.1...3.13.0)  --- updated-dependencies: - dependency-name: org.wiremock:wiremock-jetty12 dependency-version: 3.13.0 dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump jacksonVersion from 2.18.3 to 2.19.0  Bumps `jacksonVersion` from 2.18.3 to 2.19.0.  Updates `com.fasterxml.jackson:jackson-bom` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-bom/compare/jackson-bom-2.18.3...jackson-bom-2.19.0)  Updates `com.fasterxml.jackson.core:jackson-core` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-core/compare/jackson-core-2.18.3...jackson-core-2.19.0)  Updates `com.fasterxml.jackson.core:jackson-databind` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson/commits)  Updates `com.fasterxml.jackson.core:jackson-annotations` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson/commits)  Updates `com.fasterxml.jackson.module:jackson-module-parameter-names` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-modules-java8/compare/jackson-modules-java8-2.18.3...jackson-modules-java8-2.19.0)  Updates `com.fasterxml.jackson.dataformat:jackson-dataformat-yaml` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-dataformats-text/compare/jackson-dataformats-text-2.18.3...jackson-dataformats-text-2.19.0)  Updates `com.fasterxml.jackson.dataformat:jackson-dataformat-smile` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-dataformats-binary/compare/jackson-dataformats-binary-2.18.3...jackson-dataformats-binary-2.19.0)  Updates `com.fasterxml.jackson.dataformat:jackson-dataformat-cbor` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-dataformats-binary/compare/jackson-dataformats-binary-2.18.3...jackson-dataformats-binary-2.19.0)  Updates `com.fasterxml.jackson.dataformat:jackson-dataformat-ion` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-dataformat-ion/commits)  Updates `com.fasterxml.jackson.dataformat:jackson-dataformat-xml` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-dataformat-xml/compare/jackson-dataformat-xml-2.18.3...jackson-dataformat-xml-2.19.0)  Updates `com.fasterxml.jackson.datatype:jackson-datatype-guava` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-datatypes-collections/compare/jackson-datatypes-collections-2.18.3...jackson-datatypes-collections-2.19.0)  Updates `com.fasterxml.jackson.datatype:jackson-datatype-jsr310` from 2.18.3 to 2.19.0  Updates `com.fasterxml.jackson.datatype:jackson-datatype-jdk8` from 2.18.3 to 2.19.0  --- updated-dependencies: - dependency-name: com.fasterxml.jackson:jackson-bom dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.core:jackson-core dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.core:jackson-databind dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.core:jackson-annotations dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.module:jackson-module-parameter-names dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.dataformat:jackson-dataformat-yaml dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.dataformat:jackson-dataformat-smile dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.dataformat:jackson-dataformat-cbor dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.dataformat:jackson-dataformat-ion dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.dataformat:jackson-dataformat-xml dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.datatype:jackson-datatype-guava dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.datatype:jackson-datatype-jsr310 dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.datatype:jackson-datatype-jdk8 dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * chore(execution): update display names for executions. (#8527)  * fix(core): change incorrectly used search parameter (#8534)  Co-authored-by: nKwiatkowski <nkwiatkowski@kestra.io>  * fix(ui): set isCreating to false when opening flow edit mode (#8549)  * fix(core): safely access section and identifier query params (#8542)  Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  * fix(ui): update storybook editor tests with provided keys (#8550)  Co-authored-by: Piyush Bhaskar <102078527+Piyush-r-bhaskar@users.noreply.github.com>  * chore(core): localize to languages other than english (#8554)  Extended localization support by adding translations for multiple languages using English as the base. This enhances accessibility and usability for non-English-speaking users while keeping English as the source reference.  Co-authored-by: GitHub Action <actions@github.com>  * fix(triggers): amend broken filtering on triggers tab (#8553)  There was a problem with both namespace and state filters on Triggers page which is now properly sorted.  Closes https://github.com/kestra-io/kestra/issues/8529.  * chore: attempt to fix flaky tests (#8537)  SingleFlowCommandsTest:  The flow Delete -> Create -> Update sequence is weird - delete got HTTP 404. Reworked to Create -> Update -> Delete sequence.  PurgeLogsTest:  The log repository contained the prepared single entry but also might contain additional entries from previously logged messages.  * fix(namespaces): namespaceFiles with same name are wrongly overwritten (#8562)  Co-authored-by: nKwiatkowski <nkwiatkowski@kestra.io>  * feat(core): forward execution labels in Flow Trigger  * chore(triggers)*: properly handle switches for triggers disabled from within flow source (#8106)  It was not clear as to which trigger can not be enabled and why. Now  that is much more clear with the proper tooltips and disabling of switch toggling.  Closes https://github.com/kestra-io/kestra/issues/8011. Closes https://github.com/kestra-io/kestra/issues/5736.  * fix(executions): fix execution failure due to UnsupportedOperationException (#8563)  Fix: #8563  * chore(core): localize to languages other than english (#8568)  Extended localization support by adding translations for multiple languages using English as the base. This enhances accessibility and usability for non-English-speaking users while keeping English as the source reference.  Co-authored-by: GitHub Action <actions@github.com>  * feat(system): add TestSuite model taskFixture impl  * fix: redirect to edit when saving new flow (#8560)  Co-authored-by: Piyush Bhaskar <102078527+Piyush-r-bhaskar@users.noreply.github.com>  * feat(system)*: decrease defaut JDBC queue poll size  Decreasing it from 100 to 50 didn't show any performance hit but should lower the memory consumption now that we process the queue concurrently in the executor.  ## BEFORE - pollSize=100 - 10 tx/s: 150ms - 25 tx/s: 200ms - 50 tx/s: 300ms - 75 tx/s: 5.2s - 100 tx/s: 15s  ## AFTER - pollSize=50 - 10 tx/s: 150ms - 25 tx/s: 200ms - 50 tx/s: 300ms - 75 tx/s: 4.8s - 100 tx/s: 14s  * feat(flows): Allow to define an onPause task on the Pause task  The onPause task will be executed immediatly when the execution is paused. Part-of: #3601  * feat(core)!: WIP make tenant id required  * feat(core)!: WIP make tenant id required  * Feat/storage outputs (#8361)  * feat(executions): Store outputs inside the internal storage (1/2)  * feat(executions): Store outputs inside the internal storage (2/2)  * feat(test): allow passing tenantId to tests  ---------  Co-authored-by: Ludovic DEHON <tchiot.ludo@gmail.com>  * tests(webserver): fix flaky test which could query previous tests tasks  * feat(executions): Add workerId to each worker task attemps  Closes #7799  * Feat/storage outputs (#8361)  * feat(executions): Store outputs inside the internal storage (1/2)  * feat(executions): Store outputs inside the internal storage (2/2)  * feat(test): allow passing tenantId to tests  ---------  Co-authored-by: Ludovic DEHON <tchiot.ludo@gmail.com>  * wip(core): fix unit tests  * test(core): fix unit tests  * test(core): fix unit tests  * test(core): fix unit tests  * feat(core): make tenant id required everywhere  * feat(core): make tenant required in create user command  * feat(core): clean the PR  * feat(core): add tenant id to dashboard controller  * fix(core): tests after merging  * clean(core): fixes after review  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: nKwiatkowski <nkwiatkowski@kestra.io> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: Roman Acevedo <roman.acevedo62@gmail.com> Co-authored-by: Loïc Mathieu <loikeseke@gmail.com> Co-authored-by: Ludovic DEHON <tchiot.ludo@gmail.com> Co-authored-by: Miloš Paunović <paun992@hotmail.com> Co-authored-by: Piyush Bhaskar <102078527+Piyush-r-bhaskar@users.noreply.github.com> Co-authored-by: Barthélémy Ledoux <ledouxb@me.com> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: GitHub Action <actions@github.com> Co-authored-by: AJ Emerich <aemerich@kestra.io> Co-authored-by: ben8t <46634684+Ben8t@users.noreply.github.com> Co-authored-by: Bart Ledoux <bledoux@kestra.io> Co-authored-by: Satvik Kushwaha <59243339+satvik2131@users.noreply.github.com> Co-authored-by: Karuna Tata <karuna.tata@devrev.ai> Co-authored-by: Hashim Khalifa <105060840+hashimzs@users.noreply.github.com> Co-authored-by: lwyang <1670906161@qq.com> Co-authored-by: 杨利伟 <yangliwei@xiaomi.com> Co-authored-by: Florian Hussonnois <fhussonnois@kestra.io> Co-authored-by: yuri <1969yuri1969@gmail.com> Co-authored-by: AJ Emerich <aj-emerich@proton.me> Co-authored-by: rajatsingh23 <48049052+rajatsingh23@users.noreply.github.com>
kestra-io,kestra,2c53a210d7d3677d01a7e46e92111363c2e5cb6b,https://github.com/kestra-io/kestra/commit/2c53a210d7d3677d01a7e46e92111363c2e5cb6b,feat: Performance optimization for handle() method: Filter out executions with non-null next_execution_date in the query method  start a separate scheduled thread for scanning  reporting metrics and logging
kestra-io,kestra,476f34e98689845dfec2478fd81026fe4e76d4dc,https://github.com/kestra-io/kestra/commit/476f34e98689845dfec2478fd81026fe4e76d4dc,feat(system): change the way we concurrently process executor queues  Instead of consuming multiple time the queue  which lead to concurrent queries on the `queues` table  process concurrently via an ExecutorService the messages from the queue. We dind't process a new batch of messages until the existing one is totally process to be sure we process in FIFO the same execution message.  Also  go back to a poll size of 100 to mitiguate the performance hit due to this change.
kestra-io,kestra,3639abb8bb8363af95e29b9183a118f36b195f88,https://github.com/kestra-io/kestra/commit/3639abb8bb8363af95e29b9183a118f36b195f88,chore(system): improve performance of IdUtils.fromParts()  Surprisingly  this method appear in some CPU and allocation profile as having a high cost  especially on the scheduler. Switching to using a StringJoiner brings 4x perf improvements in method execution time (great improvement also on allocation but didn't have a measurement).
kestra-io,kestra,4e602021a830e345a310bcce1f1132cc31f45c32,https://github.com/kestra-io/kestra/commit/4e602021a830e345a310bcce1f1132cc31f45c32,feat(system)*: decrease defaut JDBC queue poll size  Decreasing it from 100 to 50 didn't show any performance hit but should lower the memory consumption now that we process the queue concurrently in the executor.  ## BEFORE - pollSize=100 - 10 tx/s: 150ms - 25 tx/s: 200ms - 50 tx/s: 300ms - 75 tx/s: 5.2s - 100 tx/s: 15s  ## AFTER - pollSize=50 - 10 tx/s: 150ms - 25 tx/s: 200ms - 50 tx/s: 300ms - 75 tx/s: 4.8s - 100 tx/s: 14s
kestra-io,kestra,fa07cbd3b9fce507a4feff0b14f4aa00f986d94a,https://github.com/kestra-io/kestra/commit/fa07cbd3b9fce507a4feff0b14f4aa00f986d94a,feat(core): improve performance of ExecutorService.handleChildWorkerTaskResult  Searching for a retry in all parents is a costly operation  doing it only wgen we are retrying or failing avoid it most of the time.
kestra-io,kestra,fb9691d67a96c20e7ed0041faa839dc9487822a7,https://github.com/kestra-io/kestra/commit/fb9691d67a96c20e7ed0041faa839dc9487822a7,chore(core): avoid using applicationContext.init() in the RunContext  This will improve performance as a run context is created very often.  Fixes #5492
kestra-io,kestra,34fa6ce9103e11eddebe4cd5640d289020638fb4,https://github.com/kestra-io/kestra/commit/34fa6ce9103e11eddebe4cd5640d289020638fb4,feat(jdbc): Improve execution queued performance  Add date inside the index to speed up order by in case there are a lot of execution queued. Skip locked records when selecting them as if there is a locked records it means you need to pop the next one.
kestra-io,kestra,1d7982406c5cfb31a7278b032337822c58a75fd1,https://github.com/kestra-io/kestra/commit/1d7982406c5cfb31a7278b032337822c58a75fd1,feat(jdbc-postgres): improve JSONB performance
JetBrains,intellij-community,cf1d1ab88a947be9663365700840ca6e46df4444,https://github.com/JetBrains/intellij-community/commit/cf1d1ab88a947be9663365700840ca6e46df4444,IJPL-186227 platform: disable debug color markers by-default  The performance impact might be (or not be) exaggerated by the profiler overhead  but there's no reason to keep it for everyone.  GitOrigin-RevId: 31f53201d03b76318da178bfe348f00572c20f14
JetBrains,intellij-community,ce7297003b00cb178c5014516b72fcc2ac73a1d4,https://github.com/JetBrains/intellij-community/commit/ce7297003b00cb178c5014516b72fcc2ac73a1d4,IJPL-186227 editor: do not cache TextAttributes during Editor rendering  We do not save memory by doing that  only make the cache bigger. This also saves CPU cycles on the cache map access.  This became an issue after introducing ComparableColor with slightly slower equality. This fixes 'EditorPaintingPerformanceTest.testScrollingThroughLongTextFile'  GitOrigin-RevId: 09885db0335b96a9adbf6771951986d0827fc1b5
JetBrains,intellij-community,5963235ef9b2dd4ce3d62b36a436fedd3a7f5c38,https://github.com/JetBrains/intellij-community/commit/5963235ef9b2dd4ce3d62b36a436fedd3a7f5c38,introduce ContentManager.addUiDataProvider  Old API is a performance problem.  GitOrigin-RevId: bcd953d84601c82500e0718a12e044ae6b1675f0
JetBrains,intellij-community,ef6a53e148c8ada1121b11159d2ce6adb873c582,https://github.com/JetBrains/intellij-community/commit/ef6a53e148c8ada1121b11159d2ce6adb873c582,IJPL-185303 Do not spam UiNotifyConnector instances in toolbars  When a toolbar is added/removed many times to a parent that is not currently showing  on every addNotify it would add a listener through UiNotifyConnector. Eventually there can be too many such listeners to handle.  Fix by using launchOnceOnShow instead and saving the returned job. If there's already a job scheduled  simply do nothing. Remove the job to save memory when we're done. This  of course  has a side effect of launching the same job again if the toolbar is removed and then added. That is not an issue because updateActionsFirstTime() is a no-op unless it's a really first update  so subsequent jobs won't do anything. And because there's at most one job  it'll never become a performance problem.  Dispatchers.EDT is needed in launchOnceOnShow because updateActionsImmediately() is a legacy API that requires both EDT and read actions. By default launchOnceOnShow uses Dispatchers.UI that prohibits read actions.  GitOrigin-RevId: 97f77a0d8192a0457c1a7e1e9395c21534705800
JetBrains,intellij-community,9881b881e5f325ca709053485d3be5460809a952,https://github.com/JetBrains/intellij-community/commit/9881b881e5f325ca709053485d3be5460809a952,IJPL-184075 Validate the terminal editor size when changing the font  The standard editor implementation doesn't work because it has a bug: it doesn't update the size immediately  because it calls invalidate() and not validateSize(). This is not a big deal for the editor  as it almost never uses a point that's close to the bottom  and it's actually beneficial in terms of performance. But it's a deal breaker for the terminal  as increasing the font size never scrolls correctly. Fix by introducing a flag therefore.  GitOrigin-RevId: 9b5ba77a5aa314b9ca8d3a997b5705fc54b7bea7
JetBrains,intellij-community,01ebbc638e4907662d93afda264107fab8d823b0,https://github.com/JetBrains/intellij-community/commit/01ebbc638e4907662d93afda264107fab8d823b0,[indexing-api] SingleTargetRequestResultProcessor: save service in the field  The processor is created per search session in background thread  so it's a relatively short-living thing. It's unlikely that the service should be unloaded during its function. On the other hand  processTextOccurrence might be called too often  which affects the overall search performance. May help IDEA-368981 Renaming package never finishes  stuck "Looking for Usages"  GitOrigin-RevId: f3f9643fd47a948997dbebde05f10b344e63ba53
JetBrains,intellij-community,35953e3d8b0886de60845f9187a3280ca936ff10,https://github.com/JetBrains/intellij-community/commit/35953e3d8b0886de60845f9187a3280ca936ff10,wrap LOG.debug with a guard for performance  GitOrigin-RevId: d102c1ef88db6d76750f55f4991af30771a2eda4
JetBrains,intellij-community,aaa808c3b4bc9d7d3b5a73177c5580948af60cc0,https://github.com/JetBrains/intellij-community/commit/aaa808c3b4bc9d7d3b5a73177c5580948af60cc0,Revert "PY-79480 Resolve attribute reference to ancestor attributes in constructors if they contain type annotations"  Fixes PY-79997 performance regression  GitOrigin-RevId: 9bccec0543fcdf7970311c187bd9fd9b1357c058
JetBrains,intellij-community,b66992f8eac1483b0576dc6b5b1f065427fd1675,https://github.com/JetBrains/intellij-community/commit/b66992f8eac1483b0576dc6b5b1f065427fd1675,IJPL-182127 eliminating potential performance degradation points  GitOrigin-RevId: aecf12a1de8660397e28af5216fe61eb088c3b62
JetBrains,intellij-community,d9c593bfb744289dfdf83881cdd8966cd4790067,https://github.com/JetBrains/intellij-community/commit/d9c593bfb744289dfdf83881cdd8966cd4790067,IJPL-164422 DocumentImpl: do not create new changedPart if it's a String  This code was initially written to avoid dragging along with the changed part some huge chunks of text in case this value is some complicated implementation of CharSequence. But if it's just a String  turning it into a smart immutable char sequence has no benefit but may slow down things.  Notably  this change noticeably improves the performance of LineSet.isSingleLineChange.  GitOrigin-RevId: 86ef29d3ba08dc7eaf7a51127449afcb92ae6d27
JetBrains,intellij-community,b0825ccdbb002a9c1e3de6afbff441ec85d48650,https://github.com/JetBrains/intellij-community/commit/b0825ccdbb002a9c1e3de6afbff441ec85d48650,[performance] IJPL-181579 Report triggered slow operation issues to FUS  GitOrigin-RevId: 7187af7dc142f2637a896fd0739dcd4a17f3e916
JetBrains,intellij-community,ccf87d7e638a1ad404ebebd517c403406a98e67a,https://github.com/JetBrains/intellij-community/commit/ccf87d7e638a1ad404ebebd517c403406a98e67a,[performance] IJPL-181579 Report triggered slow operation issues to FUS  GitOrigin-RevId: 6e22bb1b69f75fa0a17f5e48157f696e6eeff067
JetBrains,intellij-community,b94deb1c91c2aa000f70459e12bde3723792d75e,https://github.com/JetBrains/intellij-community/commit/b94deb1c91c2aa000f70459e12bde3723792d75e,Eel: fix Provider mismatch: WindowsFileSystemProvider != TracingFileSystemProvider(WindowsFileSystemProvider)  This error appeared in tests:  ``` java.lang.IllegalArgumentException: Provider mismatch: sun.nio.fs.WindowsFileSystemProvider@76cf91c9 != TracingFileSystemProvider(sun.nio.fs.WindowsFileSystemProvider@76cf91c9) at com.intellij.platform.core.nio.fs.MultiRoutingFileSystemProvider.getDelegate(MultiRoutingFileSystemProvider.java:197) at com.intellij.platform.core.nio.fs.DelegatingFileSystemProvider.copy(DelegatingFileSystemProvider.java:198) at java.base/java.nio.file.Files.copy(Files.java:1305) at com.intellij.platform.ijent.performance.benchmarks.IJentWslZipFileReadBenchmarkTest$measureReadFile$1.invokeSuspend(IJentWslZipFileReadBenchmarkTest.kt:48) at _COROUTINE._BOUNDARY._(CoroutineDebugging.kt:42) at com.intellij.platform.ijent.performance.BenchmarkKt$runIJentBenchmark$1.invokeSuspend(benchmark.kt:42) Caused by: java.lang.IllegalArgumentException: Provider mismatch: sun.nio.fs.WindowsFileSystemProvider@76cf91c9 != TracingFileSystemProvider(sun.nio.fs.WindowsFileSystemProvider@76cf91c9) at com.intellij.platform.core.nio.fs.MultiRoutingFileSystemProvider.getDelegate(MultiRoutingFileSystemProvider.java:197) at com.intellij.platform.core.nio.fs.DelegatingFileSystemProvider.copy(DelegatingFileSystemProvider.java:198) at java.base/java.nio.file.Files.copy(Files.java:1305) at com.intellij.platform.ijent.performance.benchmarks.IJentWslZipFileReadBenchmarkTest$measureReadFile$1.invokeSuspend(IJentWslZipFileReadBenchmarkTest.kt:48) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.internal.ScopeCoroutine.afterResume(Scopes.kt:36) at kotlinx.coroutines.AbstractCoroutine.resumeWith(AbstractCoroutine.kt:101) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:46) at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:100) at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:263) at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:112) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$BuildersKt__BuildersKt(Builders.kt:85) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:53) at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source) at com.intellij.testFramework.common.TimeoutKt.timeoutRunBlocking-rnQQ1Ag(timeout.kt:25) at com.intellij.testFramework.common.TimeoutKt.timeoutRunBlocking-rnQQ1Ag$default(timeout.kt:16) at com.intellij.platform.ijent.performance.BenchmarkKt.runIJentBenchmark(benchmark.kt:39) at com.intellij.platform.ijent.performance.benchmarks.IJentWslZipFileReadBenchmarkTest.measureReadFile(IJentWslZipFileReadBenchmarkTest.kt:30) at com.intellij.platform.ijent.performance.benchmarks.IJentWslZipFileReadBenchmarkTest.measureZipFileReading(IJentWslZipFileReadBenchmarkTest.kt:56) at com.intellij.platform.ijent.performance.benchmarks.IJentWslZipFileReadBenchmarkTest.java zip file(IJentWslZipFileReadBenchmarkTest.kt:84) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at com.intellij.ide.starter.junit5.CurrentTestMethodArgumentsProvider.intercept(CurrentTestMethodArgumentsProvider.kt:53) at com.intellij.ide.starter.junit5.CurrentTestMethodArgumentsProvider.interceptTestTemplateMethod(CurrentTestMethodArgumentsProvider.kt:23) at com.intellij.testFramework.junit5.impl.TestLoggerInterceptor.intercept$lambda$0(TestLoggerInterceptor.kt:12) at com.intellij.testFramework.TestLoggerKt.recordErrorsLoggedInTheCurrentThreadAndReportThemAsFailures(testLogger.kt:86) at com.intellij.testFramework.junit5.impl.TestLoggerInterceptor.intercept(TestLoggerInterceptor.kt:11) at com.intellij.testFramework.junit5.impl.AbstractInvocationInterceptor.interceptTestTemplateMethod(AbstractInvocationInterceptor.kt:37) at com.intellij.platform.ijent.testFramework.functional.LogTestName.runTest(IjentTestUtil.kt:63) at com.intellij.platform.ijent.testFramework.functional.LogTestName.interceptTestTemplateMethod(IjentTestUtil.kt:30) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708) at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762) at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:1024) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base/java.util.ArrayList.forEach(ArrayList.java:1596) at java.base/java.util.ArrayList.forEach(ArrayList.java:1596) ```  GitOrigin-RevId: 690ff7ad7dca00f46bf577c8f5819f3e0068b380
JetBrains,intellij-community,3c3590110fc9472d365bfa1c441e36345f9455e3,https://github.com/JetBrains/intellij-community/commit/3c3590110fc9472d365bfa1c441e36345f9455e3,do not restart the entire highlighting after the lazy quickfix registered  Instead  re-launch the additional ShowIntentionsPass to include that newly computed quickfix into the current intention info Restarting the entire highlighting was bad for performance and led to endlessly restarting CWM tests  GitOrigin-RevId: 9b86d4631b74b2c9632cd64cc518df3125d84b8c
JetBrains,intellij-community,aafc2dc34115187c438af8dbfbd9a5d1e69f54e1,https://github.com/JetBrains/intellij-community/commit/aafc2dc34115187c438af8dbfbd9a5d1e69f54e1,IJPL-179246 Highlighting sometimes disappears after typing when Find tool window is open  Sometimes a highlighting pass should be run twice  e.g. when two file editors are opened for the same document. When two GeneralHighlightingPasses for the same document are run reentrantly  they can compete for markup model  causing flicker as pass1 trying to reuse RangeHighlighter which pass2 already reused. Also  running two identical piece of computation is bad performance-wise. So when the document changed  we run highlighting passes for all file editors containing this document  but document-based passes are run only once per document  whereas editor-bound passes are run per file editor. This document-bound pass is run only once  while its instances in all other file editors are waiting for the first instance to complete  instead of calling its own collectInformation(). We assume that ProgressableTextEditorHighlightingPass inheritors are document-bound passes.  GitOrigin-RevId: 85343ab33f53ba8b8d61342d2de3131be7650c6f
JetBrains,intellij-community,e804b7c38c8cee5933ab483e9bab56bd6ebd535a,https://github.com/JetBrains/intellij-community/commit/e804b7c38c8cee5933ab483e9bab56bd6ebd535a,IJPL-164422 Optimized soft wraps for cell grid editors  When outputting large text into a terminal  it spends about 30-40% of the time calculating soft wraps. Most lines  however  don't have inlays  and because characters are aligned to cells  no font metrics is required to calculate soft wraps. We only need to know the number of columns and for every character wither it's a double-width one or not.  Introduce a special soft wrap mode that automatically kicks in as soon as the grid mode is enabled and there are no inlays in the range.  To avoid creating an iterator just to check whether there are inlays  extract inlay calculation from the constructor.  To avoid performance issues caused by repetitive codePointAt() invocations  make a copy of the whole thing using toString() instead. This will create garbage  of course  but the performance benefit seems to be very well worth it.  GitOrigin-RevId: d34e7522e32691faf70e5dd3e921dfb4856bbac7
JetBrains,intellij-community,f84c828a80e1506cacfb1dde9dce98c87a6a54af,https://github.com/JetBrains/intellij-community/commit/f84c828a80e1506cacfb1dde9dce98c87a6a54af,Avoid expensive operations if the module build script classpath is the same object  This results in a performance boost the module build classpaths are already interned in the cases where they are the same.  Also caching some jar related operations becuase a file lookup is being done.  closes https://github.com/JetBrains/intellij-community/pull/2957  GitOrigin-RevId: a48310c79379734cd028aa60defbc3f07a15dc80
JetBrains,intellij-community,426831b2deab8fe5c9536a324fe68837fa0f711d,https://github.com/JetBrains/intellij-community/commit/426831b2deab8fe5c9536a324fe68837fa0f711d,fixup! IJPL-165650 [vcs] Introduce `VcsDirtyScopeManager#rootDirty` for requesting update on a specific VCS root  The existing `VcsDirtyScopeManager#dirDirtyRecursively` method marks all internal repositories as dirty  which limits performance optimizations for Git integration.   (cherry picked from commit fc4e357d83412fbd23fb38f21e755973b6321582)  IJ-MR-154527  GitOrigin-RevId: 4a8be33c7f746399f1ff98b52aee1e440e51c4a0
JetBrains,intellij-community,18aa0440e9e814391a275a3be1b09a797b833ff7,https://github.com/JetBrains/intellij-community/commit/18aa0440e9e814391a275a3be1b09a797b833ff7,fixup! IJPL-165650 [vcs] Introduce `VcsDirtyScopeManager#rootDirty` for requesting update on a specific VCS root  The existing `VcsDirtyScopeManager#dirDirtyRecursively` method marks all internal repositories as dirty  which limits performance optimizations for Git integration.   (cherry picked from commit bd36156fa21ca49d59d348a692e0de2ec4321cf2)  IJ-MR-154527  GitOrigin-RevId: c9c0b6eee0e00ec7dd5d0270d4ff900078b27117
JetBrains,intellij-community,dd87909b676ec29cff20bacad0b45a367dd9f5e3,https://github.com/JetBrains/intellij-community/commit/dd87909b676ec29cff20bacad0b45a367dd9f5e3,IJPL-165650 [vcs] Introduce `VcsDirtyScopeManager#rootDirty` for requesting update on a specific VCS root  The existing `VcsDirtyScopeManager#dirDirtyRecursively` method marks all internal repositories as dirty  which limits performance optimizations for Git integration.   (cherry picked from commit 9f5cc85ce2ab10461ed50d276717633253c14069)  IJ-MR-154527  GitOrigin-RevId: 89209a830c41d2c94ce5e27a9a3501b59cadf116
JetBrains,intellij-community,0d621e44e8ea32d53d140eb6b67302b7c9a810f3,https://github.com/JetBrains/intellij-community/commit/0d621e44e8ea32d53d140eb6b67302b7c9a810f3,[ui] Improve performance in ColorUtil.toHex by avoiding unnecessary string concatenation  GitOrigin-RevId: 53b3ad78f328c2581b941e41532e071db8ace9fb
JetBrains,intellij-community,e2fba18524262048f94094d3f77dbf3948dc93fb,https://github.com/JetBrains/intellij-community/commit/e2fba18524262048f94094d3f77dbf3948dc93fb,IJPL-339 IDEA-367535 fix performance degradation  Inferring all classes from scratch is rather slow. Returning the original behaviour for non-multiverse case. Investigation of the proper cache is to be done.  GitOrigin-RevId: e6134ade61f36f458fc3214d1ae54180727bc2b2
JetBrains,intellij-community,1c7a08a850284d0f4cb06e06329c9be7730ca048,https://github.com/JetBrains/intellij-community/commit/1c7a08a850284d0f4cb06e06329c9be7730ca048,[threading] IJPL-148438: Extract performance monitoring from `AnyThreadWriteThreadingSupport`  GitOrigin-RevId: ceb051de5e2e773cf71f561d97334b144b107eb8
JetBrains,intellij-community,6da62a69370974abb6128eb8cea11601bddc8d19,https://github.com/JetBrains/intellij-community/commit/6da62a69370974abb6128eb8cea11601bddc8d19,[debugger] IDEA-366895 Debugger: collect performance statistics on the command execution time  GitOrigin-RevId: 1de07f7dba0a674838012e1ba228c3356febf93e
JetBrains,intellij-community,fcc1eda6ff1aa3774fdeac5e0ba1641932c49b14,https://github.com/JetBrains/intellij-community/commit/fcc1eda6ff1aa3774fdeac5e0ba1641932c49b14,IJPL-173321 Refactor ScopeEditorPanel: use selection paths directly  Using rows and then converting them to paths looks off by itself  as we don't use the row indices anywhere. Moreover  in the default Swing implementation  the getSelectionRows() function actually takes the selection paths and then takes some extra effort to convert them to rows  which we then convert back to paths. The performance impact may or may not be significant  but given that we can both get rid of it and simplify the code  it doesn't really matter.  GitOrigin-RevId: dab8e7df75e01eda099768bd9c4fbe2975e66d07
JetBrains,intellij-community,2f43f915a6dadd33057f7d337beffbf32e148fd2,https://github.com/JetBrains/intellij-community/commit/2f43f915a6dadd33057f7d337beffbf32e148fd2,[core] disable new `FilePageCache`  + `FilePageCacheLockFree` was developed to replace regular `FilePageCache` but that project was postponed for a long time in favor of using memory-mapped files for performance-critical storages. Current storages don't use `FilePageCacheLockFree`  hence it pays off disabling it and yield ~120Mb of native memory to regular `FilePageCache`  GitOrigin-RevId: ad6ceb85f891e6e737972f208c276582789cdb4f
JetBrains,intellij-community,7a256e2136e3874f2d84bffb8444db971ef7d1f3,https://github.com/JetBrains/intellij-community/commit/7a256e2136e3874f2d84bffb8444db971ef7d1f3,[java] CallMapper: optimize mapFirst for UAST  `isMethodNameOneOf` may significantly improve performance for some languages like Kotlin where `methodName` call can be expensive  ^IDEA-316635  GitOrigin-RevId: 33a016ebef048760576e2a41364b0de29341cc75
JetBrains,intellij-community,6aedab6f4ccfb601b8fb5d0cfbdd078805aa6d21,https://github.com/JetBrains/intellij-community/commit/6aedab6f4ccfb601b8fb5d0cfbdd078805aa6d21,[performance] IJPL-173892 FUS: add project count  oom error flag and last action to low.memory events  GitOrigin-RevId: 048a9136700b3d9741cd21cb74260e9a8e62a16b
JetBrains,intellij-community,0cc2444ba268291fd625af1e71e6e81058950260,https://github.com/JetBrains/intellij-community/commit/0cc2444ba268291fd625af1e71e6e81058950260,[eel  jps] IJPL-172886: Disable preloading of build process for non-local JPS projects  With preloading enabled  incremental compilation in JPS does not work correctly. I did not research why  but here we can sacrifice possible performance for correctness and observability.  GitOrigin-RevId: ee27f1f40279813c5363fc19eca3c841d23e1cb9
JetBrains,intellij-community,294c88168e0fc2cc3c386fe57cb4f76dc5c9653a,https://github.com/JetBrains/intellij-community/commit/294c88168e0fc2cc3c386fe57cb4f76dc5c9653a,IJPL-173405 [regression] Degradation in com.intellij.openapi.vfs.encoding.FileEncodingTest.testEncodingReDetectionRequestsOnDocumentChangeAreBatchedToImprovePerformance - encoding re-detect requests  GitOrigin-RevId: 94c465dc3726ee9eaca55399870343a74856de9e
JetBrains,intellij-community,50fa7b031d92f02c63721ce23f995fd48390d54a,https://github.com/JetBrains/intellij-community/commit/50fa7b031d92f02c63721ce23f995fd48390d54a,a bit of black magic to possibly help IJPL-173405 [regression] Degradation in com.intellij.openapi.vfs.encoding.FileEncodingTest.testEncodingReDetectionRequestsOnDocumentChangeAreBatchedToImprovePerformance - encoding re-detect requests  GitOrigin-RevId: 4c12e90c0fb31ae08c1605b48e74ca418f85a617
JetBrains,intellij-community,4fba5ac5007e1b9a3f3819408a68606f1e6b2d20,https://github.com/JetBrains/intellij-community/commit/4fba5ac5007e1b9a3f3819408a68606f1e6b2d20,mark naive recursive psi element visitors as such  to be able to catch more performance errors  GitOrigin-RevId: 3c28a6d1061d6d861dcc78e55b1786742c190d27
JetBrains,intellij-community,72adf36a7f3aed5ca4f7d81506c962db64dff941,https://github.com/JetBrains/intellij-community/commit/72adf36a7f3aed5ca4f7d81506c962db64dff941,[json] IJPL-172038 Prefer simple thread local cache instead of ReadActionCache to reduce FUS collection performance impact  GitOrigin-RevId: 0788ef02d6384176e0ed6fec8037c3f63b656fb9
JetBrains,intellij-community,92a193881f5f0c7ced2bc3cc35cbc401ae7eeb9b,https://github.com/JetBrains/intellij-community/commit/92a193881f5f0c7ced2bc3cc35cbc401ae7eeb9b,performance: do not mess with slow regexes in debug log  GitOrigin-RevId: 398ed69ca0154a798bbcd2d8b501a17f97eebc86
JetBrains,intellij-community,37d58601b9a3d125a2eb88a1aedf237850f8ba74,https://github.com/JetBrains/intellij-community/commit/37d58601b9a3d125a2eb88a1aedf237850f8ba74,[performance] IJPL-161370 Detect UI freezes caused by third-party plugins and disable them  GitOrigin-RevId: 57013d91d5767602b1cb6db72d644e83bed2339b
JetBrains,intellij-community,9765a5c4ceb875b02d72c90b405e958b96dbd01f,https://github.com/JetBrains/intellij-community/commit/9765a5c4ceb875b02d72c90b405e958b96dbd01f,[performance] A lot of memory allocated from InspectionVisitorOptimizer.getTargetPsiClasses for lambdas  GitOrigin-RevId: a7f330d3fd1633ee1d17040ba114a0daf4a42eb1
JetBrains,intellij-community,31ec0fab5bf50d97e4c30132497f80fa023f55c3,https://github.com/JetBrains/intellij-community/commit/31ec0fab5bf50d97e4c30132497f80fa023f55c3,[kotlin] Implement a combinable scope for source and class roots as a replacement for `ModuleWithDependenciesScope`s and to combine library scopes  - Uncombined (unions of) `ModuleWithDependenciesScope` and `LibraryWithoutSourceScope` are heavily inefficient because the `contains` function requires getting the virtual file's file info for each `contains` call. When we have uncombined scopes  the file info may be requested hundreds of times. A combined scope allows getting the file info only once  and then check the roots. - The new `ModuleSourcesScope` and the existing `LibraryWithoutSourceScope` are easily combinable  since we just have to create a combined roots map. There is no other complex magic going on to construct an efficient combined scope. - Combined scopes have a large positive impact on performance in various test cases where we have a complex module structure with many dependencies. In fact  in some of the test cases we cannot feasibly optimize anything else from snapshots since uncombined scopes are such a huge drag on performance. - Changing `ModuleWithDependenciesScope` and the default scope provided by `Module.moduleProductionSourceScope` and `moduleTestSourceScope` in the platform is difficult  because there are internal and external usages of `ModuleWithDependenciesScope`. It is supposed to be an implementation detail  but various usages cast `GlobalSearchScope`s to `ModuleWithDependenciesScope`s to get the module from the scope. Hence  it is currently much easier to make a change limited to the Kotlin plugin and reap the performance benefits now. - This has the disadvantage that we'll have duplicate implementations for `AbstractVirtualFileRootsScope` and `ModuleWithDependenciesScope`  so we should still push for integration into the platform.  ^KT-57733  GitOrigin-RevId: 0ca6b15f78803af9a47fd229b9b650368f95ca87
JetBrains,intellij-community,4819a59dee826ab50b6ecea4c85d9b0d6d774d87,https://github.com/JetBrains/intellij-community/commit/4819a59dee826ab50b6ecea4c85d9b0d6d774d87,[performance] IJPL-162450 FUS: provide low memory condition event  current size of heap and memory type  GitOrigin-RevId: aa6027c0e5651114edfa86b105af9e00bb1fbeb1
JetBrains,intellij-community,007db6c60ecb2fec90c270108fdd11116689d9db,https://github.com/JetBrains/intellij-community/commit/007db6c60ecb2fec90c270108fdd11116689d9db,RelaxNg: Avoid NPEs to improve performance when calculating element descriptors  GitOrigin-RevId: 9260e29679da728aeba7119accf2214b5dcfd460
JetBrains,intellij-community,846941b57136ee6264efe2ac325a8cc35b85efd4,https://github.com/JetBrains/intellij-community/commit/846941b57136ee6264efe2ac325a8cc35b85efd4,[matcher] fallback typo-tolerant matcher to regular matcher for long patterns  After some optimizations  for the sake of performance  a matcher stopped matching anything for long patterns. Keeping this optimization  now we fall back to the regular matcher to match at least something  even without tolerance to typos  GitOrigin-RevId: 8c3be92bccd5eb6e382df32394cc27f95113e3e4
JetBrains,intellij-community,858791cf180e0536e6749c7ad171ee96b8022eb4,https://github.com/JetBrains/intellij-community/commit/858791cf180e0536e6749c7ad171ee96b8022eb4,IDEA-354490 Performance improvements  GitOrigin-RevId: 30221b63ba9b6b8780e0d9451266c0cc032abf4b
JetBrains,intellij-community,651151bf6829c889400f09385eb675a2e4f7dd9f,https://github.com/JetBrains/intellij-community/commit/651151bf6829c889400f09385eb675a2e4f7dd9f,Add canceled/applied data for IJPL-55674 Performance metrics: Search Everywhere opening  GitOrigin-RevId: a550694f4c066c09c9ee22f9c511d388a885a8d4
JetBrains,intellij-community,e3bc6676d6a0b20e75b1447fc13ad78f54337040,https://github.com/JetBrains/intellij-community/commit/e3bc6676d6a0b20e75b1447fc13ad78f54337040,Time from the very start till the first item for IJPL-55674 Performance metrics: Search Everywhere opening  GitOrigin-RevId: 45bdbdf055f6814c581ed4134c0ffca5333b77ac
JetBrains,intellij-community,3a2a6caf80d8efabdc5cf5451e236a6fca2f0a01,https://github.com/JetBrains/intellij-community/commit/3a2a6caf80d8efabdc5cf5451e236a6fca2f0a01,Time from the very start till popup disposal for IJPL-55674 Performance metrics: Search Everywhere opening  GitOrigin-RevId: b1aa6bdbc21c8979b2e126438aa5b0525f3476bd
JetBrains,intellij-community,a818ae54ce057d486e900c2a7c3ab8b805461e69,https://github.com/JetBrains/intellij-community/commit/a818ae54ce057d486e900c2a7c3ab8b805461e69,IJPL-161292 IJent: MultiRoutingFsPath extends sun.nio.fs.BasicFileAttributesHolder  This interface is used in VFS refreshing  allowing to avoid some stat system calls. Paths from the default file system implement this interface. When MultiRoutingFileSystem became the default nio filesystem  paths lose this interface  causing a severe performance degradation.  GitOrigin-RevId: 0c53854c1bcfa1461d96fa0e8da16f1650f1e471
JetBrains,intellij-community,2039673d1b82a5372bc59ee05c5e322b96add41e,https://github.com/JetBrains/intellij-community/commit/2039673d1b82a5372bc59ee05c5e322b96add41e,[performanceTests] Add metric for afterShown method  For VCS widget the method takes about 1-3 seconds and it makes sense to track it as well  GitOrigin-RevId: d2707c09c2fa96db2f2870a36d14997706357875
JetBrains,intellij-community,5abb2bda0d2ea8d1931dd0d024b2a91e1526047a,https://github.com/JetBrains/intellij-community/commit/5abb2bda0d2ea8d1931dd0d024b2a91e1526047a,[java-analysis] VariableAccessUtils.getVariableReferences: do not query LocalRefUseInfo for non-physical files  Non-physical copies are usually created for a single purpose (completion  or a single ModCommand quick-fix to apply)  so computing all the references is likely not useful and only creates performance overhead. Should fix IDEA-357624 Slow code completion in large Java file in lines being assigned to "final var"  GitOrigin-RevId: c942fb2a91bd3bcbd4cb67d48f2801e256ab64a1
JetBrains,intellij-community,368a0df14cba11bd3978f00a704bbc16a824cf91,https://github.com/JetBrains/intellij-community/commit/368a0df14cba11bd3978f00a704bbc16a824cf91,PY-72690 Slow code analysis for Python code using many TypedDict  Performance snapshot shows that an enormous amount of time is spent calculating hashcodes for types.  GitOrigin-RevId: 47d488ada253c1aa78ae247d45af50cf4a553426
JetBrains,intellij-community,e555a9d95a7bb338e52f6e77b888e12acc72d285,https://github.com/JetBrains/intellij-community/commit/e555a9d95a7bb338e52f6e77b888e12acc72d285,[terminal] report terminal startup performance/responsiveness metrics (IJPL-159892)  GitOrigin-RevId: f4699f7023655a5a2d1e09ad40a9fb43cd0f726e
JetBrains,intellij-community,341aea199155817c707cc32c27c5adc6037db16e,https://github.com/JetBrains/intellij-community/commit/341aea199155817c707cc32c27c5adc6037db16e,[Kotlin  Java] fix performance in completion by disabling Java-specific `com.intellij.codeInsight.completion.DeprecatedSkipper` in non-Java languages  ^KTIJ-31014 fixed   (cherry picked from commit 5ae85bc4f0f7b27965595605bcf5d0fcb06c996d)  IJ-MR-142759  GitOrigin-RevId: fa8733e26b0d43bd0d9c6735da8ac110e5e2765e
JetBrains,intellij-community,0a5b243b2843eb472829bb1f95c97d8bd122243a,https://github.com/JetBrains/intellij-community/commit/0a5b243b2843eb472829bb1f95c97d8bd122243a,IJPL-159657 Fix performance issue with UrlFilter  After b77168e2 (related to KTIJ-29334)  Url filter started using url filter each time  even if file url is present. That makes the performance test of this filter with only file urls slower. Here I've added some checks before using an expensive URL filter  ^IJPL-159657 Fixed  GitOrigin-RevId: 30651ed33375244feb9e34173c6162387241360c
JetBrains,intellij-community,826ef9e8bf52878908c7455de6dba36a12dd7e26,https://github.com/JetBrains/intellij-community/commit/826ef9e8bf52878908c7455de6dba36a12dd7e26,avoid highlighting the same variable twice  for correctness and performance  GitOrigin-RevId: 6fefd372be01af9f89be1ca35020afd23b943cd8
JetBrains,intellij-community,58bcc6564da78c08882447846ccd36b90ffdc3f9,https://github.com/JetBrains/intellij-community/commit/58bcc6564da78c08882447846ccd36b90ffdc3f9,[performanceTests] Don't close SE on focus lost in perf tests  Otherwise  the command doesn't function properly since the following happens: 1. We open SE  type something  select first file 2. We open SE again but in the meantime file is opened and takes the focus  we close SE  This case can be fixed by subscription to FileEditorManagerListener but the case when we select already opened file can't. Since in this case  there is no listener or callback that will indicate that the focus is moved to the editor.  GitOrigin-RevId: 0e7959443fcd823ff9ddf10b121fa38e257f38cf
JetBrains,intellij-community,d458e5c2da20dc0fc9126507ce2178d3f1a8d607,https://github.com/JetBrains/intellij-community/commit/d458e5c2da20dc0fc9126507ce2178d3f1a8d607,[watcher]Added ability to create span in PerformanceWatcher  GitOrigin-RevId: 6e8471045d166c54eb76ce0b4ad4c528a2b29c0e
JetBrains,intellij-community,258d39252e13969ba2c514ffb7e5e367e7c449bd,https://github.com/JetBrains/intellij-community/commit/258d39252e13969ba2c514ffb7e5e367e7c449bd,[debugger] Force minimizing coroutines view at the start of the debug session  Now it has significant performance problems and low user value.  GitOrigin-RevId: 3d8c351ad84b16f76ef37ef0f2cd20db86cc60ac
JetBrains,intellij-community,cdf91bf213fd1012cb40626db22f47cb01cb145c,https://github.com/JetBrains/intellij-community/commit/cdf91bf213fd1012cb40626db22f47cb01cb145c,[benchmarks] Renaming PerformanceTest* => Benchmark*  GitOrigin-RevId: 9963b84d51e1062acc262a8d3d3de1409a708e3b
JetBrains,intellij-community,1aaaf777817ff764bbd941df776069482d3ea712,https://github.com/JetBrains/intellij-community/commit/1aaaf777817ff764bbd941df776069482d3ea712,[pycharm] restrict analysis in order to improve completion performance  GitOrigin-RevId: 1c2427d1dbb07d88672347311ec5d6f362881847
JetBrains,intellij-community,ee26890170d42fb245091c72d8df48b077c18c64,https://github.com/JetBrains/intellij-community/commit/ee26890170d42fb245091c72d8df48b077c18c64,todo: support searching for todos inside a range  to improve performance (part of IJPL-28717 Todo line lose coloring)  GitOrigin-RevId: e3d44ee80f342985497740fffbe7a76eae5c747e
JetBrains,intellij-community,23fb60afd8efcb968c2e6d6ab5e97c5aec973cc7,https://github.com/JetBrains/intellij-community/commit/23fb60afd8efcb968c2e6d6ab5e97c5aec973cc7,[json] IJPL-63554 Implemented fast exit for json schema validators  - If requested  validation will stop as soon as any error is found. This is extremelly important performance optimisation that plays well with the recenty introduced if-else branch computation. The number of calls to JsonSchemaResolver.isCorrect() increased dramatically  even more json-schema subsystem refactoring was demanded.  The existing API didn't assume any kind of laziness or cancellability. The refactoring is performed in a way to cause minimal number of changes in code and API. It'd be great to rewrite the entire validation code to sequence/analogs once and drop complicated JsonAnnotationsCollectionMode  GitOrigin-RevId: 4e62f7db76ed6b4071accbe1b80151c4b4664342
JetBrains,intellij-community,1e4b60370cbe3deba7d7d20c475e77fe3cf768c2,https://github.com/JetBrains/intellij-community/commit/1e4b60370cbe3deba7d7d20c475e77fe3cf768c2,IJPL-157491 Use the new bulk expand API for TreeUtil.restoreExpandedPaths  The main reason for this change is that the new API actually checks for every path whether it's a leaf. This prevents a bug when we first mark a path as expanded and then later we add it  so it shows as collapsed (because it's newly added)  but can't be expanded either because the tree thinks it's expanded. An example of this is the filtering tree  which tries to re-expand every expanded path after refiltering  even those paths that don't exist because of that refiltering.  A nice side effect is a considerable performance improvement in case there are many expanded paths.  GitOrigin-RevId: a0c41c33ab374e75cb385c72a796abe917e3fdad
JetBrains,intellij-community,52850e21d86aacd01bb1406106af780ad4b043c8,https://github.com/JetBrains/intellij-community/commit/52850e21d86aacd01bb1406106af780ad4b043c8,PY-62208 Include importable names in basic completion results  Previously  such names were visible only on so-called "extended" completion  activated when the hotkey for the basic completion was hit twice. The main reason was that collecting such variants from indexes was a slow process  and we didn't want to harm the responsiveness of completion for basic names. Now it becomes possible thanks to a number of performance optimizations:  * Instead of using three separate indexes for classes  functions and variables  we use one -- PyExportedModuleAttributeIndex. By definition  it includes only top-level "importable" names  so we additionally save time by not filtering out irrelevant entries. Also  it doesn't contain private definitions starting with an underscore. It might bother some users  but given that the previous completion was used extremely rarely  and the new one is going to be visible everywhere  it seems that pruning unlikely entries as much as possible is a fare tradeoff. In the future  we might enable them back on the "extended" completion if there is a demand. Also  this index binds its keys to the project (`traceKeyHashToVirtualFileMapping`)  further eliminating useless index lookups.  * Thanks to the recent fixes in the platform (IJPL-265)  it's now possible to simultaneously iterate over all keys in an index and request values for a given key without deadlocks  which is much faster than eagerly fetching all keys first.  * While scanning through all matching entries from indexes  we terminate the lookup if the number of items exceeds the size of the lookup list. We can further reduce this number by adjusting the "ide.completion.variant.limit" registry value.  * Calculating expensive "canonical" import paths (e.g. "pkg.private.Name" is importable as "pkg.Name") is offloaded to a background thread thanks to the `withExpensiveRenderer` API. We still calculate these paths synchronously  though  for names whose raw qualified names contain components starting with an underscore to decide whether these private names are publicly re-exported and  hence  should be displayed.  The rest of the work has been put into reducing the number of entries on the list  e.g.  * The prefix under caret is now matched from the beginning of a name  e.g. `Bar<caret>` matches `BarBaz`  but not `FooBar`. * We don't suggest imported names clashing with those already available in scope. * Some kinds of definitions are not suggested in specific contexts  e.g. functions and variables are not suggested inside patterns and type hints. * Nothing is suggested at the top-level of a class body  where dangling reference expressions or calls are not normally expected.  Additionally  we don't suggest names from .pyi stubs at the moment  because it pollutes the suggestion list with entries coming from the stubs for third-party packages in Typeshed. We should probably enable them back once we are able to properly disable Typeshed entries for not installed packages.  Some legacy forms of completion are left in the extended mode. In particular  qualified names of classes are offered inside string literals only in this mode. Also  module and package names are suggested only in the extended mode  because top-level packages and modules are already suggested for the basic completion by PyModuleNameCompletionContributor.  A few tests in PyClassNameCompletionTest were updated or removed entirely because * we no longer suggest private names * we no longer suggest names from private modules not re-exported in a public module * we no longer suggest names clashing with those already available in scope * prefix matching policy was changed to start at the beginning of an identifier  The whole feature can be disabled with the option "Suggest importable classes  functions and variables in basic completion" in settings.  GitOrigin-RevId: 0787d42ce337b73b01a60f0bb7aa434fee43e659
JetBrains,intellij-community,ffd202ba96b1864407ee27aa07eae57791cb2696,https://github.com/JetBrains/intellij-community/commit/ffd202ba96b1864407ee27aa07eae57791cb2696,IJPL-157271: performance experiment: avoid `synchronized` in c.i.o.fileTypes.WildcardFileNameMatcher.RegexpMatcher  This will produce more very short-living garbage which looks harmless. Matcher class has about 20 fields inside. Keep aside that some of them are arrays  Matcher probably occupies 100-150 bytes in the memory. I.e. there will be 150-225MB of garbage for mid-size project like idea ultimate (~1.5M files). Looks acceptable  because preliminary experiments showed that this will decrease 2nd scanning by ~10% (10sec -> 9sec) because of better parallelization when indexing on 19 threads. More parallel environments (like dev pods) should probably show even better parallelization.  GitOrigin-RevId: 450ccf3105d31be2703e4da859a59f1ee6351b8e
JetBrains,intellij-community,56241ab394e9985464f64d383d4d5894d360ce18,https://github.com/JetBrains/intellij-community/commit/56241ab394e9985464f64d383d4d5894d360ce18,PY-73411 Pycharm performance tests failed   Merge-request: IJ-MR-137368 Merged-by: Egor Eliseev <Egor.Eliseev@jetbrains.com>  GitOrigin-RevId: 23a7e9d443606b1a9a028c5e3275c6c408c0d796
JetBrains,intellij-community,1068a81106c40b74b48653842ee3bdf8da64af73,https://github.com/JetBrains/intellij-community/commit/1068a81106c40b74b48653842ee3bdf8da64af73,IJPL-797 `intellij.platform.ide.util.io` review internal API: deprecate `OSProcessUtil.getApplicationPid`  Initially  61bf5253ec56238d6f30a5b480272024a351a889 introduced two methods: * `private static int getCurrentProcessId()` * `public static String getApplicationPid()`  The function `getCurrentProcessId` was somehow expensive  and the function `getApplicationPid` was a caching wrapper for `getCurrentProcessId`.  Later  the commit 0b64798eb2e3d2201350b1d608c4e2412394020f made `getCurrentProcessId` public. Since that commit  new usages of both methods appeared internally and externally.  Finally  the commit a6aca19bbf2667071ccd3466c0fc29319d972bd4 replaced internals of `getCurrentProcessId` with a lightweight call.  Nowadays  there's no need in the caching function. Take a look at this JMH benchmark.  Put it into the module `intellij.platform.benchmarks`  ```java package com.intellij.openapi.vfs.newvfs.persistent;  import com.intellij.execution.process.OSProcessUtil; import org.openjdk.jmh.annotations.Benchmark; import org.openjdk.jmh.runner.Runner; import org.openjdk.jmh.runner.RunnerException; import org.openjdk.jmh.runner.options.Options; import org.openjdk.jmh.runner.options.OptionsBuilder;  import java.lang.management.ManagementFactory;  public class OsProcessUtilBench { @Benchmark public void measureGetCurrentProcessId() { OSProcessUtil.getCurrentProcessId(); }  @Benchmark public void measureGetApplicationPid() { OSProcessUtil.getApplicationPid(); }  @Benchmark public void measureInitialGetCurrentProcessId() { try { String name = ManagementFactory.getRuntimeMXBean().getName(); String result = name.split("@")[0]; } catch (Exception e) { String result = "-1"; } }  public static void main(String[] args) throws RunnerException { Options opt = new OptionsBuilder() .include(OsProcessUtilBench.class.getSimpleName()) .threads(1) .forks(0) .build();  new Runner(opt).run(); } } ```  The results are:  MacBook M2 Max  JBR-17.0.9+8-1166.2-nomod:  ``` Benchmark                                              Mode  Cnt           Score           Error  Units OsProcessUtilBench.measureGetApplicationPid           thrpt    5  2331351911 143 ± 401295172 273  ops/s OsProcessUtilBench.measureGetCurrentProcessId         thrpt    5  1840847082 291 ± 638699340 108  ops/s OsProcessUtilBench.measureInitialGetCurrentProcessId  thrpt    5     2818096 290 ±    128628 230  ops/s ```  Windows 11  i9-12900  JBR-17.0.9+8-1166.2-nomod ``` Benchmark                                              Mode  Cnt           Score           Error  Units OsProcessUtilBench.measureGetApplicationPid           thrpt    5  3409145745.079 ± 235584855.493  ops/s OsProcessUtilBench.measureGetCurrentProcessId         thrpt    5  3251065447.737 ±   4455307.411  ops/s OsProcessUtilBench.measureInitialGetCurrentProcessId  thrpt    5     3056345.314 ±      7065.322  ops/s ```  The caching granted 500x performance boost for the initial implementation and only 1.22x boost for the actual implementation. Today the caching version of this rarely used functionality isn't worth it.  GitOrigin-RevId: 84227add27cd59c0a648326ca42e3a49bc5f4c31
JetBrains,intellij-community,2ff134e7ee3303df92056d25b72db259bfa32243,https://github.com/JetBrains/intellij-community/commit/2ff134e7ee3303df92056d25b72db259bfa32243,Merge branch 'kt-master'  # Conflicts: #	.idea/libraries/kotlinc_high_level_api.xml #	.idea/libraries/kotlinc_high_level_api_fe10.xml #	.idea/libraries/kotlinc_high_level_api_fir.xml #	.idea/libraries/kotlinc_high_level_api_fir_tests.xml #	.idea/libraries/kotlinc_high_level_api_impl_base.xml #	.idea/libraries/kotlinc_high_level_api_impl_base_tests.xml #	community/.idea/libraries/kotlinc_high_level_api.xml #	community/.idea/libraries/kotlinc_high_level_api_fe10.xml #	community/.idea/libraries/kotlinc_high_level_api_fir.xml #	community/.idea/libraries/kotlinc_high_level_api_fir_tests.xml #	community/.idea/libraries/kotlinc_high_level_api_impl_base.xml #	community/.idea/libraries/kotlinc_high_level_api_impl_base_tests.xml #	community/android/android-kotlin/idea-android/k2/src/org/jetbrains/kotlin/android/inspection/K2TypeParameterFindViewByIdInspection.kt #	community/android/android-templates/intellij.android.templates.iml #	community/android/compose-designer/src/com/android/tools/idea/compose/annotator/SpringPickerLineMarkerProvider.kt #	community/android/compose-designer/src/com/android/tools/idea/compose/pickers/preview/utils/KotlinUtils.kt #	community/android/compose-designer/src/com/android/tools/idea/compose/pickers/spring/model/SpringPickerPropertiesModel.kt #	community/android/compose-ide-plugin/compiler-hosted-src/androidx/compose/compiler/plugins/kotlin/lower/IrSourcePrinter.kt #	community/android/compose-ide-plugin/src/com/android/tools/compose/ComposeColorLineMarkerProviderDescriptor.kt #	community/android/compose-ide-plugin/src/com/android/tools/compose/ComposePluginUtils.kt #	community/android/compose-ide-plugin/src/com/android/tools/compose/debug/ComposeFunctionBreakpointType.kt #	community/android/project-system-gradle/src/com/android/tools/idea/run/configuration/AndroidBaselineProfileRunLineMarkerContributor.kt #	community/platform/build-scripts/src/org/jetbrains/intellij/build/CommunityLibraryLicenses.kt #	community/plugins/dev/intellij.kotlin.dev/src/internal/KotlinGoodCodeRedVisitor.kt #	community/plugins/devkit/intellij.kotlin.devkit/src/inspections/ForbiddenInSuspectContextMethodInspection.kt #	community/plugins/devkit/intellij.kotlin.devkit/src/inspections/KtAppServiceAsStaticFinalFieldOrPropertyProvider.kt #	community/plugins/devkit/intellij.kotlin.devkit/src/inspections/KtCallingFunctionShouldBeRequiresBlockingContextVisitorProvider.kt #	community/plugins/devkit/intellij.kotlin.devkit/src/inspections/UsePlatformProcessAwaitExitInspection.kt #	community/plugins/kotlin/base/analysis-api/analysis-api-utils/src/org/jetbrains/kotlin/idea/base/analysis/api/utils/CallParameterInfoProvider.kt #	community/plugins/kotlin/base/analysis-api/analysis-api-utils/src/org/jetbrains/kotlin/idea/base/analysis/api/utils/KtSymbolFromIndexProvider.kt #	community/plugins/kotlin/base/analysis-api/analysis-api-utils/src/org/jetbrains/kotlin/idea/base/analysis/api/utils/resolveUtils.kt #	community/plugins/kotlin/base/code-insight/src/org/jetbrains/kotlin/idea/base/codeInsight/ExpectedExpressionMatcher.kt #	community/plugins/kotlin/base/code-insight/src/org/jetbrains/kotlin/idea/base/codeInsight/KotlinCallProcessor.kt #	community/plugins/kotlin/base/code-insight/src/org/jetbrains/kotlin/idea/base/codeInsight/KotlinNameSuggester.kt #	community/plugins/kotlin/base/fir/analysis-api-platform/kotlin.base.fir.analysis-api-platform.iml #	community/plugins/kotlin/base/fir/analysis-api-platform/test/org/jetbrains/kotlin/idea/base/fir/analysisApiPlatform/modificationEvents/KotlinModuleOutOfBlockModificationTest.kt #	community/plugins/kotlin/base/fir/analysis-api-platform/test/org/jetbrains/kotlin/idea/base/fir/analysisApiPlatform/sessions/CyclicDependenciesSymbolResolutionTest.kt #	community/plugins/kotlin/base/scripting/src/org/jetbrains/kotlin/idea/core/script/scriptUtils.kt #	community/plugins/kotlin/code-insight/api/src/org/jetbrains/kotlin/idea/codeinsight/api/applicable/ContextProvider.kt #	community/plugins/kotlin/code-insight/api/src/org/jetbrains/kotlin/idea/codeinsight/api/applicators/fixes/KotlinApplicatorBasedQuickFix.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/ActualAnnotationsNotMatchExpectFixFactory.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/AddDataModifierFixFactory.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/AddSuspendModifierFixFactory.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/ChangeTypeQuickFixFactories.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/InsertDelegationCallFixFactory.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/SuperClassNotInitializedFactories.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/WrapWithSafeLetCallFixFactories.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/WrongPrimitiveLiteralFix.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/imprt/ClassifierImportCandidatesProvider.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/imprt/ImportQuickFix.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeInsight/IfThenTransformationUtils.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeInsight/KotlinSuperDeclarationsInfoService.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeInsight/KotlinTypeDeclarationProvider.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeinsights/impl/base/JavaArgumentNameCommentUtils.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeinsights/impl/base/StringTemplateUtils.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeinsights/impl/base/intentions/RemoveArgumentNamesUtils.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/JoinDeclarationAndAssignmentInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/RemoveToStringInStringTemplateInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/ReplaceGetOrSetInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/SelfAssignmentInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/UsePropertyAccessSyntaxInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/KotlinConstantConditionsInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/KotlinFunctionCallInstruction.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/KtControlFlowBuilder.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/KtDfaHelpers.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/KtVariableDescriptor.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/SmartCastHelpers.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/expressions/ReplaceCallWithBinaryOperatorInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/expressions/ReplaceSizeCheckInspectionBase.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/expressions/WhenWithOnlyElseInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/jdk2k/Transformation.kt #	community/plugins/kotlin/code-insight/inspections-shared/src/org/jetbrains/kotlin/idea/codeInsight/inspections/shared/RedundantSuspendModifierInspection.kt #	community/plugins/kotlin/code-insight/inspections-shared/src/org/jetbrains/kotlin/idea/codeInsight/inspections/shared/RedundantValueArgumentInspection.kt #	community/plugins/kotlin/code-insight/inspections-shared/src/org/jetbrains/kotlin/idea/codeInsight/inspections/shared/RemoveEmptyParenthesesFromLambdaCallInspection.kt #	community/plugins/kotlin/code-insight/intentions-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/intentions/ConvertLambdaToReferenceIntention.kt #	community/plugins/kotlin/code-insight/intentions-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/intentions/ImportAllMembersIntention.kt #	community/plugins/kotlin/code-insight/intentions-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/intentions/ImportMemberIntention.kt #	community/plugins/kotlin/code-insight/intentions-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/intentions/RemoveExplicitTypeIntention.kt #	community/plugins/kotlin/code-insight/intentions-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/intentions/ReplaceUnderscoreWithTypeArgumentIntention.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/hints/KtParameterHintsProvider.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/hints/KtReferencesTypeHintsProvider.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/hints/KtValuesHintsProvider.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/quickDoc/KotlinDocumentationTarget.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/quickDoc/KotlinIdeDeclarationRenderer.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/quickFixes/createFromUsage/CreateKotlinCallableActionTextBuilder.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/quickFixes/createFromUsage/K2CreateFunctionFromUsageUtil.kt #	community/plugins/kotlin/code-insight/line-markers/src/org/jetbrains/kotlin/idea/codeInsight/lineMarkers/KotlinRecursiveCallLineMarkerProvider.kt #	community/plugins/kotlin/code-insight/live-templates-k2/src/org/jetbrains/kotlin/idea/liveTemplates/k2/macro/SymbolBasedAnonymousSuperMacro.kt #	community/plugins/kotlin/code-insight/postfix-templates/src/org/jetbrains/kotlin/idea/codeInsight/postfix/KotlinTryPostfixTemplate.kt #	community/plugins/kotlin/code-insight/postfix-templates/src/org/jetbrains/kotlin/idea/codeInsight/postfix/KotlinWhenPostfixTemplate.kt #	community/plugins/kotlin/code-insight/structural-search-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/structuralsearch/KotlinStructuralSearchUtil.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/EmptinessCheckFunctionUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/FoldIfOrWhenToFunctionCallUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/ImplicitThisUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/InlineUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/InsertExplicitTypeArgumentsUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/KotlinPsiUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/NamedArgumentUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/TypeParameterUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/TypeUtils.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/Completions.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/KotlinFirCompletionContributor.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/FirCallableCompletionContributor.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/FirClassifierCompletionContributor.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/FirNamedArgumentCompletionContributor.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/FirWhenWithSubjectConditionContributor.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/helpers/CallableMetadataProvider.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/helpers/FirSuperEntriesProvider.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/weighers/ExpectedTypeWeigher.kt #	community/plugins/kotlin/fir/src/org/jetbrains/kotlin/idea/parameterInfo/KotlinHighLevelTypeArgumentInfoHandler.kt #	community/plugins/kotlin/fir/src/org/jetbrains/kotlin/idea/parameterInfo/utils.kt #	community/plugins/kotlin/highlighting/highlighting-k2/src/org/jetbrains/kotlin/idea/highlighting/KotlinCallHighlighterExtension.kt #	community/plugins/kotlin/highlighting/highlighting-k2/src/org/jetbrains/kotlin/idea/highlighting/KotlinDiagnosticHighlightVisitor.kt #	community/plugins/kotlin/highlighting/highlighting-k2/src/org/jetbrains/kotlin/idea/highlighting/KotlinUnusedSymbolUtil.kt #	community/plugins/kotlin/highlighting/highlighting-k2/src/org/jetbrains/kotlin/idea/highlighting/highlighters/FunctionCallHighlighter.kt #	community/plugins/kotlin/highlighting/highlighting-k2/test/org/jetbrains/kotlin/idea/k2/highlighting/AbstractK2HighlightingMetaInfoWithExtensionTest.kt #	community/plugins/kotlin/injection/k2/src/org/jetbrains/kotlin/idea/k2/injection/K2KotlinLanguageInjectionContributor.kt #	community/plugins/kotlin/intellij.kotlin.plugin.community.main.iml #	community/plugins/kotlin/jvm-debugger/core-fe10/src/org/jetbrains/kotlin/idea/debugger/stepping/smartStepInto/CallableMemberInfo.kt #	community/plugins/kotlin/jvm-debugger/core/src/org/jetbrains/kotlin/idea/debugger/core/KotlinPositionManager.kt #	community/plugins/kotlin/jvm-debugger/coroutines/src/org/jetbrains/kotlin/idea/debugger/coroutine/KotlinVariableNameFinder.kt #	community/plugins/kotlin/jvm-debugger/evaluation/kotlin.jvm-debugger.evaluation.iml #	community/plugins/kotlin/jvm-debugger/evaluation/src/org/jetbrains/kotlin/idea/debugger/evaluate/kotlinExpressionWrappers.kt #	community/plugins/kotlin/k2-fe10-bindings/src/org/jetbrains/kotlin/idea/fir/fe10/KtSymbolBasedKotlinTypes.kt #	community/plugins/kotlin/k2-fe10-bindings/src/org/jetbrains/kotlin/idea/fir/fe10/binding/Fe10BindingScopeProvider.kt #	community/plugins/kotlin/k2-fe10-bindings/src/org/jetbrains/kotlin/idea/fir/fe10/binding/MiscBindingContextValueProvider.kt #	community/plugins/kotlin/k2-fe10-bindings/src/org/jetbrains/kotlin/idea/fir/fe10/binding/ResolvedCallWrappers.kt #	community/plugins/kotlin/k2-fe10-bindings/src/org/jetbrains/kotlin/idea/fir/fe10/binding/ToDescriptorBindingContextValueProviders.kt #	community/plugins/kotlin/kotlin.performanceExtendedPlugin/kotlin.performanceExtendedPlugin.iml #	community/plugins/kotlin/kotlin.searching/src/org/jetbrains/kotlin/idea/searching/inheritors/DirectKotlinClassInheritorsSearcher.kt #	community/plugins/kotlin/kotlin.searching/src/org/jetbrains/kotlin/idea/searching/usages/KotlinK2FindUsagesSupport.kt #	community/plugins/kotlin/kotlin.searching/src/org/jetbrains/kotlin/idea/searching/usages/KotlinK2SearchUsagesSupport.kt #	community/plugins/kotlin/kotlin.searching/src/org/jetbrains/kotlin/idea/searching/usages/findUsagesUtils.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.common/src/org/jetbrains/kotlin/idea/refactoring/rename/AutomaticOverloadsRenamer.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/KotlinChangeSignatureUsageSearcher.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/KotlinParameterInfo.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/KotlinTypeInfo.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/quickFix/ChangeParameterTypeFixFactory.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/quickFix/ChangeSignatureFixFactory.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/usages/KotlinFunctionCallUsage.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/extractFunction/Parameter.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/extractFunction/parametersUtil.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/inline/codeInliner/CodeInliner.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/inline/codeInliner/InlinePreprocessorUtil.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/introduce/K2SemanticMatcher.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/introduce/extractionEngine/ExtractionDataAnalyzer.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/introduce/extractionEngine/KotlinTypeDescriptor.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/introduceParameter/KotlinFirIntroduceParameterHandler.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/util/ConvertReferenceToLambdaUtil.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/util/RedundantExplicitTypeArgumentsUtil.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/utils.kt #	community/plugins/kotlin/refactorings/rename.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/rename/renameConflictUtils.kt #	community/plugins/kotlin/uast/uast-kotlin-fir/src/org/jetbrains/uast/kotlin/FirKotlinUastResolveProviderService.kt #	community/plugins/kotlin/uast/uast-kotlin-fir/src/org/jetbrains/uast/kotlin/internal/firKotlinInternalUastUtils.kt #	community/plugins/kotlin/util/project-model-updater/src/org/jetbrains/tools/model/updater/kotlincLibraries.kt #	fleet/plugins/mercury/backend/compose.plugin/src/org/jetbrains/compose/codeInsight/Common.kt #	fleet/plugins/mercury/backend/src/fleet/backend/mercury/compose/resources/ComposeResourcesUtils.kt #	plugins/frameworks/exposed/exposed-core/src/com/intellij/exposed/utils/ExposedAnalyzerUtils.kt #	plugins/frameworks/ktor/ktor-starter/src/io/ktor/ide/utils/KotlinResolve.kt  GitOrigin-RevId: 00a6e6be4d6f36368bf5b6e649eb209d83da38d2
JetBrains,intellij-community,c084e60fe0ab36ef31565f31a02a6ec31c78d88f,https://github.com/JetBrains/intellij-community/commit/c084e60fe0ab36ef31565f31a02a6ec31c78d88f,IJPL-485 Performance metrics: navigation to declaration  GitOrigin-RevId: ba0b8c5b084940711be46a7b00f2895f33ae8a1f
JetBrains,intellij-community,9347d3b5baa413ca7e8fdaacc8e6dae585295344,https://github.com/JetBrains/intellij-community/commit/9347d3b5baa413ca7e8fdaacc8e6dae585295344,[indexes] IJPL-1365: optimize .processKeys()  + seems like duplicates is not possible by design  hence an alreadyProcessedKeys set is not needed to avoid duplicates -- big performance win  GitOrigin-RevId: d9a6c2b4ed1b495349bb62314a50a5f103769d67
JetBrains,intellij-community,176d69eba6815929274f29ff6ecd1a8352365258,https://github.com/JetBrains/intellij-community/commit/176d69eba6815929274f29ff6ecd1a8352365258,[Project Structure] [IJPL-10393] Speed up the `ModuleEditor.getModule` by removing the unnecessary loop  This loop was joining with the other loop and caused performance issues on project structure dialog open in case there are a lot of modules. See IJPL-10393  However  this loop is NOT needed as it was added for a specific case that doesn't exist anymore. See the comment with the details about why this loop was introduced and why it's not needed anymore.  https://youtrack.jetbrains.com/issue/IJPL-10393/Opening-Project-Structure-in-20k-module-project-causes-freezing#focus=Comments-27-9879534.0-0  GitOrigin-RevId: f8eb0049068f9cc7d0a0d98acd318f2b710df40a
JetBrains,intellij-community,c60f9353cb7a972f0508632fa9d27d1b2ba37621,https://github.com/JetBrains/intellij-community/commit/c60f9353cb7a972f0508632fa9d27d1b2ba37621,IJPL-149878 IJent WSL: introduce TracingFileSystemProvider for controlling performance of WSL access on fs  TracingFileSystemProvider wraps both the original WindowsFileSystemProvider and IjentNioFileSystemProvider. It allows controlling sudden performance degradations in benchmark tests.  Only WSL drives are supposed to be wrapped into TracingFileSystemProvider. Regular windows drives aren't wrapped.  GitOrigin-RevId: 988278e0e88a7d6c9e01422a0dd0713e883cb275
JetBrains,intellij-community,ce6537bbde0531072298a9f9fb5916f82fc5ddb0,https://github.com/JetBrains/intellij-community/commit/ce6537bbde0531072298a9f9fb5916f82fc5ddb0,[profiler] IDEA-353438 Show performance hints only for the thread Run to Cursor was invoked on  bump async-profiler version to 3.0-4 -- support passing java thread IDs (in addition to native IDs) to the agent to profile only desired threads  GitOrigin-RevId: 7d76a8102d6c7eae44f22bfabcf14409e77ef1f9
LMAX-Exchange,disruptor,bfc35ee29a384e2151cf0dec8f19f9ce6f798b75,https://github.com/LMAX-Exchange/disruptor/commit/bfc35ee29a384e2151cf0dec8f19f9ce6f798b75,Merge pull request #483 from nicholassm/master  Add JMH benchmark to measure multi-producer batch publication performance.
dromara,Sa-Token,905f6714e2439e13d7cdd63b1ee28d3c43922766,https://github.com/dromara/Sa-Token/commit/905f6714e2439e13d7cdd63b1ee28d3c43922766,perf: sa-token-redisx 调整 SaTokenDaoOfRedisJson 类保持与 SaTokenDaoForRedisTemplate 相似的处理逻辑
dromara,Sa-Token,caeb4eba1579dafcf765af43be58188fbc3b692e,https://github.com/dromara/Sa-Token/commit/caeb4eba1579dafcf765af43be58188fbc3b692e,perf: sa-token-solon-plugin 移除 dao 下的代码（由具体插件处理）
facebook,fresco,d99df62f648c592d2a9c93ff472b5c8843beea0a,https://github.com/facebook/fresco/commit/d99df62f648c592d2a9c93ff472b5c8843beea0a,Finalize more builder classes for Android Studio Performance  Reviewed By: edelron  Differential Revision: D61660695  fbshipit-source-id: c9208d33e6c777c78b2d5b30fbfad9473e446bec
material-components,material-components-android,d16a19364c91bc6c6e2db1161726d82acb561291,https://github.com/material-components/material-components-android/commit/d16a19364c91bc6c6e2db1161726d82acb561291,[MaterialShapeDrawable] Update ShapeAppearanceModel.Builder to not use extra ContextThemeWrapper for shape appearance overlay due to performance concerns  PiperOrigin-RevId: 762498280
ben-manes,caffeine,62f401ce29cfaa2daabceff7353e69642d5261e2,https://github.com/ben-manes/caffeine/commit/62f401ce29cfaa2daabceff7353e69642d5261e2,remove sun.misc.unsafe leftover in the test and benchmark code  Removed the usage from a unit test that requires a predictable ThreadLocalRandom result. This now uses reflection and the runner must explicitly open the module for access.  Removed from a benchmark comparing table lookup mechanisms and the cache currently uses the VarHandles approach   Removed JDK 7's ConcurrentHashMap as this was only useful for judging the performance benefit of the Java 8 rewrite when trying to set baseline expectations during the library's initial development.
ben-manes,caffeine,91a36fb0957c97669989cd383b1dbe1245ac6d10,https://github.com/ben-manes/caffeine/commit/91a36fb0957c97669989cd383b1dbe1245ac6d10,optimize the frequency sketch  In an earlier analysis the block-based sketch was significantly faster than the flat (uniform) one. This was independently confirmed by a C# and Go port  who also observed a 2x speed up. However  when recently adding this benchmark to the CI it showed it as a regression. Therefore some implicit compiler optimizations are now explicit  which allows the block-based sketch to match or exceed the flat-based performance.  - We no longer rely on escape analysis to optimize away the method scoped arrays (count  index). These should have been stack allocated and broken into their components. - The arrays were meant to break a loop data dependency  but it is now faster to keep that. `Math.min` is a single cycle  branch-free instruction that the OOO pipeline seems to prefer. - `increment` is manually loop unrolled like the flat version  which shows a simiar speed up. - Previously  the flat benchmark version implemented the scaffolding interface directly  was pre-allocated  and the init guard was removed. This gave it a large advantage as it improved inlining  branch prediction  etc. The benchmark is now fair. - For jdk11 the block is always faster by at least 10M ops/s. In jdk23 the speedup only occurs as the table size increases  matching the expected gains from better cache effects. It is marginally slower on the small table size due to indexing differences.  The differences are very hardware and compiler dependent  as there are wide variations when running on Intel  Arm  Java versions  and JVMs (Graal vs C2). The user effect will be noise since this was not a performance bottleneck due to the cache's overall design.
prestodb,presto,b11f358df710e05d78a913f01ce2a12dd9f4e591,https://github.com/prestodb/presto/commit/b11f358df710e05d78a913f01ce2a12dd9f4e591,Update plugin to generate idl thrift file automatically (#25164)  ## Description 1. Automatically generate idl file for 3 classes  taskStatus  taskInfo  and taskUpdateRequest via plugin 2. Requires https://github.com/prestodb/drift/pull/63  ## Motivation and Context <!---Why is this change required? What problem does it solve?--> <!---If it fixes an open issue  please link to the issue here.-->  ## Impact <!---Describe any public API or user-facing feature change or any performance impact-->  ## Test Plan 1.  Build successfully and the generated idl file looks good. <img width="1174" alt="Screenshot 2025-05-20 at 16 16 07" src="https://github.com/user-attachments/assets/3d20b193-9aed-4635-a4f4-f45ccdde8538" />  <img width="1134" alt="Screenshot 2025-05-20 at 17 19 08" src="https://github.com/user-attachments/assets/5fe74591-5f7c-458e-bb25-4616b612ccf0" />  ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.   ``` == NO RELEASE NOTE == ```
prestodb,presto,9f77bed270474fb4f870344735d4624dffd65d4a,https://github.com/prestodb/presto/commit/9f77bed270474fb4f870344735d4624dffd65d4a,Make taskUpdateRequest and taskInfo classes Thrift ready with json fields (#25020)  ## Description 1. We are enabling thrift for task update request  and task info for critical api communication between coordinator and worker. We have two config toggles for task update request sent to worker and the task info returned to coordinator  2. However  there are some classes that are java interface/polymorphic fields. We keep them as json encoding for now and will migrate them in the next step. 3. We are also doing proper change for native worker: #25079  ## Motivation and Context 1. We observed that coordinator can spend too much cpu/heap memory on json serde for taskUpdateRequest.  ## Impact <!---Describe any public API or user-facing feature change or any performance impact-->  ## Test Plan 1. Passed verifier tests  ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.  ``` == RELEASE NOTES ==  General Changes * Improve communication between coordinator and worker with thrift serde. ```
prestodb,presto,60f1ba70c41d5153596d50c36f6bbd3fee363834,https://github.com/prestodb/presto/commit/60f1ba70c41d5153596d50c36f6bbd3fee363834,Add view text hash info to accessControlReferences (#24955)  ## Description <!---Describe your changes in detail--> In addition to the raw SQL string that the user submitted  checkQueryIntegrity needs to have view definitions used inside the of the query to validate against the credentials passed in through the identity.  ## Motivation and Context <!---Why is this change required? What problem does it solve?--> <!---If it fixes an open issue  please link to the issue here.--> This is to address the security vulnerability if a view definition gets changed in between when the approved credential was generated and when the query begins executing.  ## Impact <!---Describe any public API or user-facing feature change or any performance impact--> - Add extra view text fields to AccessControlReferences - change checkQueryintegrity API to take in view text ## Test Plan <!---Please fill in how you tested your change-->  ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.  ``` == RELEASE NOTES ==  General Changes * Add view definitions from Analyzer phase to perform full integrity check on query credentials. * Change checkQueryIntegrity function signature in AccessControl interface to pass in view definitions as params.
prestodb,presto,9f689e91f2102536a9d3116a5855cfc38895cdff,https://github.com/prestodb/presto/commit/9f689e91f2102536a9d3116a5855cfc38895cdff,Add runtime metrics for task time on event loop and log slow execution (#25009)  ## Description 1. Add runtime metrics for task execution time on event loop 2. Add logging for slow execution where the logging threshold can be controlled by config and we will log the query id  task id  and the method name if the runnable is taking too long to finish.  ## Motivation and Context 1. This increases the observability on what can be running slow on event loop  ## Impact <!---Describe any public API or user-facing feature change or any performance impact-->  ## Test Plan 1. running verifier  ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.   ``` == NO RELEASE NOTE == ```
prestodb,presto,931d4a69746f8104a81ff6188e38327a4a8bc91a,https://github.com/prestodb/presto/commit/931d4a69746f8104a81ff6188e38327a4a8bc91a,Check Query integrity in Analyzer Util (#24927)  ## Description <!---Describe your changes in detail--> An ACL check checkQueryIntegrity needs to be moved from DispatchManager to the Analyzer. This PR will pass down the raw query string as well as accessControl down to the Analyzer so that this ACL call can be performed.  ## Motivation and Context <!---Why is this change required? What problem does it solve?--> <!---If it fixes an open issue  please link to the issue here.--> The checkQueryIntegrity call needs to be done after queuing to address a potential security issue with the ACL check. If the query uses views  then the view definition could be changed between dispatch and when view definitions are read in the analyzer. Making this change will reduce the time window of this security gap.  ## Impact <!---Describe any public API or user-facing feature change or any performance impact--> Changes to createQueryExecution and various other interfaces so that accessControl and raw query string can be passed down to Analyzer and AnalyzerUtil.  ## Test Plan <!---Please fill in how you tested your change--> Using debugger in checkAccessPermissions in AnalyzerUtil  the query string and accessControlInfo are passed down properly.  ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.  ``` == RELEASE NOTES ==  General Changes * Improve ACL check by moving checkQueryIntegrity from Dispatch phase to Analyzer phase.
prestodb,presto,a33f73def6f682c48a09a1ba4c78275cf527da8c,https://github.com/prestodb/presto/commit/a33f73def6f682c48a09a1ba4c78275cf527da8c,Re-introduce improving the merging of operator stats (#24921)  ## Description 1. this pr re-introduce the #24414   which cause a sev where written partition was not logged. The bug is a corner case  where while merging only one single non-mergeable operatorInfo  the old code will NOT perform any merge operation (since the add operation will only get invoke when the second operator stats shows up) and give back the operator info itself while #24414 will actually kick off a merge and gives null result. 2. This pr reintroduce #24414 and handles this corner case and also added specific unit tests for this scenario.  ## Motivation and Context 1. re-introduce #24414  ## Impact <!---Describe any public API or user-facing feature change or any performance impact-->  ## Test Plan 1.  verifier runs log written partition correctly: <img width="1469" alt="Screenshot 2025-04-15 at 17 13 08" src="https://github.com/user-attachments/assets/f7c84a8f-7381-411a-95d1-15b075870b83" />   ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.  ``` == RELEASE NOTES ==  General Changes * Improve how we merge multiple operator stats together. * Improve metrics creation by refactoring local variables to a dedicated class.  ```
prestodb,presto,02a65c0b6f2ed9027b44f657cc87916352c230de,https://github.com/prestodb/presto/commit/02a65c0b6f2ed9027b44f657cc87916352c230de,Reintroduced json_extract to generate canonicalized output (#24879)  ## Description The original pull request [#24614](https://github.com/prestodb/presto/pull/24614) incorrectly compares canonicalizedJsonExtract and legacyJsonCast in the equals function of an object. This issue can be seen in the code [here](https://github.com/prestodb/presto/pull/24614/files#diff-e921c5d186f9d5daa836bc7330f52caf8c1b84d19cf42288d5a8a7c9a6d2a5d5R156).  As a result  whenever a SQL function requires caching  the cache is never hit  leading to the creation of new SQL function objects repeatedly. This behavior eventually causes an OOM error in the JVM metaspace. and eventually this error led to UER SEV.  After the problematic comparison was updated and tested through shadow cluster by @rschlussel   we are confident that the issue has been resolved in this PR. Therefore  we plan to bring back the json canonicalized extract   ## Motivation and Context Reintroduced json_extract to generate canonicalized output  ## Impact <!---Describe any public API or user-facing feature change or any performance impact--> low impact  ## Test Plan <!---Please fill in how you tested your change--> N/A  ## Contributor checklist  - [x] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [x] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [x] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [x] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [x] Adequate tests were added if applicable. - [x] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.  ``` == NO RELEASE NOTE == ```
prestodb,presto,41aba926697e8c14e8c860f18df874f04db351ca,https://github.com/prestodb/presto/commit/41aba926697e8c14e8c860f18df874f04db351ca,Add single node execution  To improve performance for small queries which can be executed within a single node  we introduce single worker execution mode: query will only use one node to execute and plan would be optimized accordingly.
prestodb,presto,07bf13ae09618ea51e34f74d8678d7aa19e2d2bc,https://github.com/prestodb/presto/commit/07bf13ae09618ea51e34f74d8678d7aa19e2d2bc,[Iceberg] Add statistics file caching  Adds a new connector-wide cache for statistics files. This prevents additional memory consumption and improves query planning performance by avoiding hits to the file system when generating table statistics.
prestodb,presto,1137a1f005e602054437cb68d7f96047b1e61111,https://github.com/prestodb/presto/commit/1137a1f005e602054437cb68d7f96047b1e61111,Change native spill compression kind to zstd  This helps reduce the spill bytes to speedup spill performance. For exmaple 20240608_210530_00001_iyjad execution time reduced from 12mins to 9 mins. The spilled bytes has been reduced from 2.62TB to 960GB. The total disk write time has been reduced from 12hrs to 4hrs. The max time of a single aggregation operator has been reduced from 2.4mins to 40s. And there is not much change to the compression time. And the disk read time has been reduced from 1.29 days to 14.46 hours. The max time of a single aggregation operator has been reduced from 5 mins to 2 mins. This explains the overall e2e time reduction
prestodb,presto,f87af00a462bbe4cce5e72af9af980732c69ba43,https://github.com/prestodb/presto/commit/f87af00a462bbe4cce5e72af9af980732c69ba43,Fix optimizer performance regression for large IN  This bug occurs due to large IN lists. When computing statistics for queries with this clause  we generate an instance of VariableStatisticEstimate for each expression in the list.  For example  a query with  SELECT ... IN (1  2   ... 10_000);  generates 10k VariableStatisticEstimate instances. Then  for each instance  we sum the statistics and distinct values to get a final result to determine the probability of a value occurring. Creating these VariableStatisticEstimate instances were not the root cause of the issue.  The main problem is that when the DIsjointRangeDomainHistogram was introduced it was designed as immutable. When folding all the instances together through the `reduce()` call at FilterStatsCalculator#L755  it re-creates the internal TreeRangeSet. The set of disjoint ranges is the size of the IN list  so we were re-creating the set 10k times. The first with range a set size of 1  then 2  ... etc up to 10k. So the running time of this ended up being polynomial.  The following change updates the DisjointRangeDomain histogram to use a lazily initialized RangeSet. This prevents incurring the high cost of re-creating the range set for every  new addition to the IN list.  Additionally  the StatisticRange class now serializes to a byte encoded format to decrease the amount of bytes required to serialize plans with many filters. This is mainly useful for when a query has thousands of filters in a complex plan and the filters are applied to the histogram.
questdb,questdb,fe501c849c39eac2cce23083f56105e35f5d20dd,https://github.com/questdb/questdb/commit/fe501c849c39eac2cce23083f56105e35f5d20dd,perf(pgwire): improve performance of batch inserts via PostgreSQL driver (#5564)
questdb,questdb,7805cf5970ac578d5b2c654e6675e20df736d354,https://github.com/questdb/questdb/commit/7805cf5970ac578d5b2c654e6675e20df736d354,perf(sql): performance improvement in length(varchar) function (#5188)
questdb,questdb,7112d4d5c099b8f1e16adbdca9e2f7cb9ae97106,https://github.com/questdb/questdb/commit/7112d4d5c099b8f1e16adbdca9e2f7cb9ae97106,perf(sql): improve performance of queries with negative limits and existing order by clauses (#5148)
apache,hadoop,949292eac6d26ddf5713bf2783d09c3317ffe695,https://github.com/apache/hadoop/commit/949292eac6d26ddf5713bf2783d09c3317ffe695,HADOOP-19571. Improve PrometheusMetricsSink#normalizeName performance (#7692) Contributed by Ivan Andika.  * HADOOP-19571. Improve PrometheusMetricsSink#normalizeName performance  Reviewed-by: Akira Ajisaka <aajisaka@apache.org> Signed-off-by: Shilun Fan <slfan1989@apache.org>
apache,hadoop,0e208c8abd982a658ecace110038a00c11ee41dc,https://github.com/apache/hadoop/commit/0e208c8abd982a658ecace110038a00c11ee41dc,HADOOP-19256. S3A: Support Conditional Overwrites  Amazon S3 now supports conditional overwrites  which can be be used when creating files through the createFile() API with two new builder options:  fs.option.create.conditional.overwrite:  Write if and only if there is no object at the target path. This is an atomic PUT-no-overwrite  checked in close()  not create().  fs.option.create.conditional.overwrite.etag  Write a file if and only if it is overwriting a file with a specific etag.  If the "fs.s3a.performance.flags" enumeration includes the flag "create" then file creation will use conditional creation to detect and reject overwrites.  The configuration option "fs.s3a.create.conditional.enabled" can be set to false to disable these features on third-party stores.  Contributed by Diljot Grewal  Saikat Roy and Steve Loughran
apache,hadoop,a314a1d71488f782749638ba27098515bbb3a8a4,https://github.com/apache/hadoop/commit/a314a1d71488f782749638ba27098515bbb3a8a4,YARN-11798. Precheck request separately to avoid redundant node checks and optimize performance for global scheduler. (#7516) Contributed by Tao Yang.  * YARN-11798. Precheck request separately to avoid redundant node checks and optimize performance for global scheduler.  * Add node for recording scheduler activities in RegularContainerAllocator#preCheckRequest to fix UT.  Signed-off-by: Shilun Fan <slfan1989@apache.org>
apache,hadoop,f0368bb2372f0fd2d63295eed5eec43c2afd3747,https://github.com/apache/hadoop/commit/f0368bb2372f0fd2d63295eed5eec43c2afd3747,HDFS-17405. [FGL] Using different metric name to trace performance for FGL and Global lock (#6600)
apache,hadoop,321a6cc55ed2df5222bde7b5c801322e8cf68203,https://github.com/apache/hadoop/commit/321a6cc55ed2df5222bde7b5c801322e8cf68203,HADOOP-19072. S3A: expand optimisations on stores with "fs.s3a.performance.flags" for mkdir (#6543)   If the flag list in fs.s3a.performance.flags includes "mkdir" then the safety check of a walk up the tree to look for a parent directory  -done to verify a directory isn't being created under a file- are skipped.  This saves the cost of multiple list operations.  Contributed by Viraj Jasani
apache,hadoop,a5806a9e7bc6d018de84e6511f10c359f110f78c,https://github.com/apache/hadoop/commit/a5806a9e7bc6d018de84e6511f10c359f110f78c,HADOOP-19161. S3A: option "fs.s3a.performance.flags" to take list of performance flags (#6789)    1. Configuration adds new method `getEnumSet()` to get a set of enum values from a configuration string. <E extends Enum<E>> EnumSet<E> getEnumSet(String key  Class<E> enumClass  boolean ignoreUnknown)  Whitespace is ignored  case is ignored and the value "*" is mapped to "all values of the enum". If "ignoreUnknown" is true then when parsing  unknown values are ignored. This is recommended for forward compatiblity with later versions.  2. This support is implemented in org.apache.hadoop.fs.s3a.impl.ConfigurationHelper -it can be used elsewhere in the hadoop codebase.  3. A new private FlagSet class in hadoop common manages a set of enum flags.  It implements StreamCapabilities and can be probed for a specific option being set (with a prefix)   S3A adds an option fs.s3a.performance.flags which builds a FlagSet with enum type PerformanceFlagEnum  * which initially contains {Create  Delete  Mkdir  Open} * the existing fs.s3a.create.performance option sets the flag "Create". * tests which configure fs.s3a.create.performance MUST clear fs.s3a.performance.flags in test setup.  Future performance flags are planned  with different levels of safety and/or backwards compatibility.  Contributed by Steve Loughran
apache,pulsar,1220951ac74fb4742abbbd331d6e751234c47015,https://github.com/apache/pulsar/commit/1220951ac74fb4742abbbd331d6e751234c47015,[improve][client][PIP-389] Add a producer config to improve compression performance (#23525)  PIP: https://github.com/apache/pulsar/pull/23526 ### Motivation  The motivation of this PIP is to provide a way to improve the compression performance by skipping the compression of small messages. We want to add a new configuration compressMinMsgBodySize to the producer configuration. This configuration will allow the user to set the minimum size of the message body that will be compressed. If the message body size is less than the compressMinMsgBodySize  the message will not be compressed.
apache,pulsar,3c2ec2bf8bfd94eded46b42c5089dd8321afd096,https://github.com/apache/pulsar/commit/3c2ec2bf8bfd94eded46b42c5089dd8321afd096,[improve][broker] Improve Consumer.equals performance (#23864)
apache,pulsar,d377bc9d7321a66201a301b6887fb1fea3ef8820,https://github.com/apache/pulsar/commit/d377bc9d7321a66201a301b6887fb1fea3ef8820,[improve][client] PIP-393: Improve performance of Negative Acknowledgement (#23600)  Co-authored-by: Lari Hotari <lhotari@apache.org>
apache,pulsar,73433cd06e65ce5e194372a657c5a414e820138b,https://github.com/apache/pulsar/commit/73433cd06e65ce5e194372a657c5a414e820138b,[improve] [broker] Optimize performance for checking max topics when the topic is a system topic (#23185)
apache,pulsar,1db3c5fddce45919c6cac3b5a10030183eed3d5c,https://github.com/apache/pulsar/commit/1db3c5fddce45919c6cac3b5a10030183eed3d5c,[improve][misc] Optimize TLS performance by omitting extra buffer copies (#23115)
apache,pulsar,e9deb408eaed2c04e30a27be5fba130f5d4e94b7,https://github.com/apache/pulsar/commit/e9deb408eaed2c04e30a27be5fba130f5d4e94b7,[improve][misc] Improve AES-GCM cipher performance (#23122)
apache,pulsar,77b6378ae8b9ac83962f71063ad44d6ac57f8e32,https://github.com/apache/pulsar/commit/77b6378ae8b9ac83962f71063ad44d6ac57f8e32,[improve][broker] Optimize the performance of individual acknowledgments (#23072)
quarkusio,quarkus,751e7ee760505d5e5182c26602669c02240f3f4b,https://github.com/quarkusio/quarkus/commit/751e7ee760505d5e5182c26602669c02240f3f4b,Disable checks related to unsupported bytecode enhancement  And switch back to the legacy behavior of "hoping for the best".  In Quarkus  we expect model classes to be enhanced. Lack of enhancement could lead to many problems  from bad performance  to Quarkus-specific optimizations causing errors/data loss  to incorrect generated bytecode (references to non-existing methods).  So  in ORM 6.6 we've introduced checks to detect cases where bytecode enhancement is not possible  so that developers can fix their apps.  We had a rough start  the checks were under/over-reporting  leading to bug reports  so we fixed the checks.  We're now doing the best we can  and it turns out the checks still have have many false positives -- in particular they assume property access for mapped-superclasses and embeddables  because the access type cannot be determined locally for those  which leads to reporting some very valid setups as invalid.  We've tried our best  and it turns out the remedy is worse than the disease. Let's disable these checks.  Longer-term  the solution is just to make bytecode enhancement work even in these cases: https://hibernate.atlassian.net/browse/HHH-18825
quarkusio,quarkus,064f4c80536fe98202a6976b6d1319355db6e198,https://github.com/quarkusio/quarkus/commit/064f4c80536fe98202a6976b6d1319355db6e198,Avoid repeatedly creating same DotNames in ResteasyReactiveProcessor  This is a follow-up of https://github.com/quarkusio/quarkus/pull/46464.  Given the purpose of the PR was to improve dev mode reload performance  I think it's worth avoid the extra allocations.
quarkusio,quarkus,f4a406adb96920b135b3a8a4eb4bd83e05980fbc,https://github.com/quarkusio/quarkus/commit/f4a406adb96920b135b3a8a4eb4bd83e05980fbc,Merge pull request #45311 from geoand/RequestMapper-improvement  Slightly improve performance of RequestMapper construction
quarkusio,quarkus,9849effa0ea48d711a803dc6b993d23de80ae476,https://github.com/quarkusio/quarkus/commit/9849effa0ea48d711a803dc6b993d23de80ae476,Slightly improve performance of RequestMapper construction
quarkusio,quarkus,6a4fbd63b9a939b8b8fad4035ddca96c8da7c307,https://github.com/quarkusio/quarkus/commit/6a4fbd63b9a939b8b8fad4035ddca96c8da7c307,Cache: fix CacheInterceptor in case of non-ArC interceptor bindings  RESTEasy Classic's MP RestClient implementation produces annotations at runtime  so they are not created by ArC and therefore don't extend `AbstractAnnotationLiteral`. At the same time  that implementation produces an `ArcInvocationContext` and puts interceptor bindings into its context map under the ArC key.  Some places may expect that an `ArcInvocationContext` would always contain ArC-created `AbstractAnnotationLiteral` instances  but alas  per the description above  that is not the case.  There are multiple options for fixing that collision. My preferred one would be to get rid of `AbstractAnnotationLiteral` and treat all annotations uniformly. That unfortunately has negative performance implications on the `CacheInterceptor`  so is not an option yet [1].  This commit chooses another path: it modifies the only place in Quarkus that actually depends on `AbstractAnnotationLiteral` to check whether the `Set<AbstractAnnotationLiteral>` actually contains instances of `AbstractAnnotationLiteral`. I hope that before more places in Quarkus start depending on `AbstractAnnotationLiteral`  we can get rid of it.  This commit only checks the first annotation in the set  because if the bindings come from RESTEasy Classic  then none of them are instances of `AbstractAnnotationLiteral`  and if they come from ArC  then all of them are instances of `AbstractAnnotationLiteral`.  [1] The performance issue (JDK-8180450) is fixed in JDK 23 and has not been backported to any LTS release as of this writing.
quarkusio,quarkus,d3d92be049ba5ed06b6ae35fca0161fe32bbca68,https://github.com/quarkusio/quarkus/commit/d3d92be049ba5ed06b6ae35fca0161fe32bbca68,Qute: IfSectionHelper - improve performance for common use case
quarkusio,quarkus,c96ef2a73dd34e858b8966a5674af4ed8de9ae50,https://github.com/quarkusio/quarkus/commit/c96ef2a73dd34e858b8966a5674af4ed8de9ae50,Merge pull request #42773 from peuBouzon/fix-2161  Remove jakarta.json.Json usage for performance reasons
quarkusio,quarkus,35eddccd7eb095cb0e52a3b88055756b4f3e17c1,https://github.com/quarkusio/quarkus/commit/35eddccd7eb095cb0e52a3b88055756b4f3e17c1,remove jakarta.json.Json usage for performance reasons
quarkusio,quarkus,65712cea1913eae0bdf9fdca7381f9f083755d91,https://github.com/quarkusio/quarkus/commit/65712cea1913eae0bdf9fdca7381f9f083755d91,generate jackson serializers  fix object mapper generation for all primitive types  implement nested types in generated object mapper  implement SecureField annotation support  avoid generating mapper for pojo with unknown jackson annotations  implement collections serialization  fix all tests in rest-jackson module  wip  wip  refactor and simplification  performance tuning  make reflection-free serializers generation opt-in  add @Produces annotation to SimpleJsonResource rest endpoints where appropriate  generate serializers for returned types from rest methods regardless if the @Produces annotation is present or not  wip  add javadoc
quarkusio,quarkus,9f9b4546b3cd1054d18adf6b367606914807c2c1,https://github.com/quarkusio/quarkus/commit/9f9b4546b3cd1054d18adf6b367606914807c2c1,Merge pull request #41295 from mcruzdev/feature/micrometer-perf  Micrometer performance - use Meter.MeterProvider
quarkusio,quarkus,11643b97ac00abd4f034fb9dfbe6bb26fd2caee6,https://github.com/quarkusio/quarkus/commit/11643b97ac00abd4f034fb9dfbe6bb26fd2caee6,Increase Micrometer performance using Meter.MeterProvider
elastic,logstash,7f7af057f06651799068803fa65fb89172a25d72,https://github.com/elastic/logstash/commit/7f7af057f06651799068803fa65fb89172a25d72,Feature: health report api (#16520) (#16523)  * [health] bootstrap HealthObserver from agent to API (#16141)  * [health] bootstrap HealthObserver from agent to API  * specs: mocked agent needs health observer  * add license headers  * Merge `main` into `feature/health-report-api` (#16397)  * Add GH vault plugin bot to allowed list (#16301)  * regenerate webserver test certificates (#16331)  * correctly handle stack overflow errors during pipeline compilation (#16323)  This commit improves error handling when pipelines that are too big hit the Xss limit and throw a StackOverflowError. Currently the exception is printed outside of the logger  and doesn’t even show if log.format is json  leaving the user to wonder what happened.  A couple of thoughts on the way this is implemented:  * There should be a first barrier to handle pipelines that are too large based on the PipelineIR compilation. The barrier would use the detection of Xss to determine how big a pipeline could be. This however doesn't reduce the need to still handle a StackOverflow if it happens. * The catching of StackOverflowError could also be done on the WorkerLoop. However I'd suggest that this is unrelated to the Worker initialization itself  it just so happens that compiledPipeline.buildExecution is computed inside the WorkerLoop class for performance reasons. So I'd prefer logging to not come from the existing catch  but from a dedicated catch clause.  Solves #16320  * Doc: Reposition worker-utilization in doc (#16335)  * settings: add support for observing settings after post-process hooks (#16339)  Because logging configuration occurs after loading the `logstash.yml` settings  deprecation logs from `LogStash::Settings::DeprecatedAlias#set` are effectively emitted to a null logger and lost.  By re-emitting after the post-process hooks  we can ensure that they make their way to the deprecation log. This change adds support for any setting that responds to `Object#observe_post_process` to receive it after all post-processing hooks have been executed.  Resolves: elastic/logstash#16332  * fix line used to determine ES is up (#16349)  * add retries to snyk buildkite job (#16343)  * Fix 8.13.1 release notes (#16363)  make a note of the fix that went to 8.13.1: #16026  Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com>  * Update logstash_releases.json (#16347)  * [Bugfix] Resolve the array and char (single | double quote) escaped values of ${ENV} (#16365)  * Properly resolve the values from ENV vars if literal array string provided with ENV var.  * Docker acceptance test for persisting  keys and use actual values in docker container.  * Review suggestion.  Simplify the code by stripping whitespace before `gsub`  no need to check comma and split.  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  ---------  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  * Doc: Add SNMP integration to breaking changes (#16374)  * deprecate java less-than 17 (#16370)  * Exclude substitution refinement on pipelines.yml (#16375)  * Exclude substitution refinement on pipelines.yml (applies on ENV vars and logstash.yml where env2yaml saves vars)  * Safety integration test for pipeline config.string contains ENV .  * Doc: Forwardport 8.15.0 release notes to main (#16388)  * Removing 8.14 from ci/branches.json as we have 8.15. (#16390)  ---------  Co-authored-by: ev1yehor <146825775+ev1yehor@users.noreply.github.com> Co-authored-by: João Duarte <jsvd@users.noreply.github.com> Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com> Co-authored-by: Andrea Selva <selva.andre@gmail.com> Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * Squashed merge from 8.x  * Failure injector plugin implementation. (#16466)  * Test purpose only failure injector integration (filter and output) plugins implementation. Add unit tests and include license notes.  * Fix the degrate method name typo.  Co-authored-by: Andrea Selva <selva.andre@gmail.com>  * Add explanation to the config params and rebuild plugin gem.  ---------  Co-authored-by: Andrea Selva <selva.andre@gmail.com>  * Health report integration tests bootstrapper and initial tests implementation (#16467)  * Health Report integration tests bootstrapper and initial slow start scenario implementation.  * Apply suggestions from code review  Renaming expectation check method name.  Co-authored-by: kaisecheng <69120390+kaisecheng@users.noreply.github.com>  * Changed to branch concept  YAML structure simplified as changed to Dict.  * Apply suggestions from code review  Reflect `help_url` to the integration test.  ---------  Co-authored-by: kaisecheng <69120390+kaisecheng@users.noreply.github.com>  * health api: expose `GET /_health_report` with pipelines/*/status probe (#16398)  Adds a `GET /_health_report` endpoint with per-pipeline status probes  and wires the resulting report status into the other API responses  replacing their hard-coded `green` with a meaningful status indication.  ---------  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * docs: health report API  and diagnosis links (feature-targeted) (#16518)  * docs: health report API  and diagnosis links  * Remove plus-for-passthrough markers  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  ---------  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * merge 8.x into feature branch... (#16519)  * Add GH vault plugin bot to allowed list (#16301)  * regenerate webserver test certificates (#16331)  * correctly handle stack overflow errors during pipeline compilation (#16323)  This commit improves error handling when pipelines that are too big hit the Xss limit and throw a StackOverflowError. Currently the exception is printed outside of the logger  and doesn’t even show if log.format is json  leaving the user to wonder what happened.  A couple of thoughts on the way this is implemented:  * There should be a first barrier to handle pipelines that are too large based on the PipelineIR compilation. The barrier would use the detection of Xss to determine how big a pipeline could be. This however doesn't reduce the need to still handle a StackOverflow if it happens. * The catching of StackOverflowError could also be done on the WorkerLoop. However I'd suggest that this is unrelated to the Worker initialization itself  it just so happens that compiledPipeline.buildExecution is computed inside the WorkerLoop class for performance reasons. So I'd prefer logging to not come from the existing catch  but from a dedicated catch clause.  Solves #16320  * Doc: Reposition worker-utilization in doc (#16335)  * settings: add support for observing settings after post-process hooks (#16339)  Because logging configuration occurs after loading the `logstash.yml` settings  deprecation logs from `LogStash::Settings::DeprecatedAlias#set` are effectively emitted to a null logger and lost.  By re-emitting after the post-process hooks  we can ensure that they make their way to the deprecation log. This change adds support for any setting that responds to `Object#observe_post_process` to receive it after all post-processing hooks have been executed.  Resolves: elastic/logstash#16332  * fix line used to determine ES is up (#16349)  * add retries to snyk buildkite job (#16343)  * Fix 8.13.1 release notes (#16363)  make a note of the fix that went to 8.13.1: #16026  Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com>  * Update logstash_releases.json (#16347)  * [Bugfix] Resolve the array and char (single | double quote) escaped values of ${ENV} (#16365)  * Properly resolve the values from ENV vars if literal array string provided with ENV var.  * Docker acceptance test for persisting  keys and use actual values in docker container.  * Review suggestion.  Simplify the code by stripping whitespace before `gsub`  no need to check comma and split.  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  ---------  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  * Doc: Add SNMP integration to breaking changes (#16374)  * deprecate java less-than 17 (#16370)  * Exclude substitution refinement on pipelines.yml (#16375)  * Exclude substitution refinement on pipelines.yml (applies on ENV vars and logstash.yml where env2yaml saves vars)  * Safety integration test for pipeline config.string contains ENV .  * Doc: Forwardport 8.15.0 release notes to main (#16388)  * Removing 8.14 from ci/branches.json as we have 8.15. (#16390)  * Increase Jruby -Xmx to avoid OOM during zip task in DRA (#16408)  Fix: #16406  * Generate Dataset code with meaningful fields names (#16386)  This PR is intended to help Logstash developers or users that want to better understand the code that's autogenerated to model a pipeline  assigning more meaningful names to the Datasets subclasses' fields.  Updates `FieldDefinition` to receive the name of the field from construction methods  so that it can be used during the code generation phase  instead of the existing incremental `field%n`. Updates `ClassFields` to propagate the explicit field name down to the `FieldDefinitions`. Update the `DatasetCompiler` that add fields to `ClassFields` to assign a proper name to generated Dataset's fields.  * Implements safe evaluation of conditional expressions  logging the error without killing the pipeline (#16322)  This PR protects the if statements against expression evaluation errors  cancel the event under processing and log it. This avoids to crash the pipeline which encounter a runtime error during event condition evaluation  permitting to debug the root cause reporting the offending event and removing from the current processing batch.  Translates the `org.jruby.exceptions.TypeError`  `IllegalArgumentException`  `org.jruby.exceptions.ArgumentError` that could happen during `EventCodition` evaluation into a custom `ConditionalEvaluationError` which bubbles up on AST tree nodes. It's catched in the `SplitDataset` node. Updates the generation of the `SplitDataset `so that the execution of `filterEvents` method inside the compute body is try-catch guarded and defer the execution to an instance of `AbstractPipelineExt.ConditionalEvaluationListener` to handle such error. In this particular case the error management consist in just logging the offending Event.  ---------  Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com>  * Update logstash_releases.json (#16426)  * Release notes for 8.15.1 (#16405) (#16427)  * Update release notes for 8.15.1  * update release note  ---------  Co-authored-by: logstashmachine <43502315+logstashmachine@users.noreply.github.com> Co-authored-by: Kaise Cheng <kaise.cheng@elastic.co> (cherry picked from commit 2fca7e39e87c20fcfcd934e984720173ce3417e8)  Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>  * Fix ConditionalEvaluationError to do not include the event that errored in its serialiaxed form  because it's not expected that this class is ever serialized. (#16429) (#16430)  Make inner field of ConditionalEvaluationError transient to be avoided during serialization.  (cherry picked from commit bb7ecc203f698a56f341fa538bdc1cd4da15b28c)  Co-authored-by: Andrea Selva <selva.andre@gmail.com>  * use gnu tar compatible minitar to generate tar artifact (#16432) (#16434)  Using VERSION_QUALIFIER when building the tarball distribution will fail since Ruby's TarWriter implements the older POSIX88 version of tar and paths will be longer than 100 characters.  For the long paths being used in Logstash's plugins  mainly due to nested folders from jar-dependencies  we need the tarball to follow either the 2001 ustar format or gnu tar  which is implemented by the minitar gem.  (cherry picked from commit 69f0fa54ca07cb3f822846745fdbdd1504175cfb)  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  * account for the 8.x in DRA publishing task (#16436) (#16440)  the current DRA publishing task computes the branch from the version contained in the version.yml  This is done by taking the major.minor and confirming that a branch exists with that name.  However this pattern won't be applicable for 8.x  as that branch currently points to 8.16.0 and there is no 8.16 branch.  This commit falls back to reading the buildkite injected BUILDKITE_BRANCH variable.  (cherry picked from commit 17dba9f829a2514aba295ed7a8fa21655b55c86b)  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  * Fixes the issue where LS wipes out all quotes from docker env variables. (#16456) (#16459)  * Fixes the issue where LS wipes out all quotes from docker env variables. This is an issue when running LS on docker with CONFIG_STRING  needs to keep quotes with env variable.  * Add a docker acceptance integration test.  (cherry picked from commit 7c64c7394bf47e8b5316710876ed55350df46d61)  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * Known issue for 8.15.1 related to env vars references (#16455) (#16469)  (cherry picked from commit b54caf3fd8e907c526ab2b8897ce4de4656c2fd5)  Co-authored-by: Luca Belluccini <luca.belluccini@elastic.co>  * bump .ruby_version to jruby-9.4.8.0 (#16477) (#16480)  (cherry picked from commit 51cca7320e5c54865ab3fe2d4101496bd69cacca)  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  * Release notes for 8.15.2 (#16471) (#16478)  Co-authored-by: andsel <selva.andre@gmail.com> Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com> (cherry picked from commit 01dc76f3b55333f0c49d7190c0cd4ca14b74a7c0)  * Change LogStash::Util::SubstitutionVariables#replace_placeholders refine argument to optional (#16485) (#16488)  (cherry picked from commit 8368c00367cac0c5f5e0090c26be8795b2e8c7d2)  Co-authored-by: Edmo Vamerlatti Costa <11836452+edmocosta@users.noreply.github.com>  * Use jruby-9.4.8.0 in exhaustive CIs. (#16489) (#16491)  (cherry picked from commit fd1de39005cf4646d8faa3f89b1963c716ec6088)  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * Don't use an older JRuby with oraclelinux-7 (#16499) (#16501)  A recent PR (elastic/ci-agent-images/pull/932) modernized the VM images and removed JRuby 9.4.5.0 and some older versions.  This ended up breaking exhaustive test on Oracle Linux 7 that hard coded JRuby 9.4.5.0.  PR https://github.com/elastic/logstash/pull/16489 worked around the problem by pinning to the new JRuby  but actually we don't need the conditional anymore since the original issue https://github.com/jruby/jruby/issues/7579#issuecomment-1425885324 has been resolved and none of our releasable branches (apart from 7.17 which uses `9.2.20.1`) specify `9.3.x.y` in `/.ruby-version`.  Therefore  this commit removes conditional setting of JRuby for OracleLinux 7 agents in exhaustive tests (and relies on whatever `/.ruby-version` defines).  (cherry picked from commit 07c01f8231daf14113b2ce57791712ec74365799)  Co-authored-by: Dimitrios Liappis <dimitrios.liappis@gmail.com>  * Improve pipeline bootstrap error logs (#16495) (#16504)  This PR adds the cause errors details on the pipeline converge state error logs  (cherry picked from commit e84fb458ce2f092e065c63df649222f8cbda8c44)  Co-authored-by: Edmo Vamerlatti Costa <11836452+edmocosta@users.noreply.github.com>  * Logstash Health Report Tests Buildkite pipeline setup. (#16416) (#16511)  (cherry picked from commit 5195332bc6a758198cae70fea7d88dfddf0fa15a)  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * Make health report test runner script executable. (#16446) (#16512)  (cherry picked from commit 2ebf2658ff86678125b04c8826958b468ee0da1f)  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * Backport PR #16423 to 8.x: DLQ-ing events that trigger an conditional evaluation error. (#16493)  * DLQ-ing events that trigger an conditional evaluation error. (#16423)  When a conditional evaluation encounter an error in the expression the event that triggered the issue is sent to pipeline's DLQ  if enabled for the executing pipeline.  This PR engage with the work done in #16322  the `ConditionalEvaluationListener` that is receives notifications about if-statements evaluation failure  is improved to also send the event to DLQ (if enabled in the pipeline) and not just logging it.  (cherry picked from commit b69d993d718dfd639603cdb5d340947b09a6687a)  * Fixed warning about non serializable field DeadLetterQueueWriter in serializable AbstractPipelineExt  ---------  Co-authored-by: Andrea Selva <selva.andre@gmail.com>  * add deprecation log for `--event_api.tags.illegal` (#16507) (#16515)  - move `--event_api.tags.illegal` from option to deprecated_option - add deprecation log when the flag is explicitly used relates: #16356  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com> (cherry picked from commit a4eddb8a2a79c7e1eb7696140795580427792cb1)  Co-authored-by: kaisecheng <69120390+kaisecheng@users.noreply.github.com>  ---------  Co-authored-by: ev1yehor <146825775+ev1yehor@users.noreply.github.com> Co-authored-by: João Duarte <jsvd@users.noreply.github.com> Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com> Co-authored-by: Andrea Selva <selva.andre@gmail.com> Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com> Co-authored-by: kaisecheng <69120390+kaisecheng@users.noreply.github.com> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: Luca Belluccini <luca.belluccini@elastic.co> Co-authored-by: Edmo Vamerlatti Costa <11836452+edmocosta@users.noreply.github.com> Co-authored-by: Dimitrios Liappis <dimitrios.liappis@gmail.com>  ---------  Co-authored-by: ev1yehor <146825775+ev1yehor@users.noreply.github.com> Co-authored-by: João Duarte <jsvd@users.noreply.github.com> Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com> Co-authored-by: Andrea Selva <selva.andre@gmail.com> Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com> Co-authored-by: kaisecheng <69120390+kaisecheng@users.noreply.github.com> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: Luca Belluccini <luca.belluccini@elastic.co> Co-authored-by: Edmo Vamerlatti Costa <11836452+edmocosta@users.noreply.github.com> Co-authored-by: Dimitrios Liappis <dimitrios.liappis@gmail.com> (cherry picked from commit 7eb5185b4e75061cfa0a091a8bafb622eab5a2f2)  Co-authored-by: Ry Biesemeyer <yaauie@users.noreply.github.com>
neo4j,neo4j,86018238e74b98ff0364f1c2c48fc4d222de2b8b,https://github.com/neo4j/neo4j/commit/86018238e74b98ff0364f1c2c48fc4d222de2b8b,Performance improvements to ID updates in incremental import
neo4j,neo4j,80914d4a9ecab6055d0873e5701509214689c4a5,https://github.com/neo4j/neo4j/commit/80914d4a9ecab6055d0873e5701509214689c4a5,Expose a hidden admin import option to specify the number of ranges  This can be considered a temporary option for certain edge cases to try and improve the import performance  until the range calculation covers all cases.
neo4j,neo4j,3f944c536f5b3f987f5996bdc5ab54a2eb02a0bc,https://github.com/neo4j/neo4j/commit/3f944c536f5b3f987f5996bdc5ab54a2eb02a0bc,Fix bug and performance regression in error handling
neo4j,neo4j,58c1ae80136353006cc1d189dac73c177289d9ac,https://github.com/neo4j/neo4j/commit/58c1ae80136353006cc1d189dac73c177289d9ac,SPD: Query router is bypassed on SPD shards  When Cypher is used as RPC for internal in-cluster communication  the entire query routing logic can be bypassed for performance reasons. Such queries are not created by users  but generated by another DBMS instance  they are client-side routed and a session database is always the target one.
supertokens,supertokens-core,177ee2f49a02761b7ab6a0777349794e5e08f5db,https://github.com/supertokens/supertokens-core/commit/177ee2f49a02761b7ab6a0777349794e5e08f5db,feat: webauthn base (#1115)  * feat: new dependency: webauthn4j  * feat: add tables for webauthn  * fix: typo fixes  * feat: webauthn options  * feat: registercredentials wip  * feat: passkeys register credentials wip  * feat: recipe user sign up  * recipe user creation wip  * sign up recipe user  * feat: register credentials  * fix: temp  * feat: webauthn support wip  * feat: webauthn support wip  * merging  * feat: webauthn support wip  * feat: getuserinfolist draft  * feat: get user by account info - webauthn support  * fix: generate account recovery token api  * feat: get user by account info - webauthn support  * feat: signup with credentialsregister  * fix: fixes for tests  * fix: fixes for tests  * feat: get generated options api  * feat: webauthn sign in  * fix: account recovery  * fix: fixes for tests  * fix: fixing id name in response  * fix: fixing id encoding in response  * fix: base64 url encode the challenge insted of base64 encode  * fix: account recovery impl  * fix: base64 encoding changes  * fix: fixes for tests  * fix: fixing sql issues and encoding issues  * fix: fixes for tests  * fix: integration fix for signup  * fix: webauthn flow test stub  * fix: fixes for sdk tests  * feat: add webauthn recover account apis to webserver  * feat: crud apis addition  * feat: remove options api  * fix: additional field in the sign in options response  * fix: reworked error handling  * fix: fixing GET api not to expect json body  * fix: sign in + options check  * fix: more descriptive error messages for credentialsRegister  * fix: typo fixes  * fix: fixes for tests  * fix: not letting dependencies exception to leak out  * feat: clean up expired data cron  * fix: changing recovery token consume  * fix: fixing loginmethod collection  * fix: signin fixes  * fix: webauthn sign in fixes  * fix: add recipeUserId in signIn response  * feat: enable credentials listing api  * feat: extending user listing with webauthn  * fix: don't use the counter at signin check  * fix: small fixes  * fix: setting UV and RK to false  * feat: saving userVerification and userPresence values  * fix: change a bunch of error messages for sdk integration  * fix: change a bunch of error messages for sdk integration  * fix: include userVerification and userPresence in options response  * fix: error messages changes  * fix: refactor exceptions  * feat: get credential api  * fix: options generation no longer throws invalid options error as per reference impl  * fix: more error handling for sign in  * fix: rename methods for better readability  * fix: throw the right exception  * ci: experiment with a GHA to publish test/dev images  * ci: experiment with publishing dev docker images  * ci: experiment with publishing dev docker images  * ci: experiment with a GHA to publish test/dev images  * fix: options validation  * fix: options validation rpId doesn't have to be an url  * fix: additional validation  * fix: fixes for various sdk tests  * fix: Dockerfile setupTestEnv --local  * ci: remove arm64 build from dev-docker  * fix: add webauth4jn-test dependency  * fix: fixing email verification query for webauthn  * fix: authenticator mocking and example usage  * fix: sem ver and few test fixes  * fix: test fixes  * fix: cdi version increment in webauthn test  * fix: webauthn signIn should load all loginmethods of the user  * fix: fixing table locked issue with in memory db  * fix: remove unnecessary logging  * fix: add tests and fixes  * fix: add tests and fixes for email update  * fix: additional tests and fixes related to useridmapping  * fix: add null check  * fix: add test  * fix: additional indexes for performance optimization  * ci: fix dev-docker build  * fix: self-review fixes  * fix: update pluginInterfaceSupported to the right branch  * chore: changelog  version number  * fix: review fixes  * fix: review fixes  * fix: fixing email verified flag after email change  * fix: review fixes  * fix: handling potential error while saving options  * fix: review fixes  * chore: updating supported pluginInterface  * chore: updating supported pluginInterface  * test: API tests (#1118)  * fix: API tests template  * fix: options register APIs  * test: register credential  * test: fix  * fix: test get credential  * test: list credential  * test: remove credential  * test: remove credential  * test: sign in options  * test: sign-in  * test: sign-in  * test: update email  * fix: delete  * fix: tests for mongodb  * fix: tests  * fix: tests  * fix: review fixes  * fix: review fix: token generation changes  ---------  Co-authored-by: Sattvik Chakravarthy <sattvik@gmail.com> Co-authored-by: Mihaly Lengyel <mihaly@lengyel.tech> Co-authored-by: Sattvik Chakravarthy <sattvik@supertokens.com>
theonedev,onedev,e7dec6644b6929afb650d0e288b396476e00674b,https://github.com/theonedev/onedev/commit/e7dec6644b6929afb650d0e288b396476e00674b,feat: Buffer file writing to improve performance on slow storage devices (OD-2206)
theonedev,onedev,d35cf26d803b4ac751147e5e91d6f82bf96bf1dd,https://github.com/theonedev/onedev/commit/d35cf26d803b4ac751147e5e91d6f82bf96bf1dd,Some improvements over performance and usability  feat: Issue list performance improvements (OD-2042) feat: Add criteria to query project by id (OD-2041)
theonedev,onedev,12ff4107163abc55ea1cfb1a20386236506964af,https://github.com/theonedev/onedev/commit/12ff4107163abc55ea1cfb1a20386236506964af,chore: Improve label loading performance
deeplearning4j,deeplearning4j,0ab6236295ca75736f591c2ea805e438fc580dbd,https://github.com/deeplearning4j/deeplearning4j/commit/0ab6236295ca75736f591c2ea805e438fc580dbd,Remove more elementwise stride  introduce better TAD caching (#10165)  * minor fix to modular hasher compilation/usage fix performance issues with pairwise usage (shape::rank was dominating cpu usage caching values removes this)  * Refactor index2coords/coords2index inlineiing  * cuda updates for corresponding caching of shape information  * fix performance regression with the naive non view case with reduce  * fix reduce extra params typing  * remove print statement  * fix reduce extra params typing  * remove cpp markdown initialize directshapetrie  * fix license headers  * Add new TAD caching using tries similar to shape buffers Remove unused methods from shape.h Remove helpers/TAD.h Remove pairwise_util.h Remove commented code/imports Fix indexreduce linker errors  * restore benchmarks  * fix merging artifacts  * remove old impl comments more section removals  * fix cuda chunking issues  * restore benchmarks copyright  * more reversions  * more reversions  * fix cuda chunking issues
deeplearning4j,deeplearning4j,194956504ab3401cf1ca7fc9ecec61ccaf532d2d,https://github.com/deeplearning4j/deeplearning4j/commit/194956504ab3401cf1ca7fc9ecec61ccaf532d2d,Refactoring caching with index calculations (#10164)  * minor fix to modular hasher compilation/usage fix performance issues with pairwise usage (shape::rank was dominating cpu usage caching values removes this)  * Refactor index2coords/coords2index inlineiing  * cuda updates for corresponding caching of shape information  * fix performance regression with the naive non view case with reduce  * fix reduce extra params typing  * remove print statement  * fix reduce extra params typing  * remove cpp markdown initialize directshapetrie  * fix license headers
deeplearning4j,deeplearning4j,ddab396b8df01ecaa015aea0bdd626c5120fb66d,https://github.com/deeplearning4j/deeplearning4j/commit/ddab396b8df01ecaa015aea0bdd626c5120fb66d,Remove shape descriptor creation when caching shape buffers (#10162)  * add new smoke tests fix scalar index when using the indexing api fix usage of index2coords by migrating shape information to shape::shapeOf fix usage of coords2index by migrating accidental shapeOf invocations to strides  * Fix reduce3 buffer() calls in op execution  * Add opaquendarray cachcing/deletion  on close Add new memory pressure test Update nd4j benchmarking with jemalloc docs Add licenses Remove more old aurora code Add basic reduce smoke tests  * remove print statements  * add more debugging documentaiton under troubleshooting/ rewrite benchmarks Add new memory profiler Fix deallocation crash Add environment sourcing in the custom java executable to allow for more customization  * add more debugging documentaiton under troubleshooting/ rewrite benchmarks Add new memory profiler Fix deallocation crash Add environment sourcing in the custom java executable to allow for more customization  * performance optimizations  * update benchmarks remove shape descriptor usage introduce new trie based shape buffer cache fix test java bash script to use relative path for env.sh sourcing  * fix opaquendarray caching fix javacpp compilation remove print statements  * Add ADR Fix nits  * Collapse all the descriptor hashes in to a modular hasher  * Add ADR Fix nits  * clean up ADR  * remove commented code
apache,druid,3ef2e5e504f21e6b57366a9fae92f7d5a638350e,https://github.com/apache/druid/commit/3ef2e5e504f21e6b57366a9fae92f7d5a638350e,Add policy enforcer to sanity check on policy in query execution (#17774)  * Some debug configs  * use postgresql as the default metadata store and set a few debug log  * Add s3 extension  update local storage directory  use emoji in website title  * Update favicon  easier to find the console tab  * Add indexer server  add some basic security config  updated historical and broker to use the common druid root directory  * Some policy config  * add checks for SegmentMetadataQuery  * Add thread.sleep for flaky.  * auth config  * format  and remove temp folder rules  * added NoopPolicyEnforcer and RestrictAllTablesPolicyEnforcer class  * Support pushing and streaming task payload for HDFS (#17742)  Implement pushTaskPayload/streamTaskPayload as introduced in #14887 for HDFS storage to allow larger mm-less ingestion payloads when using HDFS as the deep storage location.  * Remove usages of deprecated API Files.write() (#17761)  * Add deprecated com.google.common.io.Files#write to forbiddenApis  * Replace deprecated Files.write()  * Doc: Fix description typo for sqlserver metadata store (#17771)  Mistakenly categories under deep storage instead of metadata store.  * Fix binding of segment metadata cache on CliOverlord (#17772)  Changes --------- - Bind `SegmentMetadataCache` only once to `HeapMemorySegmentMetadataCache` in `SQLMetadataStorageDruidModule` - Invoke start and stop of the cache from `DruidOverlord` rather than on lifecycle start/stop - Do not override the binding in `CliOverlord`  * Docs: Remove semicolon from example (#17759)  * Restrict segment metadata kill query till maxInterval from last kill task time (#17770)  Changes --------- - Use `maxIntervalToKill` to determine search interval for killing unused segments. - If no segment has been killed for the datasource yet  use durationToRetain  * Update the Supervisor endpoint to not restart the Supervisor if the spec was unmodified (#17707)  Add an optional query parameter called skipRestartIfUnmodified to the /druid/indexer/v1/supervisor endpoint. Callers can set skipRestartIfUnmodified=true to not restart the supervisor if the spec is unchanged.  Example:  curl -X POST --header "Content-Type: application/json" -d @supervisor.json localhost:8888/druid/indexer/v1/supervisor?skipRestartIfUnmodified=true  * Reduce noisy coordinator logs (#17779)  * Emit time lag from Kafka supervisor (#17735)  Changes --------- - Emit time lag from Kafka similar to Kinesis as metrics `ingest/kafka/lag/time`  `ingest/kafka/maxLag/time`  `ingest/kafka/avgLag/time` - Add new method in `KafkaSupervisor` to fetch timestamps of latest records in stream to compute time lag - Add new field `emitTimeLagMetrics` in `KafkaSupervisorIOConfig` to toggle emission of new metrics  * fix processed row formatting (#17756)  * Web console: add suggestions for table status filtering. (#17765)  * suggest filter values when known  * update snapshots  * add more d  * fix load rule clamp  * better segment timeline init  * Remove all usages of skife config (#17776)   Changes --------- - Usages of skife config had been deprecated in #14695 and `LegacyBrokerParallelMergeConfig` is the last config class that still uses it. - Remove `org.skife.config` from pom  licenses  log4j2.xml  etc. - Add validation for deleted property paths in `StartupInjectorBuilder.PropertiesValidator` - Use the replacement flattened configs (which remove the `.task` and `.pool` substring)  * Add field `taskLimits` to worker select strategies (#16889)  Changes --------- - Add field `taskLimits` to the following worker select strategies `equalDistribution`  `equalDistributionWithCategorySpec`  `fillCapacityWithCategorySpec`  `fillCapacity` - Add sub-fields `maxSlotCountByType` and `maxSlotRatioByType` to `taskLimits` - Apply these limits per worker when assigning new tasks  --------- Co-authored-by: sviatahorau <mikhail.sviatahorau@deep.bi> Co-authored-by: Benedict Jin <asdf2014@apache.org> Co-authored-by: Kashif Faraz <kashif.faraz@gmail.com>  * remove NullValueHandlingConfig  NullHandlingModule  NullHandling (#17778)  * Docs: Add SQL query example (#17593)  * Docs: Add query example  * Update after review  * Update query  * Update docs/api-reference/sql-api.md  ---------  Co-authored-by: Victoria Lim <vtlim@users.noreply.github.com>  * More logging cleanup on Overlord (#17780)  * Remove maven.twttr repo from pom (#17797)  remove usage of dependency:go-offline from build scripts - as it tries to download excluded artifacts  ---------  Co-authored-by: Zoltan Haindrich <kirk@rxd.hu>  * fix bug (#17791)  * Log query stack traces for DEVELOPER and OPERATOR personas. (#17790)  Currently  query stack traces are logged only when "debug: true" is set in the query context. This patch additionally logs stack traces targeted at the DEVELOPER or OPERATOR personas  because for these personas  stack traces are useful more often than not.  We continue to omit stack traces by default for USER and ADMIN  because these personas are meant to interact with the API  not with code or logs. Skipping stack traces minimizes clutter in the logs.  * Set useMaxMemoryEstimates=false for MSQ tasks (#17792)  * Web console: fix go to task selecting correct task type (#17788)  * fix go to task selecting correct task type  * support autocompact also  * support scheduled_batch  refactor  * one more state and update tests  * Enable ComponentSuppliers to run queries using Dart (#17787)    Enables Calcite*Test-s and quidem tests to run queries with Dart.  needed some minor tweaks:  changed to use interfaces at some places renamed DartWorkerClient to DartWorkerClientImpl and made DartWorkerClient an interface reused existing parts of the MSQ test system to run the query  * Fix single container config creates failing peon tasks (#17794)  * Fix single container config creates failing peon tasks  * More obvious array error output  * Update `k8s-jobs.md` reference (#17805)  Signed-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>  * Footer Copyright Year Update (#17751)  * Update docusaurus.config.js  * Update docusaurus.config.js  * [Revert] Reduce number of metadata transaction retries (#17808)  * Revert "Run JDK 21 workflows with latest JDK. (#17694)" (#17806)  * Revert "Run JDK 21 workflows with latest JDK. (#17694)"  This reverts commit 31ede5cb  * Review comments.  * Review comments.  * Revert "reject publishing actions with a retriable error code if a earlier task is still publishing (#17509)"  This reverts commit aca56d6bb842231853d624e7da07748ba002ac4f.  * Fix unstable tests after #17787 and dart usage in quidem-ut (#17814)  * fixes  * fix cleanup  * Use "mix" shuffle spec for target size with nil clusterBy. (#17810)  When a nil clusterBy is used  we have no way of achieving a particular target size  so we need to fall back to a "mix" spec (unsorted single partition).  This comes up for queries like "SELECT COUNT(*) FROM FOO LIMIT 1" when results use a target size  such as when we are inserting into another table or when we are writing to durable storage.  * Docs: Recommend using runtime property javaOptsArray instead of javaOpts  * Add minor checks in jetty utils (#17817)  Add minor checks in jetty utils class  * CI improvement: Leverage cancelled() instead of always() for CI jobs (#17819)  * Make MSQ tests use the same datasets as other similar tests (#17818)  MSQ tests had their own way of creating the segments/etc - this have lead to that custom datasets didn't worked with them. This patch alters a few things to make it possible to access CompleteSegment for the active segments - which fixed the issue and also enabled the removal of the extra loading codes.  * Add unnest tests to quidem (#17825)  This PR adds the sql-native unnest tests to quidem. This set of tests has 6392 queries in total  with 5247 positive tests and 1145 negative tests.  * Web console: show loader on aux queries (#17804)  * show loader on aux queries  * show supervisors if not on page 0  * refactor  * fix bug fetching data when columns are added or removed  * update test  * Use compaction dynamic config to enable compaction supervisors (#17782)  Changes --------- - Remove runtime property object `CompactionSupervisorConfig` - Add fields `useSupervisors` and `engine` to cluster-level compaction dynamic config - Remove unused field `useAutoScaleSlots`  * Retry segment publish task actions without holding locks (#17816)  #17802 reverted a retry of failed segment publish actions.  This patch attempts to address the original issue by retrying the segment publish task actions on the client (i.e. task) side without holding any locks so that other transactions are not blocked. Changes  Add retries to TransactionalSegmentPublisher Add field retryable to SegmentPublishResult Remove class DataStoreMetadataUpdateResult and use SegmentPublishResult instead  * Add the capability to turboload segments onto historicals (#17775)  Add the capability to set Historicals into a turbo loading mode  to focus on loading segments at the cost of query performance.  Context -------- Currently  when a new Historical is started  it initially starts out using a bootstrap thread pool. It uses this thread pool to load any existing cached segments and broadcast segments. Once it loads any segments from both these sources  the historical switches to a smaller thread-pool and begins to serve queries.  In certain cases  it would be useful to have the historical switch back to this mode  and focus on loading segments  either to continue loading the initial non-bootstrap segments  or to catch up with assigned segments.  This PR adds a coordinator dynamic config that allows servers to be configured to use the larger bootstrap threadpool to load segments faster.  Changes --------- - Added a new dynamic coordinator configuration  `turboLoadingNodes`. - Ignore  `druid.coordinator.loadqueuepeon.http.batchSize` for servers in `turboLoadingNodes` - Add API on historical to return loading capabilities i.e. num loading threads in normal and turbo mode  * Fix resource leak for GroupBy query merge buffer when query matched result cache (#17823)  * Fix resource leak for GroupBy query merge buffer when match result cache  * Fix resource leak for GroupBy query merge buffer when match result cache  * Add test  * Add test  * Add comment  * Add test  * Add metric and simulation test for turbo loading mode (#17830)  Changes --------- - Add field `loadingMode` to `SegmentChangeStatus` - Including loading mode in `DataSegmentChangeResponse` - Include loading mode in the `description` of metrics emitted from `HttpLoadQueuePeon` - Add simulation test to verify loading mode metrics  * Update query example (#17811)  * String util upgrade for jdk9+ (#17795)  * Update StringUtils.replace() after fix in JDK9  * Upgrade optimized string replace algorithm  * Update methods by re-using declared StringUtils#replace method  * Replace hard-coded UTF-8 encodings with StandardCharsets  * Documentation Fix (#17826)  * Enable to run quidem tests against multiple configurations; add conditionals; cleanup framework init (#17829)  * cleans up `SqlTestFramework` initialization to leave the `OverrideModule` empty - so that tests could more easily take over parts * remove the `QueryComponentSupplier#createEngine`  factory method - instead uses a `Class<SqlEngine>` and use the `injector` to initialize it * enables the usage of `!disabled <supplier> <message>` - to mark cases which are not yet supported with a specific configuration for some reason * fixes that `datasets` was not respecting the `rollup` specification of the ingest * enables to use `MultiComponentSupplier` backed tests - these will turn into matrix tests over multiple componentsuppliers - enabling running the same testcase in different scenarios  * Fix failing test in DimensionSchemaUtilsTest (#17832)  * Improve performance of segment metadata cache on Overlord (#17785)  Description ----------- #17653 introduces a cache for segment metadata on the Overlord. This patch is a follow up to that to make the cache more robust  performant and debug-friendly.  Changes --------- - Do not cache unused segments This significantly reduces sync time in cases where the cluster has a lot of unused segments. Unused segments are needed only during segment allocation to ensure that a duplicate ID is not allocated. This is a rare DB query which is supported by sufficient indexes and thus need not be cached at the moment. - Update cache directly when segments are marked as unused to avoid race conditions with DB sync. - Fix NPE when using segment metadata cache with concurrent locks. - Atomically update segment IDs and pending segments in a `HeapMemoryDatasourceSegmentCache` using methods `syncSegmentIds()` and `syncPendingSegments()` rather than updating one by one. This ensures that the locks are held for a shorter period and the update made to the cache is atomic.  Main updated classes ---------------------- - `IndexerMetadataStorageCoordinator` - `OverlordDataSourcesResource` - `HeapMemorySegmentMetadataCache` - `HeapMemoryDatasourceSegmentCache`  Cleaner cache sync -------------------- In every sync  the following steps are performed for each datasource:  - Retrieve ALL used segment IDs from metadata store - Atomically update segment IDs in cache and determine list of segment IDs which need to be refreshed. - Fetch payloads of segments that need to be refreshed - Atomically update fetched payloads into the cache - Fetch ALL pending segments - Atomically update pending segments into the cache - Clean up empty intervals from datasource caches  * GroupBy: Fix offsets on outer queries. (#17837)  Prior to this patch  an offset specified on a groupBy that itself has an inner groupBy would lead to an error like "Cannot push down offsets". This happened because of a violated assumption: the processing logic assumes that offsets have been pushed into limits (so limit pushdown optimizations can safely be used).  This patch adjusts processing to incorporate offsets into limits during processing of subqueries. Later on  in post-processing  offsets are applied as written.  * Enable build cache for web-console (#17831)  * run audit fix (#17836)  * Do not block task actions on Overlord if segment metadata cache is syncing (#17824)  * Do not use segment metadata cache until leader has synced  * Read from cache only when synced  but write even if sync is pending  * Fix compilation  * Fix checkstyle  test  * Revert some extra changes  * Add 3 modes of cache usage  * Move enum to SegmentMetadataCache  * Run tests in all 3 cache modes  * Fix docs and IT configs  * Fix config binding  * Remove forbidden api  * Fix typos  docs and enum casing  * Fix doc  * Add json  array  aggregation function tests to quidem (#17842)  This PR adds the sql-native portion of the json  array  and aggregation function tests to quidem.  It adds a total of 9965 queries  with 6752 positive tests and 3213 negative tests.  * Optionally include Content-Disposition header in statement results API response (#17840)  Adds support for an optional filename query parameter to the /druid/v2/sql/statements/{queryId}/results API. When provided  the response will include a header Content-Disposition: attachment; filename="{filename}"  which will instruct a web browser to save the response as a file rather than displaying it inline.  This save-as-attachment behavior could be achieved by adding a "download" attribute to the results link  but this only works for same-origin URLs (as in the Web Console). If the UI origin is different from the Druid API origin  browsers will ignore the attribute and serve the results inline  which is poor UX for files that are potentially very large.  For the sake of consistency  all successful responses in SqlStatementResource.doGetResults may include this header  even if there are no results. Release note  Improved: The "Get query results" statements API supports an optional filename query parameter. When provided  the response will instruct web browsers to save the results as a file instead of showing them inline (via the Content-Disposition header).  * Web console: download follow up (#17845)  * set filename  * update download button  * added markdown support  * add test  * better download  * fix TSV  * better download behaviour and tests  * always show download all button  * Fix flaky unit tests in SegmentBootstrapperTest and KinesisIndexTaskTest (#17841)  Changes: - Fix flakiness in SegmentBootstrapperTest - Make TestSegmentCacheManager thread safe by moving from ArrayList to CopyOnWriteArrayList - Modify assertions to disregard list ordering since order of list modifications is not always deterministic - Fix flaky KinesisIndexTask tests.  * Web console: responding to user feedback about the explore view and fixing bugs (#17844)  * better debounce  * better cumpose filter  * hook up preview filters  * better stack handling  * fix some props  * refactor stack to facet  * fix hover part 1  * line hover part 2  * start adding moduleWhere  * info popover  * add filter icon  * toggle button  * module filter bar  * update TestSegmentCacheManager  * revert some style changes  * validate datasource in CachingClusteredClient as well  * fix build failure and update style  * changes  * add inlineds test  * add sanity check on segment  * inject policy enforcer  * add PolicyEnforcer binding in MSQTestBase  * add check in SinkQuerySegmentWalker  * more tests in realtime server  * revert config change in examples  * revert config change in integration test config  * more tests in msq  * another test for unnest in msq  * add support for policy from extension  * more test  * refactor MSQTaskQueryMakerTest to use an instance of MSQTaskQueryMaker  * Add test for JoinDataSource  * add policyEnforcer to withPolicies  and validate segment after segment mapping  * fix binding and test  * add policy module  * mock planner toolbox  * revert some injection  * add test for stream appenderator  * update PolicyEnforcer to take ReferenceCountingSegment as param  * update to QueryLifecycleTest  * update to SqlTestFramework  * pass enforcer to BroadcastJoinSegmentMapFnProcessor and add test. PolicyEnforcer should also deal with multiple layer wrapped segments/  * ReferenceCountingSegment is not allowed to wrap with a SegmentReference  and PolicyEnforcer now validates all segments  remove test cases for inline/lookup.  * moving ReferenceCountingSegment to another pr  * Revert "Merge remote-tracking branch 'cecemei/debug' into policy"  This reverts commit 25ffb7ca8e5228786c1da65bc000b7de596dcd95  reversing changes made to 1e6632fcb779919f3fc54050ab9b44aba19d3265.  ---------  Signed-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com> Co-authored-by: Virushade <70288012+GWphua@users.noreply.github.com> Co-authored-by: Eyal Yurman <eyal.yurman@gmail.com> Co-authored-by: Kashif Faraz <kashif.faraz@gmail.com> Co-authored-by: Frank Chen <frank.chen021@outlook.com> Co-authored-by: Chetan Patidar <122344823+chetanpatidar26@users.noreply.github.com> Co-authored-by: aho135 <ash023@ucsd.edu> Co-authored-by: Adithya Chakilam <35785271+adithyachakilam@users.noreply.github.com> Co-authored-by: Vadim Ogievetsky <vadim@ogievetsky.com> Co-authored-by: Misha <mikhailsviatohorof@gmail.com> Co-authored-by: sviatahorau <mikhail.sviatahorau@deep.bi> Co-authored-by: Benedict Jin <asdf2014@apache.org> Co-authored-by: Clint Wylie <cwylie@apache.org> Co-authored-by: Katya Macedo <38017980+ektravel@users.noreply.github.com> Co-authored-by: Victoria Lim <vtlim@users.noreply.github.com> Co-authored-by: Zoltan Haindrich <kirk@rxd.hu> Co-authored-by: Gian Merlino <gianmerlino@gmail.com> Co-authored-by: Emmanuel Ferdman <emmanuelferdman@gmail.com> Co-authored-by: Om Kenge <88768848+omkenge@users.noreply.github.com> Co-authored-by: Karan Kumar <karankumar1100@gmail.com> Co-authored-by: Lars Francke <lars.francke@stackable.tech> Co-authored-by: Adarsh Sanjeev <adarshsanjeev@gmail.com> Co-authored-by: Akshat Jain <akjn11@gmail.com> Co-authored-by: Andy Tsai <61856143+weishiuntsai@users.noreply.github.com> Co-authored-by: Maytas Monsereenusorn <maytasm@apache.org> Co-authored-by: jtuglu-netflix <jtuglu@netflix.com> Co-authored-by: Lucas Capistrant <capistrant@users.noreply.github.com>
apache,druid,c0cc27ce87ba4aa38cf3073f8f8a7c9d8ec43822,https://github.com/apache/druid/commit/c0cc27ce87ba4aa38cf3073f8f8a7c9d8ec43822,Improve performance of segment metadata cache on Overlord (#17785)  Description ----------- #17653 introduces a cache for segment metadata on the Overlord. This patch is a follow up to that to make the cache more robust  performant and debug-friendly.  Changes --------- - Do not cache unused segments This significantly reduces sync time in cases where the cluster has a lot of unused segments. Unused segments are needed only during segment allocation to ensure that a duplicate ID is not allocated. This is a rare DB query which is supported by sufficient indexes and thus need not be cached at the moment. - Update cache directly when segments are marked as unused to avoid race conditions with DB sync. - Fix NPE when using segment metadata cache with concurrent locks. - Atomically update segment IDs and pending segments in a `HeapMemoryDatasourceSegmentCache` using methods `syncSegmentIds()` and `syncPendingSegments()` rather than updating one by one. This ensures that the locks are held for a shorter period and the update made to the cache is atomic.  Main updated classes ---------------------- - `IndexerMetadataStorageCoordinator` - `OverlordDataSourcesResource` - `HeapMemorySegmentMetadataCache` - `HeapMemoryDatasourceSegmentCache`  Cleaner cache sync -------------------- In every sync  the following steps are performed for each datasource:  - Retrieve ALL used segment IDs from metadata store - Atomically update segment IDs in cache and determine list of segment IDs which need to be refreshed. - Fetch payloads of segments that need to be refreshed - Atomically update fetched payloads into the cache - Fetch ALL pending segments - Atomically update pending segments into the cache - Clean up empty intervals from datasource caches
apache,druid,08af98c73bf7f35e4c7e2b136a63158d11b02348,https://github.com/apache/druid/commit/08af98c73bf7f35e4c7e2b136a63158d11b02348,Add the capability to turboload segments onto historicals (#17775)  Add the capability to set Historicals into a turbo loading mode  to focus on loading segments at the cost of query performance.  Context -------- Currently  when a new Historical is started  it initially starts out using a bootstrap thread pool. It uses this thread pool to load any existing cached segments and broadcast segments. Once it loads any segments from both these sources  the historical switches to a smaller thread-pool and begins to serve queries.  In certain cases  it would be useful to have the historical switch back to this mode  and focus on loading segments  either to continue loading the initial non-bootstrap segments  or to catch up with assigned segments.  This PR adds a coordinator dynamic config that allows servers to be configured to use the larger bootstrap threadpool to load segments faster.  Changes --------- - Added a new dynamic coordinator configuration  `turboLoadingNodes`. - Ignore  `druid.coordinator.loadqueuepeon.http.batchSize` for servers in `turboLoadingNodes` - Add API on historical to return loading capabilities i.e. num loading threads in normal and turbo mode
apache,druid,9c25226e06da1eb7d3a47742dd4e65337585142c,https://github.com/apache/druid/commit/9c25226e06da1eb7d3a47742dd4e65337585142c,QueryableIndexSegment: Re-use time boundary inspector. (#17397)  This patch re-uses timeBoundaryInspector for each cursor holder  which enables caching of minDataTimestamp and maxDataTimestamp.  Fixes a performance regression introduced in #16533  where these fields stopped being cached across cursors. Prior to that patch  they were cached in the QueryableIndexStorageAdapter.
apache,druid,db7cc4634c5df247376e251fb1f502cfc7ae934d,https://github.com/apache/druid/commit/db7cc4634c5df247376e251fb1f502cfc7ae934d,Dart: Smoother handling of stage early-exit. (#17228)  Stages can be instructed to exit before they finish  especially when a downstream stage includes a "LIMIT". This patch has improvements related to early-exiting stages.  Bug fix:  - WorkerStageKernel: Don't allow fail() to set an exception if the stage is already in a terminal state (FINISHED or FAILED). If fail() is called while in a terminal state  log the exception  then throw it away. If it's a cancellation exception  don't even log it. This fixes a bug where a stage that exited early could transition to FINISHED and then to FAILED  causing the overall query to fail.  Performance:  - DartWorkerManager previously sent stopWorker commands to workers even when "interrupt" was false. Now it only sends those commands when "interrupt" is true. The method javadoc already claimed this is what the method did  but the implementation did not match the javadoc. This reduces the number of RPCs by 1 per worker per query.  Quieter logging:  - In ReadableByteChunksFrameChannel  skip logging exception from setError if the channel has been closed. Channels are closed when readers are done with them  so at that point  we wouldn't be interested in the errors.  - In RunWorkOrder  skip calling notifyListener on failure of the main work  in the case when stop() has already been called. The stop() method will set its own error using CanceledFault. This enables callers to detect when a stage was canceled vs. failed for some other reason.  - In WorkerStageKernel  skip logging cancellation errors in fail(). This is made possible by the previous change in RunWorkOrder.
apache,druid,878adff9aaa8d28ddbba0119c44dae4809dd5744,https://github.com/apache/druid/commit/878adff9aaa8d28ddbba0119c44dae4809dd5744,MSQ profile for Brokers and Historicals. (#17140)  This patch adds a profile of MSQ named "Dart" that runs on Brokers and Historicals  and which is compatible with the standard SQL query API. For more high-level description  and notes on future work  refer to #17139.  This patch contains the following changes  grouped into packages.  Controller (org.apache.druid.msq.dart.controller):  The controller runs on Brokers. Main classes are   - DartSqlResource  which serves /druid/v2/sql/dart/. - DartSqlEngine and DartQueryMaker  the entry points from SQL that actually run the MSQ controller code. - DartControllerContext  which configures the MSQ controller. - DartMessageRelays  which sets up relays (see "message relays" below) to read messages from workers' DartControllerClients. - DartTableInputSpecSlicer  which assigns work based on a TimelineServerView.  Worker (org.apache.druid.msq.dart.worker)  The worker runs on Historicals. Main classes are   - DartWorkerResource  which supplies the regular MSQ WorkerResource  plus Dart-specific APIs. - DartWorkerRunner  which runs MSQ worker code. - DartWorkerContext  which configures the MSQ worker. - DartProcessingBuffersProvider  which provides processing buffers from sliced-up merge buffers. - DartDataSegmentProvider  which provides segments from the Historical's local cache.  Message relays (org.apache.druid.messages):  To avoid the need for Historicals to contact Brokers during a query  which would create opportunities for queries to get stuck  all connections are opened from Broker to Historical. This is made possible by a message relay system  where the relay server (worker) has an outbox of messages.  The relay client (controller) connects to the outbox and retrieves messages. Code for this system lives in the "server" package to keep it separate from the MSQ extension and make it easier to maintain. The worker-to-controller ControllerClient is implemented using message relays.  Other changes:  - Controller: Added the method "hasWorker". Used by the ControllerMessageListener to notify the appropriate controllers when a worker fails. - WorkerResource: No longer tries to respond more than once in the "httpGetChannelData" API. This comes up when a response due to resolved future is ready at about the same time as a timeout occurs. - MSQTaskQueryMaker: Refactor to separate out some useful functions for reuse in DartQueryMaker. - SqlEngine: Add "queryContext" to "resultTypeForSelect" and "resultTypeForInsert". This allows the DartSqlEngine to modify result format based on whether a "fullReport" context parameter is set. - LimitedOutputStream: New utility class. Used when in "fullReport" mode. - TimelineServerView: Add getDruidServerMetadata as a performance optimization. - CliHistorical: Add SegmentWrangler  so it can query inline data  lookups  etc. - ServiceLocation: Add "fromUri" method  relocating some code from ServiceClientImpl. - FixedServiceLocator: New locator for a fixed set of service locations. Useful for URI locations.
apache,druid,b9a4c73e525d7addd9cde078e62490e2943da6e9,https://github.com/apache/druid/commit/b9a4c73e525d7addd9cde078e62490e2943da6e9,Window Functions : Improve performance by comparing Strings in frame bytes without converting them (#17091)
apache,druid,0603d5153d8177856687c957ae0061492c81e1d4,https://github.com/apache/druid/commit/0603d5153d8177856687c957ae0061492c81e1d4,Segments sorted by non-time columns. (#16849)  * Segments primarily sorted by non-time columns.  Currently  segments are always sorted by __time  followed by the sort order provided by the user via dimensionsSpec or CLUSTERED BY. Sorting by __time enables efficient execution of queries involving time-ordering or granularity. Time-ordering is a simple matter of reading the rows in stored order  and granular cursors can be generated in streaming fashion.  However  for various workloads  it's better for storage footprint and query performance to sort by arbitrary orders that do not start with __time. With this patch  users can sort segments by such orders.  For spec-based ingestion  users add "useExplicitSegmentSortOrder: true" to dimensionsSpec. The "dimensions" list determines the sort order. To define a sort order that includes "__time"  users explicitly include a dimension named "__time".  For SQL-based ingestion  users set the context parameter "useExplicitSegmentSortOrder: true". The CLUSTERED BY clause is then used as the explicit segment sort order.  In both cases  when the new "useExplicitSegmentSortOrder" parameter is false (the default)  __time is implicitly prepended to the sort order  as it always was prior to this patch.  The new parameter is experimental for two main reasons. First  such segments can cause errors when loaded by older servers  due to violating their expectations that timestamps are always monotonically increasing. Second  even on newer servers  not all queries can run on non-time-sorted segments. Scan queries involving time-ordering and any query involving granularity will not run. (To partially mitigate this  a currently-undocumented SQL feature "sqlUseGranularity" is provided. When set to false the SQL planner avoids using "granularity".)  Changes on the write path:  1) DimensionsSpec can now optionally contain a __time dimension  which controls the placement of __time in the sort order. If not present  __time is considered to be first in the sort order  as it has always been.  2) IncrementalIndex and IndexMerger are updated to sort facts more flexibly; not always by time first.  3) Metadata (stored in metadata.drd) gains a "sortOrder" field.  4) MSQ can generate range-based shard specs even when not all columns are singly-valued strings. It merely stops accepting new clustering key fields when it encounters the first one that isn't a singly-valued string. This is useful because it enables range shard specs on "someDim" to be created for clauses like "CLUSTERED BY someDim  __time".  Changes on the read path:  1) Add StorageAdapter#getSortOrder so query engines can tell how a segment is sorted.  2) Update QueryableIndexStorageAdapter  IncrementalIndexStorageAdapter  and VectorCursorGranularizer to throw errors when using granularities on non-time-ordered segments.  3) Update ScanQueryEngine to throw an error when using the time-ordering "order" parameter on non-time-ordered segments.  4) Update TimeBoundaryQueryRunnerFactory to perform a segment scan when running on a non-time-ordered segment.  5) Add "sqlUseGranularity" context parameter that causes the SQL planner to avoid using granularities other than ALL.  Other changes:  1) Rename DimensionsSpec "hasCustomDimensions" to "hasFixedDimensions" and change the meaning subtly: it now returns true if the DimensionsSpec represents an unchanging list of dimensions  or false if there is some discovery happening. This is what call sites had expected anyway.  * Fixups from CI.  * Fixes.  * Fix missing arg.  * Additional changes.  * Fix logic.  * Fixes.  * Fix test.  * Adjust test.  * Remove throws.  * Fix styles.  * Fix javadocs.  * Cleanup.  * Smoother handling of null ordering.  * Fix tests.  * Missed a spot on the merge.  * Fixups.  * Avoid needless Filters.and.  * Add timeBoundaryInspector to test.  * Fix tests.  * Fix FrameStorageAdapterTest.  * Fix various tests.  * Use forceSegmentSortByTime instead of useExplicitSegmentSortOrder.  * Pom fix.  * Fix doc.
apache,druid,e2516d9a674ce49342ac80e7509f1ccf5375a1c0,https://github.com/apache/druid/commit/e2516d9a674ce49342ac80e7509f1ccf5375a1c0,WriteOutBytes improvements  This PR generally improves the working of WriteOutBytes and WriteOutMedium. Some analysis of usage of TmpFileSegmentWriteOutMedium shows that they periodically get used for very small things. The overhead of creating a tmp file is actually very large. To improve the performance in these cases  this PR modifies TmpFileSegmentWriteOutMedium to return a heap-based WriteOutBytes that falls back to making a tmp file when it actually fills up. --------- Co-authored-by: imply-cheddar <eric.tschetter@imply.io>
apache,druid,eaa09937bc6081a8401e40bd0be49fbd5ce8a65c,https://github.com/apache/druid/commit/eaa09937bc6081a8401e40bd0be49fbd5ce8a65c,SuperSorter: direct merging  increased parallelism. (#16775)  Two performance enhancements:  1) Direct merging of input frames to output channels  without any temporary files  if all input frames fit in memory.  2) When doing multi-level merging (now called "external mode")  improve parallelism by boosting up the number of mergers in the penultimate level.  To support direct merging  FrameChannelMerger is enhanced such that the output partition min/max values are used to filter input frames. This is necessary because all direct mergers read all input frames  but only rows corresponding to a single output partition.
apache,druid,dca31d466c90f9bf2faa1fa0a0f6e7aab62c8fb5,https://github.com/apache/druid/commit/dca31d466c90f9bf2faa1fa0a0f6e7aab62c8fb5,minor adjustments for performance (#16714)  changes: * switch to stop using some string.format * switch some streams to classic loops
apache,doris,4da1c8ab1942108811007ccf69879929fce38c6d,https://github.com/apache/doris/commit/4da1c8ab1942108811007ccf69879929fce38c6d,[opt](mtmv) optimize mtmv rewrite performance (#49514)  ### What problem does this PR solve?  - Obtaining available materialized view adjustments after partition pruning RBO rewrite rules. This reduces partition version comparison overhead for partitioned materializations  thereby improving performance.  - Set maximum time threshold for transparent query rewriting with default value of 1000ms. Rewrite attempts will be terminated if exceeding this threshold. set materialized_view_rewrite_duration_threshold_ms = 1000  - Optimize code structure in transparent rewriting framework: 1. Replace Lambda expressions with conventional for-loops 2. Remove redundant member variables  - Cache available partitions of materialized views. Since calculating available partitions for materialized views is time-consuming  caching can significantly enhance performance.   Co-authored-by: zhangdong <zhangdong@selectdb.com>
apache,doris,5f0b89f06ac12b46a37b032c553c5b11471a26c1,https://github.com/apache/doris/commit/5f0b89f06ac12b46a37b032c553c5b11471a26c1,[improvement](statistics)Agg table set preagg on when doing sample analyzing. (#49918)  ### What problem does this PR solve?  This pr includes 3 changes. 1. Nereids support set ScanNode preagg on by hint  like this: select * from table1 /*+PREAGGOPEN*/ 2. When sample analyze agg table and mor unique table  set preagg on to improve performance. 3. Skip sample analyzing agg table and mor unique table's value columns.
apache,doris,3aa18fc50d926b374438bcb0965d4c0d30421222,https://github.com/apache/doris/commit/3aa18fc50d926b374438bcb0965d4c0d30421222,[feature](agg function) support corr_welford agg function (#49712)  In the past  due to performance considerations  the implementation of corr was relatively simple  avoiding division operations except in the final step. However  this approach resulted in larger errors  so corr_welford was introduced to improve the accuracy of the calculations. ``` 0.7605339114107809 corr_result 0.7605339114107733 corr_welford_result ```
apache,doris,0d8d4827e75e3cbc54ad601b19488ec0b3c7c944,https://github.com/apache/doris/commit/0d8d4827e75e3cbc54ad601b19488ec0b3c7c944,[Chore](runtime-filter) enlarge default value of runtimeBloomFilterMaxSize  runtimeFilterMaxInNum (#49689)  ### What problem does this PR solve? enlarge default value of runtimeBloomFilterMaxSize  runtimeFilterMaxInNum In some scenarios with small data volume  using in filter performance is better than bloom filter. In some scenarios with large data volume  we need a larger runtimeBloomFilterMaxSize so that the bloom filter can filter data normally.  ### Check List (For Author)  - Test <!-- At least one of them must be included. --> - [ ] Regression test - [ ] Unit Test - [ ] Manual test (add detailed scripts or steps below) - [x] No need to test or manual test. Explain why: - [ ] This is a refactor/code format and no logic has been changed. - [x] Previous test can cover this change. - [ ] No code files have been changed. - [ ] Other reason <!-- Add your reason?  -->  - Behavior changed: - [x] No. - [ ] Yes. <!-- Explain the behavior change -->  - Does this need documentation? - [x] No. - [ ] Yes. <!-- Add document PR link here. eg: https://github.com/apache/doris-website/pull/1214 -->  ### Check List (For Reviewer who merge this PR)  - [ ] Confirm the release note - [ ] Confirm test cases - [ ] Confirm document - [ ] Add branch pick label <!-- Add branch pick label that this PR should merge into -->
apache,doris,a65e0806ce55224b3b10be5d896f33ce7f30a2cd,https://github.com/apache/doris/commit/a65e0806ce55224b3b10be5d896f33ce7f30a2cd,[opt](metrics) optimize performance of metrics endpoint (#49380)  the `http://<fe_ip>:<fe_http_port>/metrics` maybe made the frontends hung  when the threads num too large  this pr replace `ThreadMXBean.getThreadInfos` to `ThreadMXBean.dumpAllThreads` to optimize performance  see also: 1. https://issues.apache.org/jira/browse/HADOOP-16850 2. https://bugs.openjdk.org/browse/JDK-8185005 3. https://github.com/apache/skywalking/discussions/9190
apache,doris,0c958d21467311a728e61bdb0da64c745b1fd317,https://github.com/apache/doris/commit/0c958d21467311a728e61bdb0da64c745b1fd317,[opt](mtmv) Opt materialized view rewrite performance when the num of struct infos are huge (#48782)  ### What problem does this PR solve?  Opt materialized view rewrite performance when the num of struct infos are huge  Optimize the recursive algorithm to reduce the number of recursive calls. If a group has already been refreshed  skip subsequent refreshes.
apache,doris,ccbc3286a92d881a93537bb83b8e26e61b08970b,https://github.com/apache/doris/commit/ccbc3286a92d881a93537bb83b8e26e61b08970b,[enhance](mtmv) insert overwrite of mtmv force drop partition (#48074)  insert overwrite of mtmv force drop partition  because recyclebin process partition lead to performance issue.
apache,doris,3a723b05a4fb4794b5698b06f966887d760487bd,https://github.com/apache/doris/commit/3a723b05a4fb4794b5698b06f966887d760487bd,[fix](Outfile) add two fields to `SELECT INTO OUTFILE` (#48144)  Problem Summary:  To better monitor the performance of Outfile  we've added two new fields to the Outfile return results: WriteTime and WriteSpeed. WriteTime is the time each writer takes to write data  measured in seconds. WriteSpeed is the average data write speed for each writer  measured in KB/s.
apache,doris,235ffb81c320cabdd48781644409e90855f59712,https://github.com/apache/doris/commit/235ffb81c320cabdd48781644409e90855f59712,[fix](Export) change the export logical (#48022)  Problem Summary:  Previously  Export would split the task into multiple threads  and each thread would further split the task into multiple outfiles  which were executed sequentially. Each thread will decide how many outfiles to split into according to `maximum_tablets_of_outfile_in_export` and the actual number of partitions/buckets of the data. Each thread executes multiple outfiles sequentially  with each Outfile task waiting for the previous one to complete before starting. This not only reduces export performance but also worsens the user experience  as users cannot clearly determine the actual concurrency performance of an Export Job.  Now  this logic has been simplified  and each thread will only handle one outfile without splitting it into multiple Outfile tasks. The parallelism value specified by the user in the `EXPORT` statement corresponds to the number of threads  with each thread handling one Outfile statement. This not only improves the user experience but also increases CPU utilization.
apache,doris,2945de9e2359aac1be78061c10b63db0bb5043b4,https://github.com/apache/doris/commit/2945de9e2359aac1be78061c10b63db0bb5043b4,[opt](iceberg)Improve performance by not retrieving table objects for hms (#47782)  ### What problem does this PR solve?  When Iceberg retrieves all tables in a database  it first fetches all table names  then retrieves the corresponding table objects based on the table names  and finally determines whether a table is an Iceberg table by checking if the type attribute of the table is "iceberg".  However  for lower - version HMS (Hive Metastore Service)  it doesn't have the `getTableObjectsByName` method and can only use the `get_table` method. When there are 1000 tables in a database  the `get_table` method will be called 1000 times  which is extremely time consuming.  Therefore  by default  the table type is not checked  and all tables are displayed.  If you still want to perform this check  you can add a configuration to the catalog:`"list-all-tables"="false"`
apache,doris,1ff07a391a540882b40f7a3f5ab693dbee69ffbf,https://github.com/apache/doris/commit/1ff07a391a540882b40f7a3f5ab693dbee69ffbf,[Opt](multi-catalog)Improve performance by introducing cache of list directory files when getting split for each query. (#43913)  ### What problem does this PR solve?  Refer to trino to implement the cache mechanism of multiple hive tables at the query level to obtain the file split list of each partition. Because files within a query should have the same visibility  the split list of partitions that see the same table should be consistent across the query scope. So this cache is reasonable and should be enabled by default. The mechanism in Trino is transactional level. A transaction can see the same table  so the command is `TransactionScopeCachingDirectoryLister`. This name is retained for Doris to expand to the transaction concept in the future. In addition  for this scenario  because the caffeine cache currently used by doris has an elimination phase strategy  the existing cache items in the window area may be eliminated immediately after the weight is updated. Therefore  `EvictableCache` which based on guava was introduced and eliminated based on segment LRU.
apache,doris,6cbde0c4fdbcabca947a82245486268c236ced22,https://github.com/apache/doris/commit/6cbde0c4fdbcabca947a82245486268c236ced22,[opt](paimon)Upgrade the Paimon version to 1.0.0 and Iceberg to 1.6.1 (#46990)  ### What problem does this PR solve?  Problem Summary:  Upgrade the Paimon version to 1.0.0  By default  paimon uses a caching catalog to cache some data to improve read performance. FYI: https://paimon.apache.org/docs/1.0/maintenance/configurations/#catalogoptions  If you do not want to use this catalog  you can add a configuration `paimon.cache-enabled ` to turn it off: ``` CREATE CATALOG `c1` PROPERTIES ( "type" = "paimon"  "paimon.catalog.type" = "xxx"  "paimon.cache-enabled" = "false"  "warehouse" = "xxx" ); ``` If you want to modify cache-related parameters  you can add the `paimon.` prefix to the parameters supported by paimon  such as: ``` CREATE CATALOG `c1` PROPERTIES ( "type" = "paimon"  "paimon.catalog.type" = "xxx"  "warehouse" = "xxx"  "paimon.cache.expiration-interval" = "20 min"  "paimon.cache.manifest.small-file-memory"="10 mb" ); ```  Note: During the doris upgrade process  this error may occur:  ![image](https://github.com/user-attachments/assets/47ec8216-9e3f-4d8e-95ef-17cce6b7c486)  This is because doris will upgrade be first  and then upgrade fe. During this process  the version of paimon on be may be higher than that on fe. This is normal. Because bucketkey judgment is newly added in the higher version of paimon  which is not available in the lower version. After the fe upgrade is completed normally  there will be no more errors.
apache,doris,b4dcf89aa93e0692a73c6cf4321a623792f2dd3a,https://github.com/apache/doris/commit/b4dcf89aa93e0692a73c6cf4321a623792f2dd3a,[opt](nereids) optimize one bucket tpcds performance (#47371)  ### What problem does this PR solve?  use one bucket shuffle hash join is slow than shuffle hash join  so we should downgrade to shuffle hash join when the table only contains one bucket.  this pr is optimized for nereids distribute planner in master branch
apache,doris,e35e01920d5d895b9333ac89e2278ed426a51f7b,https://github.com/apache/doris/commit/e35e01920d5d895b9333ac89e2278ed426a51f7b,[opt](load)  Add config to control commit lock scope for tables (#46996)   Problem Summary: Previously  all tables required commit locks during transaction commit  which helped reduce conflicts at the MetaService level. However  this approach may not be optimal for all scenarios since only MOW (Merge-on-Write) tables truly need strict concurrency control.  This PR adds a new config `enable_commit_lock_for_all_tables` (default: true) to control the commit lock strategy:  - When enabled (default): All tables will acquire commit locks during transaction commit  which helps reduce conflicts at MetaService level by queueing transactions at FE level - When disabled: Only MOW tables will acquire commit locks  which may improve concurrency for non-MOW tables but could increase conflicts at MetaService level  The default setting maintains the original behavior to avoid potential performance impact from increased MetaService conflicts  while providing flexibility to optimize for different deployment scenarios.
apache,doris,565edd9d13353f481093731d08915e9ef77e803b,https://github.com/apache/doris/commit/565edd9d13353f481093731d08915e9ef77e803b,[feature](nereids) in predicate extract non constant expressions (#46794)  Problem Summary: if an in predicate contains non-literal  backend process it will reduce performance. so we need to extract the non constant from the in predicate.  this pr add an expression rewrite rule InPredicateExtractNonConstant  it will extract all the non-constant out of the in predicate. for example:  ``` k1  in (k2   k3 + 3    1  2  3 + 3)  => k1 in (1  2  3 + 3) or k1 = k2 or k1 = k3 + 1 ```
apache,doris,584a256a25274722d22c36eafe57ba3cb8e0dc88,https://github.com/apache/doris/commit/584a256a25274722d22c36eafe57ba3cb8e0dc88,[opt](coordinator) optimize parallel degree of shuffle when use nereids (#44754)  optimize parallel degree of shuffle when use nereids   this pr can fix some performance rollback when upgrade doris from 1.2 to 2.x/3.x
apache,doris,c28c00aa1e77e1404c8e669ad4c76e3087a4e222,https://github.com/apache/doris/commit/c28c00aa1e77e1404c8e669ad4c76e3087a4e222,[enhance](nereids) add rule MultiDistinctSplit (#45209)  ### What problem does this PR solve?  Problem Summary:  This pr add a rewrite rule  which can do this 2 type of rewrite: 1. This rewrite can greatly improve the execution speed of multiple count(distinct) operations. When 3be  ndv=10000000  the performance can be improved by three to four times.  select count(distinct a) count(distinct b) count(distinct c) from t; -> with tmp as (select * from t) select * from (select count(distinct a) from tmp) t1 cross join  (select count(distinct b) from tmp) t2 cross join  (select count(distinct c) from tmp) t3   2.Before this PR  the following SQL statement would fail to execute due to an error: "The query contains multi count distinct or sum distinct  each can't have multi columns". This PR rewrites this type of SQL statement as follows  making it executable without an error.  select count(distinct a d) count(distinct b c) count(distinct c) from t; -> with tmp as (select * from t) select * from (select count(distinct a d) from tmp) t1 cross join  (select count(distinct b c) from tmp) t2 cross join  (select count(distinct c) from tmp) t3  ### Release note  Support multi count distinct with different parameters
apache,doris,44cc25494b21aa46c677633498ed2af3703d607a,https://github.com/apache/doris/commit/44cc25494b21aa46c677633498ed2af3703d607a,[Improve](nereids) use hash set replace three set in DiscreteValue  to improve in predicate performance (#45181)
apache,doris,17667aeb92efc5580b8615e53a3ef4f514a27204,https://github.com/apache/doris/commit/17667aeb92efc5580b8615e53a3ef4f514a27204,[Fix](catalog)Remove the fs.disable.cache parameter to prevent excessive FS-associated objects and memory leaks (#46184)  ### Background In the current file system implementation  the fs.disable.cache parameter allows disabling FS caching. While this provides flexibility  it introduces several critical issues: ```  1:      22537201      721190432  java.util.HashMap$Node 2:      21559238      689895616  javax.management.MBeanAttributeInfo 3:      21559098      517418352  javax.management.Attribute 4:      19380247      465125928  org.apache.hadoop.metrics2.impl.MetricCounterLong 5:        122603      461180096  [J 6:        294309      255533536  [B 7:        724598      252264048  [Ljava.lang.Object; 8:       2012368      189047432  [C 9:        159442      131064400  [Ljava.util.HashMap$Node; 10:        114752       88075072  [Ljavax.management.MBeanAttributeInfo; 11:       1899581       45589944  java.lang.String 12:       1720140       41283360  org.apache.hadoop.metrics2.impl.MetricGaugeLong ```  #### Unbounded FS Instance Creation When fs.disable.cache=true  a new FS instance is created for every access  preventing instance reuse. ```  String disableCacheName = String.format("fs.%s.impl.disable.cache"  scheme); if (conf.getBoolean(disableCacheName  false)) { LOGGER.debug("Bypassing cache to create filesystem {}"  uri); return createFileSystem(uri  conf); } ```  #### Resource Leakage Associated objects  such as thread metrics and connection pools  are not properly released due to excessive FS instance creation  leading to memory leaks.  #### Performance Degradation Frequent creation and destruction of FS instances impose significant overhead  especially in high-concurrency scenarios.    ### Release note  None  ### Check List (For Author)  - Test <!-- At least one of them must be included. --> - [ ] Regression test - [ ] Unit Test - [x] Manual test (add detailed scripts or steps below) ``` CREATE CATALOG `iceberg_cos` PROPERTIES ( "warehouse" = "cosn://ha/ha/ha/stress/multi_fs"  "type" = "iceberg"  "iceberg.catalog.type" = "hadoop"  "cos.secret_key" = "*XXX"  "cos.region" = "ap-beijing"  "cos.endpoint" = "cos.ap-beijing.myqcloud.com"  "cos.access_key" = "**************" );  Create a catalog using object storage  then write a scheduled script to continuously refresh the catalog. Query the catalog periodically and monitor whether the thread memory behaves as expected. ``` <img width="1131" alt="image" src="https://github.com/user-attachments/assets/c7b04a5a-449f-432c-975b-524fdb81247a" />  At 22:30  I replaced it with the fixed version.
apache,doris,cd9e5bb895ac02e901f0fb05e5abd806a04031d0,https://github.com/apache/doris/commit/cd9e5bb895ac02e901f0fb05e5abd806a04031d0,[improvement](statistics)Async drop table stats while doing truncate and schema change. (#45923)  ### What problem does this PR solve?  Async drop table stats while doing truncate and schema change. Truncate can schema change operation may hold table's write lock. And these two operations will trigger drop old stats info. Drop stats with write lock holding may bring performance issue.  Issue Number: close #xxx  Related PR: #xxx  Problem Summary:  ### Release note  None
apache,doris,2a1209d3cc77dac4f3ee7073240cd354bd6575c8,https://github.com/apache/doris/commit/2a1209d3cc77dac4f3ee7073240cd354bd6575c8,[opt](catalog) cache the Configuration object (#45433)  ### What problem does this PR solve?  Problem Summary: Creating Configuration object is very costly  so we cache it for better performance
apache,doris,f85b40877c7ee20489d51fb287579f6ac92935f0,https://github.com/apache/doris/commit/f85b40877c7ee20489d51fb287579f6ac92935f0,[chore](arrow-flight-sql) Add Arrow Flight Sql demo for Java (#45306)  ### What problem does this PR solve?  # How to use:  1. mvn clean install -U 2. mvn package 3. java --add-opens=java.base/java.nio=org.apache.arrow.memory.core ALL-UNNAMED -cp java-0.1.jar doris.arrowflight.demo.Main "sql" "fe_ip" "fe_arrow_flight_port" "fe_query_port"  # What can this demo do:  This is a java demo for doris arrow flight sql  you can use this to test various connection methods for sending queries to the doris arrow flight server  help you understand how to use arrow flight sql and test performance. You should install maven prior to run this demo.  # Performance test  Section 6.2 of https://github.com/apache/doris/issues/25514 is the performance test results of the Doris Arrow Flight SQL using java.  # Output  ``` WARNING: Unknown module: org.apache.arrow.memory.core specified to --add-opens ************************************* |          FlightAdbcDriver         | ************************************* FlightAdbcDriver > loadArrowBatch SLF4J(W): No SLF4J providers were found. SLF4J(W): Defaulting to no-operation (NOP) logger implementation SLF4J(W): See https://www.slf4j.org/codes.html#noProviders for further details. > Schema<l_shipdate: Date(DAY) not null> > 1994-08-17  > batchCount: 25  rowCount: 100000 > cost: 1704 ms.  FlightAdbcDriver > loadArrowBatchToString > Schema<l_shipdate: Date(DAY) not null> > 1992-01-02  > batchCount: 25  rowCount: 100000 > cost: 1692 ms.  ************************************* |          FlightJdbcDriver         | ************************************* FlightJdbcDriver > loadArrowBatch > Schema<l_shipdate: Date(DAY) not null> > 1997-01-30  > batchCount: 98  rowCount: 100000 > cost: 1840 ms.  FlightJdbcDriver > loadArrowBatchToString > Schema<l_shipdate: Date(DAY) not null> > 1992-01-02  > batchCount: 98  rowCount: 100000 > cost: 1712 ms.  ************************************* |          JdbcDriverManager        | ************************************* JdbcDriverManager > jdbc:mysql > loadJdbcResult > rowCount: 100000  columnCount: 1 > cost: 11431 ms.  JdbcDriverManager > jdbc:mysql > loadJdbcResultToString > 1992-01-02  > rowCount: 100000  columnCount: 1 resultSize: 100000 > cost: 5164 ms.  JdbcDriverManager > jdbc:arrow-flight-sql > loadJdbcResultToString > rowCount: 100000  columnCount: 1 > cost: 1736 ms.  JdbcDriverManager > jdbc:arrow-flight-sql > loadJdbcResultToString > 1997-01-29  > rowCount: 100000  columnCount: 1 resultSize: 100000 > cost: 2442 ms.   ************************************* |           FlightSqlClient         | ************************************* FlightSqlClient > getFlightInfoFromDorisFe > Schema<l_shipdate: Date(DAY) not null> > 1994-08-15  > batchCount: 25  rowCount: 100000  FlightSqlClient > constructDummyFlightInfo  don't be afraid! expected to get error `INVALID_ARGUMENT: Malformed ticket` org.apache.arrow.flight.FlightRuntimeException: INVALID_ARGUMENT: Malformed ticket  size: 1 at org.apache.arrow.flight.CallStatus.toRuntimeException(CallStatus.java:121) at org.apache.arrow.flight.grpc.StatusUtils.fromGrpcRuntimeException(StatusUtils.java:161) at org.apache.arrow.flight.grpc.StatusUtils.fromThrowable(StatusUtils.java:182) at org.apache.arrow.flight.FlightStream$Observer.onError(FlightStream.java:489) at org.apache.arrow.flight.FlightClient$1.onError(FlightClient.java:371) at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481) at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40) at org.apache.arrow.flight.grpc.ClientInterceptorAdapter$FlightClientCallListener.onClose(ClientInterceptorAdapter.java:118) at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564) at io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72) at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729) at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710) at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base/java.lang.Thread.run(Thread.java:842)  Process finished with exit code 0 ```
apache,doris,cc51e99dae339381c22a494460914e8a0431e6fb,https://github.com/apache/doris/commit/cc51e99dae339381c22a494460914e8a0431e6fb,[feat](nereids) expression add min max scope for slot reference (#45081)  expression add min max scope for slot reference  so that olap scan can reduce field's searching scope.  detail: in simplify range rule  record min and max range of the slot reference  then add 'e > min and e < max' to the expression.  example:  ``` TA >= 10 and TA <= 20 or TA >= 50 and TA <= 60 or TA >= 100 and TA <= 120 => (TA <= 20 or TA >= 50 and TA <= 60 or TA >= 100) AND TA >= 10 and TA <= 120  TA in (10  50  100) or TA >= 70 and TA <= 90 => (TA in (10  50  100) or TA >= 70 AND TA <= 90) AND TA >= 10 AND TA <= 100  TA between 10 and 20 and TB between 10 and 20 or TA between 100 and 120 and TB between 100 and 120 => (TA <= 20 and TB <= 20 or TA >= 100 and TB >= 100) AND TA >= 10 AND TA <= 120 AND TB >= 10 AND TB <= 120  ISNULL (TA > 10) and TA > 10 and TA < 20 or TA > 50 and TA < 60 or TA > 100 and TA < 120 => (ISNULL(TA > 10) and TA < 20 or TA > 50 and TA < 60 or TA > 100) AND TA > 10 and TA < 120 ```  benchmark on tpch tpcds 1T   performance no change.
apache,doris,df90de94618865e4832648e967b279c68e1c8703,https://github.com/apache/doris/commit/df90de94618865e4832648e967b279c68e1c8703,[performance](load) increase max_broker_concurrency to 100 (#44929)  Increase default `max_broker_concurrency` to 100 to improve broker load performance. This option will affect the max number of scan / sink instances allowed in a broker load.  ``` parallel instance = min(max_broker_concurrency  source file size / min_bytes_per_broker_scanner  num backends * load_parallelism) ```  S3 load time of tpcds_1000g catalog_sales: * before: 438s * after: 225s
apache,doris,0671f57221378144e7369490a28c2292b940f90a,https://github.com/apache/doris/commit/0671f57221378144e7369490a28c2292b940f90a,[feat](catalog)Replace HadoopUGI with HadoopKerberosAuthenticator to Support Kerberos Ticket Auto-Renewal (#44916)  ### Background The current implementation uses the HadoopUGI method  which invokes the ugiDoAs function for each operation to log in and execute actions based on the configuration. However  this approach has the following issues:  - Lack of Auto-Renewal: If the Kerberos TGT (Ticket Granting Ticket) expires  manual re-login is required as there is no support for automatic ticket renewal. - Redundant Login Overhead: Each operation requires reinitializing or checking UserGroupInformation  potentially causing performance bottlenecks. - Complex Management: The HadoopUGI design does not unify the lifecycle management of UGI instances  leading to duplicated logic across the codebase. ### Objective  - Auto-Renewal: Automatically renew Kerberos credentials when the TGT is expired or near expiry. - UGI Caching: Maintain reusable UserGroupInformation instances during their lifecycle to avoid repetitive logins. - Unified Management: Simplify the management of UGI instances and Kerberos credentials.
apache,doris,6a8ae7705a9641ce31aa1d9693044c8ddd54e394,https://github.com/apache/doris/commit/6a8ae7705a9641ce31aa1d9693044c8ddd54e394,[Enchancement](runtime-filter) improvement for datetimev2 bloom filter hash method (#44924)  ### What problem does this PR solve? improvement for datetimev2 bloom filter hash method In the past  datetimev2 use to_int64 to get a 64bit data and truncate to 32bit data  then use this 32bit data to build the bloom filter. this can lead to poor performance and bad filterability.
apache,doris,d77bfa09d85d9759965c109cc50ee89e8fbde499,https://github.com/apache/doris/commit/d77bfa09d85d9759965c109cc50ee89e8fbde499,[Improvement](shuffle) Use a knob to decide whether a serial exchange… (#44676)  … should be used  This improvement was completed in #43199 and reverted by #44075 due to performance fallback. After fixing it  this improvement is re-submited.  A new knob to control a exchange node should be serial or not. For example  a partitioned hash join should be executed like below: ``` ┌────────────────────────────┐                  ┌────────────────────────────┐ │                            │                  │                            │ │Exchange(HASH PARTITIONED N)│                  │Exchange(HASH PARTITIONED N)│ │                            │                  │                            │ └────────────────────────────┴─────────┬────────┴────────────────────────────┘ │ │ │ │ │ │ ┌──────▼──────┐ │             │ │ HASH  JOIN  │ │             │ └─────────────┘ ```  After turning on this knob  the real plan should be: ``` ┌──────────────────────────────┐                        ┌──────────────────────────────┐ │                              │                        │                              │ │ Exchange (HASH PARTITIONED 1)│                        │ Exchange (HASH PARTITIONED 1)│ │                              │                        │                              │ └────────────┬─────────────────┘                        └────────────┬─────────────────┘ │                                                       │ │                                                       │ │                                                       │ │                                                       │ │                                                       │ ┌──────────────▼─────────────────────┐                  ┌──────────────▼─────────────────────┐ │                                    │                  │                                    │ │ Local  Exchange(HASH PARTITIONED N)│                  │ Local  Exchange(HASH PARTITIONED N)│ │              1 -> N                │                  │              1 -> N                │ └────────────────────────────────────┴─────────┬────────┴────────────────────────────────────┴ │ │ │ │ │ │ ┌──────▼──────┐ │             │ │ HASH  JOIN  │ │             │ └─────────────┘ ```  For large cluster  X (mappers) * Y (reducers) rpc channels can be reduced to X (mappers) * Z (BEs).
apache,doris,faa8a6425e692f9604e342800c6143b79ca9008e,https://github.com/apache/doris/commit/faa8a6425e692f9604e342800c6143b79ca9008e,[chore](sink) `enable_parallel_result_sink` default value is changed to false (#43933)  ### What problem does this PR solve?  Problem Summary:  For most queries  result sink will not become a performance bottleneck  but the parallel result sink will increase the pressure of RPC between fe and be.
apache,doris,05b6f5aca1ea609abbdd447c64d4d57fd474b295,https://github.com/apache/doris/commit/05b6f5aca1ea609abbdd447c64d4d57fd474b295,[fix](external) fix count(*) performance fallback in external table query (#44172)  ### What problem does this PR solve?  Related PR: #41789  Problem Summary:  This PR #41789 change the local shuffle logic  but forget to implement `numScanBackends()` in FileQueryScanNode  which causing `select count(*) from hive_table` query performance fallback.  This PR implement this method in `FileQueryScanNode()`
apache,doris,07ec65712d67efd072667247704b40adaf3e35a7,https://github.com/apache/doris/commit/07ec65712d67efd072667247704b40adaf3e35a7,[opt](Nereids) optimize performance in new distribute planner (#44048)  optimize new distribute planner performance in tpc-h  because #41730 made some performance rollback has occurred  1. fix the wrong runtime filter thrift parameters 2. not default to print distribute plan in profile  you should config `set profile_level=3` to see it 3. for shuffle join which two sides distribution of natural + execution_bucketed  support compare cost between plans of shuffle to left/right
apache,doris,aab5ba3a7d4d9ccd26c2bf800e798004885769be,https://github.com/apache/doris/commit/aab5ba3a7d4d9ccd26c2bf800e798004885769be,[performance](load) fix broker load scan ranges for unsplittable files (#43161)
apache,doris,cd514b6a3dbd06aabefe67600ef1d1c262fa1aa2,https://github.com/apache/doris/commit/cd514b6a3dbd06aabefe67600ef1d1c262fa1aa2,[feature](function) add approx_top_k aggregation function (#40813)  ## Proposed changes  1. select approx_top_k(clientip  status  size  10  300) from tbl;  This code implements an approximate Top-N query function based on the SpaceSaving algorithm. SpaceSaving is an efficient streaming algorithm commonly used to handle frequent element query problems in large datasets. Below is a description of the main functionalities:  (1) Data Structures and Memory Management: The SpaceSavingArena class provides a memory pool to manage memory allocation and deallocation. For keys of type StringRef  it handles the memory by copying the string into the memory pool. The Counter struct stores the key  count  and error for each element  and provides serialization and deserialization functions.  (2) Insertion and Updates: The insert method is used to insert new elements or update the count of existing elements. If the current capacity is not full  it inserts the new element; if it is full  it replaces the element with the smallest count based on the element's count and error.  (3) Merge Operation: The merge method allows merging two SpaceSaving objects. During the merge  it adjusts the counts and errors of the existing elements  ensuring that the result maintains the correct order.  (4) Top-K Query: The top_k method returns the current Top-K most frequent elements  sorted by their count and error.  (5) Capacity Expansion and Shrinking: The resize method allows adjusting the storage capacity  and it recalculates the size of the alpha_map accordingly.  (6) Serialization and Deserialization: The write and read methods are provided for serializing the SpaceSaving structure to disk or reading data from disk.  (7) Optimization and Performance: The code uses a hash table-based approach for lookup and storage  and dynamically adjusts the alpha_map size to optimize performance and reduce memory waste.  In summary  the SpaceSaving class efficiently implements Top-N queries for large data streams within limited memory  with efficient insertion  updating  and merging mechanisms.  Co-authored-by: zzzxl1993 <yangsiyu@selectdb.com>
apache,doris,5f07b884819a0187862d0005ec60cae96edcafea,https://github.com/apache/doris/commit/5f07b884819a0187862d0005ec60cae96edcafea,[improvement](external)add some improvements for external scan (#38946)  ## Proposed changes  1. add session variable: `use_consistent_hash_for_external_scan`  which can specify consistent hash for external scan. 2. add session variable: `ignore_split_type`  which can ignore splits of the specified type  use for performance tuning. 3. add split weight for paimon split with consistent hash. 4. add `executeFilter` for paimon jni split.
apache,doris,19016b147731ec825845686dc61bbf6e3d1116b9,https://github.com/apache/doris/commit/19016b147731ec825845686dc61bbf6e3d1116b9,[improve](routine load) adjust default values to make routine load more convenient to use (#42491)  For a routine load job  it will be divided into many tasks  each of which is a transaction. Currently  the default time consumed(max_batch_interval) is 10 seconds. The benefits of increasing this value are: 1. Larger batch consumption can lead to better performance. 2. Reducing the number of transactions can alleviate the pressure of compaction and the conflicts of concurrent transaction submissions.  related doc: https://github.com/apache/doris-website/pull/1236/files
apache,doris,5ede16abe01d7bd9ef3a4d85bc481c83025aeb71,https://github.com/apache/doris/commit/5ede16abe01d7bd9ef3a4d85bc481c83025aeb71,[improvement](jdbc catalog) Add catalog property to enable jdbc connection pool (#41992)  We initially introduced jdbc connection pool to improve the connection performance of jdbc catalog  but we always found that connection pool would bring some unexpected errors  so we chose to add a catalog property: `enable_connection_pool` to choose whether to enable the jdbc connection pool of jdbc catalog  and the default false.However  the created catalog will still open the connection pool when it is upgraded  and only the newly created catalog will be false  And we conducted performance tests on this  the performance loss is within the expected range.  - Enable connection pool: mysqlslap -uroot -h127.0.0.1 -P9030 --concurrency=1 --iterations=100 --query='SELECT * FROM mysql.test.test limit 1;' --create-schema=mysql --delimiter=";" --verbose Benchmark Average number of seconds to run all queries: 0.008 seconds Minimum number of seconds to run all queries: 0.004 seconds Maximum number of seconds to run all queries: 0.133 seconds Number of clients running queries: 1 Average number of queries per client: 1  - Disable connection pool: mysqlslap -uroot -h127.0.0.1 -P9030 --concurrency=1 --iterations=100 --query='SELECT * FROM mysql_no_pool.test.test limit 1;' --create-schema=mysql --delimiter=";" --verbose Benchmark Average number of seconds to run all queries: 0.054 seconds Minimum number of seconds to run all queries: 0.047 seconds Maximum number of seconds to run all queries: 0.184 seconds Number of clients running queries: 1 Average number of queries per client: 1
apache,doris,b7faf57163a6825d19782ede7b0a6a0c77315690,https://github.com/apache/doris/commit/b7faf57163a6825d19782ede7b0a6a0c77315690,[improvement](jdbc catalog) Disallow non-constant type conversion pushdown and implicit conversion pushdown (#42102)  Add a variable `enable_jdbc_cast_predicate_push_down`  the default value is false  which prohibits the pushdown of non-constant predicates with type conversion and all predicates with implicit conversion. This change can prevent the wrong predicates from being pushed down to the Jdbc data source  resulting in query data errors  because the predicates with cast were not correctly pushed down to the data source before. If you find that the data is read correctly and the performance is better before this change  you can manually set this variable to true  ``` | Expression                                          | Can Push Down | |-----------------------------------------------------|---------------| | column type equals const type                       | Yes           | | column type equals cast const type                  | Yes           | | cast column type equals const type                  | No            | | cast column type equals cast const type             | No            | | column type not equals column type                  | No            | | column type not equals cast const type              | No            | | cast column type not equals const type              | No            | | cast column type not equals cast const type         | No            |  ```
apache,doris,22aabb56fdffe5f41e4fde9c1a6e4c5b47d078de,https://github.com/apache/doris/commit/22aabb56fdffe5f41e4fde9c1a6e4c5b47d078de,[opt](Catalog) Remove unnecessary conjuncts handling on External Scan (#41218)  In the previous FileScanNode  some parts that used conjuncts for predicate conversion were placed in the init phase. However  for the Nereids planner  pushing the filter down to the scan happens in the Translator  which means that the ScanNode can only get the complete conjuncts in the finalized phase. Therefore  in this PR  I have removed all conjuncts variables in External for the Nereids planner. They no longer need to store conjuncts themselves or add them to the ScanNode. Instead  all places in the ScanNode that use conjuncts should be moved to the finalized phase.  This refactor also fix a performance issue introduced from #40176 After introducing the change of generating SelectNode for consecutive projects or filters  FileScan still adds conjuncts too early in the init phase  resulting in the discovery of consecutive filters when the upper layer continues to translate  a selectnode was unexpectedly generated on the scannode  causing the project to be unable to prune the scannode columns. However  the Project node trims columns of SelectNode and ScanNode differently  which causes ScanNode to scan unnecessary columns.  My modification removes the addition of conjuncts in the scannode step  so that we can keep the structure from ScanNode to Project and achieve correct column trimming.
apache,doris,127ac8d93ece5310afe20eb5521f6ec883d7282a,https://github.com/apache/doris/commit/127ac8d93ece5310afe20eb5521f6ec883d7282a,[Enhancement](ExternalTable)Optimize the performance of getCachedRowCount when reading ExternalTable (#41659)  ## Proposed changes Because ExternalTable will initialize the previously uninitialized table when `getCachedRowCount()`  which is unnecessary. So for the uninitialized table  we directly return -1. This will increase the speed of our query `information_schema.tables`.
apache,doris,6ed0bc813c484777a93605ada8fb629c627d8fd7,https://github.com/apache/doris/commit/6ed0bc813c484777a93605ada8fb629c627d8fd7,[enhance](auth) Optimize the authentication logic of Ranger Doris (#41207)  - Set the authentication type for row policy and datamask as `select` to avoid traversing all permission items within the ranger - The permission items on the ranger page are consistent with the grant syntax permission items on Doris - Add global permissions on the ranger side  and the ranger page needs to be specified in the input box* - No longer using cache to cache datamask and row policy  as changing the log level and specifying permission items has already made the speed fast enough - When using a ranger  there is no need to create a role with the same name within Doris - If you have sub level permissions  you can see the current level when showing. For example  if you have query permissions for table1 under db1  db1 will be displayed when showing databases  ranger ui: morningman/ranger#1  performance testing:  Each time the ranger authentication method is called  it takes less than 1ms   If the user has global/db/table permissions  querying a large wide table takes approximately 1ms.  If the user only has partial column query permission  it will take approximately 80ms to query 50 columns
apache,doris,3b18b1f000427e8d3b7d9d06ac57c08ec0959703,https://github.com/apache/doris/commit/3b18b1f000427e8d3b7d9d06ac57c08ec0959703,[fix](scanner) Fix incorrect _max_thread_num in scanner context when many queries are running. (#41273)  1. Minor refactor for scanner constructor  calculation of _max_thread_num is moved to init method 2. The expected value of _max_thread_num is changed. There is no need to submit too many scan task to scan scheduler  since thread num is limited. 3. Calculation of _max_bytes_in_queue is changed. _max_bytes_in_queue for each scan instance is limited to 100MB by default.  ``` mysql [tpch]>select count(*) from supplier; -------------- select count(*) from supplier --------------  +----------+ | count(*) | +----------+ |  1000000 | +----------+ 1 row in set (0.04 sec)  mysql [tpch]>select count(*) from revenue0; -------------- select count(*) from revenue0 --------------  +----------+ | count(*) | +----------+ |  1000000 | +----------+ 1 row in set (0.19 sec) ``` To illustrate the effect  we need to create much scanners  so ``` set global experimental_parallel_scan_min_rows_per_scanner=29715 ``` default value is `2097152`  we can make scanner num almost equal to `experimental_parallel_scan_max_scanners_count` which is 48.  Lets use mysqlslap to do concurrent test.  Current master: ```text [hezhiqiang@VM-10-8-centos be_1]$ mysqlslap -hxxxx -uroot -Pyyyy  --create-schema=tpch -c 20 -i 5 -q "select     s_suppkey      s_name      s_address      s_phone      total_revenue from     supplier      revenue0 where     s_suppkey = supplier_no     and total_revenue = (         select             max(total_revenue)         from             revenue0     ) order by     s_suppkey;" Benchmark Average number of seconds to run all queries: 12.480 seconds Minimum number of seconds to run all queries: 12.159 seconds Maximum number of seconds to run all queries: 12.843 seconds Number of clients running queries: 20 Average number of queries per client: 1  [hezhiqiang@VM-10-8-centos be_1]$ mysqlslap -hyyyy -uroot -Pyyyy  --create-schema=tpch -c 25 -i 5 -q "select     s_suppkey      s_name      s_address      s_phone      total_revenue from     supplier      revenue0 where     s_suppkey = supplier_no     and total_revenue = (         select             max(total_revenue)         from             revenue0     ) order by     s_suppkey;" mysqlslap: Cannot run query select     s_suppkey      s_name      s_address      s_phone      total_revenue from     supplier      revenue0 where     s_suppkey = supplier_no     and total_revenue = (         select             max(total_revenue)         from             revenue0     ) order by     s_suppkey; ERROR : errCode = 2  detailMessage = (10.16.10.8)[TOO_MANY_TASKS]Failed to submit scanner to scanner pool reason:Thread pool Scan_normal is at capacity (192/192 tasks running  102400/102400 tasks queued)|type:0 ```  After this pr ``` [hezhiqiang@VM-10-8-centos lib]$ mysqlslap -hxxx -uroot -Pxxx  --create-schema=tpch -c 50 -i 5 -q "select     s_suppkey      s_name      s_address      s_phone      total_revenue from     supplier      revenue0 where     s_suppkey = supplier_no     and total_revenue = (         select             max(total_revenue)         from             revenue0     ) order by     s_suppkey;" Benchmark Average number of seconds to run all queries: 31.520 seconds Minimum number of seconds to run all queries: 30.164 seconds Maximum number of seconds to run all queries: 34.131 seconds Number of clients running queries: 50 Average number of queries per client: 1 ```  The max concurrency increased from 25 to 50.  Actually  for sequential query test  the performance does not decrease  `submit_many_scan_tasks_for_potential_performance_issue` can be remove in the future.
apache,doris,f9bd4efb9e57f5c64ed4d3610b6dff9dedca8878,https://github.com/apache/doris/commit/f9bd4efb9e57f5c64ed4d3610b6dff9dedca8878,[fix](oracle scan) Fix performance issues caused by version judgment (#41407)
apache,doris,8e33cda7ac237d273a6bb3413d6436d833c4a021,https://github.com/apache/doris/commit/8e33cda7ac237d273a6bb3413d6436d833c4a021,[improvement](statistics)Reduce partition column sample BE memory consumption. (#41203)  For string type columns  use xxhash_64 to transfer column value to an integer  and then calculate the NDV based on the integer hash value. In this case  we can reduce the memory cost of sample analyze and improve the performance. For example  l_comment column of TPCH 100G lineitem table. The memory cost to calculate its NDV is reduced to 8GB from 22GB
apache,doris,5868e10fb48da5c5454c477f656d1307403313be,https://github.com/apache/doris/commit/5868e10fb48da5c5454c477f656d1307403313be,[opt](nereids) refine operator estimation (#40762)  Stats deriving refinement step 3: refine operator estimation  a. refine filter estimation - refine column-column/column-constant stats estimation  refine in predicate estimation for future extension - unify original "enforceValid" and "normalizeByRatio" function => normalizeColumnStatistics  which can be able to update column statistics  such as ndv  numNulls  based on self's stats and current row count info. - unify original "cover" function => intersect in StatisticsRange - add normalizeColumnStatistics at each partial step of estimation  such as middle stage of and/or estimation  to ensure the returning stats is valid and avoid serious stats deviation from unhandled column statistics. - standardize the comparison function name  by using more meaningful name  such as column/constant. - to avoid serious estimation problem from inaccurate minmax  the range related estimation path has been splitted by judging instance of RangeScalable  and the non RangeScalable type  such as string type  will use alternative way more conservatively to do the estimation. - the notNullSelectivity's computing has been refined at former pr  in this pr  the refined getNotNullSelectivity will use original numNulls  ndv  rowCount  to computing a more accurate notNullSelectivity. however  current column2column notNullSelectivity has not been activated and will do it in the future. - not/is null estimation will use "normalizeColumnStatistics" to do the normalization.  b. refine join  estimation - unify join's estimation function. - standardize the naming  such as removing the physical concepts  such as hash/nestloop  to logical concepts. - add other join type's updateJoinConditionColumnStatistics action  besides inner join  during join stats' estimation  the updateJoinConditionColumnStatistics will have different behavior for different join types. - semi/anti's estimation method has been updated but currently commented  will be activated in the future.  c. row count and ndv value normalization ([0 1] => 1) - not activated in this pr  will be opened in the future.  Benchmark performance impact: - tpcds 1t query58: 0.60s -> 0.66s - tpcds 1t query64: no impact  Co-authored-by: zhongjian.xzj <zhongjian.xzj@zhongjianxzjdeMacBook-Pro.local>
apache,doris,51ba957fd6b274886d89ad28b6c8c4899bd5bdba,https://github.com/apache/doris/commit/51ba957fd6b274886d89ad28b6c8c4899bd5bdba,[improve](partition_topn) Add partition threshold check in hash table to control partition nums (#39057)  ## Proposed changes 1.  Add a session variable to control partition_topn partition threshold 2. move the partition threshold check at emplace data to hash table to control partition nums  so get check every rows. this could improve some bad case about 50%+ performance improvement， and some better case before  after move the check in hash table  maybe have performance degradation almost 10%  I think this is within the acceptable result。  <!--Describe your changes.-->
apache,doris,80482c5fc4f0d4098fc8a7c06d1c1f5c07b73833,https://github.com/apache/doris/commit/80482c5fc4f0d4098fc8a7c06d1c1f5c07b73833,[opt](nereids) clean count usage in ColumnStatistic during stats deriving (#40654)  ## Proposed changes  Stats deriving refinement step 1: clean up count usage in ColumnStatistic during stats deriving(mainly for stats-available)  to avoid serious stats deriving problem. a. use Statistics rowCount instead of count in ColumnStatistic in stats deriving  since these two infos may be inconsistent and lead to stats deriving problem. b. remove setCount interface to avoid using this count field during deriving unexpectedly in the future. c. refine notNullSelectivity computing and corresponding estimation.  Benchmark plan shape change: - tpcds query74: no performance impact.  ---------  Co-authored-by: zhongjian.xzj <zhongjian.xzj@zhongjianxzjdeMacBook-Pro.local>
apache,doris,b92b63e153789646a96a520456f549202b5eef06,https://github.com/apache/doris/commit/b92b63e153789646a96a520456f549202b5eef06,Revert "[fix](scanner) Fix incorrect _max_thread_num in scanner context" (#40804)  Reverts apache/doris#40569  We need more test to avoid performance issue
apache,doris,efd1cd0c692b2f67c0b423acd41784c5919e799a,https://github.com/apache/doris/commit/efd1cd0c692b2f67c0b423acd41784c5919e799a,[improve](routine load) delay schedule EOF tasks to avoid too many small transactions (#39975)  We encountered a scenario where a large number of small transactions were generated  resulting in an impact on query performance: Kafka's data comes in batches of very small data every very short time  which leads to tasks being frequently scheduled and ending very quickly  resulting in a large number of small transactions.  To solve this problem  we delay the scheduling of tasks that perceive EOF  which would not delay data consumption  for perceiving EOF indicates that the consumption speed is greater than the production speed.
apache,doris,53dcf497958237634266636d9626b530cc8de881,https://github.com/apache/doris/commit/53dcf497958237634266636d9626b530cc8de881,[Feature](Variant) Implement inner nested data type for variant type (#39022)  # Background Currently  importing nested data formats  such as:  ``` json { "a": [{"nested1": 1}  {"nested2": "123"}] } ``` This results in the a column type becoming JSON  which has worse compression and query performance compared to native arrays  mainly due to the inability to leverage low cardinality optimizations and the overhead of parsing JSON during queries.  A common example:  ``` json { "eventId": 1  "firstName": "Name1"  "lastName": "Surname1"  "body": { "phoneNumbers": [ { "number": "5550219210"  "type": "GSM"  "callLimit": 5 }  { "number": "02124713252"  "type": "HOME"  "callLimit": 3 }  { "number": "05550219211"  "type": "WORK"  "callLimit": 2 } ] } } ```   # Design Consider storing the expanded nested structure so that the schema merge logic can be utilized directly  and querying becomes easier  for example: ``` json { "n": [{"a": 1  "b": 2}  {"a": 10  "b": 11  "c": 12}  {"a": 1001  "d": "12"}] }  { "n": [{"x": 1  "y": 2}] } ``` Data would be stored as follows  with following storage format Column | Row 0 | Row 1 -- | -- | -- n.a (array<int>) | [1  10  1001] | [null] n.b (int) | [2  11  null] | [null] n.c (int) | [null  12  null] | [null] n.d (text) | [null  null  "12"] | [null] n.x | [null  null  null] | [1] n.y | [null  null  null] | [1]  Data offsets are aligned (equal size).  # Compaction To maintain the relationship between nested nodes  such as n.a  n.b  n.c  and n.d  during compaction  if any of these columns are missing  their offsets are filled using any sibling column's offset.  # Queries ```sql SELECT v['n']['a'] FROM tbl; --- This outputs [1  10  1001]. ```  ```  sql SELECT v['n'] FROM tbl; --- This outputs [{"a" : 1  "b" : 2}  {"a" : 10  "b" : 11  "c" : 12}  {"a":1001  "d" : "12"}]. ```  During queries  the path's nested information is not perceived because this information is ignored during path evaluation (not stored in the subcolumn tree).
apache,doris,45ddb8ce9c82b250d9d4c7ce3157615dda5cf692,https://github.com/apache/doris/commit/45ddb8ce9c82b250d9d4c7ce3157615dda5cf692,[fix](mtmv) Add debug log decide for performance when query rewrite by materialized view (#39914)  ## Proposed changes In method `AbstractMaterializedViewRule#isMaterializationValid` Should add `LOG.isDebugEnabled()` before print debug log. Because `Plan#treeString` in debug log is performance consume.
apache,doris,2c154c6d9710c16d6057e0d08ba866521a12bbb0,https://github.com/apache/doris/commit/2c154c6d9710c16d6057e0d08ba866521a12bbb0,[fix](jdbc catalog) Fix Memory Leak by Enabling Weak References in HikariCP (#39582)  This PR addresses a memory leak issue caused by FastList objects in HikariCP being retained by ThreadLocal variables  which are not easily garbage collected in long-running JNI threads. To mitigate this  a system property com.zaxxer.hikari.useWeakReferences is set to true  ensuring that WeakReference is used for ThreadLocal objects  allowing the garbage collector to reclaim memory more effectively. Even though setting this will affect some performance  solving resource leaks is relatively more important Performance difference before and after setting Before setting: 10 concurrency 0.02-0.05 100 concurrency 0.18-0.4 After setting: 10 concurrency 0.02-0.07 100 concurrency 0.18-0.7
apache,doris,3e8c19f6977494968f701f04985be43d5b0c4f85,https://github.com/apache/doris/commit/3e8c19f6977494968f701f04985be43d5b0c4f85, [improvement](mtmv) Only Generate rewritten plan when generate mv plan for performance (#39541)  Before query rewrite by materialized view  we collecet the table which query used by method org.apache.doris.mtmv.MTMVCache#from.  In MTMVCache#from we calcute the cost of plan which is useless for collecting table. So add boolean needCost param in method MTMVCache#from to identify that if need cost of plan or not for performance.
apache,doris,109dba687f596df30268601fc00fffb0ebd5bc58,https://github.com/apache/doris/commit/109dba687f596df30268601fc00fffb0ebd5bc58,[enhance](mtmv)Improve the performance of obtaining partition/table versions (#39301)  Batch retrieve version information of all tables and partitions used by MTMV and store it in MTMVRefreshContext
apache,doris,f3dd685645bb5f6ea6ae54b8ae15c6d6497154ef,https://github.com/apache/doris/commit/f3dd685645bb5f6ea6ae54b8ae15c6d6497154ef,[Improvement](runtime-filter) do not use bloom to replace in_or_bloom when rf need merge (#39147)  ## Proposed changes do not use bloom to replace in_or_bloom when rf need merge Because in some cases  this will lead to poor performance  <img width="298" alt="图片" src="https://github.com/user-attachments/assets/bcee330f-bb38-4e51-af76-1a181bd205f9"> <img width="298" alt="图片" src="https://github.com/user-attachments/assets/481a4b06-929d-4f4a-8d10-bf2901e68fdf">
apache,doris,2fb98f5558e79ae64429ecccd87397008e8c0f29,https://github.com/apache/doris/commit/2fb98f5558e79ae64429ecccd87397008e8c0f29,[Feature](Cloud) Support session variable disable_file_cache and enable_segment_cache in query (#37141)  Currently  whether to read from file cache or remote storage is controlled by the BE config `enable_file_cache` in cloud mode. This PR proposed to control the file cache behavior via session variables when executing queries in cloud mode. It's more convenient when have such a session variable  cache behavior could be controlled per query/session without changing BE configs  such as: 1. **Performance test**. Test the query performance when read from local file cache or remote storage for queries. 2. **Data correctness**. Check if it's file cache issue for certain tables or queries.  The read path has three kinds of caches: segment cache  page cache and file cache.  | module       | cache| BE config    | session variable| |------------|------|----------| ---- | | Segment | segment cache | disable_segment_cache | **enable_segment_cache** (supportted by this PR) | | PageIO | page cache | disable_storage_page_cache | enable_page_cache | | FileReader | file cache | enable_file_cache | **disable_file_cache** (supportted by this PR) |  The modification of the PR:  - **enable_segment_cache**: add a new session variable enable_segment_cache to control use segment cache or not. - **disable_file_cache**: disable_file_cache was for write path in cloud mode. It's supported for read path when executing queries in the PR.  With this PR   data is read from remote storage without cache: ```sql set enable_segment_cache=false; set enable_page_cache=false; set disable_file_cache=true; ```  Co-authored-by: Gavin Chou <gavineaglechou@gmail.com>
apache,doris,f18d7b6c72866b29b9639bcfccf279a8742f5304,https://github.com/apache/doris/commit/f18d7b6c72866b29b9639bcfccf279a8742f5304,[fix](group commit) Fix group commit debug log and improve performance (#38754)  ## Proposed changes  1. show `query_id` in the debug log of when group commit insert does not work 2. remove string.format to improve performance
apache,doris,faaa24cbed541748f9f56aa9ae524d357c242717,https://github.com/apache/doris/commit/faaa24cbed541748f9f56aa9ae524d357c242717,[Enhancement](audit log) Add print audit log sesssion variable (#38419)  ## Proposed changes  For the `insert into` statements during group commit load via JDBC. Printing audit logs can severely impact performance. Therefore  we have introduced a session variable to control whether to print audit logs. It is recommended to turn off audit logs only during group commit load via JDBC.  <!--Describe your changes.-->
apache,doris,d6dd62e368e977fcc8eebb951854c2e10d77f0c2,https://github.com/apache/doris/commit/d6dd62e368e977fcc8eebb951854c2e10d77f0c2,[enhencement](trino-connector) trino-connector supports push down projection to connectors (#37874)  Invoke the `applyProjection` method of connectorMetadata` to push the projection down to the connector. This reduces the amount of data retrieved by the connector and enhances query performance.  Projection pushdown is particularly important for the BigQuery connector.
apache,doris,7211c2d6594821a78a00423754d9f0e608e45192,https://github.com/apache/doris/commit/7211c2d6594821a78a00423754d9f0e608e45192,[conf](parallel) Reduce parallel tasks for large cluster (#38196)  For large cluster  too many parallel tasks will cause performance issue. So this PR limit the max parallel tasks in Doris.
apache,doris,e2bdb95abb83823b7e443ade2af059cdad6458fc,https://github.com/apache/doris/commit/e2bdb95abb83823b7e443ade2af059cdad6458fc,[opt](nereids) refine left semi/anti cost under short-cut opt (#37951)  Refine left semi/anti cost computing under short-cut opt  for the case whose semi/anti join has the small left side and big right side  which original solution can't support. This pr reduce the left style cost by reduce the right side cost and improve the possibility of choosing left style joins.  Pass the performance test on tpch/tpcds/usercase.  previous work: #37060
apache,doris,a1b0264544ce14245c4198f2e84399d400dfdf9a,https://github.com/apache/doris/commit/a1b0264544ce14245c4198f2e84399d400dfdf9a,[improvement](statistics)Async drop stats while truncating table. (#37715)  Drop stats for table with many partitions may slow  because to invalidate partition stats cache is time consuming. Truncate table operation do the drop stats synchronously  so the truncate table may be very slow for partition tables. This pr is to improvement the performance of truncate table. Do the drop stats asynchronously.  Time consumed for truncate a table with 10000 partitions and 10 columns reduced to 2.5s from 10s.
apache,doris,2f0e57455475c0a1037b98874eb84e947925ccc4,https://github.com/apache/doris/commit/2f0e57455475c0a1037b98874eb84e947925ccc4,[enhancement](log)Updated the default setting of sys_log_mode in fe.conf for better performance (#37793)  ## Proposed changes  <!--Describe your changes.--> Updated the default setting of `sys_log_mode` in `fe.conf` from `NORMAL` to `ASYNC` for better performance.
apache,doris,5367f7ed8b15f91e63ee5c2ff98de82ec6450742,https://github.com/apache/doris/commit/5367f7ed8b15f91e63ee5c2ff98de82ec6450742,[performance](broker-load) increase default broker load batch size (#36477)
apache,doris,701c7db45a9debc9c5d8b374b889c2dd0a7419e2,https://github.com/apache/doris/commit/701c7db45a9debc9c5d8b374b889c2dd0a7419e2,[improvement](mtmv) improve mv rewrite performance by reuse the shuttled expression (#37197)  Optimizations: 1. Expression shuttle is expensive in materialized view rewritting  So reuse the shuttled expression. 2. Generate shuttledExpressions by planOutput is also expensive  so generate and store in struct info once and used later
apache,doris,94ce626b49b72bdc4bdbc78bc7c9669c2296c9a3,https://github.com/apache/doris/commit/94ce626b49b72bdc4bdbc78bc7c9669c2296c9a3,[improvement](statistics)Enable estimate hive table row count using file size. (#37218)  Enable fetch hive table row count through file list by default. Change the sample partition count to 30 to reduce resource consumption. Using a table with 100000 partition to test the performance. Row count could be fetch within 0.2 second and the cpu used is less than 1 core.
apache,doris,40dbb5998df09bc002c8bd8f4c56c86d8c2c90ae,https://github.com/apache/doris/commit/40dbb5998df09bc002c8bd8f4c56c86d8c2c90ae,[feat](Nereids) Add support for slot pruning in functional dependencies (#37045)  Implement slot pruning functionality for functional dependencies to optimize performance and resource utilization. This enhancement allows for more efficient handling of dependencies by removing unnecessary slots.
apache,doris,6889225b19e5826d74582c518f3d38982a1e3886,https://github.com/apache/doris/commit/6889225b19e5826d74582c518f3d38982a1e3886,[feat](Nereids) Optimize query by pushing down aggregation through join on foreign key (#36035)  ## Proposed changes  This PR optimizes query performance by pushing down aggregations through joins when grouped by a foreign key. This adjustment reduces data processing overhead above the join  improving both speed and resource efficiency.  Transformation Example:  Before Optimization: ``` Aggregation(group by fk) | Join(pk = fk) /  \ pk  fk ``` After Optimization: ``` Join(pk = fk) /     \ pk  Aggregation(group by fk) | fk ```
apache,doris,c8f1b9f4ae0075d051a2714dceae2a8b208f71fd,https://github.com/apache/doris/commit/c8f1b9f4ae0075d051a2714dceae2a8b208f71fd,[opt](hive) save hive table schema in transaction (#37008)  Save the table schema  reduce the number of HMS calls  and improve write performance.
apache,doris,3da83514cb5baa85dbd914a8dd086156bb4fc282,https://github.com/apache/doris/commit/3da83514cb5baa85dbd914a8dd086156bb4fc282,[fix](mtmv) Fix high nest level materialized view can not be rewritten  because low level mv aggregate roll up (#36567)  Query is aggregate  the query group by expression is less than materialzied view group by expression. when the more dimensions than queries in materialzied view can be eliminated with functional dependencies. it can be rewritten with out roll up aggregate. For example as following: mv def is  CREATE MATERIALIZED VIEW mv BUILD IMMEDIATE REFRESH AUTO ON MANUAL DISTRIBUTED BY RANDOM BUCKETS 2 PROPERTIES ('replication_num' = '1') AS select l_orderkey  l_partkey  l_suppkey  o_orderkey  o_custkey  ps_partkey  cast( sum( IFNULL(ps_suppkey  0) * IFNULL(ps_partkey  0) ) as decimal(28  8) ) as agg2 from lineitem_1 inner join orders_1 on lineitem_1.l_orderkey = orders_1.o_orderkey inner join partsupp_1 on l_partkey = partsupp_1.ps_partkey and l_suppkey = partsupp_1.ps_suppkey where partsupp_1.ps_suppkey > 1 group by l_orderkey  l_partkey  l_suppkey  o_orderkey  o_custkey  ps_partkey;  query is as following:  select l_orderkey  l_partkey  l_suppkey  o_orderkey  o_custkey  cast( sum( IFNULL(ps_suppkey  0) * IFNULL(ps_partkey  0) ) as decimal(28  8) ) as agg2 from lineitem_1 inner join orders_1 on lineitem_1.l_orderkey = orders_1.o_orderkey inner join partsupp_1 on l_partkey = partsupp_1.ps_partkey and l_suppkey = partsupp_1.ps_suppkey where partsupp_1.ps_suppkey > 1 group by l_orderkey  l_partkey  l_suppkey  o_orderkey  o_custkey;  we can see that query doesn't use `ps_partkey` which is in mv group by expression. Normally will add roll up aggragate on materialized view if the gorup by dimension in mv is mucher than query group by dimension. And  in this scane we can get the function dependency on `l_suppkey = ps_suppkey `. and we doesn't need to add roll up aggregate on materialized view in rewritten plan. this improve performance and is beneficial for nest materialized view rewrite.
apache,doris,9b5a764623873f3ec3165e9d8eca3980cb67fcd7,https://github.com/apache/doris/commit/9b5a764623873f3ec3165e9d8eca3980cb67fcd7,[feat](Nereids) Optimize Sum Literal Rewriting by Excluding Single Instances (#35559)  ## Proposed changes  This PR introduces a change in the method removeOneSumLiteral to enhance the performance of sum literal rewriting in SQL queries. The modification ensures that sum literals appearing only once  such as in expressions like select count(id1 + 1)  count(id2 + 1) from t  are not rewritten.
apache,doris,41d4618f7cf599df35c9b9401d7c760a23f12dc0,https://github.com/apache/doris/commit/41d4618f7cf599df35c9b9401d7c760a23f12dc0,[opt](Nereids) Optimize Join Penalty Calculation Based on Build Side Data Volume (#35773)  This PR introduces an optimization that adjusts the penalty applied during join operations based on the volume of data on the build side. Specifically  when the number of rows and width of the tables being joined are equal  the materialization costs are now considered more accurately. The update ensures that joins with a larger dataset on the build side incur a higher penalty  improving overall query performance and resource allocation.
apache,doris,a939b59dd4cb291a74ff694a1662bf2228230cea,https://github.com/apache/doris/commit/a939b59dd4cb291a74ff694a1662bf2228230cea, [opt](mtmv) Improve the mv rewrite performance by optimize code usage (#35674)  Improve the performance from two points  one is optimize decide model method and another is to reuse the mv struc info:  1. Instead of use java.util.List#containsAll by java.util.Set#containsAll in method AbstractMaterializedViewRule#decideMatchMode  2. Reuse the mv struct info in different query  because mv struct info is immutable.  Notes: tableBitSet in struct info is relevant to the statementContext in cascadesContext  if reuse the mv struct info for different query  we should re generate table bitset and construct new struct info with method StructInfo#withTableBitSet
apache,doris,4665fa162afbbf10aa4a1714be5a69b3cfe9b5a8,https://github.com/apache/doris/commit/4665fa162afbbf10aa4a1714be5a69b3cfe9b5a8,[fix](Nereids) Optimize BFS Memory Usage to Mitigate Exponential Data Growth (#35440)  The origin pr is #34948 and the temporary solution is #35408.  In our effort to streamline and optimize dependency handling  we implement the following steps:  - Detect Circular Dependencies: Identify any circular references within functional dependencies. If any are found  we remove the specific dependencies responsible for creating these cycles. - Clean Up Group By Dependencies: Remove all dependencies listed in the 'group by' clauses to simplify and enhance query performance.
pinpoint-apm,pinpoint,710a2c8bec968a8fc42b75d4032631b93d0bef64,https://github.com/pinpoint-apm/pinpoint/commit/710a2c8bec968a8fc42b75d4032631b93d0bef64,[#12159] Change sort key combination to improve query performance
pinpoint-apm,pinpoint,98cc821a34c0e0f436937e0b3de4836170f0f2e6,https://github.com/pinpoint-apm/pinpoint/commit/98cc821a34c0e0f436937e0b3de4836170f0f2e6,[#10318] Improved serialization performance of primitive values
pinpoint-apm,pinpoint,06584b08cf2437654c0ca2bc6620c16592158878,https://github.com/pinpoint-apm/pinpoint/commit/06584b08cf2437654c0ca2bc6620c16592158878,[#11497]  Improve atomicity and performance of Redis call in ActiveThread
pinpoint-apm,pinpoint,19250305230e94f3b6e5152f600a9d9f953dd2e3,https://github.com/pinpoint-apm/pinpoint/commit/19250305230e94f3b6e5152f600a9d9f953dd2e3,[#11411] Improve write performance of ServerMap Link
pinpoint-apm,pinpoint,ccd7e28f4074477b0a78cae5966bad898cb7d07a,https://github.com/pinpoint-apm/pinpoint/commit/ccd7e28f4074477b0a78cae5966bad898cb7d07a,[#11337] Improve performance for ActiveAgent
languagetool-org,languagetool,5f79678349473d697fa8a047da3e9c9fc1a841dd,https://github.com/languagetool-org/languagetool/commit/5f79678349473d697fa8a047da3e9c9fc1a841dd,Revert "Memory and performance optimizations (#11027)" (#11043)  This reverts commit 6f823cfdc5f7d70eaea63cc9b49cfa16a94b2a86.
languagetool-org,languagetool,6f823cfdc5f7d70eaea63cc9b49cfa16a94b2a86,https://github.com/languagetool-org/languagetool/commit/6f823cfdc5f7d70eaea63cc9b49cfa16a94b2a86,Memory and performance optimizations (#11027)  * Do not instantiate French language manually multiple times  * Do not instantiate GermanyGerman language multiple times  Do not preserve language in static fields.  Such approach causes multiple instantiation of other classes when initialization of classA loading classB and classB has a static field with instance of classA (this is a simplified example)  * Do not instantiate languages multiple times on `Languages` class loading  * Added multiple instantiation protection for the French languages
languagetool-org,languagetool,edac13dd6f0e9f80b10bd6c1b49f5a53af6d815f,https://github.com/languagetool-org/languagetool/commit/edac13dd6f0e9f80b10bd6c1b49f5a53af6d815f,perf: various optimizations for hot spots in profiling data (#10704)  * perf: various optimizations for hot spots in profiling data  avoid creating regexes via Pattern.split and String.replace remove unneccessary lock in wordsToAdd cache data in HunspellRule initialization  * perf: remove other synchronized blocks in wordsToAdd
google,guice,17aa84c3ce2bdb7cb81acd7d9cde92e4f53359cb,https://github.com/google/guice/commit/17aa84c3ce2bdb7cb81acd7d9cde92e4f53359cb,Simplify the 'factory' MethodHandle signatures to drop 'dependency' types from them  I thought modeling native types would be better but on reflection it just adds complexity for little gain.  So turn all generated factories into `(InternalContext)->Object` types.  This eliminates a bunch of `asType` adapters and should improve linkage performance a bit (not that this is a huge concern)  but the major benefit is just simplicity.  Performance is an interesting question here.  In principle we are just adding 'no op' cast to `Object` on constructor and provides method invocations  and then `checkCast` instructions at constructor and method parameter injection points.  So the waste is those extra `cast` operations.  However  in the JVM those are nearly free (simple pointer check  can often be elided as part of normal invocation type checking). So my guess is that this is a no-op performance wise.  Some limited benchmarking of `@Provides` and `@Inject` constructors bears this out.  PiperOrigin-RevId: 740791709
google,guice,ab1ef98c117d3d62b786a78f71ae64cd3f48d7c5,https://github.com/google/guice/commit/ab1ef98c117d3d62b786a78f71ae64cd3f48d7c5,Introduce `LinkageContext` to resolve cycles that occur during `getHandle` calls.  If 2 `InternalFactory` objects get linked to each other recursively we need to handle and detect cycles to prevent `StackOverflow`  For `get` calls this is managed by `InternalContext` and it is extremely performance sensitive.  For `getHandle` calls  this is currently unhandled which can lead to StackOverflow as more InternalFactory types are migrated.  This change addresses that via a new class `LinkageContext` which can detect and automatically resolve a cycle.  The specific strategy is to use a `MutableCallSite` as a layer of indirection to link back to the original method handle once it is done being constructed.  This constructs a recursive `MethodHandle`  much in the same way the normal cycle resolution uses a `Proxy` for that same layer of indirection.  See also [The farnsworth parabox](https://en.wikipedia.org/wiki/The_Farnsworth_Parabox).  Testing this scenario right now is difficult since we don't support many kinds of factories  but the next change introducing provider methods requires this.  PiperOrigin-RevId: 737708332
google,guice,41e8c06f6ff8d349ba89aaaa5b91f04c25868d27,https://github.com/google/guice/commit/41e8c06f6ff8d349ba89aaaa5b91f04c25868d27,Improve the performance of `isCircularProxy`  The map implementation used by `MapMaker` is fairly old and requires 2 levels of indirection (segments->table).  Switching to `ConcurrentHashMap` would be better but this will still be redundant with logic inside of `Proxy` so instead we just use that.  Based on some simple benchmarks this is about 5X as fast  and should save a bit memory.  A number of alternatives were considered  * adding a marker interface to the list of proxy interfaces This is a completely ideal solution but fails due to our need to support proxying interfaces from all `ClassLoaders` and it isn't possible to proxy interfaces that cannot mutually 'see' each other.  So we would need to 'inject' our interface into the Bootstrap classloader which seems impossible and risky  or create special 'child' classloaders that can bridge it  but this triggers issues with proxying package-private interfaces... sigh. * using a `ClassValue`  bootstrapping the value is tricky and while this is faster than the status quo  it would allocate an entry for every class we queried which could add up in a large application  and the approach in this change is faster. * using `Proxy.isProxyInstance` this works but is slower than an `instanceof` query.  PiperOrigin-RevId: 721984180
redis,jedis,eb34e054763fdc3b50284ece1b5c0dd08615c037,https://github.com/redis/jedis/commit/eb34e054763fdc3b50284ece1b5c0dd08615c037,Run pipeline in current thread if all the keys on same node (#4149)  * perf:last node run in current thread directly  * fix: connection leak  we should return it to connection pool  * noop sync when pipelinedResponses.isEmpty()  * revert rename  * clean  * remove last node run in current thread when multi node  * add test for pipeline all keys at same node  * fix: make all keys on same node  * formatting  ---------  Co-authored-by: ggivo <ivo.gaydazhiev@redis.com>
grpc,grpc-java,f866c805c2f78271de9f2b61254363d009cee8c6,https://github.com/grpc/grpc-java/commit/f866c805c2f78271de9f2b61254363d009cee8c6,util: SocketAddress.toString() cannot be used for equality  Some addresses are equal even though their toString is different (InetSocketAddress ignores the hostname when it has an address). And some addresses are not equal even though their toString might be the same (AnonymousInProcessSocketAddress doesn't override toString()).  InetSocketAddress/InetAddress do not cache the toString() result. Thus  even in the worst case that uses a HashSet  this should use less memory than the earlier approach  as no strings are formatted. It probably also significantly improves performance in the reasonably common case when an Endpoint is created just for looking up a key  because the string creation in the constructor isn't then amorized. updateChildrenWithResolvedAddresses()  for example  creates n^2 Endpoint objects for lookups.
plantuml,plantuml,47df55c69cc0db30d41cdc4f0b1728343c83883f,https://github.com/plantuml/plantuml/commit/47df55c69cc0db30d41cdc4f0b1728343c83883f,fix: performance issue on substring https://github.com/plantuml/plantuml/issues/1819
debezium,debezium,6a01cf5a6e3521c3a57f3a6373ece6607cb3cfb4,https://github.com/debezium/debezium/commit/6a01cf5a6e3521c3a57f3a6373ece6607cb3cfb4,DBZ-9030 Fix performance regression with transaction inserts
debezium,debezium,1d587fb6ff0a200b58d22c5e6e6a18059e782c98,https://github.com/debezium/debezium/commit/1d587fb6ff0a200b58d22c5e6e6a18059e782c98,DBZ-8879: Fix performance regression in debezium-core
debezium,debezium,16654038c8df55ef8cbacd6f06e6d2136713d6ba,https://github.com/debezium/debezium/commit/16654038c8df55ef8cbacd6f06e6d2136713d6ba,DBZ-8879: Fix performance regression in debezium-core
debezium,debezium,695e56cb22804471fc69443e0bf1558b8fc1a350,https://github.com/debezium/debezium/commit/695e56cb22804471fc69443e0bf1558b8fc1a350,DBZ-8879: Fix performance regression in debezium-core
debezium,debezium,d0676405ebd835fba7ec878fbd9af8f7acafb841,https://github.com/debezium/debezium/commit/d0676405ebd835fba7ec878fbd9af8f7acafb841,DBZ-8925 Improve Hybrid strategy performance with the Oracle ObjectId cache
debezium,debezium,f2109c1b46ec419eff33471d5584707f4daaaf17,https://github.com/debezium/debezium/commit/f2109c1b46ec419eff33471d5584707f4daaaf17,DBZ-8860 Improve transaction/event cache performance
debezium,debezium,56a2746e2d5b443f51db56756242c9881b52198b,https://github.com/debezium/debezium/commit/56a2746e2d5b443f51db56756242c9881b52198b,DBZ-8665 Improve performance for handling constraint violations
debezium,debezium,f4a2854c856cebeb8fda9c1af04d86ad769e4956,https://github.com/debezium/debezium/commit/f4a2854c856cebeb8fda9c1af04d86ad769e4956,DBZ-8071 Improve hybrid mining performance with object-id-to-table-id cache
trinodb,trino,73015615900443f2921c71f3cc71bb12b8a47a29,https://github.com/trinodb/trino/commit/73015615900443f2921c71f3cc71bb12b8a47a29,Parallelize materialized view base table freshness retrieval in Iceberg  Previously  base table freshness was retrieved sequentially  which could make materialized view refreshes inefficient  especially when base tables changed frequently or loaded slowly or the materialized view involves other materialized views.  This change parallelizes the retrieval process  improving refresh performance  particularly for workloads with frequently changing or slow-loading base tables.  Benchmark Results  with 20 base tables in a materialized view using `REFRESH MATERIALIZED VIEW`: * Avg table load time: 10ms → Refresh time reduced from 560ms to 310ms. * Avg table load time: 100ms → Refresh time reduced by more than 1s.
trinodb,trino,819b1935110eff239b710ae540da5c0285e3c02f,https://github.com/trinodb/trino/commit/819b1935110eff239b710ae540da5c0285e3c02f,Print splits count and distribution time in EXPLAIN ANALYZE  Split distribution time can be a bottleneck in some scenarios. Having it in the EXPLAIN output makes it easier to diagnose without accessing query json. Split count is helpful to analyze query performance when there is either a big number of small splits or there is just one split like with basic JDBC connectors. The split count is visible also in the "Input rows distribution" metric but it is only available in the VERBOSE mode.
trinodb,trino,4f480877d01f28fa68665af19f41f217c4156201,https://github.com/trinodb/trino/commit/4f480877d01f28fa68665af19f41f217c4156201,Improve performance when listing columns in Iceberg
trinodb,trino,61a79dd875fa48e2e0c2b3230051a526ecd51ad7,https://github.com/trinodb/trino/commit/61a79dd875fa48e2e0c2b3230051a526ecd51ad7,Make IcebergSplitSource async  Previously  all IcebergSplitSource scan planning activies happened on the calling scheduler thread  which meant that all table scan planning would happen sequentially across table scans. This hurts performance when queries contain multiple iceberg (or other connector) table scans that could proceed with split generation in parallel to one another.
trinodb,trino,dce5a39f697fee3bdbf4029618526c6745f6ce47,https://github.com/trinodb/trino/commit/dce5a39f697fee3bdbf4029618526c6745f6ce47,Extract IANA timezone ID from timestamp(p) and time(p) with tz  This commit adds functions to extract the IANA timezone ID from timestamp(p) with time zone and time(p) with time zone. Previously  only the timezone offsets could be extracted (TIMEZONE_HOUR and TIMEZONE_MINUTE)  which loses information about the actual timezone.  Resolves: #20893  Refactor timezone extraction logic and update documentation  - Removed unnecessary line-breaks to improve code style - Replaced String.format with optimized string operations for better performance - Updated documentation to clarify method functionality and usage  Resolves: #20893  Resolve function name conflict  timezone(timestamp(p) with time zone) and timezone(time(p) with time zone) in conflict which lead to build failure in trino-docs. :no-index: prevents this conflict.  Resolves: #20893
trinodb,trino,7ec87bcee3b38f1e5f1bec1df7638fdb83cf7887,https://github.com/trinodb/trino/commit/7ec87bcee3b38f1e5f1bec1df7638fdb83cf7887,Use dynamic filters to reduce hive partitions metadata listing  Improves performance of splits generation on large partitioned hive tables by using dynamic filters to reduce the partitions metadata fetched by io.trino.metastore.HiveMetastore#getPartitionsByNames
trinodb,trino,a524522a56f21623ebe6298c30f50f2bb2cc698d,https://github.com/trinodb/trino/commit/a524522a56f21623ebe6298c30f50f2bb2cc698d,Do not materialize stream early  It doesn't improve performance much
trinodb,trino,42c13dfe008c2579dee4df1f641fd00a8b3f041e,https://github.com/trinodb/trino/commit/42c13dfe008c2579dee4df1f641fd00a8b3f041e,Materialize input stream early  According to a Jackson's performance guidelines  parsing from byte[] is faster than parsing from InputStream.
trinodb,trino,185e071a379fc49e977a6740d1cfd81a4016ded0,https://github.com/trinodb/trino/commit/185e071a379fc49e977a6740d1cfd81a4016ded0,Add MultipleDistinctAggregationsToSubqueries  The rule splits distinct aggregations on different arguments to sub-queries and joins the grouped results using grouping keys if any. This allows SingleDistinctAggregationToGroupBy to kick in and improve parallelism and performance significantly when the grouped query is cheap to duplicate.
trinodb,trino,401669e72faf819f583c7d99b748901d719ccc63,https://github.com/trinodb/trino/commit/401669e72faf819f583c7d99b748901d719ccc63,Skip PreAggregateCaseAggregations it aggregations are not reduced  If the number of pre-aggregations is equal to number of case aggregations  then there is no performance gain in rule execution. Additionally  firing rule in such case could lead to infinite rule execution loop  since new pre-aggregations could be eligable for further optimization.
trinodb,trino,cae2e9b70671e8dfefd690d2d2d8f0b464ededb5,https://github.com/trinodb/trino/commit/cae2e9b70671e8dfefd690d2d2d8f0b464ededb5,Reduce thread congestion in HashBuilderOperator  Memory management in HashBuilderOperator.addInput caused significant thread congestion. When memory tracking accuracy is traded for calling delegate.setBytes() less frequently  we are getting better performance for queries which tend to process tiny pages in HashBuilder operator.
trinodb,trino,bd289dc4773baca88c94670f625538a4b98f8398,https://github.com/trinodb/trino/commit/bd289dc4773baca88c94670f625538a4b98f8398,Increase the max number of http connections in Azure native FS  By default  OkHttp only allows 5 concurrent HTTP connections per host  and 64 total concurrent connections. Increase these values to avoid a performance degradation compared to the legacy FS  when executing queries with a large number of splits  on nodes with large number of split runner threads.
trinodb,trino,623bcc2fb328665467d12ae02cb8700d6ae0a389,https://github.com/trinodb/trino/commit/623bcc2fb328665467d12ae02cb8700d6ae0a389,Improve performance of json functions  Avoid allocating heap ByteBuffer used by InputStreamReader.
trinodb,trino,cd415149a1eca481c2b69cdde8c504ea70e597d3,https://github.com/trinodb/trino/commit/cd415149a1eca481c2b69cdde8c504ea70e597d3,Improve performance of SortedRangeSet discrete union
jhy,jsoup,0679bef07f1e29ae72ae54102d5af9a1f80d45d4,https://github.com/jhy/jsoup/commit/0679bef07f1e29ae72ae54102d5af9a1f80d45d4,Perf: removed redundant lowercase normalization
jhy,jsoup,d80275e16ebd34bae5b48f29f3e4437e1b207955,https://github.com/jhy/jsoup/commit/d80275e16ebd34bae5b48f29f3e4437e1b207955,Performance tweak when appending tag names  For some crafted HTML  this path was accumulating an ultra-long tag name. Removed redundant
jhy,jsoup,36ba3edad1de83e61dff71ca929909587eebd834,https://github.com/jhy/jsoup/commit/36ba3edad1de83e61dff71ca929909587eebd834,Optimize `#id .class` selector performance  Fixes #2254
apereo,cas,2445f633a7685adc31e663f6c4085fefc7b6e08e,https://github.com/apereo/cas/commit/2445f633a7685adc31e663f6c4085fefc7b6e08e,improve redis ticket registry performance - #2
apereo,cas,db973e2a019d7b972e84e7a5a862605267390bcb,https://github.com/apereo/cas/commit/db973e2a019d7b972e84e7a5a862605267390bcb,improve redis ticket registry performance - #1
apereo,cas,e1c6d28fb40b591d962293ed7666644df3837cdc,https://github.com/apereo/cas/commit/e1c6d28fb40b591d962293ed7666644df3837cdc,improve performance of MongoDb/GoogleCloudFirestore ticket registry (#6129)
apereo,cas,6294bf6a7e9b579b985c6a97f9ccc8383e87e78a,https://github.com/apereo/cas/commit/6294bf6a7e9b579b985c6a97f9ccc8383e87e78a,minor hz performance improvements
apereo,cas,d49f0c1cf7a7a8fdc7d3125e435b897bc446ea12,https://github.com/apereo/cas/commit/d49f0c1cf7a7a8fdc7d3125e435b897bc446ea12,Improve sessions counting by using Jet for the Hazelcast ticket registry  The performance of the sessions counting can be greatly improved by using Jet for the Hazelcast ticket registry.  The `HazelcastTicketRegistryTests` already covered this use case.
xpipe-io,xpipe,b39c74b4ff5caefcf8078d8ebdc9346987330f3e,https://github.com/xpipe-io/xpipe/commit/b39c74b4ff5caefcf8078d8ebdc9346987330f3e,Improve list box performance by synchronizing less
xpipe-io,xpipe,e69da5d5b63e3433f51bede3716034d0d16daa08,https://github.com/xpipe-io/xpipe/commit/e69da5d5b63e3433f51bede3716034d0d16daa08,Don't animate transition in performance mode
xpipe-io,xpipe,39ee41e7b4871235062dcc29d67dcc1a38b94520,https://github.com/xpipe-io/xpipe/commit/39ee41e7b4871235062dcc29d67dcc1a38b94520,More performance adjustments
xpipe-io,xpipe,ee418ed0ffd9c25b309988af32070c2b0acaa294,https://github.com/xpipe-io/xpipe/commit/ee418ed0ffd9c25b309988af32070c2b0acaa294,Section performance fixes
xpipe-io,xpipe,a898341011e19cb99307eff7f67443bcc2d81edb,https://github.com/xpipe-io/xpipe/commit/a898341011e19cb99307eff7f67443bcc2d81edb,Various performance fixes
xpipe-io,xpipe,afaf206a4253f85558a4eb52e2b850a53d18a2ed,https://github.com/xpipe-io/xpipe/commit/afaf206a4253f85558a4eb52e2b850a53d18a2ed,More performance fixes
xpipe-io,xpipe,572a0f03418e99b589cb142f087574f54d21e8d4,https://github.com/xpipe-io/xpipe/commit/572a0f03418e99b589cb142f087574f54d21e8d4,Various performance fixes
opensearch-project,OpenSearch,560ac1030c941c8cb01c4fcd29a919bd45ccabee,https://github.com/opensearch-project/OpenSearch/commit/560ac1030c941c8cb01c4fcd29a919bd45ccabee,Improve sort-query performance by retaining the default totalHitsThreshold for approximated match_all queries (#18189)  Signed-off-by: Prudhvi Godithi <pgodithi@amazon.com>
opensearch-project,OpenSearch,27946550b686df41fd0ff53cb340ce52c73a7b45,https://github.com/opensearch-project/OpenSearch/commit/27946550b686df41fd0ff53cb340ce52c73a7b45,Improve flat_object field parsing performance by reducing two passes to a single pass (#16297)
opensearch-project,OpenSearch,92088be9e859d95422aa94f3e2db3619d1a90209,https://github.com/opensearch-project/OpenSearch/commit/92088be9e859d95422aa94f3e2db3619d1a90209,Optimize innerhits query performance (#16937)  Signed-off-by: kkewwei <kewei.11@bytedance.com> Signed-off-by: kkewwei <kkewwei@163.com>
opensearch-project,OpenSearch,ba0c4f3e0e2a34d85c55ccecd344df8db9d256cf,https://github.com/opensearch-project/OpenSearch/commit/ba0c4f3e0e2a34d85c55ccecd344df8db9d256cf,Improve performance of bitmap terms filtering (#16936)   ---------  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>
opensearch-project,OpenSearch,e07499a771afbc335e1f7f08a82f8197e5826939,https://github.com/opensearch-project/OpenSearch/commit/e07499a771afbc335e1f7f08a82f8197e5826939,Improve performance for resolving derived fields (#16564)  Doing the type check before the string comparison makes it much faster to resolve derived fields.  Signed-off-by: Robson Araujo <robson.araujo@glean.com>
opensearch-project,OpenSearch,6f1b59e54bec41d40772f8571c7b65d4b523f8b1,https://github.com/opensearch-project/OpenSearch/commit/6f1b59e54bec41d40772f8571c7b65d4b523f8b1,Add logic in master service to optimize performance and retain detailed logging for critical cluster operations. (#16421)  Signed-off-by: Sumit Bansal <sumitsb@amazon.com> Signed-off-by: shwetathareja <shwetathareja@live.com> Co-authored-by: shwetathareja <shwetathareja@live.com>
opensearch-project,OpenSearch,dc8a435f9f14eb6eb679c63d06f4baca3def3215,https://github.com/opensearch-project/OpenSearch/commit/dc8a435f9f14eb6eb679c63d06f4baca3def3215,[Star tree] Performance optimizations during flush flow (#16037)  ---------  Signed-off-by: Bharathwaj G <bharath78910@gmail.com>
opensearch-project,OpenSearch,2e9db40a50735eacc95a4fc8926e8bb7042a696a,https://github.com/opensearch-project/OpenSearch/commit/2e9db40a50735eacc95a4fc8926e8bb7042a696a,Introduce ApproximateRangeQuery and ApproximateQuery (#13788)  This introduces a basic "approximation" framework that improves query performance by modifying the query in a way that should be functionally equivalent.  To start  we can reduce the bounds of a range query in order to satisfy the `track_total_hits` value (which defaults to 10 000).  ---------  Signed-off-by: Harsha Vamsi Kalluri <harshavamsi096@gmail.com> Signed-off-by: Michael Froh <froh@amazon.com> Co-authored-by: Michael Froh <froh@amazon.com>
opensearch-project,OpenSearch,eb306d2bab43de789b59adc01265c683a8fb69fb,https://github.com/opensearch-project/OpenSearch/commit/eb306d2bab43de789b59adc01265c683a8fb69fb,Add queryGroupId to search workload tasks at co-ordinator and data node level (#14708)  * add logic to add headers to Task  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add logic to add queryGroupId to task headers  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * remove redundant code  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add changelog entry  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * address comments  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * fix precommit  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add UTs for RemoteIndexMetadataManager (#14660)  Signed-off-by: Shivansh Arora <hishiv@amazon.com> Co-authored-by: Arpit-Bandejiya <abandeji@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix match_phrase_prefix_query not working on text field with multiple values and index_prefixes (#10959)  * Fix match_phrase_prefix_query not working on text field with multiple values and index_prefixes Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Add more test  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * modify change log  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Fix test failure  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Change the indexAnalyzer used by prefix field  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Skip old version for yaml test  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Optimize some code  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Fix test failure  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Modify yaml test description  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Remove the name parameter for setAnalyzer()  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  ---------  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Offline calculation of total shard per node and caching it for weight calculation inside LocalShardBalancer (#14675)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [bug fix] validate lower bound for top n size (#14587)  Signed-off-by: Chenyang Ji <cyji@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Create SystemIndexRegistry with helper method matchesSystemIndex (#14415)  * Create new extension point in SystemIndexPlugin for a single plugin to get registered system indices  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add to CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * WIP on system indices from IndexNameExpressionResolver  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add test in IndexNameExpressionResolverTests  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Remove changes in SystemIndexPlugin  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add method in IndexNameExpressionResolver to get matching system indices  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Show how resolver can be chained to get system indices  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Fix forbiddenApis check  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Update CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Make SystemIndices internal  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Remove unneeded changes  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Fix CI failures  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Fix precommit errors  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Use Regex instead of WildcardMatcher  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Address code review feedback  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Allow caller to pass index expressions  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Create SystemIndexRegistry  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Update CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Remove singleton limitation  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add javadoc  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add @ExperimentalApi annotation  Signed-off-by: Craig Perkins <cwperx@amazon.com>  ---------  Signed-off-by: Craig Perkins <cwperx@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Refactor Grok validate pattern to iterative approach (#14206)  * grok validate patterns recusrion to iterative  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * Add max depth in resolving a pattern to avoid OOM  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * change path from deque to arraylist  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * rename queue to stack  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * Change max depth to 500  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * typo originPatternName fix  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * spotless  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  ---------  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump opentelemetry from 1.39.0 to 1.40.0 (#14674)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump jackson from 2.17.1 to 2.17.2 (#14687)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add release notes for release 1.3.18 (#14699)  Signed-off-by: Zelin Hao <zelinhao@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump reactor from 3.5.19 to 3.5.20 (#14697)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add unit tests for read flow of RemoteClusterStateService and bug fix for transient settings (#14476)  Signed-off-by: Shivansh Arora <hishiv@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Update version check for the bug fix of match_phrase_prefix_query not working on text field with multiple values and index_prefixes (#14703)  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Remove unnecessary cast to int from test (#14696)  Signed-off-by: Lukáš Vlček <lukas.vlcek@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * print reason why parent task was cancelled (#14604)  Signed-off-by: kkewwei <kkewwei@163.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Use set of shard routing for shard in unassigned shard batch check. (#14533)  Signed-off-by: Swetha Guptha <gupthasg@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add versioning for UploadedIndexMetadata (#14677)  * Add versioning for UploadedIndexMetadata * Handle componentPrefix for backward compatibility  Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix: update help output for _cat (#14722)  * fixed help output for _cat  Signed-off-by: ahmedsobeh <ahmed.sobeh@aiven.io>  * updated changelog  Signed-off-by: ahmedsobeh <ahmed.sobeh@aiven.io>  * updated changelog  Signed-off-by: ahmedsobeh <ahmed.sobeh@aiven.io>  ---------  Signed-off-by: ahmedsobeh <ahmed.sobeh@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix hdfs-fixture kerb-admin & hadoop-minicluster dependencies are not being updated / false positive reports on CVEs (#14729)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Update to Gradle 8.9 (#14574)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix hdfs-fixture hadoop-minicluster dependencies are not being updated / false positive reports on CVEs (#14732)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add `strict_allow_templates` dynamic mapping option (#14555)  * The dynamic mapping parameter supports strict_allow_templates  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Modify change log  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Modify skip version in yml test file  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Refactor some code  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Keep the old methods  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * change public to private  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Optimize some code  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Do not override toString method for Dynamic  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Optimize some code and modify the changelog  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  ---------  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump net.minidev:json-smart from 2.5.0 to 2.5.1 in /plugins/repository-azure (#14748)  * Bump net.minidev:json-smart in /plugins/repository-azure  Bumps [net.minidev:json-smart](https://github.com/netplex/json-smart-v2) from 2.5.0 to 2.5.1. - [Release notes](https://github.com/netplex/json-smart-v2/releases) - [Commits](https://github.com/netplex/json-smart-v2/compare/2.5.0...2.5.1)  --- updated-dependencies: - dependency-name: net.minidev:json-smart dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * remove query insights plugin from core (#14743)  Signed-off-by: Chenyang Ji <cyji@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add `strict_allow_templates` dynamic mapping option (#14555) (#14737) (#14742)  * The dynamic mapping parameter supports strict_allow_templates  * Modify change log  * Modify skip version in yml test file  * Refactor some code  * Keep the old methods  * change public to private  * Optimize some code  * Do not override toString method for Dynamic  * Optimize some code and modify the changelog  ---------  (cherry picked from commit 6b8b3efe01a62c221f308a2e3b019d75a7f5ad8a)  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Co-authored-by: opensearch-trigger-bot[bot] <98922864+opensearch-trigger-bot[bot]@users.noreply.github.com> Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix create or update alias API doesn't throw exception for unsupported parameters (#14719)  * Fix create or update alias API doesn't throw exception for unsupported parameters  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Update version check in yml test  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * modify change log  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  ---------  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Remove query categorization from core (#14759)  * Remove query categorization from core  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Add changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Trigger Build  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  ---------  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add changes to propagate queryGroupId across child requests and nodes (#14614)  * add query group header propagator  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * apply spotless check  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add new propagator in ThreadContext  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * spotlessApply  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * address comments  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.microsoft.azure:msal4j from 1.15.1 to 1.16.0 in /plugins/repository-azure (#14610)  * Bump com.microsoft.azure:msal4j in /plugins/repository-azure  Bumps [com.microsoft.azure:msal4j](https://github.com/AzureAD/microsoft-authentication-library-for-java) from 1.15.1 to 1.16.0. - [Release notes](https://github.com/AzureAD/microsoft-authentication-library-for-java/releases) - [Changelog](https://github.com/AzureAD/microsoft-authentication-library-for-java/blob/dev/changelog.txt) - [Commits](https://github.com/AzureAD/microsoft-authentication-library-for-java/compare/v1.15.1...v1.16.0)  --- updated-dependencies: - dependency-name: com.microsoft.azure:msal4j dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Bugfix] Fix ICacheKeySerializerTests flakiness (#14564)  * Fix testInvalidInput flakiness  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  * Addressed andrross's comment  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  * rerun security check  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  ---------  Signed-off-by: Peter Alfonsi <petealft@amazon.com> Co-authored-by: Peter Alfonsi <petealft@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Correct typo in method name (#14621)  Signed-off-by: vatsal <vatsal.v.anand@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Refactoring FilterPath.parse by using an iterative approach instead of recursion. (#14200)  * Refactor FilterPath parse function (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Implement unit tests for FilterPathTests (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Write warn log if Filter is empty; Add comments (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Add changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove unnecessary log statement  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove unused logger  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Spotless apply  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove incorrect changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  ---------  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Co-authored-by: Robin Friedmann <robinfriedmann.rf@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Removing String format in RemoteStoreMigrationAllocationDecider to optimise performance(#14612)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Clear templates before Adding; Use NamedWriteableAwareStreamInput for RemoteCustomMetadata; Correct the check for deciding upload of HashesOfConsistentSettings (#14513)  * Clear templates before Adding; Use NamedWriteableAwareStreamInput for RemoteCustomMetadata * Correct the check for deciding upload of hashes of consistent settings  Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add changelog  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add PR link changelog  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Improve reroute performance by optimising List.removeAll in LocalShardsBalancer to filter remote search shard from relocation decision (#14613)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix assertion failure while deleting remote backed index (#14601)  Signed-off-by: Sachin Kale <kalsac@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Allow system index warning in OpenSearchRestTestCase.refreshAllIndices (#14635)  * Allow system index warning  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add to CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Address code review comments  Signed-off-by: Craig Perkins <cwperx@amazon.com>  ---------  Signed-off-by: Craig Perkins <cwperx@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Star tree codec changes (#14514)  --------- Signed-off-by: Bharathwaj G <bharath78910@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.github.spullara.mustache.java:compiler from 0.9.13 to 0.9.14 in /modules/lang-mustache (#14672)  * Bump com.github.spullara.mustache.java:compiler  Bumps [com.github.spullara.mustache.java:compiler](https://github.com/spullara/mustache.java) from 0.9.13 to 0.9.14. - [Commits](https://github.com/spullara/mustache.java/compare/mustache.java-0.9.13...mustache.java-0.9.14)  --- updated-dependencies: - dependency-name: com.github.spullara.mustache.java:compiler dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump net.minidev:accessors-smart from 2.5.0 to 2.5.1 in /plugins/repository-azure (#14673)  * Bump net.minidev:accessors-smart in /plugins/repository-azure  Bumps [net.minidev:accessors-smart](https://github.com/netplex/json-smart-v2) from 2.5.0 to 2.5.1. - [Release notes](https://github.com/netplex/json-smart-v2/releases) - [Commits](https://github.com/netplex/json-smart-v2/compare/2.5.0...2.5.1)  --- updated-dependencies: - dependency-name: net.minidev:accessors-smart dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * move query group thread context propagator out of ThreadContext  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  ---------  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com> Signed-off-by: dependabot[bot] <support@github.com> Signed-off-by: Peter Alfonsi <petealft@amazon.com> Signed-off-by: vatsal <vatsal.v.anand@gmail.com> Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: Sachin Kale <kalsac@amazon.com> Signed-off-by: Craig Perkins <cwperx@amazon.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Co-authored-by: Peter Alfonsi <peter.alfonsi@gmail.com> Co-authored-by: Peter Alfonsi <petealft@amazon.com> Co-authored-by: Vatsal <36672090+imvtsl@users.noreply.github.com> Co-authored-by: Siddhant Deshmukh <deshsid@amazon.com> Co-authored-by: Robin Friedmann <robinfriedmann.rf@gmail.com> Co-authored-by: rishavz_sagar <rishavsagar4b1@gmail.com> Co-authored-by: Sooraj Sinha <81695996+soosinha@users.noreply.github.com> Co-authored-by: Sachin Kale <sachinpkale@gmail.com> Co-authored-by: Craig Perkins <cwperx@amazon.com> Co-authored-by: Bharathwaj G <bharath78910@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add consumers to remote store based index settings (#14764)  Signed-off-by: Shourya Dutta Biswas <114977491+shourya035@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add matchesPluginSystemIndexPattern to SystemIndexRegistry (#14750)  * Add matchesPluginSystemIndexPattern to SystemIndexRegistry  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add to CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Use single data structure to keep track of system indices  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Address code review comments  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add test for getAllDescriptors  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Update server/src/main/java/org/opensearch/indices/SystemIndexRegistry.java  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Craig Perkins <craig5008@gmail.com>  ---------  Signed-off-by: Craig Perkins <cwperx@amazon.com> Signed-off-by: Craig Perkins <craig5008@gmail.com> Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * SPI for loading ABC templates (#14659)  * SPI for loading ABC templates  Signed-off-by: mgodwan <mgodwan@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix bulk upsert ignores the default_pipeline and final_pipeline when the auto-created index matches the index template (#12891)  * Fix bulk upsert ignores the default_pipeline and final_pipeline when auto-created index matches with the index template  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Modify changelog & comment  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Use new approach  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Fix test failure  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  ---------  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix flaky test due to node being used across all tests (#14787)  Signed-off-by: Mohit Godwani <mgodwan@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Star Tree Implementation [OnHeap] (#14512)  --------- Signed-off-by: Sarthak Aggarwal <sarthagg@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add Gao Binlong as maintainer (#14796)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Clear ehcache disk cache files during initialization (#14738)  * Clear ehcache disk cache files during initialization  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Adding UT to fix line coverage  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Addressing comment  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Adding more Uts for better line coverage  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Throwing exception in case we fail to clear cache files during startup  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Adding more UTs  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Adding a UT for more coverage  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Fixing gradle build  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Update ehcache disk cache close() logic  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  ---------  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Refactor remote-routing-table service inline with remote state interfaces (#14668)  --------- Signed-off-by: Arpit Bandejiya <abandeji@amazon.com> Signed-off-by: Arpit-Bandejiya <abandeji@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Set version to 2.15 for determining metadata during migration to remote store  Signed-off-by: Sandeep Kumawat <skumwt@amazon.com> Co-authored-by: Sandeep Kumawat <skumwt@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix bulk upsert ignores the default_pipeline and final_pipeline when the auto-created index matches the index template (#14793)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix create or update alias API doesn't throw exception for unsupported parameters (#14769)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Change RCSS info logs to debug (#14814)  Signed-off-by: Shivansh Arora <hishiv@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Bugfix] Fix NPE in ReplicaShardAllocator (#13993) (#14385)  * [Bugfix] Fix NPE in ReplicaShardAllocator (#13993)  Signed-off-by: Daniil Roman <daniilroman.cv@gmail.com>  * Add fix info to CHANGELOG.md  Signed-off-by: Daniil Roman <danroman17397@gmail.com>  ---------  Signed-off-by: Daniil Roman <daniilroman.cv@gmail.com> Signed-off-by: Daniil Roman <danroman17397@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Run performance benchmark on pull requests (#14760)  * add performance benchmark workflow for pull requests  Signed-off-by: Rishabh Singh <sngri@amazon.com>  * Update PERFORMANCE_BENCHMARKS.md  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  * Update PERFORMANCE_BENCHMARKS.md  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  * Update .github/workflows/benchmark-pull-request.yml  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  * Update .github/workflows/benchmark-pull-request.yml  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  * Update .github/workflows/benchmark-pull-request.yml  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  * Update .github/workflows/benchmark-pull-request.yml  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  ---------  Signed-off-by: Rishabh Singh <sngri@amazon.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com> Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * fix constant_keyword field type (#14807)  Signed-off-by: kkewwei <kkewwei@163.com>  test  Signed-off-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Co-authored-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Remote Store Migration] Reconcile remote store based index settings during STRICT mode switch (#14792)  Signed-off-by: Shourya Dutta Biswas <114977491+shourya035@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add prefix mode verification setting for repository verification (#14790)  * Add prefix mode verification setting for repository verification  Signed-off-by: Ashish Singh <ssashish@amazon.com>  * Add UTs and randomise prefix mode repository verification  Signed-off-by: Ashish Singh <ssashish@amazon.com>  * Incorporate PR review feedback  Signed-off-by: Ashish Singh <ssashish@amazon.com>  ---------  Signed-off-by: Ashish Singh <ssashish@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add length check on comment body for benchmark workflow (#14834)  Signed-off-by: Rishabh Singh <sngri@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add restore-from-snapshot test procedure for snapshot run benchmark config (#14842)  Signed-off-by: Rishabh Singh <sngri@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix env variable name typo (#14843)  Signed-off-by: Rishabh Singh <sngri@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Use circuit breaker in InternalHistogram when adding empty buckets (#14754)  * introduce circuit breaker in InternalHistogram  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * use circuit breaker from reduce context  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * add test  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * revert use_real_memory change in OpenSearchNode  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * add change log  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  ---------  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Remote State] Create interface RemoteEntitiesManager (#14671)  * Create interface RemoteEntitiesManager  Signed-off-by: Shivansh Arora <hishiv@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Optimise TransportNodesAction to not send DiscoveryNodes for NodeStat… (#14749)  * Optimize TransportNodesAction to not send DiscoveryNodes for NodeStats  NodesInfo and ClusterStats call  Signed-off-by: Pranshu Shukla <pranshushukla06@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Enabling term version check on local state for all ClusterManager Read Transport Actions (#14273)  * enabling term version check on local state for all admin read actions  Signed-off-by: Rajiv Kumar Vaidyanathan <rajivkv@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Reduce logging in DEBUG for MasterService:run (#14795)  * Reduce logging in DEBUG for MasteService:run by introducing short and long summary in Taskbatcher  Signed-off-by: Sumit Bansal <sumitsb@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add SplitResponseProcessor to Search Pipelines (#14800)  * Add SplitResponseProcessor for search pipelines  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Register the split processor factory  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Address code review comments  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Avoid list copy by casting array  Signed-off-by: Daniel Widdis <widdis@gmail.com>  ---------  Signed-off-by: Daniel Widdis <widdis@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add integration tests for RemoteRoutingTable Service. (#14631)  Signed-off-by: Shailendra Singh <singhlhs@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add SortResponseProcessor to Search Pipelines (#14785)  * Add SortResponseProcessor for search pipelines  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Add stupid and unnecessary javadocs to satisfy overly strict CI  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Split casting and sorting methods for readability  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Register the sort processor factory  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Address code review comments  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Cast individual list elements to avoid creating two lists  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Add yamlRestTests  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Clarify why there's unusual sorting  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Use instanceof instead of isAssignableFrom  Signed-off-by: Daniel Widdis <widdis@gmail.com>  ---------  Signed-off-by: Daniel Widdis <widdis@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix allowUnmappedFields  mapUnmappedFieldAsString settings to be applied when parsing query string query (#13957)  * Modify to invoke QueryShardContext.fieldMapper() method to apply allowUnmappedFields and mapUnmappedFieldAsString settings  Signed-off-by: imyp92 <pyw5420@gmail.com>  * Add test cases to verify returning 400 responses if unmapped fields are included for some types of query  Signed-off-by: imyp92 <pyw5420@gmail.com>  * Add changelog  Signed-off-by: imyp92 <pyw5420@gmail.com>  ---------  Signed-off-by: imyp92 <pyw5420@gmail.com> Signed-off-by: gaobinlong <gbinlong@amazon.com> Co-authored-by: gaobinlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.microsoft.azure:msal4j from 1.16.0 to 1.16.1 in /plugins/repository-azure (#14857)  * Bump com.microsoft.azure:msal4j in /plugins/repository-azure  Bumps [com.microsoft.azure:msal4j](https://github.com/AzureAD/microsoft-authentication-library-for-java) from 1.16.0 to 1.16.1. - [Release notes](https://github.com/AzureAD/microsoft-authentication-library-for-java/releases) - [Changelog](https://github.com/AzureAD/microsoft-authentication-library-for-java/blob/dev/changelog.txt) - [Commits](https://github.com/AzureAD/microsoft-authentication-library-for-java/compare/v1.16.0...v1.16.1)  --- updated-dependencies: - dependency-name: com.microsoft.azure:msal4j dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.gradle.develocity from 3.17.5 to 3.17.6 (#14856)  * Bump com.gradle.develocity from 3.17.5 to 3.17.6  Bumps com.gradle.develocity from 3.17.5 to 3.17.6.  --- updated-dependencies: - dependency-name: com.gradle.develocity dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump org.jline:jline in /test/fixtures/hdfs-fixture (#14859)  Bumps [org.jline:jline](https://github.com/jline/jline3) from 3.26.2 to 3.26.3. - [Release notes](https://github.com/jline/jline3/releases) - [Changelog](https://github.com/jline/jline3/blob/master/changelog.md) - [Commits](https://github.com/jline/jline3/compare/jline-parent-3.26.2...jline-parent-3.26.3)  --- updated-dependencies: - dependency-name: org.jline:jline dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Use Lucene provided Persian stem (#14847)  Lucene provided Persian stem apparently isn't hooked yet and this change is doing that based on what is done for Arabic stem support.  Signed-off-by: Ebrahim Byagowi <ebrahim@gnu.org> Signed-off-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Co-authored-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump actions/checkout from 2 to 4 (#14858)  * Bump actions/checkout from 2 to 4  Bumps [actions/checkout](https://github.com/actions/checkout) from 2 to 4. - [Release notes](https://github.com/actions/checkout/releases) - [Changelog](https://github.com/actions/checkout/blob/main/CHANGELOG.md) - [Commits](https://github.com/actions/checkout/compare/v2...v4)  --- updated-dependencies: - dependency-name: actions/checkout dependency-type: direct:production update-type: version-update:semver-major ...  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Deprecate batch_size parameter on bulk API (#14725)  By default the full _bulk payload will be passed to ingest processors as a batch  with any sub batching logic to be implemented by each processor if necessary.  Signed-off-by: Liyun Xiu <xiliyun@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add perms for remote snapshot cache eviction on scripted query (#14411)  Signed-off-by: Finn Carroll <carrofin@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add transport interceptor to populate queryGroupId in task headers  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add rest  transport layer changes for Hot to warm tiering - dedicated setup (#13980)  Signed-off-by: Neetika Singhal <neetiks@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Create listener to refresh search thread resource usage (#14832)  * [bug fix] fix incorrect coordinator node search resource usages  Signed-off-by: Chenyang Ji <cyji@amazon.com>  * fix bug on serialization when passing task resource usage to coordinator  Signed-off-by: Chenyang Ji <cyji@amazon.com>  * add more unit tests  Signed-off-by: Chenyang Ji <cyji@amazon.com>  * remove query insights plugin related code  Signed-off-by: Chenyang Ji <cyji@amazon.com>  * create per request listener to refresh task resource usage  Signed-off-by: Chenyang Ji <cyji@amazon.com>  * Make new listener API public  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Add changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove wrong files added  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Address review comments  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Build fix  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Make singleton  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Address review comments  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Make sure listener runs before plugin listeners  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Spotless  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Minor fix  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  ---------  Signed-off-by: Chenyang Ji <cyji@amazon.com> Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Signed-off-by: Jay Deng <jayd0104@gmail.com> Co-authored-by: Chenyang Ji <cyji@amazon.com> Co-authored-by: Jay Deng <jayd0104@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Caching avg total bytes and avg free bytes inside ClusterInfo (#14851)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Use default value when index.number_of_replicas is null (#14812)  * Use default value when index.number_of_replicas is null  Signed-off-by: Liyun Xiu <xiliyun@amazon.com>  * Add integration test  Signed-off-by: Liyun Xiu <xiliyun@amazon.com>  * Add changelog  Signed-off-by: Liyun Xiu <xiliyun@amazon.com>  ---------  Signed-off-by: Liyun Xiu <xiliyun@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Remote Routing Table] Implement write and read flow for shard diff file. (#14684)  * Implement write and read flow to upload/download shard diff file.  Signed-off-by: Shailendra Singh <singhlhs@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Optimized ClusterStatsIndices to precomute shard stats (#14426)  * Optimize Cluster Stats Indices to precomute node level stats  Signed-off-by: Pranshu Shukla <pranshushukla06@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix constraint bug which allows more primary shards than average primary shards per index (#14908)  Signed-off-by: Gaurav Bafna <gbbafna@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Optmising AwarenessAllocationDecider for hashmap.get call (#14761)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * update comment  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix IngestServiceTests.testBulkRequestExecutionWithFailures (#14918)  The test would previously fail if the randomness led to only a single indexing request being included in the bulk payload. This change guarantees multiple indexing requests in order to ensure the batch logic kicks in.  Also replace some unneeded mocks with real classes.  Signed-off-by: Andrew Ross <andrross@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add queryGroupTask  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * remove unnecessary imports  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add QueryGroupTask tests  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * rename WLM transport request handler  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add CHANGELOG entry  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * fix ut  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * address comments  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * fix UT to remove the verify for final method  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * apply spotless  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  ---------  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com> Signed-off-by: Shivansh Arora <hishiv@amazon.com> Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Chenyang Ji <cyji@amazon.com> Signed-off-by: Craig Perkins <cwperx@amazon.com> Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com> Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Zelin Hao <zelinhao@amazon.com> Signed-off-by: Lukáš Vlček <lukas.vlcek@aiven.io> Signed-off-by: kkewwei <kkewwei@163.com> Signed-off-by: Swetha Guptha <gupthasg@amazon.com> Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: ahmedsobeh <ahmed.sobeh@aiven.io> Signed-off-by: dependabot[bot] <support@github.com> Signed-off-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Signed-off-by: Peter Alfonsi <petealft@amazon.com> Signed-off-by: vatsal <vatsal.v.anand@gmail.com> Signed-off-by: Sachin Kale <kalsac@amazon.com> Signed-off-by: Shourya Dutta Biswas <114977491+shourya035@users.noreply.github.com> Signed-off-by: Craig Perkins <craig5008@gmail.com> Signed-off-by: mgodwan <mgodwan@amazon.com> Signed-off-by: Mohit Godwani <mgodwan@amazon.com> Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com> Signed-off-by: Sandeep Kumawat <skumwt@amazon.com> Signed-off-by: Daniil Roman <daniilroman.cv@gmail.com> Signed-off-by: Daniil Roman <danroman17397@gmail.com> Signed-off-by: Rishabh Singh <sngri@amazon.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com> Signed-off-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Signed-off-by: Ashish Singh <ssashish@amazon.com> Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com> Signed-off-by: Pranshu Shukla <pranshushukla06@gmail.com> Signed-off-by: Rajiv Kumar Vaidyanathan <rajivkv@amazon.com> Signed-off-by: Sumit Bansal <sumitsb@amazon.com> Signed-off-by: Daniel Widdis <widdis@gmail.com> Signed-off-by: Shailendra Singh <singhlhs@amazon.com> Signed-off-by: imyp92 <pyw5420@gmail.com> Signed-off-by: gaobinlong <gbinlong@amazon.com> Signed-off-by: Ebrahim Byagowi <ebrahim@gnu.org> Signed-off-by: Liyun Xiu <xiliyun@amazon.com> Signed-off-by: Finn Carroll <carrofin@amazon.com> Signed-off-by: Neetika Singhal <neetiks@amazon.com> Signed-off-by: Jay Deng <jayd0104@gmail.com> Signed-off-by: Gaurav Bafna <gbbafna@amazon.com> Signed-off-by: Andrew Ross <andrross@amazon.com> Co-authored-by: Shivansh Arora <hishiv@amazon.com> Co-authored-by: Arpit-Bandejiya <abandeji@amazon.com> Co-authored-by: gaobinlong <gbinlong@amazon.com> Co-authored-by: rishavz_sagar <rishavsagar4b1@gmail.com> Co-authored-by: Chenyang Ji <cyji@amazon.com> Co-authored-by: Craig Perkins <cwperx@amazon.com> Co-authored-by: Sandesh Kumar <sandeshkr419@gmail.com> Co-authored-by: Andriy Redko <andriy.redko@aiven.io> Co-authored-by: Zelin Hao <zelinhao@amazon.com> Co-authored-by: Lukáš Vlček <lukas.vlcek@aiven.io> Co-authored-by: kkewwei <kkewwei@163.com> Co-authored-by: SwethaGuptha <156877431+SwethaGuptha@users.noreply.github.com> Co-authored-by: Sooraj Sinha <81695996+soosinha@users.noreply.github.com> Co-authored-by: Ahmed Sobeh <ahmed.sobeh@aiven.io> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Co-authored-by: opensearch-trigger-bot[bot] <98922864+opensearch-trigger-bot[bot]@users.noreply.github.com> Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Co-authored-by: Siddhant Deshmukh <deshsid@amazon.com> Co-authored-by: Peter Alfonsi <peter.alfonsi@gmail.com> Co-authored-by: Peter Alfonsi <petealft@amazon.com> Co-authored-by: Vatsal <36672090+imvtsl@users.noreply.github.com> Co-authored-by: Robin Friedmann <robinfriedmann.rf@gmail.com> Co-authored-by: Sachin Kale <sachinpkale@gmail.com> Co-authored-by: Bharathwaj G <bharath78910@gmail.com> Co-authored-by: Shourya Dutta Biswas <114977491+shourya035@users.noreply.github.com> Co-authored-by: Craig Perkins <craig5008@gmail.com> Co-authored-by: Andriy Redko <drreta@gmail.com> Co-authored-by: Mohit Godwani <81609427+mgodwan@users.noreply.github.com> Co-authored-by: Sarthak Aggarwal <sarthagg@amazon.com> Co-authored-by: Sagar <99425694+sgup432@users.noreply.github.com> Co-authored-by: Sandeep Kumawat <2025sandeepkumawat@gmail.com> Co-authored-by: Sandeep Kumawat <skumwt@amazon.com> Co-authored-by: Daniil Roman <danroman17397@gmail.com> Co-authored-by: Rishabh Singh <rishabhksingh@gmail.com> Co-authored-by: kkewwei <kewei.11@bytedance.com> Co-authored-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Co-authored-by: Ashish Singh <ssashish@amazon.com> Co-authored-by: bowenlan-amzn <bowenlan23@gmail.com> Co-authored-by: Pranshu Shukla <55992439+Pranshu-S@users.noreply.github.com> Co-authored-by: rajiv-kv <157019998+rajiv-kv@users.noreply.github.com> Co-authored-by: Sumit Bansal <sumit.asr@gmail.com> Co-authored-by: Daniel Widdis <widdis@gmail.com> Co-authored-by: shailendra0811 <167273922+shailendra0811@users.noreply.github.com> Co-authored-by: Park  Yeongwu <pyw5420@gmail.com> Co-authored-by: ebraminio <ebraminio@gmail.com> Co-authored-by: Liyun Xiu <xiliyun@amazon.com> Co-authored-by: Finn <carrofin@amazon.com> Co-authored-by: Neetika Singhal <neetiks@amazon.com> Co-authored-by: Jay Deng <jayd0104@gmail.com> Co-authored-by: Gaurav Bafna <85113518+gbbafna@users.noreply.github.com> Co-authored-by: Andrew Ross <andrross@amazon.com>
opensearch-project,OpenSearch,d33d24e9ff48bd20c12636349ec8c0eb67b38eb2,https://github.com/opensearch-project/OpenSearch/commit/d33d24e9ff48bd20c12636349ec8c0eb67b38eb2,Add changes to propagate queryGroupId across child requests and nodes (#14614)  * add query group header propagator  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * apply spotless check  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add new propagator in ThreadContext  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * spotlessApply  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * address comments  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.microsoft.azure:msal4j from 1.15.1 to 1.16.0 in /plugins/repository-azure (#14610)  * Bump com.microsoft.azure:msal4j in /plugins/repository-azure  Bumps [com.microsoft.azure:msal4j](https://github.com/AzureAD/microsoft-authentication-library-for-java) from 1.15.1 to 1.16.0. - [Release notes](https://github.com/AzureAD/microsoft-authentication-library-for-java/releases) - [Changelog](https://github.com/AzureAD/microsoft-authentication-library-for-java/blob/dev/changelog.txt) - [Commits](https://github.com/AzureAD/microsoft-authentication-library-for-java/compare/v1.15.1...v1.16.0)  --- updated-dependencies: - dependency-name: com.microsoft.azure:msal4j dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Bugfix] Fix ICacheKeySerializerTests flakiness (#14564)  * Fix testInvalidInput flakiness  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  * Addressed andrross's comment  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  * rerun security check  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  ---------  Signed-off-by: Peter Alfonsi <petealft@amazon.com> Co-authored-by: Peter Alfonsi <petealft@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Correct typo in method name (#14621)  Signed-off-by: vatsal <vatsal.v.anand@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Refactoring FilterPath.parse by using an iterative approach instead of recursion. (#14200)  * Refactor FilterPath parse function (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Implement unit tests for FilterPathTests (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Write warn log if Filter is empty; Add comments (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Add changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove unnecessary log statement  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove unused logger  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Spotless apply  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove incorrect changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  ---------  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Co-authored-by: Robin Friedmann <robinfriedmann.rf@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Removing String format in RemoteStoreMigrationAllocationDecider to optimise performance(#14612)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Clear templates before Adding; Use NamedWriteableAwareStreamInput for RemoteCustomMetadata; Correct the check for deciding upload of HashesOfConsistentSettings (#14513)  * Clear templates before Adding; Use NamedWriteableAwareStreamInput for RemoteCustomMetadata * Correct the check for deciding upload of hashes of consistent settings  Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add changelog  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add PR link changelog  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Improve reroute performance by optimising List.removeAll in LocalShardsBalancer to filter remote search shard from relocation decision (#14613)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix assertion failure while deleting remote backed index (#14601)  Signed-off-by: Sachin Kale <kalsac@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Allow system index warning in OpenSearchRestTestCase.refreshAllIndices (#14635)  * Allow system index warning  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add to CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Address code review comments  Signed-off-by: Craig Perkins <cwperx@amazon.com>  ---------  Signed-off-by: Craig Perkins <cwperx@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Star tree codec changes (#14514)  --------- Signed-off-by: Bharathwaj G <bharath78910@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.github.spullara.mustache.java:compiler from 0.9.13 to 0.9.14 in /modules/lang-mustache (#14672)  * Bump com.github.spullara.mustache.java:compiler  Bumps [com.github.spullara.mustache.java:compiler](https://github.com/spullara/mustache.java) from 0.9.13 to 0.9.14. - [Commits](https://github.com/spullara/mustache.java/compare/mustache.java-0.9.13...mustache.java-0.9.14)  --- updated-dependencies: - dependency-name: com.github.spullara.mustache.java:compiler dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump net.minidev:accessors-smart from 2.5.0 to 2.5.1 in /plugins/repository-azure (#14673)  * Bump net.minidev:accessors-smart in /plugins/repository-azure  Bumps [net.minidev:accessors-smart](https://github.com/netplex/json-smart-v2) from 2.5.0 to 2.5.1. - [Release notes](https://github.com/netplex/json-smart-v2/releases) - [Commits](https://github.com/netplex/json-smart-v2/compare/2.5.0...2.5.1)  --- updated-dependencies: - dependency-name: net.minidev:accessors-smart dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * move query group thread context propagator out of ThreadContext  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  ---------  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com> Signed-off-by: dependabot[bot] <support@github.com> Signed-off-by: Peter Alfonsi <petealft@amazon.com> Signed-off-by: vatsal <vatsal.v.anand@gmail.com> Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: Sachin Kale <kalsac@amazon.com> Signed-off-by: Craig Perkins <cwperx@amazon.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Co-authored-by: Peter Alfonsi <peter.alfonsi@gmail.com> Co-authored-by: Peter Alfonsi <petealft@amazon.com> Co-authored-by: Vatsal <36672090+imvtsl@users.noreply.github.com> Co-authored-by: Siddhant Deshmukh <deshsid@amazon.com> Co-authored-by: Robin Friedmann <robinfriedmann.rf@gmail.com> Co-authored-by: rishavz_sagar <rishavsagar4b1@gmail.com> Co-authored-by: Sooraj Sinha <81695996+soosinha@users.noreply.github.com> Co-authored-by: Sachin Kale <sachinpkale@gmail.com> Co-authored-by: Craig Perkins <cwperx@amazon.com> Co-authored-by: Bharathwaj G <bharath78910@gmail.com>
opensearch-project,OpenSearch,58d1164f74e921a26b7a73b6185b38cc87bbc7a9,https://github.com/opensearch-project/OpenSearch/commit/58d1164f74e921a26b7a73b6185b38cc87bbc7a9,Improve reroute performance by optimising List.removeAll in LocalShardsBalancer to filter remote search shard from relocation decision (#14613)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com>
opensearch-project,OpenSearch,57fb50b22bf30148a632bd4c5e78bde53116f00f,https://github.com/opensearch-project/OpenSearch/commit/57fb50b22bf30148a632bd4c5e78bde53116f00f,Apply the date histogram rewrite optimization to range aggregation (#13865)  * Refactor the ranges representation  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Refactor try fast filter  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Main work finished; left the handling of different numeric data types  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * buildRanges accepts field type  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * first working draft probably  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * add change log  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * accommodate geo distance agg  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Fix test  support all numeric types minus one on the upper range  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * [Refactor] range is lower inclusive  right exclusive  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * adding test  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Adding test and refactor  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * refactor  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * add test  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * add test and update the compare logic in tree traversal  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * fix test  add random test  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * refactor to address comments  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * small potential performance update  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * fix precommit  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * refactor  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * refactor  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * set refresh_interval to -1  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * address comment  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * address comment  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * address comment  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Fix test  To understand fully about the double and bigdecimal usage in scaled float field will take more time.  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  ---------  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>
apple,pkl,1bc473ba545167af3a48f7f24048b3b1c42d139a,https://github.com/apple/pkl/commit/1bc473ba545167af3a48f7f24048b3b1c42d139a,Improve lazy type checking of listings and mappings (#789)  Motivation: - simplify implementation of lazy type checking - fix correctness issues of lazy type checking (#785)  Changes: - implement listing/mapping type cast via amendment (`parent`) instead of delegation (`delegate`) - handle type checking of *computed* elements/entries in the same way as type checking of computed properties - ElementOrEntryNode is the equivalent of TypeCheckedPropertyNode - remove fields VmListingOrMapping.delegate/typeNodeFrame/cachedMembers/checkedMembers - fix #785 by executing all type casts between a member's owner and receiver - fix #823 by storing owner and receiver directly instead of storing the mutable frame containing them (typeNodeFrame) - remove overrides of VmObject methods that are no longer required - good for Truffle partial evaluation and JVM inlining - revert a85a173faa785c2e8b14221d6c213c8837925d72 except for added tests - move `VmUtils.setOwner` and `VmUtils.setReceiver` and make them private - these methods aren't generally safe to use  Result: - simpler code with greater optimization potential - VmListingOrMapping can now have both a type node and new members - fewer changes to surrounding code - smaller memory footprint - better performance in some cases - fixes https://github.com/apple/pkl/issues/785 - fixes https://github.com/apple/pkl/issues/823  Potential future optimizations: - avoid lazy type checking overhead for untyped listings/mappings - improve efficiency of forcing a typed listing/mapping - currently  lazy type checking will traverse the parent chain once per member  reducing the performance benefit of shallow-forcing a listing/mapping over evaluating each member individually - avoid creating an intermediate untyped listing/mapping in the following cases: - `new Listing<X> {...}` - amendment of `property: Listing<X>`
apple,pkl,b93cb9b32237760e357d9229b87686b81390a519,https://github.com/apple/pkl/commit/b93cb9b32237760e357d9229b87686b81390a519,Exclude non file-based modules from synthesized *GatherImports task (#821)  This fixes an issue where certain modules tasks fail due to the plugin attempting to analyze their imports  but the arguments may not actually be Pkl modules.  For example  the pkldoc task accepts entire packages in its "sourceMoules" property.  This changes the gather imports logic to only analyze file-based modules. This is also a performance improvement; non file-based modules are unlikely to import files due to insufficient trust levels.  Also: fix a bug when generating pkldoc on Windows
datahub-project,datahub,36ae5afbb573f53b8078e060f24e142f847f5229,https://github.com/datahub-project/datahub/commit/36ae5afbb573f53b8078e060f24e142f847f5229,feat(neo4j): improve neo4j read query performance by specifying labels (#10593)
Activiti,Activiti,cbfdba0e28097475c08da9f7c961a02357500a08,https://github.com/Activiti/Activiti/commit/cbfdba0e28097475c08da9f7c961a02357500a08,MNT-24335: improve historical tasks performance (#4702) (#4716)  * MNT-24335: improve historical tasks performance (#4702)  (cherry picked from commit 84b6e253bc3631e45769c2750edbf939949def42)  * MNT-24335: removed support for mysql5 * MNT-24335: added missing Activiti 7.11.1 version on db schema
StarRocks,starrocks,0c48197c56d31c7e1da5842261fee6f63b6471c6,https://github.com/StarRocks/starrocks/commit/0c48197c56d31c7e1da5842261fee6f63b6471c6,[Enhancement] Change proc_profile collect time to 2min (#59005)  ## Why I'm doing:  Some cluster memory usage spikes suddenly  and it may take only 1–2 minutes from the start of the memory spike to the process getting stuck. If the profile collection time is too long  it can cause the collected process to also get stuck  resulting in no output.  ## What I'm doing: Change the default collect time to 2min. Remove `proc_profile_collect_interval_s` param. In actual scenarios  profile collection must not have intervals. If users feel that collecting profiles affects performance  they can simply disable this function.  Signed-off-by: gengjun-git <gengjun@starrocks.com>
StarRocks,starrocks,c1bc8d31d114dba7232a683d4cb3581ace2132d7,https://github.com/StarRocks/starrocks/commit/c1bc8d31d114dba7232a683d4cb3581ace2132d7,[Enhancement] adjust translate function syntax structure to prevent parser performance rollback (#54830)  Signed-off-by: stephen <stephen5217@163.com>
StarRocks,starrocks,d928cac11183d39783f2b27fda9975f3a6c0f9ac,https://github.com/StarRocks/starrocks/commit/d928cac11183d39783f2b27fda9975f3a6c0f9ac,[Enhancement] Optimize partition retention condition compensation rewrite performance in force_mv mode (#54072)  Signed-off-by: shuming.li <ming.moriarty@gmail.com>
StarRocks,starrocks,c70eadf8ee80322562ddbaf500b98767ee1a2d05,https://github.com/StarRocks/starrocks/commit/c70eadf8ee80322562ddbaf500b98767ee1a2d05,[Enhancement] improve performance of show materialized views statement (#54374)  Signed-off-by: Murphy <mofei@starrocks.com>
StarRocks,starrocks,fb89e478fbe1ed60fd883cc056029b92dd56fd73,https://github.com/StarRocks/starrocks/commit/fb89e478fbe1ed60fd883cc056029b92dd56fd73,[Enhancement] Improve command SHOW PROC "/statistics" performance and some code refinements (#51718)  ## Why I'm doing: If there are many tablet in a single db  like hundreds of thousand  there would be millions replica. Then  command SHOW PROC '/statistic' may be very slow to finish  in the meanwhile  it will occupy the db lock during execution  which may slow down other query executions.  The performance bootle neck is that it calls method SystemInfoService.getBackendIds too many times (one call for each replica). If there are 200 backends  each call may take hundred milliseconds: ![image](https://github.com/user-attachments/assets/11c56743-8a13-4077-bb7e-fb10a374e76f)   To improve command `SHOW PROC "/statistics" `performance  and other general operation which may call `SystemInfoService.getBackendIds`. The main performance bottle neck of method `SystemInfoService.getBackendIds` is that it calls `this.getBackend(...)`  which is unnecessary. <img width="576" alt="image" src="https://github.com/user-attachments/assets/1851dff7-ac28-44ff-899b-78254e47e5e1">  I made a micro benchmark for this improvement  if there are 200 backends  and 1000000 replicas  we can gain 5X performance improvement. ![image](https://github.com/user-attachments/assets/b0468bc5-f5b7-41f1-a92d-e9a60dc40146) ![image](https://github.com/user-attachments/assets/ef207e86-e06a-4637-be44-cac9707fc1c9) ![image](https://github.com/user-attachments/assets/52b0382b-bd58-4881-a447-bb32c532acad)  Fixes #51715  Signed-off-by: ganggewang <ganggewang@tencent.com>
StarRocks,starrocks,e3c6b4ead3b2d1c9824e66b8cdf0d3aacec10ea2,https://github.com/StarRocks/starrocks/commit/e3c6b4ead3b2d1c9824e66b8cdf0d3aacec10ea2,[Enhancement] Optimize iceberg mor performance of iceberg equality delete (#51050)  Signed-off-by: stephen <stephen5217@163.com>
StarRocks,starrocks,f0cb5e97c8fd7176d01d5e3b9b9677cf82f174b7,https://github.com/StarRocks/starrocks/commit/f0cb5e97c8fd7176d01d5e3b9b9677cf82f174b7,[Enhancement] Optimize memory tracker (#49841)  ## Why I'm doing: Currently  memory statistics consume a lot of performance because they are all calculated each time.  ## What I'm doing: Get some samples to estimate the manager's memory  even though it is inaccurate  but it's helpful to solve memory leak bugs.  Signed-off-by: gengjun-git <gengjun@starrocks.com>
StarRocks,starrocks,556928ff43157296907dc4229ec207b9eb3a6d09,https://github.com/StarRocks/starrocks/commit/556928ff43157296907dc4229ec207b9eb3a6d09,[Enhancement] Optimize view based mv rewrite performance (#50256)  Signed-off-by: shuming.li <ming.moriarty@gmail.com>
StarRocks,starrocks,89a6b7741abdfefe28d4347b4b3f987a5126c434,https://github.com/StarRocks/starrocks/commit/89a6b7741abdfefe28d4347b4b3f987a5126c434,[Enhancement] Support configurable hll_sketch and optimize hll_sketch performance (#48939)  Signed-off-by: shuming.li <ming.moriarty@gmail.com>
StarRocks,starrocks,888e20136b5d2ea776038542a2880aba05fed1f5,https://github.com/StarRocks/starrocks/commit/888e20136b5d2ea776038542a2880aba05fed1f5,[Enhancement] Improve cache select performance (#48262)  Signed-off-by: Smith Cruise <chendingchao1@126.com>
StarRocks,starrocks,70c621c0795edf99c2fe24a8b2e29eba3c14985d,https://github.com/StarRocks/starrocks/commit/70c621c0795edf99c2fe24a8b2e29eba3c14985d,[Enhancement] Optimize performance of get physical partition from olap table (#47198)  Signed-off-by: meegoo <meegoo.sr@gmail.com>
StarRocks,starrocks,a8e64b69a8874481b59062898f1230f28466af61,https://github.com/StarRocks/starrocks/commit/a8e64b69a8874481b59062898f1230f28466af61,[Enhancement] Optimize text based mv rewrite performance (#49330)  Signed-off-by: shuming.li <ming.moriarty@gmail.com>
StarRocks,starrocks,4d1ea245db8142970a113679b004ac02349e6533,https://github.com/StarRocks/starrocks/commit/4d1ea245db8142970a113679b004ac02349e6533,[BugFix] Fix incorrect connection state update when register/unregister conn (#48056)  Why I'm doing: Connection state update is incorrect.  What I'm doing: Currently we need to update a few maps and counters when register/unregister connection from FE  we don't have some sync mechanism to protect this update  which may cause incorrect connection state and prompt "connection reach limit" unexpectly to user login. Add lock protection when register/unregister connection  the performance should be ok  we have moved some heavy operations like cleaning tmp table outside of lock  and the rest are just some simple map get/put operations.
StarRocks,starrocks,8e88f8b8c82167988cfb6b770ba1c27b20b9276d,https://github.com/StarRocks/starrocks/commit/8e88f8b8c82167988cfb6b770ba1c27b20b9276d,[Enhancement] optimize the performance of refreshExternalTable stage of MV refresh (#47809)  Signed-off-by: Murphy <mofei@starrocks.com>
StarRocks,starrocks,a79e8eb38b66496a3865269fc5148920b8a5a88e,https://github.com/StarRocks/starrocks/commit/a79e8eb38b66496a3865269fc5148920b8a5a88e,[Enhancement] optimize performance and memeory use in FE (#47012)  Signed-off-by: Seaven <seaven_7@qq.com>
StarRocks,starrocks,2d074050257936f0da73a536e026a999771fdb31,https://github.com/StarRocks/starrocks/commit/2d074050257936f0da73a536e026a999771fdb31,[Enhancement] Performance optimization to Reduce SqlToScalarOperatorTranslator function calls in QueryTransformer the about window function. (#46527)  Signed-off-by: edwinhzhang <edwinhzhang@tencent.com>
StarRocks,starrocks,d4fbb1268243d5cdda7c40c75518e97322e96960,https://github.com/StarRocks/starrocks/commit/d4fbb1268243d5cdda7c40c75518e97322e96960,[Enhancement] optimize performance in cast large string to array (#46957)  Signed-off-by: packy92 <wangchao@starrocks.com>
StarRocks,starrocks,ecc1c148156b8abbbfe9614f8fa2eced34f6308d,https://github.com/StarRocks/starrocks/commit/ecc1c148156b8abbbfe9614f8fa2eced34f6308d,[Enhancement] Optimize performance of get physical partition from temp partition (#47148)  Signed-off-by: meegoo <meegoo.sr@gmail.com>
StarRocks,starrocks,455be1732b1afb7daf6092acb909f3d293f1fdd4,https://github.com/StarRocks/starrocks/commit/455be1732b1afb7daf6092acb909f3d293f1fdd4,[Enhancement] use hashset instead of list to fix the performance bottleneck (#46845)  Signed-off-by: Murphy <mofei@starrocks.com>
signalapp,Signal-Server,886984861f52ba7e722e95f94595815a798506eb,https://github.com/signalapp/Signal-Server/commit/886984861f52ba7e722e95f94595815a798506eb,remove performance based turn routing from CallRoutingControllerV2
SonarSource,sonarqube,9042dc991247cb82fe011994c8e0414658164e83,https://github.com/SonarSource/sonarqube/commit/9042dc991247cb82fe011994c8e0414658164e83,SCA-125 Perf: releases endpoint (#13178)
SonarSource,sonarqube,24bd8856af66bf2e1148c42279739225b254e84e,https://github.com/SonarSource/sonarqube/commit/24bd8856af66bf2e1148c42279739225b254e84e,NO-JIRA improved performance of tests in sonar-scanner-engine module
spinnaker,spinnaker,93f27cca937ff4b4411ce046ee662dba615a47d1,https://github.com/spinnaker/spinnaker/commit/93f27cca937ff4b4411ce046ee662dba615a47d1,perf(pipeline): Improve execution times for dense pipeline graphs (#4824)  * test(pipeline): Define StartStageHandler performance  when given a complex pipeline with multiple layers of upstream stages  * fix(pipeline): Memoize anyUpstreamStagesFailed results  This turns the anyUpstreamStagesFailed calculation from one that scales exponentially based on the number of (branches+downstream stages) in a pipeline to one that scales linearly based on the total number of stages in a pipeline. This is a significant performance improvement  especially for very large and complicated pipelines  * refactor(stage): Move anyUpstreamStagesFailed to StartStageHandler  since that's the only place where it gets used  * fix(stage): Avoid using a ConcurrentHashMap  for memoization. The recursive anyUpstreamStagesFailed(StageExecution) function runs in a single thread  so ConcurrentHashMap is not necessary here  * docs(test): Use a more concise test name  * perf(stage): First check if a stage has been visited  before checking for parent stages. stage.getRequisiteStageRefIds is a more expensive call because the underlying implementation creates a copy of a List. Therefore  start with the cheaper operation first hoping to short-circuit and avoid the more expensive check  * perf(stage): Filter out non-synthetic stages  The javadocs state that the syntheticStageOwner property is null for non-synthetic stages. Use this information to filter out non-synthetic stages before attempting a potentially expensive operation to check for synthetic parents of previousStages  * perf(stage): Precompute requisiteStageRefIds  StageExecutionImpl.getRequisiteStageRefIds() returns a copy of a Set. This is a costly operation that has the potential to get repeated for every unvisited stage. To avoid this  compute the value before entering a loop  * perf(stage): Only use withAuth when needed  withAuth is only necessary when starting a stage. Since withAuth is very computationally expensive for complex pipelines  only call it when it is absolutely necessary.  * perf(stage): Remove duplicate call to withStage  StartStageHandler already makes a call to message.withStage at the beginning of the handle() method. Therefore  this call within the catch block is unnecessary  * chore(import): Clean up unused imports  ---------  Co-authored-by: Daniel Zheng <d.zheng@salesforce.com>
spinnaker,spinnaker,11707d495925eee4665e1e8faf09c0c61ca73a04,https://github.com/spinnaker/spinnaker/commit/11707d495925eee4665e1e8faf09c0c61ca73a04,perf(cache): Optimise heap usage in KubernetesCachingAgent (#6255)  This pull request addresses a performance issue discovered in the Clouddriver  specifically within the `KubernetesServiceHandler.addAllReplicaSetLabels` method.  With allocation profiling I found that 40% of allocations happen for converting `replicaSet` object to `KubernetesManifest` class. The thing is that this conversion is redundant because `replicaSet` is already `KubernetesManifest`.  The objects aren't changed later  so it should be safe to stop parsing and making a copy. After this optimization we reduced memory usage by half and also got rid of regular GC pauses.
apache,cassandra,f327b63db09a907206749a3c88aba38a4554e548,https://github.com/apache/cassandra/commit/f327b63db09a907206749a3c88aba38a4554e548,Introduce SSTableSimpleScanner for compaction  This removes the usage of index files during compaction and simplifies and improves the performance of compaction.  patch by Branimir Lambov; reviewed by Sylvain Lebresne for CASSANDRA-20092
apache,cassandra,513509ee2cd46da4604a73c7873a9bb634ddad0b,https://github.com/apache/cassandra/commit/513509ee2cd46da4604a73c7873a9bb634ddad0b,Accord's ConfigService lock is held over large areas which cause deadlocks and performance issues  patch by David Capwell; reviewed by Benedict Elliott Smith for CASSANDRA-20065
apache,cassandra,91def312841123088562f5c3e59bb1b003772be3,https://github.com/apache/cassandra/commit/91def312841123088562f5c3e59bb1b003772be3,do not schedule additional durability attempts while some in flight; plus minor performance improvements
apache,cassandra,8404d2fd5cbda4ba5210522ee612ac2fd169278e,https://github.com/apache/cassandra/commit/8404d2fd5cbda4ba5210522ee612ac2fd169278e,Improve performance when getting writePlacementAllSettled from ClusterMetadata in large cluster with many range movements  Patch by marcuse; reviewed by Sam Tunnicliffe for CASSANDRA-20526
apache,cassandra,f1bec5d0c5dccce4128b3ff9bc087cdf0577f2b0,https://github.com/apache/cassandra/commit/f1bec5d0c5dccce4128b3ff9bc087cdf0577f2b0,Improve performance of DistributedSchema.validate for large schemas  patch by Abe Ratnofsky; reviewed by Caleb Rackliffe  Benedict Elliott Smith  Matt Byrd  Sam Tunnicliffe for CASSANDRA-20360
apache,cassandra,cc86fc8865e2794a83078db5629ba5096498045e,https://github.com/apache/cassandra/commit/cc86fc8865e2794a83078db5629ba5096498045e,Merge branch 'cassandra-5.0' into trunk  * cassandra-5.0: Add selected SAI index state and query performance metrics to nodetool tablestats
apache,cassandra,79630fb42ae9e42691f516590100c279f372ac89,https://github.com/apache/cassandra/commit/79630fb42ae9e42691f516590100c279f372ac89,Add selected SAI index state and query performance metrics to nodetool tablestats  patch by Sunil Ramchandra Pawar; reviewed by Caleb Rackliffe  Matt Byrd  and Maxim Muzafarov for CASSANDRA-20026
apache,cassandra,3078aea1cfc70092a185bab8ac5dc8a35627330f,https://github.com/apache/cassandra/commit/3078aea1cfc70092a185bab8ac5dc8a35627330f,Introduce SSTableSimpleScanner for compaction  This removes the usage of index files during compaction and simplifies and improves the performance of compaction.  patch by Branimir Lambov; reviewed by Sylvain Lebresne for CASSANDRA-20092
spring-projects,spring-security,8917cdb404979ec23694ffe081ac891aa2e82bbd,https://github.com/spring-projects/spring-security/commit/8917cdb404979ec23694ffe081ac891aa2e82bbd,Improve Performance of IPv4 Check  Closes gh-15324
flyway,flyway,2dbbab3e138519b8c658d80add3bd93427561403,https://github.com/flyway/flyway/commit/2dbbab3e138519b8c658d80add3bd93427561403,Bump version to flyway-10.21.0  Please see the GH release for the release notes  Update H2 2.3.224 to 2.3.232  Improve repair performance  Upgrade snowflake-jdbc 3.14.3 to 3.20.0 to fix CVE-2024-43382
java-native-access,jna,c9e389567554df573e4466b44d78e937812325c8,https://github.com/java-native-access/jna/commit/c9e389567554df573e4466b44d78e937812325c8,Merge pull request #1626 from brettwooldridge/master  Improve ``Structure`` (subclass) construction performance.
apache,shenyu,5bd78ff6bc303925a4e419153ca48b645ec92c4a,https://github.com/apache/shenyu/commit/5bd78ff6bc303925a4e419153ca48b645ec92c4a,perf(logging): Optimize the performance of log collection (#5931)  - Adjust the serialization timing of the request body and response body to avoid unnecessary string operations. - Perform string conversion only after confirming the need to record logs to reduce resource consumption.  Co-authored-by: 宗杰 <1491040549@qq.com>
apache,shenyu,8f3a1d07e2e29d68c7db26a44d4085bf085598cf,https://github.com/apache/shenyu/commit/8f3a1d07e2e29d68c7db26a44d4085bf085598cf,[type:optimize] Optimize BodyParamUtils with Caffeine cache (#5905)  * [type:optimize] Optimize BodyParamUtils with Caffeine cache  - Added Caffeine cache for base type checking - Improved isBaseType() performance with 5000-entry cache - Reduced class loading overhead  * [type:fix] Fix for style check.  ---------  Co-authored-by: aias00 <liuhongyu@apache.org>
apache,seatunnel,dc3c23981b3a78a3f1af69bcde57fbd5bf57d7b4,https://github.com/apache/seatunnel/commit/dc3c23981b3a78a3f1af69bcde57fbd5bf57d7b4,[Improve][Jdbc] Skip all index when auto create table to improve performance of write (#7288)
flowable,flowable-engine,70a529076d0adc7af555c5e383d3f66a9cd7a57f,https://github.com/flowable/flowable-engine/commit/70a529076d0adc7af555c5e383d3f66a9cd7a57f,Add support for SQL Server NVarchar (#3909)  * Work in progress: to support differentiating between varchar/nvarchar  (which is needed to improve sql server performance)  all mappings need to get the correct type. For other databases  regular varchar is fine  hence why the typehandler is always the same for those databases  * Nvarchar mappings for TimerJob  * event subscription and cmmn engine updates  * Nvarchar mappings for Execution  * Nvarchar mappings for Batch  * Nvarchar mappings for ExternalWorkerJob  * Nvarchar mappings for ByteArray  * cmmn engine updates  * Nvarchar mappings for Job  * cmmn engine updates  * Nvarchar mappings for Property and Comment  * cmmn engine updates  * Nvarchar mappings for HistoricDetail and HistoricTaskInstance  * cmmn engine updates  * cmmn engine updates  * cmmn engine updates  * Nvarchar mappings for HistoricProcessInstance and Resource  * Nvarchar mappings for Model and ProcessDefinitionInfo  * Simplify nvarchar test  * Nvarchar mappings for Model and HistoricEntityLink  * cmmn engine updates  * Nvarchar mappings for VariableInstance  * cmmn engine updates  * Nvarchar mappings for HistoricActivityInstance  * Nvarchar mappings for DMN  * App engine nvarchar changes  * Comment out varchar to nvarchar changes  * IDM nvarchar changes  * event registry updates  * fixes  ---------  Co-authored-by: Joram Barrez <jbarrez@users.noreply.github.com>
apache,beam,83b373570fef4405482ebee4207b33d4b3212700,https://github.com/apache/beam/commit/83b373570fef4405482ebee4207b33d4b3212700,[flink-runner] Improve Datastream for batch performances (#32440)  * [Flink] Set return type of bounded sources * [Flink] Use a lazy split enumerator for bounded sources * [Flink] fix lazy enumerator package * [Flink] Default to maxParallelism = parallelism in batch * [Flink] Avoid re-serializing trigger on every element * [Flink] Avoid re-evaluating options every time a new state is stored * [Flink] Only serialize states namespace keys if necessary * [Flink] Make ToKeyedWorkItem part of the DoFnOperator * [Flink] Remove ToBinaryKV * [Flink] Refactor CombinePerKeyTranslator * [Flink] Combine before Reduce (no side-input only) * [Flink] Implement partial reduce * [Flink] dead code cleanup * [Flink] persistent PartialReduceBundleOperator operator state * [Flink] Combine before GBK * [Flink] Combine before reduce (with side input) * [Flink] Force slot sharing group in batch mode * [Flink] Disable bundling in batch mode * [Flink] Lower default max bundle size in batch mode * [Flink] Code cleanup * [Flink] fix WindowDoFnOperatorTest * [Flink] Remove 1.14 compat code * [Flink] Fix flaky test * [Flink] Use a custom key type to better distribute load * [Flink] fix GBK streaming with side input * [Flink] fix error management in lazy source * [Flink] disable operator chaining in validatesRunner * [Flink] fix lazy source enumerator behaviour on error * [Flink] set validates runner parallelism to 1 * [Flink] add org.apache.beam.sdk.transforms.ParDoTest to sickbay * [Flink] fix lazy source enumerator behaviour on error (again) * [Flink] Add org.apache.beam.sdk.transforms.ViewTest.testTriggeredLatestSingleton to sickbay
apache,beam,2fc7646c9b828a8e7d4bd05f41f0dd8e70d93c02,https://github.com/apache/beam/commit/2fc7646c9b828a8e7d4bd05f41f0dd8e70d93c02,Merge pull request #33626 Expose Coder.getEncodedElementByteSize publicly  Make Coder.getEncodedElementByteSize publicly available to allow performance improvements in higher level coders.  Actually making this method public would be a backwards incompatible change.
apache,beam,bf7e317a7aedd104099c4c1d029bb9716016f617,https://github.com/apache/beam/commit/bf7e317a7aedd104099c4c1d029bb9716016f617,[Managed BigQuery] use file loads with Avro format for better performance (#33392)  * use avro file format  * add comment  * add unit test
apache,beam,f3e6c66c0a5d3a8638fd94978adf503be5081274,https://github.com/apache/beam/commit/f3e6c66c0a5d3a8638fd94978adf503be5081274,Improve performance of BigQueryIO connector when withPropagateSuccessfulStorageApiWrites(true) is used (#31840)  * Performance improvements related to conversion of BigQuery's Storage Write API proto's to TableRows when withPropagateSuccessfulStorageApiWrites(true) is used.  * Fix spotless findings.  * Update CHANGES.md  * Update CHANGES.md - moved the entry to 2.59.0 section.
pentaho,pentaho-kettle,db5f39cc7d9f6223ef20e6da5102d5910e91af27,https://github.com/pentaho/pentaho-kettle/commit/db5f39cc7d9f6223ef20e6da5102d5910e91af27,[PDI-20322] -Disable gather performance metrics when scheduling jobs/ktrs from Spoon's schedule perspective
apache,tomcat,82367b3891b9ffd501ee7046b71ee2461303562c,https://github.com/apache/tomcat/commit/82367b3891b9ffd501ee7046b71ee2461303562c,Remove the case sensitivity check  The performance impact is minimal and getting the check right in all cases is difficult due to various edge cases
apache,tomcat,5144218a399027a59199cb5cf3aaafe9033f7ffb,https://github.com/apache/tomcat/commit/5144218a399027a59199cb5cf3aaafe9033f7ffb,Follow-up to BZ 69381. Additional location for performance improvement
apache,tomcat,de680af8843f1f61311d43f54ddee772bf81315b,https://github.com/apache/tomcat/commit/de680af8843f1f61311d43f54ddee772bf81315b,BZ 69419 - further performance improvements
apache,tomcat,3b30c3d6ef600b0820417c698236c91d5a82a5c8,https://github.com/apache/tomcat/commit/3b30c3d6ef600b0820417c698236c91d5a82a5c8,BZ 69419: getAttribute() performance for nested ApplicationHttpRequest  Avoid repeated getSpecial() calls with nested ApplicationHttpRequest https://bz.apache.org/bugzilla/show_bug.cgi?id=69419 Based on a patch by John Engebretson
apache,tomcat,5b30d52b5140a9472a19e8c214f98e27b30ad0f5,https://github.com/apache/tomcat/commit/5b30d52b5140a9472a19e8c214f98e27b30ad0f5,Fix BZ 69381 Improve method lookup performance  When the method has no arguments there is no requirement to consider casting or coercion. Shortcut the method lookup process in that case.  Based on PR #770 by John Engebretson.
apache,tomcat,9ab668b2e9b21d5259acca3221a4554a6d41f158,https://github.com/apache/tomcat/commit/9ab668b2e9b21d5259acca3221a4554a6d41f158,Partial fix for BZ 69338. Better performance for >2 operand And/Or  https://bz.apache.org/bugzilla/show_bug.cgi?id=69338
cabaletta,baritone,1e2ae34dbe452a1eebc12019ae632b2279210cad,https://github.com/cabaletta/baritone/commit/1e2ae34dbe452a1eebc12019ae632b2279210cad,crucial performance optimization
cabaletta,baritone,99f9dd1671b9b2adb216dcbbdf789c781ebe18ee,https://github.com/cabaletta/baritone/commit/99f9dd1671b9b2adb216dcbbdf789c781ebe18ee,Performance
cabaletta,baritone,b25a6305ce23d68ffb2c948418668599789bcee7,https://github.com/cabaletta/baritone/commit/b25a6305ce23d68ffb2c948418668599789bcee7,Don't bother testing reachability for far away blocks  This is a massive performance improvement for big farms.
software-mansion,react-native-svg,7b5d4daaed9fd62d1b6a07ffb3742f56c49eda20,https://github.com/software-mansion/react-native-svg/commit/7b5d4daaed9fd62d1b6a07ffb3742f56c49eda20,fix: scaling when mask is set (#2299)  # Summary  This PR resolves an issue raised in #1451. Currently  when a mask is used  we render the element as a bitmap (or platform equivalent)  but the bitmap's size does not update accordingly with transformations. With these changes  the problem is addressed as follows: * **Android**: We utilize the original canvas layers to render the mask and element with the appropriate blending mode. * **iOS**: We create an offscreen context with the size multiplied by the screen scale and apply the original UIGraphics CTM (current transformation matrix) to the offscreen context. This ensures that the same transformations are applied as on the original context.  Additionally  there is a significant performance improvement on Android as we are not creating three new Bitmaps and three new Canvases.  ## Test Plan  There are many ways for testing these changes  but the required ones are: * `TestsExample` app -> `Test1451.tsx` * `Example` app -> Mask section * `FabricExample` app -> Mask section  ## Compatibility  | OS      | Implemented | | ------- | :---------: | | Android |    ✅     | | iOS     |    ✅     |  ## Preview  <img width="337" alt="image" src="https://github.com/software-mansion/react-native-svg/assets/39670088/93dbae85-edbd-452a-84b0-9a50107b1361"> <img width="337" alt="image" src="https://github.com/software-mansion/react-native-svg/assets/39670088/07838dff-cb2d-4072-a2fc-5c16a76f6c33">
Graylog2,graylog2-server,7845ab99117250085170ffede89de0e8ae84de7f,https://github.com/Graylog2/graylog2-server/commit/7845ab99117250085170ffede89de0e8ae84de7f,Improve performance for working with indices (#21195)  * Issue #18563: Don't iterate over all IndexSets for a given indexName  instead use `getForIndex(String)` directly. Also converted some loops to streaming.  * Add changelog  * Review comments  * Review comments part two
langchain4j,langchain4j,770c77038ad00b3512e6997f043ea4c31003fe6b,https://github.com/langchain4j/langchain4j/commit/770c77038ad00b3512e6997f043ea4c31003fe6b,Rewrite the PgVectorEmbeddingStore search query to enable index (#2485)  Closes #2484  ## Change Now the query looks like this and it corresponds the pgvector documentation so it uses the index (not just seq scan) which will improve the performance for large tables.  ``` SELECT (embedding <=> '[...]') AS score  embedding_id  embedding  text FROM document_embeddings WHERE (embedding <=> '[...]') <= 0.5 ORDER BY embedding <=> '[...]' LIMIT 10; ```  And here is the execution plan: ``` Limit  (cost=411.07..412.29 rows=10 width=47) (actual time=5.074..5.183 rows=10 loops=1) ->  Index Scan using document_embeddings_ivfflat_index on document_embeddings  (cost=411.07..2014.16 rows=13180 width=47) (actual time=5.073..5.180 rows=10 loops=1) "        Order By: (embedding <=> '[...]'::vector)" "        Filter: ((embedding <=> '[...]'::vector) <= '0.5'::double precision)" Planning Time: 0.095 ms Execution Time: 5.225 ms ```  Note: I haven't added new tests since I didn't add or remove any functionality. This PR is just a search performance fix.  ## General checklist  - [x] There are no breaking changes - [ ] I have added unit and/or integration tests for my change - [ ] The tests cover both positive and negative cases - [x] I have manually run all the unit and integration tests in the module I have added/changed  and they are all green - [x] I have manually run all the unit and integration tests in the [core](https://github.com/langchain4j/langchain4j/tree/main/langchain4j-core) and [main](https://github.com/langchain4j/langchain4j/tree/main/langchain4j) modules  and they are all green
langchain4j,langchain4j,f3312919fdc8514a633293b16cfe7788c9cf4e8d,https://github.com/langchain4j/langchain4j/commit/f3312919fdc8514a633293b16cfe7788c9cf4e8d,Optimization of UUID Generation Using HexFormat (#2024)  ## Issue  This pull request introduces the HexFormat class (Java 17+) to optimize the hexadecimal encoding of SHA-256 hashes used for UUID generation in the generateUUIDFrom method. By replacing the manual byte-to-hex conversion with HexFormat  this update aims to improve code readability and potentially enhance performance.   I created JMH test to ensure that HexFormat is more efficient than format of String.  ```java package dev.langchain4j;     import org.openjdk.jmh.annotations.*; import org.openjdk.jmh.annotations.Benchmark; import org.openjdk.jmh.runner.Runner; import org.openjdk.jmh.runner.RunnerException; import org.openjdk.jmh.runner.options.CommandLineOptionException; import org.openjdk.jmh.runner.options.CommandLineOptions; import org.openjdk.jmh.runner.options.Options; import org.openjdk.jmh.runner.options.OptionsBuilder;  import java.security.MessageDigest; import java.security.NoSuchAlgorithmException; import java.util.HexFormat; import java.util.UUID; import java.util.concurrent.TimeUnit;  import static java.nio.charset.StandardCharsets.UTF_8;  @State(Scope.Benchmark) @BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.NANOSECONDS) @Fork(1) @Warmup(iterations = 5) @Measurement(iterations = 2) public class BenchmarkHex {  private static String largeString;  @Setup(Level.Trial) public void setup() { largeString = generateLargeString(100000); }  @Benchmark public void benchmarkOldBehavior(){ boolean optimized = false; generateUUIDFrom(largeString  optimized); }   @Benchmark public void benchmarkOptimizedBehavior(){ boolean optimized = true; generateUUIDFrom(largeString  optimized); }  private String generateUUIDFrom(String input  boolean optimized) { byte[] hashBytes = getSha256Instance().digest(input.getBytes(UTF_8)); if (optimized) { return UUID.nameUUIDFromBytes(HexFormat.of().formatHex(hashBytes).getBytes(UTF_8)).toString(); } StringBuilder sb = new StringBuilder(); for (byte b : hashBytes) { sb.append("%02x".formatted(b)); } return UUID.nameUUIDFromBytes(sb.toString().getBytes(UTF_8)).toString(); }  private MessageDigest getSha256Instance() { try { return MessageDigest.getInstance("SHA-256"); } catch (NoSuchAlgorithmException e) { throw new IllegalArgumentException(e); } }  private String generateLargeString(int length) { StringBuilder sb = new StringBuilder(); for (int i = 0; i < length; i++) { sb.append(UUID.randomUUID()); } return sb.toString(); }  public static void main(String[] args) throws RunnerException  CommandLineOptionException { Options opt = new OptionsBuilder() .parent(new CommandLineOptions(args)) .timeUnit(TimeUnit.NANOSECONDS) .include(BenchmarkHex.class.getSimpleName()) .build();  new Runner(opt).run(); }    }  ```  Results: ```java Benchmark                                Mode  Cnt  Score   Error  Units BenchmarkHex.benchmarkOldBehavior        avgt       1.811          ms/op BenchmarkHex.benchmarkOptimizedBehavior  avgt       1.625          ms/op ```   It's not a really big enhancement  but it's can be here for improving the code    ## Change <!-- Please describe the changes you made. -->   ## General checklist - [x] There are no breaking changes - [ ] I have added unit and integration tests for my change - [x] I have manually run all the unit and integration tests in the module I have added/changed  and they are all green - [x] I have manually run all the unit and integration tests in the [core](https://github.com/langchain4j/langchain4j/tree/main/langchain4j-core) and [main](https://github.com/langchain4j/langchain4j/tree/main/langchain4j) modules  and they are all green <!-- Before adding documentation and example(s) (below)  please wait until the PR is reviewed and approved. --> - [ ] I have added/updated the [documentation](https://github.com/langchain4j/langchain4j/tree/main/docs/docs) - [ ] I have added an example in the [examples repo](https://github.com/langchain4j/langchain4j-examples) (only for "big" features) - [ ] I have added/updated [Spring Boot starter(s)](https://github.com/langchain4j/langchain4j-spring) (if applicable)
langchain4j,langchain4j,2c8ff58c028c6d1927d20ec7ad392864114eef6a,https://github.com/langchain4j/langchain4j/commit/2c8ff58c028c6d1927d20ec7ad392864114eef6a,Milvus: improve insert performance
google,closure-compiler,7a2a710232dd39a2b7a4822a428df363dfaca6f4,https://github.com/google/closure-compiler/commit/7a2a710232dd39a2b7a4822a428df363dfaca6f4,Use a RangeSet to improve the performance of checking whether a given line and column is within a closure-unaware source range.  PiperOrigin-RevId: 712966714
google,closure-compiler,8eda0eac26a389e45cd71b4a2f7d508f8be53b2d,https://github.com/google/closure-compiler/commit/8eda0eac26a389e45cd71b4a2f7d508f8be53b2d,Remove unnecessary String.split calls in StripCode  This is a slight performance improvement.  PiperOrigin-RevId: 704434673
google,closure-compiler,84866704f0857985019755099993f78e134a6a89,https://github.com/google/closure-compiler/commit/84866704f0857985019755099993f78e134a6a89,Avoiding rerunning `ProcessDefines` in `J2clUtilGetDefineRewriterPass`  The way `J2clUtilGetDefineRewriterPass` currently works is pretty messy as the rerun of `ProcessDefines` does not work as one would expect. When we actually rerun it all of the `goog.define` calls have already been removed  so the define name is actually based on the name of the node annotated with `@define`.  Instead of doing this we can have `ProcessDefines` stash the name of all the defines that it found. That's the only information that `J2clUtilGetDefineRewriterPass` needs to operate  and no new defines are going to be added along the way.  This does not change the requirement that the J2CL requires the define name to match a `goog.provide` of the same name. Resolving that problem is quite a bit more involved. For now this change just improves the improves the performance and reduces the brittleness of the existing solution.  PiperOrigin-RevId: 700492292
google,closure-compiler,0238a87b57031fe900362a2fd28151245448f522,https://github.com/google/closure-compiler/commit/0238a87b57031fe900362a2fd28151245448f522,Merge transpiler passes that do small  independent  local rewritings into a PeepholeTranspilationsPass.java doing a single traversal.  Most notably  for build performance  this means that the single peephole pass traversal performs all the small rewritings instead of a doing a separate AST traversal for each rewriting.  PiperOrigin-RevId: 657759081
apache,iceberg,c1d4182b3fb9fcb162a0233b8aa304a65ffa56f1,https://github.com/apache/iceberg/commit/c1d4182b3fb9fcb162a0233b8aa304a65ffa56f1,Parquet: Fix performance regression in reader init (#12305)
apache,iceberg,ea5da1789f3eb80fc3196bcdd787a95ac0c493ba,https://github.com/apache/iceberg/commit/ea5da1789f3eb80fc3196bcdd787a95ac0c493ba,AWS: Switch to base2 entropy in ObjectStoreLocationProvider for optimized S3 performance (#11112)  Co-authored-by: Drew Schleit <aschleit@amazon.com>
gocd,gocd,02d4acbb976d58586ce58a9cc64dc444714643c7,https://github.com/gocd/gocd/commit/02d4acbb976d58586ce58a9cc64dc444714643c7,Remove need for Guava within agents  - Still need to review performance and synchronize the PluginRoleUsersStore
wiremock,wiremock,f76cb7f2f9e342df72ff780b35b6116f1bde6745,https://github.com/wiremock/wiremock/commit/f76cb7f2f9e342df72ff780b35b6116f1bde6745,perf: Reinstate improvements on EqualToXmlPattern
wiremock,wiremock,9ccb1c89a3f15db0c396af3999939fe464f87e48,https://github.com/wiremock/wiremock/commit/9ccb1c89a3f15db0c396af3999939fe464f87e48,improve performance of EqualToXmlPattern. (#2944)  removes use of DiffBuilder.ignoreComments which performs an xslt transform on the provided XML documents which is very memory intensive. now comments are ignored at read/parse time.  as part of this change  the Xml utility class was also refactored to (hopefully) make it more reusable.
wiremock,wiremock,5610f61083dcc55664a1110569aeac5852689262,https://github.com/wiremock/wiremock/commit/5610f61083dcc55664a1110569aeac5852689262,Fix HttpClientBuilder сode and add options for Connection Management to increase the proxying performance (#2744)  * Fix code in HttpClientBuilder that override ConnectionManager parameters and add two command lines options (maxHttpClientConnections and disableConnectionReuse) to flexibly manage HttpClient  ---------  Co-authored-by: Tom Akehurst <tom@wiremock.org>
AutoMQ,automq,e83322518f816eb1372ed92112a2d1060a0717e6,https://github.com/AutoMQ/automq/commit/e83322518f816eb1372ed92112a2d1060a0717e6,feat: Add --catchup-topic-prefix for performance testing and solve the issue  (#2514)  feat: Add --catchup-topic-prefix for performance testing
AutoMQ,automq,c4bf688e36b50e418d1d79b2da856cd37d245fad,https://github.com/AutoMQ/automq/commit/c4bf688e36b50e418d1d79b2da856cd37d245fad,perf: Migrate to Monitored Thread Pools by Replacing Native ExecutorService (#2485)  * docs: Update IDE launch configuration in contribution guide  - Adjust the memory settings in VM Options and change `-Xmx1` to `-Xmx1G`  Signed-off-by: ZetoHkr <147681181+shawngao-org@users.noreply.github.com>  * refactor(thread): optimize thread pool closing logic  - Introduce the ThreadUtils class to uniformly handle thread pool shutdown  #2358  * perf(s3): Optimize TrafficRateLimiter test code import  * perf(s3): Optimize TrafficRateLimiter test code import  * perf: Migrate to Monitored Thread Pools by Replacing Native ExecutorService.  ---------  Signed-off-by: ZetoHkr <147681181+shawngao-org@users.noreply.github.com>
AutoMQ,automq,24cb55f02081b051850014c8a81ea7a86e090f1c,https://github.com/AutoMQ/automq/commit/24cb55f02081b051850014c8a81ea7a86e090f1c,perf: Standardize ExecutorService Shutdown with Safety Wrapper. (#2430)  * docs: Update IDE launch configuration in contribution guide  - Adjust the memory settings in VM Options and change `-Xmx1` to `-Xmx1G`  Signed-off-by: ZetoHkr <147681181+shawngao-org@users.noreply.github.com>  * refactor(thread): optimize thread pool closing logic  - Introduce the ThreadUtils class to uniformly handle thread pool shutdown  #2358  * perf(s3): Optimize TrafficRateLimiter test code import  * perf(s3): Optimize TrafficRateLimiter test code import  ---------  Signed-off-by: ZetoHkr <147681181+shawngao-org@users.noreply.github.com>
AutoMQ,automq,c41faffb2a2e658c2e065059434e42d217a711f3,https://github.com/AutoMQ/automq/commit/c41faffb2a2e658c2e065059434e42d217a711f3,perf: Replace Unsafe ScheduledExecutorService Initialization with Exception-Handling Wrapper (#2414)  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357
AutoMQ,automq,14249c5786821aba071afaaf5454347b6a48daf3,https://github.com/AutoMQ/automq/commit/14249c5786821aba071afaaf5454347b6a48daf3,perf: add log level checks to optimize AutoMQ logging performance (#2378)  * perf: add log level checks to optimize AutoMQ logging performance  * fix: add @SuppressWarnings("NPathComplexity") to fix checkstyle issues
AutoMQ,automq,c4e2bcfa802f05ebfe2fe47c9fec7c6ff10e3cf4,https://github.com/AutoMQ/automq/commit/c4e2bcfa802f05ebfe2fe47c9fec7c6ff10e3cf4,perf(s3stream): limit write traffic to object storage (#2340)  perf(s3stream): limit write traffic to object storage (#2335)  * chore(objectstorage): log next retry delay    * feat: a `TrafficLimiter` to limit the network traffic    * feat: a `TrafficMonitor` to monitor the network traffic    * feat: record success and failed write requests    * feat: queued pending write tasks    * feat: run write tasks one by one    * feat: use a `TrafficRegulator` to control the rate of write requests    * feat: limit the inflight force upload tasks    * fix: correct retry count    * chore: fix commit object logs    * chore: log force uploads    * fix: catch exceptions    * style: fix lint    * fix: fix re-trigger run write task    * perf: increate if no traffic    * refactor: remove useless try-catch    * perf: ensure only one inflight force upload tasks    * refactor: move inner classes outside    * perf: increase rate limit slower    * chore: add a prefix in `AbstractObjectStorage#logger`    * chore: reduce useless logs    * perf: reduce the sample count on warmup    * feat: introduce `TrafficVolumeLimiter` base on IBM `AsyncSemaphore`    * feat: limit the inflight write requests    * chore: reduce useless logs    * fix: fix release size    * fix: release permits once the request failed    * perf: increase to max after 2 hours    * fix: limit the request size    * perf: adjust constants    ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,b470ba4854a2c6d94c3059b3e8e7a5374c460701,https://github.com/AutoMQ/automq/commit/b470ba4854a2c6d94c3059b3e8e7a5374c460701,feat(backpressure): add metrics (#2198)  * feat(backpressure): log it on recovery from backpressure  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: add metric fetch_limiter_waiting_task_num  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: add metric fetch_limiter_timeout_count  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: add metric fetch_limiter_time  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: add metric back_pressure_state  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: add metric broker_quota_limit  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix(backpressure): run checkers with fixed delay  Signed-off-by: Ning Yu <ningyu@automq.com>  * style: fix lint  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf: drop too large values  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: record -1 for other states  Signed-off-by: Ning Yu <ningyu@automq.com>  * test: fix tests  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,a7250cd750dbf0716959128e334cbc323134d8ea,https://github.com/AutoMQ/automq/commit/a7250cd750dbf0716959128e334cbc323134d8ea,perf: limit the inflight requests (#2100)  * docs: add todos  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(network): limit the inflight requests by size  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(ReplicaManager): limit the queue size of the `fetchExecutor`s  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(KafkaApis): limit the queue size of async request handlers  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor(network): make "queued.max.requests.size.bytes" configurable  Signed-off-by: Ning Yu <ningyu@automq.com>  * style: fix lint  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix(network): limit the min queued request size per queue  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,e8e7aabc87b7d9c294ec7dbe2402541323e48d97,https://github.com/AutoMQ/automq/commit/e8e7aabc87b7d9c294ec7dbe2402541323e48d97,refactor(s3stream/wal): use bucket4j to limit the write rate and bandwidth (#2034)  * perf(s3stream/wal): limit iops by bucket4j  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix: make `pendingBlocks` and `currentBlock` volatile  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: no need to batch the `Block`  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix: leave some buffer for other write operations  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: limit the write bandwidth in `SlidingWindowService` rather than `WALBlockDeviceChannel`  Signed-off-by: Ning Yu <ningyu@automq.com>  * chore: add option "--bandwidth" in `WriteBench`  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix: limit the max refilling rate  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix: fix the case that `softLimit=0`  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf: increase the buckets' capacity  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,619138b92b123dc97281fab35eb3e5c059b90438,https://github.com/AutoMQ/automq/commit/619138b92b123dc97281fab35eb3e5c059b90438,fix(log): use the same view to calculate trim offsets (#1973)  * fix(log): use the same view to calculate trim offsets  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix: calculate trim offsets in the lock  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf: use `Iterator` to reduce the overhead caused by passing intermediate data  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf: merge `calTrimOffset` and `calStreamsMinOffset`  Signed-off-by: Ning Yu <ningyu@automq.com>  * test: fix tests  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,09b685ef9f2a32595f122a8eb71a9677158ed1b8,https://github.com/AutoMQ/automq/commit/09b685ef9f2a32595f122a8eb71a9677158ed1b8,chore(tools/perf): increase the message sending rate during the warmup to accelerate JVM warmup (#1883)  chore(tools/perf): increase the message sending rate during the warmup to accelerate JVM warmup (#1881)  * perf: increase the message sending rate during the warmup to accelerate JVM warmup    * chore: rename "catchup-rate" to "send-rate-during-catchup"    * fix: fix compile error    ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,2d3a49669c6ddc0198462d97a01983191c6af212,https://github.com/AutoMQ/automq/commit/2d3a49669c6ddc0198462d97a01983191c6af212,perf(s3stream/wal): increase the size of the buffer pool as CPU cores (#1882)  perf: increase the size of the buffer pool as CPU cores  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,962fad347106c6111dde5e7d46fc8253e0331e8f,https://github.com/AutoMQ/automq/commit/962fad347106c6111dde5e7d46fc8253e0331e8f,perf(s3stream/wal): reuse the `ByteBuf` for record headers (#1877)  * refactor: manage the record headers' lifecycle in `Block`  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(s3stream/wal): reuse the `ByteBuf` for record headers  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf: remove the max size limit  Signed-off-by: Ning Yu <ningyu@automq.com>  * test: test `FixedSizeByteBufPool`  Signed-off-by: Ning Yu <ningyu@automq.com>  * revert: "perf: remove the max size limit"  This reverts commit ed6311210a77fd5547b45d4857cf2011fc072dc8.  * feat: use a separate `poolSize` to limit the size of the pool  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,9ee6cfe3250aaab3bc612c86c4f4741dc6a394e9,https://github.com/AutoMQ/automq/commit/9ee6cfe3250aaab3bc612c86c4f4741dc6a394e9,perf: use a new `GrowableMultiBufferSupplier` to avoid memory waste (#1800)  * perf: use a new `GrowableMultiBufferSupplier` to avoid memory waste  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: always try the first buffer in cache  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,dd6b5c1c1e1cab1a9077272252dc5e535ed106f2,https://github.com/AutoMQ/automq/commit/dd6b5c1c1e1cab1a9077272252dc5e535ed106f2,fix(core): Revert "perf: use netty buffer to pool memory (#1788)" (#1797)  Revert "perf: use netty buffer to pool memory (#1788)"  This reverts commit 5317eff6d037726a8f278b80a4ad00677eecbc67.
AutoMQ,automq,5317eff6d037726a8f278b80a4ad00677eecbc67,https://github.com/AutoMQ/automq/commit/5317eff6d037726a8f278b80a4ad00677eecbc67,perf: use netty buffer to pool memory (#1788)  * perf: use netty buffer to pool memory  Signed-off-by: Ning Yu <ningyu@automq.com>  * test: test `NettyBufferSupplier`  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: only replace the `BufferSupplier` in `KRaftControlRecordStateMachine`  Signed-off-by: Ning Yu <ningyu@automq.com>  * docs: add automq inject comments  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: replace all `BufferSupplier` used by `RecordsIterator`  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,a2a2eae47c64b2cb3a17401903deacb5cba87ea4,https://github.com/AutoMQ/automq/commit/a2a2eae47c64b2cb3a17401903deacb5cba87ea4,feat(core): optimize indexing performance by using sparse index cache (#1585)  Signed-off-by: Shichao Nie <niesc@automq.com>
AutoMQ,automq,76119686e5a467da8112046b839d42ec8f551b3c,https://github.com/AutoMQ/automq/commit/76119686e5a467da8112046b839d42ec8f551b3c,feat(s3stream): optimize get objects performance (#1512)  Signed-off-by: Shichao Nie <niesc@automq.com>
AutoMQ,automq,3835515feaf7cb5bb7de3c4d63794e79100eb62a,https://github.com/AutoMQ/automq/commit/3835515feaf7cb5bb7de3c4d63794e79100eb62a,KAFKA-16541 Fix potential leader-epoch checkpoint file corruption (#15993)  A patch for KAFKA-15046 got rid of fsync on LeaderEpochFileCache#truncateFromStart/End for performance reason  but it turned out this could cause corrupted leader-epoch checkpoint file on ungraceful OS shutdown  i.e. OS shuts down in the middle when kernel is writing dirty pages back to the device.  To address this problem  this PR makes below changes: (1) Revert LeaderEpochCheckpoint#write to always fsync (2) truncateFromStart/End now call LeaderEpochCheckpoint#write asynchronously on scheduler thread (3) UnifiedLog#maybeCreateLeaderEpochCache now loads epoch entries from checkpoint file only when current cache is absent  Reviewers: Jun Rao <junrao@gmail.com>
AutoMQ,automq,078dd9a311a5e9ddb9d0b49a1164c50dadde3a27,https://github.com/AutoMQ/automq/commit/078dd9a311a5e9ddb9d0b49a1164c50dadde3a27,KAFKA-16821; Member Subscription Spec Interface (#16068)  This patch reworks the `PartitionAssignor` interface to use interfaces instead of POJOs. It mainly introduces the `MemberSubscriptionSpec` interface that represents a member subscription and changes the `GroupSpec` interfaces to expose the subscriptions and the assignments via different methods.  The patch does not change the performance.  before: ``` Benchmark                                     (memberCount)  (partitionsToMemberRatio)  (topicCount)  Mode  Cnt  Score   Error  Units TargetAssignmentBuilderBenchmark.build                10000                         10           100  avgt    5  3.462 ± 0.687  ms/op TargetAssignmentBuilderBenchmark.build                10000                         10          1000  avgt    5  3.626 ± 0.412  ms/op JMH benchmarks done ```  after: ``` Benchmark                                     (memberCount)  (partitionsToMemberRatio)  (topicCount)  Mode  Cnt  Score   Error  Units TargetAssignmentBuilderBenchmark.build                10000                         10           100  avgt    5  3.677 ± 0.683  ms/op TargetAssignmentBuilderBenchmark.build                10000                         10          1000  avgt    5  3.991 ± 0.065  ms/op JMH benchmarks done ```  Reviewers: David Jacot <djacot@confluent.io>
AutoMQ,automq,abc3a6c14ff5b837bac612876e6384407be6f6f4,https://github.com/AutoMQ/automq/commit/abc3a6c14ff5b837bac612876e6384407be6f6f4,fix(s3stream): temporarily disable percentile metrics for performance issue (#1353)  Signed-off-by: Shichao Nie <niesc@automq.com>
AutoMQ,automq,979f8d9aa3e8840951a151ab83eb006f8e7c1314,https://github.com/AutoMQ/automq/commit/979f8d9aa3e8840951a151ab83eb006f8e7c1314,MINOR: Small refactor in TargetAssignmentBuilder (#16174)  This patch is a small refactoring which mainly aims at avoid to construct a copy of the new target assignment in the TargetAssignmentBuilder because the copy is not used by the caller. The change relies on the exiting tests and it does not really have an impact on performance (e.g. validated with TargetAssignmentBuilderBenchmark).  Reviewers: Chia-Ping Tsai <chia7712@gmail.com>
AutoMQ,automq,c8af740bd44dae92bbe68254114c0fd7f7c32345,https://github.com/AutoMQ/automq/commit/c8af740bd44dae92bbe68254114c0fd7f7c32345,Improve producer ID expiration performance (#16075)  Skip using stream when expiring the producer ID. This can improve the performance significantly when the count is high. Before  Benchmark                                        (numProducerIds)  Mode  Cnt      Score       Error  Units ProducerStateManagerBench.testDeleteExpiringIds             10000  avgt    3    101.253 ±    28.031  us/op ProducerStateManagerBench.testDeleteExpiringIds            100000  avgt    3   2297.219 ±  1690.486  us/op ProducerStateManagerBench.testDeleteExpiringIds           1000000  avgt    3  30688.865 ± 16348.768  us/op After  Benchmark                                        (numProducerIds)  Mode  Cnt     Score     Error  Units ProducerStateManagerBench.testDeleteExpiringIds             10000  avgt    3    39.122 ±   1.151  us/op ProducerStateManagerBench.testDeleteExpiringIds            100000  avgt    3   464.363 ±  98.857  us/op ProducerStateManagerBench.testDeleteExpiringIds           1000000  avgt    3  5731.169 ± 674.380  us/op Also  made a change to the JMH testing which excludes the producer ID populating from the testing.  Reviewers: Artem Livshits <alivshits@confluent.io>  Justine Olshan <jolshan@confluent.io>
quartz-scheduler,quartz,eb576ff498541c141edfa0a532ecf370f388f827,https://github.com/quartz-scheduler/quartz/commit/eb576ff498541c141edfa0a532ecf370f388f827,performance: replace  fo loop by bulk operation with .addAll() method  Signed-off-by: Carlos Garcia <bcode@protonmail.com>
quartz-scheduler,quartz,2bccc097601b550e2eb494eca8d608d909e184f8,https://github.com/quartz-scheduler/quartz/commit/2bccc097601b550e2eb494eca8d608d909e184f8,performance: use StringBuilder instead of String concatenation  Signed-off-by: Carlos Garcia <bcode@protonmail.com>
Col-E,Recaf,9f8a4a7890a28ef9558281aa10e0204303f48828,https://github.com/Col-E/Recaf/commit/9f8a4a7890a28ef9558281aa10e0204303f48828,Replace transformer collections of ClassPathNode with identity based collections over hash based collections  This massively improves performance due to the expensive nature of path hashing.  The creation of these collections is such that no duplicate paths should ever be attempted to go into the same collection. Additionally the user is not expected to do path based lookups in these maps. They are for primarily intended for iteration/inspection only.
Col-E,Recaf,570dafb8839a95c352194b8885ddac129fa2cd7b,https://github.com/Col-E/Recaf/commit/570dafb8839a95c352194b8885ddac129fa2cd7b,Remove dirs.dev dependency for performance concerns
jOOQ,jOOQ,db62f6c1076be4bf151d4118daa767cec2f3be08,https://github.com/jOOQ/jOOQ/commit/db62f6c1076be4bf151d4118daa767cec2f3be08,Merge pull request #17670 from ggsurrel/main  Performance improvement in BatchSingle class
jOOQ,jOOQ,daa947f1ce9baff14e359014e8299c6a177e51f3,https://github.com/jOOQ/jOOQ/commit/daa947f1ce9baff14e359014e8299c6a177e51f3,Performance improvement in BatchSingle.java
hazelcast,hazelcast,2d2918ff9e3c4cb3f0fd895de96d1a1127a8cfe8,https://github.com/hazelcast/hazelcast/commit/2d2918ff9e3c4cb3f0fd895de96d1a1127a8cfe8,Improve `Stream` `sum` performance by avoiding redundant boxing (micro optimization) (#4262)  When `sum`ming a `Stream`  calling `reduce` adds unnecessary object allocation overhead via boxing.  Updated to make faster and more efficient.  <details> <summary>JMH Benchmark</summary>  ```java import org.openjdk.jmh.annotations.*; import java.util.Collection; import java.util.concurrent.TimeUnit; import java.util.stream.*;  @BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.NANOSECONDS) @Warmup(iterations = 5  time = 1  timeUnit = TimeUnit.SECONDS) @Measurement(iterations = 3  time = 1  timeUnit = TimeUnit.SECONDS) @State(Scope.Benchmark) public class SumStreamBenchmark { private static final Collection<Long> values = LongStream.range(1  100000) .boxed() .collect(Collectors.toSet());  @Benchmark public long reduce() { return values.stream() .reduce(0L  Long::sum); }  @Benchmark public long mapToLong() { return values.stream() .mapToLong(Long::longValue) .sum(); } } ``` </details>  | Benchmark           | Time (ms/op) | Allocation Rate (B/op) | |---------------------|--------------|------------------------| | `reduce` (before)   | 0.719        | 2 400 000              | | `mapToLong` (after) | 0.483        | 233                    |  GitOrigin-RevId: 7d76911bdcd53cf0c483ab766f11a5834062a432
hazelcast,hazelcast,8ca36349f45bb169562d6797bdcb57cfcf26616a,https://github.com/hazelcast/hazelcast/commit/8ca36349f45bb169562d6797bdcb57cfcf26616a,Improve Metrics performance by replacing `AtomicLongFieldUpdater#lazySet` with `VarHandle#setOpaque` (#4074)  In https://github.com/hazelcast/hazelcast/pull/24620 we updated `SwCounter` to use `VarHandle#setOpaque` instead of `AtomicLongFieldUpdater#lazySet`.  We can apply this same change to other metrics  too.  This (micro-optimisation) shows a ~75% improvement in [benchmarks](https://github.com/hazelcast/internal-benchmarks/pull/56): | Method                              	| ns/op 	| |-------------------------------------	|-------	| | `AtomicIntegerFieldUpdater#lazySet` 	| 0.61 	| | `VarHandle#setOpaque`               	| 0.35	|  GitOrigin-RevId: d7538028b203110a3b98d102321b870332cf7b3e
hazelcast,hazelcast,576bed00fbae560622d3446a87ff9477fc3f6d62,https://github.com/hazelcast/hazelcast/commit/576bed00fbae560622d3446a87ff9477fc3f6d62,Correct ByteArrayObjectDataOutput capacity for large inputs [HZG-346] (#4099)  There is a demonstrable large performance hit (100x) when an input length of more than `Integer.MAX_VALUE / 2` is provided. This is due to resizing being done in increments of `len` passed to `ensureAvailable(int len)` when doubling the current buffer length with a bit shift would overflow and become a negative value.  This means that the buffer is potentially continuously extended by small increments  instead of a single large increment (which is the intention with a doubling strategy).  This PR resolves the issue by catching the overflow case and setting the buffer's length to a `MAX_ARRAY_SIZE` value  which is approximately the largest supported array size possible.  This is the simplest solution to the problem encountered  and fits with the current doubling strategy within the limits of the JVM. We could overhaul our buffering mechanism entirely and use a segmented approach  trading some additional overheads for a more robust solution that supports larger inputs - this is definitely a more complex solution  and is likely overkill for Hazelcast's use-case. I discussed these options with @gbarnett-hz and he agreed that the simple approach is sufficient for now.  This PR also includes a test case that increases the existing `ByteArrayObjectDataOutput` test coverage.  Fixes https://hazelcast.atlassian.net/browse/HZG-346 Closes https://github.com/hazelcast/hazelcast/issues/26422  GitOrigin-RevId: aaf54ed54a7f6c9238858e854938bdfd1d66a01a
hazelcast,hazelcast,8b9a0fc4055e30d1ed58c9a49b23311b811c99c7,https://github.com/hazelcast/hazelcast/commit/8b9a0fc4055e30d1ed58c9a49b23311b811c99c7,Use dedicated thread pool for vector collection search [AI-214] (#4084)  Introduces new `hz:vector-query` executor with default size equal to number of physical CPUs. This implements the decision from ADR-00046. This change will decrease vector search performance in default configuration but will avoid very high CPU utilisation. For achieving best performance adjusting of `hz:vector-query` pool size is recommended.  Small generalisation of `HealthMonitor` was needed to provide vector query executor statistics in Enterprise version only if vector collection license is present.  `SamplingNodeExtension` did not forward `getLicense()` invocation to wrapped `NodeExtension`. Apparently this has not yet caused any problems but could cause problems in the future  so it was fixed.  Fixes https://hazelcast.atlassian.net/browse/AI-214  Breaking changes: * different thread pool will be used for vector searches: `hz:vector-query` instead of `hz:query`. If `hz:query` executor was customised/tuned for vector collection the settings will have to be migrated to `hz:vector-query`.  GitOrigin-RevId: e34987e321fdfbd356a03b1a63a60cc0b2ade8b9
hazelcast,hazelcast,9b1d5f63e7c0c65cb1881dd0fd69cb0e968fed40,https://github.com/hazelcast/hazelcast/commit/9b1d5f63e7c0c65cb1881dd0fd69cb0e968fed40,Add detailed vector search metrics [AI-296] (#3877)  Adds 2 useful metrics for understanding vector search performance: - `searchIndexVisitedNodes` - number of visited nodes in graph which is equal to number of distance calculations  this in turn is one of the most time-consuming operations during search - `searchIndexQueryCount` - number of searches on vector index. Can be useful to understand retries due to concurrent modifications. Without retires and PartitionPredicate filters `searchIndexQueryCount = searchCount * partitionCount` (non-empty partitions only)  anything more than that indicates that there were some retries.  Fixes https://hazelcast.atlassian.net/browse/AI-296  GitOrigin-RevId: 61c987790399f20539aa610ca8d13f6700defb5a
hazelcast,hazelcast,51c6f26c53e6604152148a1d2c100f3db7be7b5a,https://github.com/hazelcast/hazelcast/commit/51c6f26c53e6604152148a1d2c100f3db7be7b5a,Add support for blocking backup operations [AI-192] (#3846)  This PR introduces support for blocking and optionally offloaded backup operations.  ### Blocking backup operations  Up until now the only available mechanism for long-running backup operations was offloading  optionally using Step Engine implemented for IMap. Blocking backup operations introduced in this PR use some of Step Engine support code (in particular delayed sending of backup ACK) and use `OperationParker` to implementing waiting. `OperationParker` was extended to be aware of backup operations. Before its logic was well suited only for primary operations.  Blocking backup operation have some unique traits: - backup operation should not be cancelled when original member or client disconnects  unlike for primary operations - they can be parked both on source and destination on migration. Primary operations can ever be parked only on migration source - they execute on the owner and owner is never a destination of migration. - some migrations are backup->backup (changing replica index). In some such cases parked operations should be kept on the migration source (but not destination) as partition data is not updated by migration on source. This uses similar checks to the ones determining if the partition data should be retained or deleted after migration commit and is implemented by `BlockingBackupOperation.shouldKeepAfterMigration` - the operations should be properly cleared during migration to avoid executing stray operations when the partition/replica is migrated again to the same node - the operations should be cleared also after replica sync on destination - if the operation is rejected from execution (eg. by `metWithPreconditions` or other exception) it should mark replica as dirty to trigger anti-entropy task (note that it is not triggered immediately). This is similar to offloaded backup operations. This is the last resort if the backup operation cannot be executed properly and in order. This should happen rarely and is quite costly as it triggers replication of entire fragment partition (usually partition of single data structure). - backup operations must preserve order or alternatively be commutative (rare case). This affects requirements on `shouldWait` implementation - it is not allowed to return `true` for given operation once it returned `false` - because of the ordering requirements all non-commutative blocking backup operations for given data structure partition must block on the same condition and `WaitNotifyKey`  Additionally blocking backups can also be offloaded  but offloaded backups need to take care for ACK sending and error handling on their own if not using Step Engine.  With blocking backup operations `isClusterSafe`/`isMemberSafe` checks are not fully reliable. They will return true even if there are some parked blocking backups. Such backup operations can be lost in case of owner crash. This is hard to observe and can happen only for invocations that did not get their backup ACK in time.  Note: proper discarding of backup operations during migration and promotion and ensuring that the replicas are not silently out-of-sync is the most tricky and hard to test part of this change.  ### Impact on existing workloads  Currently no data structure uses blocking backups  so there will be no direct impact.  However some impact on migration and replication performance is possible in cases with large number of blocking operations. After the change the list of parked operations will have to be traversed in more cases than before  not only after migration on destination (as originally) but also on source and after replica sync on backup. `WaitSet` is not partitioned based on partitionId or replicaIndex or namespace.  Part of https://hazelcast.atlassian.net/browse/AI-192  GitOrigin-RevId: 4cbf961ac0d7ddb88448c8a00961ba62bc925dd6
hazelcast,hazelcast,694e349f4251fe3643d818e4235b211219c35338,https://github.com/hazelcast/hazelcast/commit/694e349f4251fe3643d818e4235b211219c35338,Log basic statistics after backup promotions [AI-293] (#3848)  The statistics can be useful for understanding timing and performance of backup promotions. We already log statistics for migrations. These statistics will be logged only in case of member leaving the cluster ungracefully so should have a minimal impact on normal operation.  Fixes: https://hazelcast.atlassian.net/browse/AI-293 GitOrigin-RevId: 070c61c850b1f71504c2c685f7c9544023afa834
hazelcast,hazelcast,82925e2c6da852347e8a12a1e15475e812a0524a,https://github.com/hazelcast/hazelcast/commit/82925e2c6da852347e8a12a1e15475e812a0524a,Unwrap OperationFactoryWrapper in InvocationProfiler and OperationProfiler output [HZG-258] (#3655)  Currently when enabled the `InvocationProfiler` and `OperationProfiler` `DiagnosticPlugins` record latency distributions of invocations and operations. Unfortunately some operations end up wrapped in the `OperationFactoryWrapper` class. This makes it hard to precisely understand what the cluster is doing and to diagnose performance issues.  This change will incur a small increase in heap usage to store the finer-grained `LatencyDistribution` objects when the profiler is enabled. Analysis shows each `LatencyDistribution` only retains a few hundred bytes so overall impact on node memory usage will be negligible.  Fixes https://hazelcast.atlassian.net/browse/HZG-258  Checklist: - [x] Labels (`Team:`  `Type:`  `Source:`  `Module:`) and Milestone set - [x] Add `Add to Release Notes` label if changes should be mentioned in release notes or `Not Release Notes content` if changes are not relevant for release notes - [x] Architecture Design Record (ADR) reviewed and signed-off if this PR represents a significant architectural change - [x] Request reviewers if possible - [ ] Send backports/forwardports if fix needs to be applied to past/future releases GitOrigin-RevId: a0b64502534c9f287b67db90f5e2fc0ae2e234e0
hazelcast,hazelcast,324dd593fba6c5c903d3b166fe7280c5dbaab418,https://github.com/hazelcast/hazelcast/commit/324dd593fba6c5c903d3b166fe7280c5dbaab418,Avoid passing explicit default value to new `Atomic` constructors to improve performance (#3535)  `new AtomicInteger(0)` is equivalent to `new AtomicInteger()`  but has a greater overhead due to an additional (redundant) `volatile` write.  Using the implicit default in these scenarios offers a performance improvement [benchmarked at ~27%](https://github.com/hazelcast/internal-benchmarks/pull/52) - however  our usage of these constructors is typically limited to fields inside objects which means the throughput improvement to the application is unlikely to be significant.  Updated [`Atomic` classes](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/package-frame.html) constructors' to use an implicit default definition  _where possible_  using: ``` find . -name "*.java" -exec sed -i '' \ -e 's/new AtomicBoolean(false)/new AtomicBoolean()/g' \ -e 's/new AtomicInteger(0)/new AtomicInteger()/g' \ -e 's/new AtomicIntegerArray(null)/new AtomicIntegerArray()/g' \ -e 's/new AtomicLong(0)/new AtomicLong()/g' \ -e 's/new AtomicLong(0l)/new AtomicLong()/g' \ -e 's/new AtomicLong(0L)/new AtomicLong()/g' \ -e 's/new AtomicLongArray(null)/new AtomicLongArray()/g' \ -e 's/new AtomicReference(null)/new AtomicReference()/g' \ -e 's/new AtomicReferenceArray(null)/new AtomicReferenceArray()/g' \ {} + ```  This was [originally raised](https://github.com/hazelcast/hazelcast/pull/22373) by @dreis2211  so full credit to Christoph for flagging this. Due to a delay in reviewing  the PR could not be merged: - too many conflicts - pre-`hazelcast-mono`  I've re-implemented this in a new PR to update `hazelcast-mono`'s sources as well (rather than just `hazelcast`).  Fixes: https://github.com/hazelcast/hazelcast/pull/22373  Co-authored-by: Christoph Dreis <6304496+dreis2211@users.noreply.github.com> GitOrigin-RevId: 4c9b504342ca2e6fad6f23cfe34730802153a8bf
hazelcast,hazelcast,0a75bf11300a8d325ee6ef4dbb9838149476cd44,https://github.com/hazelcast/hazelcast/commit/0a75bf11300a8d325ee6ef4dbb9838149476cd44,Add control of search list size in similarity search [AI-140] (#3229)  The most important parameter that affects search results quality is search list size. Before this change it was always equal to `topK` or `partitionLimit` (if supplied)  which produced poorer quality results with smaller values.  Explicit control of that parameter is introduced via `efSearch` hint. This should improve in particular searches with small topK (e.g. 10)  where in some benchmarks it was hard to get >95% precision at all.  `partitionLimit` hint remains unchanged and can be used for further performance tuning. During initial tests it was observed that with good `efSearch` value  `partitionLimit` can be smaller than `topK` without compromising the quality.  Benchmarking was performed using `dbpedia-openai-1M-1536` dataset which has default topK = 10 on Apple M1 laptop  JDK 21 and Panama enabled. Single-member cluster with `-Xmx10g` and 16 partitions was used. Index parameters were max_degree = 16 and 32 (this is after upgrade to JVector 3  equivalent to 8 and 16 respectively in previous benchmarks)  ef_construction = 128 and disabled deduplication.  ![image](https://github.com/user-attachments/assets/462d2502-5865-4301-a063-6a4ad61c77a1)  ![image](https://github.com/user-attachments/assets/040f2121-730a-4be1-9b2b-f5093a3651df)  Labels on the line are `efSearch` value.  The results show that precision depends predominantly on `efSearch`. `partitionLimit` has very small impact when `efSearch` is sufficiently large. For reference  results with topK=100 are provided (previously recommended way to improve quality in low-topK searches). Quality is similar for the same effective search list size  but searches with inflated `topK` are slower.  Increasing `efSearch` past 100 brings very little improvement in quality but significant increase in run time (in this case).  Important to note about these benchmarks:  1. the results are quite noisy  likely due to running on a laptop  but they prove the main point about precision. Proper benchmarks will be done later. 2. search with `partitionLimit=10` is faster (!) than with `partitionLimit=5`. This however may be an artifact of benchmark methodology  lack of warmup  JIT compilation. This will be investigated further. 3. from initial tests is seems that `efSearch` value around 50-100 should be a good choice  but the sample is very small (only a few tests) so this may not turn out to be true. 4. `partitionLimit=3` results in slightly but visibly decreased quality  even though 3*16=48 entries in total are fetched from partitions for topK=10 query. It seems that the probability of omitting good candidate (because it is 4th best in partition) is non-negligible in this case. GitOrigin-RevId: a9e809c711d825f81a3f6b548ec0ee95c68a37d6
hazelcast,internal-benchmarks,00e5df77344b093bca9e361d6f7cc8cbe7d4454c,https://github.com/hazelcast/internal-benchmarks/commit/00e5df77344b093bca9e361d6f7cc8cbe7d4454c,Improve float array serialization performance in client protocol (#2334)  Use `ByteBuffer.asFloatBuffer()` to serialize float array more efficiently in client protocol codecs in similar way to https://github.com/hazelcast/hazelcast-mono/pull/2099.  Benchmarks:
hazelcast,hazelcast,c7d012c34077b8a36b7f124943e02dd3b6b6e7e4,https://github.com/hazelcast/hazelcast/commit/c7d012c34077b8a36b7f124943e02dd3b6b6e7e4,Improve float array serialization performance (#2099)  Use `ByteBuffer. asFloatBuffer()` to serialize float array more efficiently. Inspired by https://github.com/hazelcast/hazelcast-mono/pull/1402/commits/5cbb2dbd038aa4fc52b48640fa24c09df220a82c  Important caveats: - only `ByteArrayObjectDataInput` and `Output` have optimised implementation. Unsafe and Fixed variants and `ObjectDataOutputStream` have unchanged implementation - some performance tests should be performed  in particular if this implementation behaves has really better performance for different array sizes (big and small - vectors are expected to be a on the order of 1k floats) and if it works equally well for different combinations of native and requested endianness - this implementation is used in many places  eg. compact serialization  so the impact may be broad  but float array is probably not often used - part of the performance improvement might be caused by invoking `ensureAvailable` once instead of for each float. Also  with the default buffer size being 4kB  for common vector sizes at least 1 resize is expected (with factor 2) - other arrays (int  long  etc.) were not analyzed. They might benefit from similar operations as well  ## Performance tests  Synthetic benchmark shows that 1 reallocation during serialization is ~2x more costly than entire serialization itself  so the buffer size was chosen to avoid reallocation. This may be interesting tuning parameter eg. to avoid reallcation when serializing vector collection search request.  Benchmark settings - running on Apple M1:  ``` # JMH version: 1.37 # VM version: JDK 21.0.2  OpenJDK 64-Bit Server VM  21.0.2+13-58 # Blackhole mode: compiler (auto-detected  use -Djmh.blackhole.autoDetect=false to disable) # Warmup: 3 iterations  10 s each # Measurement: 3 iterations  10 s each # Timeout: 10 min per iteration # Threads: 1 thread  will synchronize iterations # Benchmark mode: Average time  time/op ```  ### Serialization  #### Before  ``` Benchmark                                       (arraySize)  (nativeOrder)  Mode  Cnt     Score   Error  Units FloatArraySerializationBenchmark.serialization            1           true  avgt    6     3.677 ± 0.011  ns/op FloatArraySerializationBenchmark.serialization            2           true  avgt    6     4.977 ± 0.012  ns/op FloatArraySerializationBenchmark.serialization            8           true  avgt    6    12.558 ± 0.026  ns/op FloatArraySerializationBenchmark.serialization           16           true  avgt    6    22.719 ± 0.024  ns/op FloatArraySerializationBenchmark.serialization          100           true  avgt    6   128.823 ± 0.476  ns/op FloatArraySerializationBenchmark.serialization         1000           true  avgt    6  1268.861 ± 2.724  ns/op  FloatArraySerializationBenchmark.serialization            1          false  avgt    6     3.702 ± 0.010  ns/op FloatArraySerializationBenchmark.serialization            2          false  avgt    6     4.989 ± 0.007  ns/op FloatArraySerializationBenchmark.serialization            8          false  avgt    6    12.656 ± 0.102  ns/op FloatArraySerializationBenchmark.serialization           16          false  avgt    6    22.709 ± 0.031  ns/op FloatArraySerializationBenchmark.serialization          100          false  avgt    6   128.768 ± 0.337  ns/op FloatArraySerializationBenchmark.serialization         1000          false  avgt    6  1280.513 ± 3.518  ns/op ```  #### After  ``` Benchmark                                       (arraySize)  (nativeOrder)  Mode  Cnt    Score   Error  Units FloatArraySerializationBenchmark.serialization            1           true  avgt    6    4.739 ± 0.011  ns/op FloatArraySerializationBenchmark.serialization            2           true  avgt    6    5.673 ± 0.009  ns/op FloatArraySerializationBenchmark.serialization            8           true  avgt    6    5.896 ± 0.012  ns/op FloatArraySerializationBenchmark.serialization           16           true  avgt    6    6.107 ± 0.023  ns/op FloatArraySerializationBenchmark.serialization          100           true  avgt    6   11.906 ± 0.441  ns/op FloatArraySerializationBenchmark.serialization         1000           true  avgt    6   81.133 ± 8.741  ns/op  FloatArraySerializationBenchmark.serialization            1          false  avgt    6    4.774 ± 0.011  ns/op FloatArraySerializationBenchmark.serialization            2          false  avgt    6   14.922 ± 0.161  ns/op FloatArraySerializationBenchmark.serialization            8          false  avgt    6   15.273 ± 0.106  ns/op FloatArraySerializationBenchmark.serialization           16          false  avgt    6   16.174 ± 0.051  ns/op FloatArraySerializationBenchmark.serialization          100          false  avgt    6   25.014 ± 3.435  ns/op FloatArraySerializationBenchmark.serialization         1000          false  avgt    6  121.114 ± 1.065  ns/op ```  ### Deserialization  #### Before  ``` Benchmark                                           (arraySize)  (nativeOrder)  Mode  Cnt    Score    Error  Units FloatArrayDeserializationBenchmark.deserialization            1           true  avgt    6   12.173 ±  0.238  ns/op FloatArrayDeserializationBenchmark.deserialization            2           true  avgt    6   18.845 ±  0.323  ns/op FloatArrayDeserializationBenchmark.deserialization            8           true  avgt    6   20.399 ±  0.979  ns/op FloatArrayDeserializationBenchmark.deserialization           16           true  avgt    6   26.874 ±  7.764  ns/op FloatArrayDeserializationBenchmark.deserialization          100           true  avgt    6   73.798 ±  0.220  ns/op FloatArrayDeserializationBenchmark.deserialization         1000           true  avgt    6  731.697 ± 10.041  ns/op  FloatArrayDeserializationBenchmark.deserialization            1          false  avgt    6   11.945 ±  0.306  ns/op FloatArrayDeserializationBenchmark.deserialization            2          false  avgt    6   16.146 ±  0.560  ns/op FloatArrayDeserializationBenchmark.deserialization            8          false  avgt    6   20.968 ±  0.384  ns/op FloatArrayDeserializationBenchmark.deserialization           16          false  avgt    6   24.412 ±  0.733  ns/op FloatArrayDeserializationBenchmark.deserialization          100          false  avgt    6   73.652 ±  0.106  ns/op FloatArrayDeserializationBenchmark.deserialization         1000          false  avgt    6  729.102 ±  5.406  ns/op ```  #### After  ``` Benchmark                                           (arraySize)  (nativeOrder)  Mode  Cnt    Score     Error  Units FloatArrayDeserializationBenchmark.deserialization            1           true  avgt    6   14.846 ±   0.505  ns/op FloatArrayDeserializationBenchmark.deserialization            2           true  avgt    6   23.800 ±   0.100  ns/op FloatArrayDeserializationBenchmark.deserialization            8           true  avgt    6   16.897 ±   1.230  ns/op FloatArrayDeserializationBenchmark.deserialization           16           true  avgt    6   24.230 ±   0.556  ns/op FloatArrayDeserializationBenchmark.deserialization          100           true  avgt    6   34.877 ±   0.348  ns/op FloatArrayDeserializationBenchmark.deserialization         1000           true  avgt    6  216.405 ±   7.042  ns/op  FloatArrayDeserializationBenchmark.deserialization            1          false  avgt    6   16.093 ±   0.110  ns/op FloatArrayDeserializationBenchmark.deserialization            2          false  avgt    6   29.848 ±   0.072  ns/op FloatArrayDeserializationBenchmark.deserialization            8          false  avgt    6   30.159 ±   0.351  ns/op FloatArrayDeserializationBenchmark.deserialization           16          false  avgt    6   30.666 ±   0.305  ns/op FloatArrayDeserializationBenchmark.deserialization          100          false  avgt    6   41.933 ±   0.262  ns/op FloatArrayDeserializationBenchmark.deserialization         1000          false  avgt    6  314.484 ± 112.088  ns/op ```  ## Observations  - Byte-by-byte serialisation: - is nicely inlined (no method calls) but is not optimised to int/long operations nor vectorized (at least in JVM21 on aarch64 - Apple M1). This is highly inefficient when endianness matches native. This explains also lack of difference between matching and mismatched endianness (before change). - Additionally  there are extra range checks for each byte (see `array[(int) offset]` in `ByteArrayAccessStrategy`). They are optimised using uncommon trap  but int range comparison remains. - ByteBuffer implementation: - depends on endianness  calls to appropriate native memory copy routines  depending if endianness matches or not. - There is some initial cost (maybe alignment  extra checks or pre and post loops in native implementation or possibly some Java implementation used for small sizes) which makes it more costly for small array sizes. Around 8 (endianness matches) or 16 (mismatch) `ByteBuffer` starts outperforming original implementation - Significant stdev is observed for optimised 1000-element array in native order. This effect is repeatable. The theory is that there may be some threshold or alignment which is different on each iteration (CPU cache line alignment  saturated memory bandwidth?): when it is good  the results are ~20% better than otherwise. Another theory is that some random memory activity during the test slows is down. This does not invalidate the general conclusion that new implementation performs better. - Serialisation benchmark does not create any garbage  while deserialisation benchmark creates a new float array for each invocation. That might explain less spectacular gains (2-4x) than for serialization (5-10x).  ## Conclusions  The general trend is that for expected vector sizes for vector search use of the new serialization implementation is ~5-10x faster in serialization and ~2-4x faster in deserialization (not that this is synthetic benchmars  VectorValues and VectorDocument ser/de handles more that just float array). We might consider adding a threshold of minimum size (8-16) below which iterative implementation is used  but that can be done later if small float arrays are found significant in practice and the difference is not very big.  ## Benchmark code  https://github.com/hazelcast/internal-benchmarks/pull/49  GitOrigin-RevId: 71db0212e49bd411ccba00cb92496bded576f902
micronaut-projects,micronaut-core,6cbbbd8924145fb1d99d42a7f09f2c7682e18d58,https://github.com/micronaut-projects/micronaut-core/commit/6cbbbd8924145fb1d99d42a7f09f2c7682e18d58,ThreadLocal bean improvements (#11685)  Improve performance and lifecycle management of ThreadLocal beans.  - Add a benchmark - Cache AbstractInitializableBeanDefinition.getScope. This is the majority of the performance improvement - Implement ThreadLocalCustomScope without the AbstractConcurrentCustomScope superclass. This leads to a minor performance improvement (no more locks) - Add optional advanced lifecycle handling. Can be turned on with the ThreadLocal.lifecycle flag. There are two cleanup paths: When a thread terminates  its beans are destroyed through a Cleaner on GC. Separately  when the app context terminates  all those cleaner tasks are invoked immediately.  There's still some performance left on the table  see the benchmark  but it's much better than before.  Fixes #11293
micronaut-projects,micronaut-core,5fdb596ebe12882117aa8d06816cd6f2667e0345,https://github.com/micronaut-projects/micronaut-core/commit/5fdb596ebe12882117aa8d06816cd6f2667e0345,Improve event loop locality of client requests (#11300)  If a request is made on an event loop that is part of the same event loop group that the client is configured to use  prefer making the connection on that same event loop. This keeps all the relevant processing on the same thread and may improve performance.If a request is made on an event loop that is part of the same event loop group that the client is configured to use  prefer making the connection on that same event loop. This keeps all the relevant processing on the same thread and may improve performance.
micronaut-projects,micronaut-core,7fae86545edaaa0ce514cbd457a9422d5be45cda,https://github.com/micronaut-projects/micronaut-core/commit/7fae86545edaaa0ce514cbd457a9422d5be45cda,Uri template rewrite (#10921)  This is an attempt to get rid of the old UriTemplate  UriMatchTemplate  and UriTypeMatchTemplate (not used in Core). The implementation of matching/expanding is very confusing and hard to understand  with all the things being changed in the inherited constructor methods  etc. It does not allow adding any changes that we might need.  The new implementation is split into a parser  an expander  and an alternative URI template matcher. The matcher now matches segments of the path  allowing to match without always using the regexp; this should improve the performance of matching basic /hello or /hello/{world}. In the future  we can implement routing  which will combine similar segments into one  reducing the complexity for N to something better  considering most routes share the same prefix.  The new matcher passes previous tests and is now used to match the routes. However  there are still some cases where the old template is used: resolving conflicts and the URI builder. Next  I will investigate what needs to be aligned with the JAX-RS implementation.  Unfortunately  the classes are public and cannot be removed or changed  so we have a new implementation until v5.
vespa-engine,vespa,8c9d3d9a1d9301682ca0bbac822471f98edc0cd7,https://github.com/vespa-engine/vespa/commit/8c9d3d9a1d9301682ca0bbac822471f98edc0cd7,Merge pull request #33947 from vespa-engine/bratseth/use-performance-optimized-costs  Prioritize flavors by performance optimized cost
vespa-engine,vespa,5083587e953b6f7ba6f6db6ba6656f4a72c01fb2,https://github.com/vespa-engine/vespa/commit/5083587e953b6f7ba6f6db6ba6656f4a72c01fb2,Prioritize flavors by performance optimized cost
vespa-engine,vespa,9e3f277a568397c03cbc35d14806e337a690b9b1,https://github.com/vespa-engine/vespa/commit/9e3f277a568397c03cbc35d14806e337a690b9b1,Only overcommit on bare metal  Prerequisite to tracking performance differences on non-bare metal.
vespa-engine,vespa,b85ad806c49628eb4f65764106c78c803c6aa057,https://github.com/vespa-engine/vespa/commit/b85ad806c49628eb4f65764106c78c803c6aa057,Support weakand configuration for a rank profile.  Makes it possible to configure the following for improved performance: - stopword-limit: Specifies what the weakand considers to be a stopword and dropped during evaluation. - adjust-target: Used to adjust the initial score threshold of the weakand heap.
vespa-engine,vespa,d79c181312f5d44ed573d0a7816dc1ee29368e4e,https://github.com/vespa-engine/vespa/commit/d79c181312f5d44ed573d0a7816dc1ee29368e4e,fix: performance improvements by removing calls to positionAddOffset and removing redundant parsing
vespa-engine,vespa,3a0c22d8744d2070cbb11a7d10609072606677fc,https://github.com/vespa-engine/vespa/commit/3a0c22d8744d2070cbb11a7d10609072606677fc,feat: simple function completion and minor performance improvements
graphql-java,graphql-java,0ad655c9ff9f37385447900c4289080fc5108a54,https://github.com/graphql-java/graphql-java/commit/0ad655c9ff9f37385447900c4289080fc5108a54,Merge pull request #3930 from graphql-java/remove-optional-streams-style-code  Removing some of the Optional.map() and .stream() for performance reasons
graphql-java,graphql-java,6fcb38196f6e372eaba3041d6eb3419c2da54a5c,https://github.com/graphql-java/graphql-java/commit/6fcb38196f6e372eaba3041d6eb3419c2da54a5c,Merge pull request #3932 from graphql-java/fpkit-no-longer-uses-streams  FpKit now longer uses streams for performance reasons
graphql-java,graphql-java,24b955a4d926a13b47f746c465ae6ae8fdb4ccf8,https://github.com/graphql-java/graphql-java/commit/24b955a4d926a13b47f746c465ae6ae8fdb4ccf8,Merge pull request #3948 from graphql-java/reduce-performance-forks  reduce forks to 2 to make perf tests faster
graphql-java,graphql-java,ade6100ff573e35e8f3e55ba3570be624ad24e5d,https://github.com/graphql-java/graphql-java/commit/ade6100ff573e35e8f3e55ba3570be624ad24e5d,This is a performance improvement for property data fetchers to not create `DataFetcherFactoryEnvironment` objects for simple property fetchers
graphql-java,graphql-java,c81cee8e93f6a4646c9defe51f080c366b57ddbd,https://github.com/graphql-java/graphql-java/commit/c81cee8e93f6a4646c9defe51f080c366b57ddbd,FpKit now longer uses streams for performance reasons - tweak - removed unused code
graphql-java,graphql-java,36fc91c482a22b3119bddad60f74b827d1521ffe,https://github.com/graphql-java/graphql-java/commit/36fc91c482a22b3119bddad60f74b827d1521ffe,FpKit now longer uses streams for performance reasons - tweak
graphql-java,graphql-java,26bb00178f5e146a4820cde488db4924112d732e,https://github.com/graphql-java/graphql-java/commit/26bb00178f5e146a4820cde488db4924112d732e,Revert "Revert "FpKit now longer uses streams for performance reasons""  This reverts commit 262ff2f5a5fcec6df6f39970577af6f803aaab0d.
graphql-java,graphql-java,262ff2f5a5fcec6df6f39970577af6f803aaab0d,https://github.com/graphql-java/graphql-java/commit/262ff2f5a5fcec6df6f39970577af6f803aaab0d,Revert "FpKit now longer uses streams for performance reasons"  This reverts commit 5e7ce823c9f33ab3491d0f5fad129b96359135c8.
graphql-java,graphql-java,5e7ce823c9f33ab3491d0f5fad129b96359135c8,https://github.com/graphql-java/graphql-java/commit/5e7ce823c9f33ab3491d0f5fad129b96359135c8,FpKit now longer uses streams for performance reasons
graphql-java,graphql-java,66d527c23b9e367ec6f98a72c5ab120bb93cbccb,https://github.com/graphql-java/graphql-java/commit/66d527c23b9e367ec6f98a72c5ab120bb93cbccb,Removing some fo the Optional.map() and .stream() for performance reasons.
graphql-java,graphql-java,f1730ccc4d666deb62169cc6841e4f56e91d7c51,https://github.com/graphql-java/graphql-java/commit/f1730ccc4d666deb62169cc6841e4f56e91d7c51,Large in memory query benchmark - moving class to performance
graphql-java,graphql-java,1f4d18e51bd46c5bf00ed0fc5c417e5867b349d8,https://github.com/graphql-java/graphql-java/commit/1f4d18e51bd46c5bf00ed0fc5c417e5867b349d8,some performance naming cleanup  docs
graphql-java,graphql-java,c2d1a3b7a277ffb797f13873463e4c5566e6e28c,https://github.com/graphql-java/graphql-java/commit/c2d1a3b7a277ffb797f13873463e4c5566e6e28c,add ENF performance tests
graphql-java,graphql-java,ec8bafb0ef8931466b9ec52d59aa1c05bffb2c96,https://github.com/graphql-java/graphql-java/commit/ec8bafb0ef8931466b9ec52d59aa1c05bffb2c96,fix sorting improve performance for calc lower bounds costs
hibernate,hibernate-orm,6877c201b7b9a96e538ccb360deb4d1f4816f3cd,https://github.com/hibernate/hibernate-orm/commit/6877c201b7b9a96e538ccb360deb4d1f4816f3cd,HHH-19208 Adapt javadoc of QuerySettings.QUERY_PLAN_CACHE_ENABLED  More info see: https://discourse.hibernate.org/t/after-upgrade-to-hibernate-6-slower-performance-when-executing-query-the-first-time-due-to-antlr/10614/8
hibernate,hibernate-orm,4c77552746efffb640c37f88edd3614e8c388dbf,https://github.com/hibernate/hibernate-orm/commit/4c77552746efffb640c37f88edd3614e8c388dbf,do more caching of commonly-used services for performance and convenience  expose them via BootstrapContext and SessionFactoryImplementor
hibernate,hibernate-orm,05b8d0dbe92fc8e70d32d55a41f7af82f9882130,https://github.com/hibernate/hibernate-orm/commit/05b8d0dbe92fc8e70d32d55a41f7af82f9882130,minor change  for possibly better performance
hibernate,hibernate-orm,aa6e46eb79d0b4346118ccff6b1eda1827d5236a,https://github.com/hibernate/hibernate-orm/commit/aa6e46eb79d0b4346118ccff6b1eda1827d5236a,get rid of use of tracev()  - at most one of these is worse without it - there was even a performance bug due to missing isTraceEnabled()
hibernate,hibernate-orm,9828ad7b3371eb248c85dd78332e8c03f3fbd0c1,https://github.com/hibernate/hibernate-orm/commit/9828ad7b3371eb248c85dd78332e8c03f3fbd0c1,document performance implications of id batching i.e. BatchSize  Signed-off-by: Gavin King <gavin@hibernate.org>
hibernate,hibernate-orm,94b444b4d8e0ac7ec5817642ca711f626ee327f3,https://github.com/hibernate/hibernate-orm/commit/94b444b4d8e0ac7ec5817642ca711f626ee327f3,HHH-18506 Improve flush performance by reducing itable stubs
apache,flink-cdc,250ab43e185ee39b23c4fb6376b84ea9e6741f58,https://github.com/apache/flink-cdc/commit/250ab43e185ee39b23c4fb6376b84ea9e6741f58,[FLINK-37741][cdc-runtime] Fix transform operator performance degradation  This closes  #4007
apache,flink-cdc,77218abd68999efd8cf9ad46f7e830caa024fd7c,https://github.com/apache/flink-cdc/commit/77218abd68999efd8cf9ad46f7e830caa024fd7c,[FLINK-37539][pipeline-connector/paimon] Replace stream with parallelStream to optimize the performance  This closes  #3966
apache,flink-cdc,602abde36ffae5a7395b922e7ac50db6214a3df3,https://github.com/apache/flink-cdc/commit/602abde36ffae5a7395b922e7ac50db6214a3df3,[FLINK-37278][cdc-runtime] Optimize regular schema evolution topology's performance  This closes  #3912.
apache,flink-cdc,fc71888d7a9a84b73f1f6a16f7c755e2b1b40c02,https://github.com/apache/flink-cdc/commit/fc71888d7a9a84b73f1f6a16f7c755e2b1b40c02,[hotfix][cdc-common] Remove duplicated code to improve performance  This closes #3840.  Co-authored-by: zhangchaoming.zcm <zhangchaoming.zcm@antgroup.com>
apache,flink-cdc,2a5828c0ac445461c12a6d387a7b8f12f4dca158,https://github.com/apache/flink-cdc/commit/2a5828c0ac445461c12a6d387a7b8f12f4dca158,[FLINK-35291][runtime] Improve the ROW data deserialization performance of DebeziumEventDeserializationScheme (#3289)  Co-authored-by: liuzeshan <liuzeshan@bytedance.com>
apache,incubator-kie-drools,23ff9dc9ffc3c0bd7a8e91fd65af5ffc2d0d3d5f,https://github.com/apache/incubator-kie-drools/commit/23ff9dc9ffc3c0bd7a8e91fd65af5ffc2d0d3d5f,[incubator-kie-drools-6007] Executable model doesn't report an error when duplicated (#6013)  * removing kie-ci from dependency  because it causes a test failure in KieBaseIncludeTest  * Use canonicalKieModule.getKiePackages() rather than getKieBase()  * null check for kiePackage  * move populateIncludedRuleNameMap out of packages loop  * removed unused FileManager  * performance improvement. Use getModelForKBase instead of getKiePackages  * Fit into build phases  * clean up
robolectric,robolectric,0d813b7ebaff90b82b96869173c3e445295025da,https://github.com/robolectric/robolectric/commit/0d813b7ebaff90b82b96869173c3e445295025da,Set the default Choreographer frame delay in Robolectric to 15 ms (60 fps)  Previously  the default Choreographer frame delay was 1ms  which approximated 1000fps. This was overkill and caused an excessive number of animation frames to be run  which caused performance issues when running tests that contained long animations. Reduce the default frame delay to 15ms  which approximates 60ps.  This only applies to paused looper mode. Also  this value is still configurable by a system property  since some tests depend on the 1ms value.  Note if your test fails as a result of this CL  just add:  jvm_flags = ["-Drobolectric.defaultFrameDelayMs=1"]  to your android_local_test target(s) that are failing.  PiperOrigin-RevId: 727008546
robolectric,robolectric,cae7375dbbdcc3d3b57d742b656641867e6d5206,https://github.com/robolectric/robolectric/commit/cae7375dbbdcc3d3b57d742b656641867e6d5206,Make the Choreographer frame delay configurable with a system property  The default 1ms frame delay is overkill  it causes an unnecessarily large number of animation frames. Add an option to configure the default frame delay. This is part of a larger effort to increaser the frame delay to something that approximates 60fps. The benefit of this is that animations will require much fewer frames  which will result in better performance.  PiperOrigin-RevId: 725688137
robolectric,robolectric,da246cf14092ebc191765dadfdec1a993a6dbd2b,https://github.com/robolectric/robolectric/commit/da246cf14092ebc191765dadfdec1a993a6dbd2b,Add consistent performance measurement for binary vs native resources.  Add three different performance measurements for the suspected resources hotspots: - load apk assets - get resource value - apply style PiperOrigin-RevId: 682027218
robolectric,robolectric,65abe75a74c365abdb616731d5621d16754a23e6,https://github.com/robolectric/robolectric/commit/65abe75a74c365abdb616731d5621d16754a23e6,Add perf stat for Choreographer.doFrame for S+  There are some active performance investigations related to Choreographer and animations. Previously we were only keeping track of doFrame for R and below. Add metric collections for S and above.  PiperOrigin-RevId: 665899605
vavr-io,vavr,86d1bf3f27fdff24cf78c3baf2db457c70b334d3,https://github.com/vavr-io/vavr/commit/86d1bf3f27fdff24cf78c3baf2db457c70b334d3,Faster LinkedHashMap tail() (#2725)  Change: LinkedHashMap tail() is now constant-time  from being O(size) time and space.  Motivation: We have a use case which looks somewhat like the following.  1. It's effectively a Queue use case (fifo). 2. Iteration order needs to be stable. 3. Elements in the queue frequently need to be verified as present in the queue. 4. Occasionally  removals must occur  which are usually from the head of the queue although occasionally randomly.  We started with a Queue and eventually found performance issues from too much linear scanning. This was not obviously going to be a bottleneck since our queues are usually of very small size  but ended up so. The next type we considered was a LinkedHashSet  using the `head` and `tail` methods to implement a dequeue-like operation and the set operations being sufficient for the other operations. Unfortunately  it seems that `tail` at present copies the entire queue despite not seemingly needing to.  We moved to our own hand-crafted HashSet+Queue structure  but this PR seems like it'd generally be beneficial for users of Vavr!
vavr-io,vavr,2e2373d9f3f9cb6997fc399b51237d20e15aad2b,https://github.com/vavr-io/vavr/commit/2e2373d9f3f9cb6997fc399b51237d20e15aad2b,Performance improvement for List::unfold  List::unfoldLeft (#2689)
apache,camel,65123ab6ccd5b62c69edb81aff52bb2b3c54245f,https://github.com/apache/camel/commit/65123ab6ccd5b62c69edb81aff52bb2b3c54245f,CAMEL-21845: camel-sql - Improve performance of batch inserts (#17390)  * CAMEL-21845: camel-sql - Improve performance of batch inserts
apache,camel,5e3074215a60a97b511abca9d6b195f30db3edd1,https://github.com/apache/camel/commit/5e3074215a60a97b511abca9d6b195f30db3edd1,CAMEL-21663: fix NPE hurting sjms/sjms2 performance
apache,camel,f617e42d6dc67827475b5dea509de51d5b1d5ebf,https://github.com/apache/camel/commit/f617e42d6dc67827475b5dea509de51d5b1d5ebf,Iterate over the "entrySet" instead of the "keySet"  it improves performance in case there are a lot of beans and routes registered  Signed-off-by: Aurélien Pupier <apupier@redhat.com>
apache,camel,632e5ece55a0d207afcacec851f6904a0bcbb8de,https://github.com/apache/camel/commit/632e5ece55a0d207afcacec851f6904a0bcbb8de,CAMEL-20884 - Replace this call to "replaceAll()" by a call to the "replace()" method.  The underlying implementation of String::replaceAll calls the java.util.regex.Pattern.compile() method each time it is called even if the first argument is not a regular expression. This has a significant performance cost and therefore should be used with care.  Signed-off-by: Aurélien Pupier <apupier@redhat.com>
apache,hudi,129c0da70c2d68b3c95ba093182222f0c24645dc,https://github.com/apache/hudi/commit/129c0da70c2d68b3c95ba093182222f0c24645dc,[HUDI-9437] Improve Flink Clustering Performance Using Row Reader (#13344)
apache,hudi,6fd14638c36b81ef5024873f4b42ec1af31328f8,https://github.com/apache/hudi/commit/6fd14638c36b81ef5024873f4b42ec1af31328f8,[HUDI-9364] Improve FileSystemView loading performance with large partitions (#13247)
apache,hudi,7dbd6f3c77004c72f8525efbb7662d1c3797c84b,https://github.com/apache/hudi/commit/7dbd6f3c77004c72f8525efbb7662d1c3797c84b,[HUDI-9322] Optimizing performance for hoodie log files writing (#13170)  * [HUDI-9322] Optimizing performance for hoodie log files writing  * eliminate the byte[] copy for avro data block serialization * add BaseAvroPayload#getRecordBytes * refactor the #bufferRecord of HoodieAppendHandle to make it more clear  ---------  Co-authored-by: danny0405 <yuzhao.cyz@gmail.com>
apache,hudi,024ecd8729668b36fb12c97e10dd7a34a4ae5135,https://github.com/apache/hudi/commit/024ecd8729668b36fb12c97e10dd7a34a4ae5135,[HUDI-9152] Improve read/write/compaction performance by reusing avro schema (#12949)  1. Introduce JVM level caching for avro schema to reduce the cost of schema comparison.  NOTE: Use cache to cache references to the schema on key links where the schema may be created repeatedly. This ensures that only one variable instance of the same Schema will be used during a JVM lifetime  thus reducing the overhead of schema comparison on important io paths. For most of the cases  we only need to compare whether it is the same reference  there is no need to call the `Schema::equals` method.  2. Cache the frequently reused Schema on the IO code path.  ---------  Signed-off-by: TheR1sing3un <chaoyang@apache.org>
apache,hudi,257979fc3e2951054f7b8e7d9afe611f1415f34c,https://github.com/apache/hudi/commit/257979fc3e2951054f7b8e7d9afe611f1415f34c,[HUDI-8800] Introduce SingleSparkConsistentBucketClusteringExecutionStrategy to improve performance (#12537)
apache,hudi,24f0db68904b78ef10c7594b26660ddbcb0c00c7,https://github.com/apache/hudi/commit/24f0db68904b78ef10c7594b26660ddbcb0c00c7,[HUDI-8678] feat: improve consistent-bucket resizing performance by reducing unnecessary record collecting (#12451)  * feat: improve consistent-bucket resizing performance by reducing unnecessary record collecting * refactor: remove ConsistentHashingBucketInsertPartitioner  ---------  Signed-off-by: TheR1sing3un <chaoyang@apache.org>
apache,hudi,79bcb69361203adc75f7b387bfa91eaa02993bb5,https://github.com/apache/hudi/commit/79bcb69361203adc75f7b387bfa91eaa02993bb5,[HUDI-8787] Improve compaction performance by reducing unnecessary disk access (#12531)  * perf: improve compaction performance by avoid unnecessary disk visiting  1. improve compaction performance by avoid unnecessary disk visiting 2. support push down predicate to `ExternalSpillableMap`  Signed-off-by: TheR1sing3un <chaoyang@apache.org>  * Cosmetic changes  * feat: support RocksDbDiskMap::iterator(filter)  1. support RocksDbDiskMap::iterator(filter) 2. refactor RocksDBDao::iterator to return key-value pairs rather than only values  Signed-off-by: TheR1sing3un <chaoyang@apache.org>  * refactor: remove unused changes  1. remove unused changes  Signed-off-by: TheR1sing3un <chaoyang@apache.org>  ---------  Signed-off-by: TheR1sing3un <chaoyang@apache.org> Co-authored-by: danny0405 <yuzhao.cyz@gmail.com>
apache,hudi,9da3221a79465f3326ae3ac206b08d60864ddcaa,https://github.com/apache/hudi/commit/9da3221a79465f3326ae3ac206b08d60864ddcaa,[HUDI-8781] Optimize executor memory usage during executing clustering (#12515)  * perf: optimize executor memory usage during executing clustering  1. optimize executor memory usage during executing clustering  Signed-off-by: TheR1sing3un <chaoyang@apache.org>  * Cosmetic changes  ---------  Signed-off-by: TheR1sing3un <chaoyang@apache.org> Co-authored-by: danny0405 <yuzhao.cyz@gmail.com>
apache,hudi,76dbdaa65e3e2865d250f862ff23ab0039679e87,https://github.com/apache/hudi/commit/76dbdaa65e3e2865d250f862ff23ab0039679e87,[HUDI-8622] Fix performance regression of tag when written into consistent bucket index table (#12389)  * fix: fix performance regression of tag when written into consistent bucket index table  1. fix performance regression of tag when written into consistent bucket index table 2. unified the tag logic of the bucket index and lazily loaded the required mapper information  ---------  Signed-off-by: TheR1sing3un <chaoyang@apache.org> Co-authored-by: danny0405 <yuzhao.cyz@gmail.com>
apache,hudi,00709d2928d95d36146c23ff6a1ae0d1537a51a0,https://github.com/apache/hudi/commit/00709d2928d95d36146c23ff6a1ae0d1537a51a0,[HUDI-8676] Improve ValidationUtils performance by lazy appending msg (#12450)
apache,hudi,36db1317318a024f6fdd2e356a7c3f792af6a6e5,https://github.com/apache/hudi/commit/36db1317318a024f6fdd2e356a7c3f792af6a6e5,[HUDI-8573] Fix performance degradation in RowDataKeyGen (#12325)  [HUDI-8573] Fix performance degradation in RowDataKeyGen
apache,hudi,0cd28e52873e0c6ef61184047953d5a3feac895b,https://github.com/apache/hudi/commit/0cd28e52873e0c6ef61184047953d5a3feac895b,[MINOR] HoodieAvroUtils performance optimization of createFullName method (#11747)
apache,hudi,cbd2573bdeb2de97aba77f5ba9702c87f26e4ab7,https://github.com/apache/hudi/commit/cbd2573bdeb2de97aba77f5ba9702c87f26e4ab7,[MINOR] Improve performance of generateBucketKey (#11748)  Co-authored-by: Sergey Troshkov <troshkov.sergey@huawei.com>
apache,hudi,8e36fe91715d96785fab63f51b3ab6ae61f2b53c,https://github.com/apache/hudi/commit/8e36fe91715d96785fab63f51b3ab6ae61f2b53c,[HUDI-7924] Capture Latency and Failure Metrics For Hive Table recreation (#11498)  Added latency and failure metrics for recreate table on meta sync failure. Results in pushing new metrics to prometheus which helps in monitoring the performance of recreating table.  ---------  Co-authored-by: Vamsi <vamsi@Vamsis-MacBook-Pro.local> Co-authored-by: Vamsi <vamsi@onehouse.ai>
apache,iotdb,56158b46eab7788770ce4c0387d472ac8cbfe5d1,https://github.com/apache/iotdb/commit/56158b46eab7788770ce4c0387d472ac8cbfe5d1,perf: Optimize the code structure and add comments for CaseWhen
apache,iotdb,7ba971748598654a82c401e0a4aafa94afc70581,https://github.com/apache/iotdb/commit/7ba971748598654a82c401e0a4aafa94afc70581,Optimize partition cache getRegionReplicaSet interface performance by batching (#15396)  * finish  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * enhance  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix ci  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix new arrayList redundantly  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix new arrayList redundantly  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * refine code  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  ---------  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>
apache,iotdb,74a0d306f8138c14406d4fb2ccfc120b9adf5183,https://github.com/apache/iotdb/commit/74a0d306f8138c14406d4fb2ccfc120b9adf5183,perf: avoid reading tsfile on distinct aggregation on tag/attribute column
apache,iotdb,ef8dc52e89f8fa074c9beb5c814b8780aa972ef0,https://github.com/apache/iotdb/commit/ef8dc52e89f8fa074c9beb5c814b8780aa972ef0,perf: add encodeBatch interface to accelerate flushing for sequential insert (#15243)  * perf: batch encode to improve insert performance  * fix IoTDBSimpleQueryIT  * move batchEncodeInfo / times as MemTableFlushTask member
apache,iotdb,da246d7ae06d36852f3bdae4b4910b301c3567ad,https://github.com/apache/iotdb/commit/da246d7ae06d36852f3bdae4b4910b301c3567ad,perf: various types of iterator for multiple tvlists in memchunk (#15114)  * perf: batch iterator for multiple TVLists/alignedTVList  * fix bug 1  * current time  * single TVList/alignedTVList iterator  * fix: queryAlignChuckWithDeletionTest  * fix: pr review  * fix: test cases  * remove clone in sortTvListForFlush
apache,iotdb,c557e3e42fcf7687ea772b0822e083d70ab41710,https://github.com/apache/iotdb/commit/c557e3e42fcf7687ea772b0822e083d70ab41710,perf: more adjustment for memtable/tvlist  (#15035)  * fix: no need to synchronize list since sort is already an synchronized method  * fix: optimize mininum time update in TVList  * perf: special case for merge sort iterator when no handover occurs
apache,iotdb,1adc74dffd2a032306aa536be97d879369fd9386,https://github.com/apache/iotdb/commit/1adc74dffd2a032306aa536be97d879369fd9386,fix: memtable enhancement issues (#14994)  * perf: improve seq inserting  * rename MergeSortTvListIterator to MergeSortTVListIterator  * fix: concurrent indices modification during query sort and flush sort
apache,iotdb,b498285c4f413251e2fe3ebe52a5b1530a0b762f,https://github.com/apache/iotdb/commit/b498285c4f413251e2fe3ebe52a5b1530a0b762f,Pipe: Modify MaxAllowedPinnedMemTableCount to adapt to changes in the number of DRs & Modify the implementation of the poll method in PipeRealtimePriorityBlockingQueue to reduce commit queue backlog & Adjust the default thread count related to Pipe for better performance & Significantly reduce pipeMemoryAllocateRetryIntervalMs & Provide a switch for memory control of ConnectorReadFileBuffer (#14917)
apache,iotdb,85326097340d22813446140060ffd3396ce4c536,https://github.com/apache/iotdb/commit/85326097340d22813446140060ffd3396ce4c536,Memtable enhancement for query (#14591)  * Split non_aligned charge text chunk  * dev non_aligned  * dev aligned chunk split  * new type  * dev aligned binary chunk split  * Fix binary size calculatation  * fix IT  * update IoTDBDuplicateTimeIT.java  * fix pipe IT  * change method names  * add ut  * add UT  * remove useless methods  * fix UT  * fix /FileReaderManagerTest  * fix win UT  * add binary test  * Add Aligned UTs  * fix win ut  * improve coverage  * fix comments  * fix windows UT  * fix review  * fix review  * fix review  * target chunk size count non binary  * fix compile  * fix UT  * Tvlist feat new (#14616)  * null bitmap for int tvlist  * update min/max timestamp and sequential part of tvlist during insert  * mutable & immutable tvlists in writable memchunk  * copy-on-write array list  * review comments part 1  * fix unit test errors  * review comments part 2  * push down global time filter  * fix MemPageReaderTest case  * fix memory page offsets error  * synchronized sort & MergeSortTvListIterator bug  * tvlist_sort_threshold config property  * bug fix: * out of mempage bounds check * overlapped data error during query  * optimize TVListIterator & MergeSortTvListIterator  * retrofit encode when tvlist_sort_threshold is zero  * delay sort & statistic generation to query execution  * fix: skip deleted data during encode  * aligned time series part  * fix: MemAlignedChunkReader page offset  * performance issue: * change some list to array * remember row count in tvlist iterator  * fix: memory chunk reader may read more points than expected in one page  * update chunk & page statistic for aligend memchunk by column  * revert: getAlignedValueForQuery  * fix: * CopyOnWriteArrayList for AlignedTVList bitmaps * memory control of column access  * refactor: Tim/Quick/Backward TVList  * refactor: synchronized tvlist method: sort  putXXX  * refactor: change list to array in AlignedTVList iterator  * revert: remove CopyOnWriteArrayList  * refactor: clone MergeSort iterator from ReadOnlyChunk  * fix: clone working tvlist during flush if there is query on it  * fix: writable mem chunk flush conditions  * refactor: add annotation and variable/function rename  * fix: * remove delete method in BinaryTVList * filter deleted data in WritableMemChunk encode  * fix: remove getSortedTvListForQuery in SeriesRegionScan  * fix: TsFileProcessorTest unit test  * fix: IoTDBNullIdQueryIT.noMeasurementColumnsSelectTest  * fix: delete column of aligned time series  * fix: aligned timeseries encode bug  * fix: IoTDBGroupByNaturalMonthIT  * remove avgSeriesPointNumberThreshold setting  * fix: IoTDBDeleteAlignedTimeseriesIT & AlignedTVListTest  * fix: Copy globalTimeFilter due to GroupByMonthFilter  * reset tmpLength for backward sort  * * fix TVList clear * bitmap mark * sequence row count  * hot-load TVLIST_SORT_THRESHOLD  * fix: isNullValue caller  * fix unit test  * refactor: abstract prepareTvListMapForQuery method  * refactor:  clear/clone/expand indices and bitmap  * merge sort using min heap  * fix: WritableMemChunk deserialize  * feat: add index mem cost for TVList  * fix: hot-load tvlist_sort_threshold setting  * remove needless line in property template  ---------  Co-authored-by: shizy <shizy04@gmail.com>
apache,iotdb,f84a52eca0f1b5c25262b4209f501c6f95624136,https://github.com/apache/iotdb/commit/f84a52eca0f1b5c25262b4209f501c6f95624136,Pipe: Fix potential NPE from WALEntryHandler#getInsertNodeViaCacheIfPossible & Improve performance for pipe slightly (#14312)
apache,iotdb,c7e2d8e67c5efafaf7b16bddb576b998881106cc,https://github.com/apache/iotdb/commit/c7e2d8e67c5efafaf7b16bddb576b998881106cc,Optimize insertRelationalTablet performance (#14197)  * Optimize single device insert relation tablet performance  * optimize ttl check
apache,iotdb,22cf9449c40dbd5376f6b4a52f64897b41544065,https://github.com/apache/iotdb/commit/22cf9449c40dbd5376f6b4a52f64897b41544065,Pipe: Improve performance when syncing table data between clusters with param 'table-name' = '.*' or with param 'database-name' = '.*' by reducing unnecessary tsfile parse & Fix data filter with both tree and table patterns (#14150)  Co-authored-by: Steve Yurong Su <rong@apache.org>
apache,iotdb,63da4a42c941bc0e8135769365eac52808362e98,https://github.com/apache/iotdb/commit/63da4a42c941bc0e8135769365eac52808362e98,Table model data deletion (#13878)  * temp save  * temp save  * Change modification format  * update tsfile version  * fix deviceId match  * fix identitySinOperatorTest  * refactor interface hierachy  * refactor package structure  * spotless  * support table deletion  * fix test  * remove v1 mod file  * fix read empty mod  * add table deletion IT  * Fix nullability check in buildTsBlock().  * fix partialPath type in TreeDeletionEntry  * add predicate ut  * allow multiple mods in a plan node  * implment deleteDataForDropTable & fix IT  * fix log level  * ignore one test  * Added table IT (#13978)  * Update IoTDBTableIT.java  * Update DataNodeInternalRPCServiceImpl.java  * fix RelationalDeleteDataPlan serialization  * spotless  * add license  * Support more compicated deletion predicate  * fix tests  * update tsfile version & update tests  * fix modEntry merge  * parallel file deletion & fix ut  * add performance test remove redundant force optimize the procdedure of writing modfile  * all shared mod file framework  * fix TsFileResourceSerialization  * add mod file manager  * fix TsFileResource deserialization  * fix exception handle  * spotless  * fulfill deletion framework  * ignore perf test  * use buffered stream to read mods  * fix ut  * spotless  * add mod file exists marker and log condition  * fix tests  * fix test  * Drop column adaptation (#14073)  * drop column  * Update IoTDBTableIT.java  * Update IoTDBTableIT.java  * adaptation (#14077)  * fix comments  * Fixed the adaptation of delete device (#14081)  * adaptation  * Fix  * Update DeleteDevice.java  * Update AnalyzeUtils.java  * Update IoTDBDeviceIT.java  * update maxTime in TVList after deletion  * fix comment  * fix comment  ---------  Co-authored-by: Caideyipi <87789683+Caideyipi@users.noreply.github.com>
apache,iotdb,124a25e0efe6dee66dc9bd8a740feb2318f7252e,https://github.com/apache/iotdb/commit/124a25e0efe6dee66dc9bd8a740feb2318f7252e,Support update attribute on standalone version & Enable cache update / mlog writing of createOrUpdate device & Improved the performance / semantic of schema device query & Introduce limit/offset of show device
apache,iotdb,7d552c7887fc0b2663fb2b93e5115c46ba5ac561,https://github.com/apache/iotdb/commit/7d552c7887fc0b2663fb2b93e5115c46ba5ac561,Pipe / Load: Improved the performance of invalidating last cache when loading tsFile (#12833)
apache,iotdb,c0743d98c2c249958d6063dfacd86d5ca0c3fb43,https://github.com/apache/iotdb/commit/c0743d98c2c249958d6063dfacd86d5ca0c3fb43,Enhance wal compression (#12830)  * use directbuffer to optimize performance  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * optimize log  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix review  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  ---------  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>
apache,iotdb,2e1ebf46e930f49c47fa1ff54da902d97cfc65b1,https://github.com/apache/iotdb/commit/2e1ebf46e930f49c47fa1ff54da902d97cfc65b1,Pipe: Intoduce TsFileInsertionScanDataContainer to read data from tsfile sequentially to improve pattern parse performance when filter rate is high (#12781)  Co-authored-by: Steve Yurong Su <rong@apache.org>
apache,pinot,290914c4f2b907bcd52a3527d28a99787651da70,https://github.com/apache/pinot/commit/290914c4f2b907bcd52a3527d28a99787651da70,Normalize excessive whitespaces in sql to avoid regex performance issues (#15498)
apache,pinot,2c7b0ddc72b69f8bb080158d1b20de27c1d69431,https://github.com/apache/pinot/commit/2c7b0ddc72b69f8bb080158d1b20de27c1d69431,Improve performance of multi-stage queries with large IN clauses (#14615)
apache,pinot,8b2d5b6d9ee980ee8004d44edb0c0e32ad5f7db0,https://github.com/apache/pinot/commit/8b2d5b6d9ee980ee8004d44edb0c0e32ad5f7db0,Improve JSON_MATCH performance. (#15049)
apache,pinot,8774d320c4f789772f3a512fd290f6182625c140,https://github.com/apache/pinot/commit/8774d320c4f789772f3a512fd290f6182625c140,Add and use CLPMutableForwardIndexV2 by default to improve ingestion performance and efficiency (#14241)  * Add the initial implementation of CLPMutableForwardIndexV2  * Upgraded clp-ffi.version from 0.4.6 to 0.4.7  * Updated comments  * Address code review concerns  * Enable CLPMutableForwardIndexV2 to be compatible with CLPMutableForwardIndex during mutable->immutable segment conversion.  * Change Pinot to use CLPMutableForwardIndexV2 by default  * Fix integration issues related to dictionary encoding.  * Fix left-over bugs introduced by class name changes.  * Fixed small bug in CLPMutableForwardIndexV2#appendEncodedMessage related to Pinot null value handling.  * Fix style
apache,pinot,e5df02c594e52ca85ae98800052dddafe0a5e533,https://github.com/apache/pinot/commit/e5df02c594e52ca85ae98800052dddafe0a5e533,Improve performance of DataBlock serde (#13303)  This commit includes several changes in the code that builds  serializes and deserializes DataBlocks in order to improve the performace.  Changes here should not change the binary format (test included verify that). Instead we've changed how the code to reduce allocation and copies.
apache,pinot,cf52567dadbaec2673904162f88077d4c2426632,https://github.com/apache/pinot/commit/cf52567dadbaec2673904162f88077d4c2426632,Improve null handling performance for nullable single input aggregation functions (#13791)  Modify aggregation functions that were not extending NullableSingleInputAggregationFunction to do so and optimize their code to use the methods included there.
logisim-evolution,logisim-evolution,5146a8f22ac76ab02483dff5825e2abbb7aa48da,https://github.com/logisim-evolution/logisim-evolution/commit/5146a8f22ac76ab02483dff5825e2abbb7aa48da,Improve performance of regtab. HC:b310161
logisim-evolution,logisim-evolution,cd3a09e84f727a402b7d8c39d71646411c142429,https://github.com/logisim-evolution/logisim-evolution/commit/cd3a09e84f727a402b7d8c39d71646411c142429,Remove performance hostspot in ram simulation. HC:8d83b0d
logisim-evolution,logisim-evolution,4298b73ee79f2646783b69b2b9b0f35a7a17fbe6,https://github.com/logisim-evolution/logisim-evolution/commit/4298b73ee79f2646783b69b2b9b0f35a7a17fbe6,Minor performance improvement. HC:c6f6175
logisim-evolution,logisim-evolution,ac9119da9453ee1353e2dc2175399df07e271809,https://github.com/logisim-evolution/logisim-evolution/commit/ac9119da9453ee1353e2dc2175399df07e271809,Minor performance tweaks. HC:de8cd66
apache,hive,208488089df71e8a0dafd95b2812e5e4864baed9,https://github.com/apache/hive/commit/208488089df71e8a0dafd95b2812e5e4864baed9,HIVE-27102: Upgrade Calcite version from 1.25.0 to 1.33.0 (#5196)  1. Upgrade Calcite version from 1.25.0 to 1.33.0 and update code to address breaking changes and avoid compilation failures: * Add HiveJdbcImplementor#visit(JdbcTableScan) to address breaking change from CALCITE-4640. * Add HiveRuleConfig to address the removal of RelRule.Config.EMPTY by CALCITE-4839. This allows to instantiate RelRule subclasses without relying on the immutables annotation processor. * Adapt RelNode constructors and pass empty hints to address breaking change from in CALCITE-4640. * Simplify HiveDruidRules by using the config from respective rules in Calcite. * Adapt HiveRelFactories due to changes in ProjectFactory API (CALCITE-4199  CALCITE-5127). 2. Upgrade Avatica version from 1.12.0 to 1.23.0 (required by Calcite). 3. Add explicit dependency to org.immutables:value-annotations to avoid compilation failures due to missing Value annotation. Since we don't want to propagate the dependency to dependent projects we declared it at provided scope. The problem appears cause annotation pre-processing is enabled. 4. Add explicit dependency to org.locationtech.jts:jts-core and declare it at runtime scope. The jts-core is a transitive dependency used by calcite-core. In order for CBO to function properly the jar must be present in the classpath. If the jar is missing  Hiveserver2 will fail during startup with ClassNotFoundException. Normally we wouldn't need to explicitly declare transitive dependencies  but we are forced to do so cause calcite is partially shaded in hive-exec module. We are already doing the same for various other calcite deps such as janino  avatica  etc. 5. Register HiveIn as SqlKind.OTHER_FUNCTION and distinguish it from SqlKind.IN which is now reserved for usage inside RexSubQuery 6. Replace checks for SqlKind.IN in RexCall  which can no longer appear  with checks on HiveIn operator directly. 7. Implement new rules and converters to transform/expand the internal SEARCH operator to traditional conjunctions/disjunctions of standard comparisons operators  IN  and BETWEEN. 8. Copy and use Lopt classes from Calcite with the fix for (CALCITE-6737) to avoid changes in join order when queries contain self-joins. 9. Move rollup aggregation logic from HiveRelBuilder to the appropriate SqlAggFunction subclass and remove redundant overrides in HiveMaterializedViewRule. CALCITE-4342 made rollup strategy a first class citizen of the SqlAggFunction interface so each function can return its own rollup if available. It's no longer necessary to pass from the RelBuilder and basically whenever we need to obtain a rollup we have to ask directly the SqlAggFunction. As part of CALCITE-4342  the MaterializedViewRule#getRollup method was deprecated so this refactoring is necessary for the Hive MV rules to function properly. 10. Make HivePreFilteringRule a reduction rule to address fixpoint bug (StackOverflowError) triggered by changes in RelMetadataQuery#getPulledUpPredicates (CALCITE-5036). 11. Add HiveTypeFactory to avoid redundant nullability CAST to ARRAY that in some cases (lateral_view_outer.q) leads to wrong results. The redundant CAST is a side-effect from fixing CALCITE-4603. 12. Handle ROW type literals in RexNodeConverter#transformInToOrOperands After the upgrade a ROW RexCall with constants is reduced/folded to a RexLiteral of ROW types so we must adapt the IN transformation logic otherwise the ROW specific code will not kick in. Although this is merely an optimization it has impact on correctness (due to existing Hive bugs) so it's necessary to keep it working. 13. Enhance FilterSelectivityEstimator to provide better estimates for NOT 14. Use RelBuilder instead of RexBuilder in TestHivePushdownSnapshotFilterRule. Code is more readable and avoids creating invalid expressions with wrong types as it was the case before. 15. Disable RelBuilder simplifications in TestHivePointLookupOptimizerRule to avoid the introduction of SEARCH in the input plans. The rule is meant to handle disjunctions (OR) so we want to keep the input plans intact in order to properly test the rule. If the input plan is simplified to SEARCH or something else then test coverage will change that is not desired.  The plan and behavior changes that occur as part of the upgrade can be summarized into the following categories. An additional (R*) classifier  is used to denote subtle regressions that will be fixed by follow-up patches. Each change is associated with some represantive .q.out files that illustrate the overall impact on query plans.  Category I: Changes that are mostly cosmetic and do not affect the performance or behavior of the queries.  1. Changes in the output of EXPLAIN EXTENDED (OPTIMIZED SQL) * Less parentheses [auto_join_reordering_values.q.out] * Less spaces when formatting IN  ARRAY  etc. [alter_partition_coltype.q.out  autoColumnStats_5a.q.out] * VALUES clause with one entry becomes SELECT with literals [acid_nullscan.q.out] * New OPTIMIZED SQL entries since various bugs in RelToSqlConverter were fixed [pcs.q.out] * Explicit CROSS JOIN clause instead of comma separated sub-queries [tez_fixed_bucket_pruning.q.out] 2. Different formatting (toString) for DOUBLE literals [rule_exclusion_config.q.out] 3. Extra "fields" attributes in the output of EXPLAIN FORMATTED (R*) [concat_op.q.out] 4. IN clause literals sorted alphabetically [dynamic_partition_pruning_2.q.out] 5. Different aliases for expressions inside projects (e.g.  lots of EXPR$ in CBO plans) [cardinality_preserving_join_opt.q.out] 6. Result changes due to missing ORDER BY in queries [partition_wise_fileformat2.q.out]  Category II: Changes that can slightly affect the performance of queries increasing/decreasing CPU cycles.  1. Predicate order changes in filters [auto_join5.q.out] 2. Redundant IS NOT NULL predicates (R*) [explainuser_4.q.out  pointlookup5.q.out] 3. Redundant CAST around COALESCE (R*) [cbo_aggregate_reduce_functions_rule.q.out] 3. Pull-up constants from nullsafe predicates [is_distinct_from.q.out  join_nullsafe.q.out  orc_predicate_pushdown.q.out] 4. Residual filter predicates in joins (R*) [ppd_join.q.out]  Category III: Changes that can lead to significant changes in query performance.  1. Expression simplifications in project/filter/join * IN  BETWEEN  and comparison operators (< > >= <= == <>) are simplified together by exploiting the SEARCH operator [bucketsortoptimize_insert_7.q.out] * x NOT BETWEEN 10 AND 20 is transformed to x < 10 OR x > 20 [udf_between.q.out] * x <> 10 is transformed to x > 10 OR x < 10 (R*) [ppd_join3.q.out] * Simplifications with arithmetic operators [materialized_view_rewrite_8.q.out  tez_vector_dynpart_hashjoin_2.q.out] 2. Disjunctive predicate pushdown for joins (R*) [cbo_join_transitive_pred_loop_1.q.out  correlationoptimizer8.q.out] 3. LEFT OUTER to INNER joins due to CALCITE-5247 [subquery_ANY.q.out] 4. Better orders for LEFT/RIGHT/ANTI joins due to CALCITE-4208 [auto_join30.q.out  subquery_join_rewrite.q.out] 5. Unconditional OR to IN tranformations [in_typecheck_char.q.out  pointlookup2.q.out] 6. Extra partition pruning due to simplifications [select_unquote_or.q.out]  Co-authored-by: Stamatis Zampetakis <zabetak@gmail.com> Co-authored-by: Krisztian Kasa <kasakrisz2@gmail.com>
apache,hive,e8ccca82be80611e79178c1ab1acc75c5d62b2e0,https://github.com/apache/hive/commit/e8ccca82be80611e79178c1ab1acc75c5d62b2e0,HIVE-27874: Support datatype conversion on fetch threads (Kurt Deschler  reviewed by Attila Turoczy  Denys Kuzmenko)  This patch provides a mechanism to move expensive datatype conversions (i.e.  Timestamp) to the fetch threads where the work can be done in parallel. This can substantially improve performance in cases where the client thread is the bottleneck and resources are available for multiple fetch threads. Implementation is in form of ConvertedResultSet  which is agnostic to the underlying protocol result and can be dynamically substituted into the fetch path.  Closes #4902
apache,hive,7871199a49c692deef70628bc12ef022f4099bd3,https://github.com/apache/hive/commit/7871199a49c692deef70628bc12ef022f4099bd3,HIVE-27872: Support multi-stream parallel fetch in JDBC driver (Kurt Deschler  reviewed by Attila Turoczy  Denys Kuzmenko)  This patch enables JDBC to open multiple sockets to an HS2 service and performance concurrent fetch results for a single query. This can significantly speed up fetching of large results that are bottlenecked on Thrift serialization  de-serialization  and string conversion. With adequate threads  fetch performance will now only be limited by the single-threaded client-side result processing and server-size row materialization.  Added JDBC Client parameter fetchThreads to control the number of threads allocated for fetching. Setting fetchThreads=1 will pipeline the Fetch using the existing connection asynchronously. Setting fetchThreads>1 will cause an additional Thrift connection to be opened to the server for each thread. Care should be taken not to over-allocate connections to the server.  Added new HiveConf parameter hive.jdbc.fetch.threads to allow config of fetchThrads from server conf.  Closes #4902
apache,hive,6166da46337a75d0131c591e4b3aa339961514d2,https://github.com/apache/hive/commit/6166da46337a75d0131c591e4b3aa339961514d2,HIVE-27873: Fix getOperationStatus and optimize fetch (Kurt Deschler  reviewed by Attila Turoczy  Denys Kuzmenko)  This patch fixes a major performance issue fetching result from Impala. The problem was that Impala does not set isHasResultSet during getOperationStatus() calls  resulting in that RPC getting called and logging a completion message for every row fetched. Optimizes the fetch path to minimize conditional checks in the fast path.  Closes #4902
apache,hive,454415b4798dbb7accf5e5de23db3dc4801b7d68,https://github.com/apache/hive/commit/454415b4798dbb7accf5e5de23db3dc4801b7d68,HIVE-28571: Basic UNIONTYPE support in CBO (Stamatis Zampetakis reviewed by Soumyakanti Das  Attila Turoczy  Alessandro Solimando  Shohei Okumiya)  Support UNIONTYPE in the CBO path to take advantage of the powerful optimizations that are performed in the CBO layer and avoid relying on the fallback mechanism. The changes do not aim to cover new use-cases but just to ensure that existing queries that involve UNIONTYPE can exploit the CBO.  1. Model union as struct at the CBO layer 2. Remove CalciteSemanticException from TypeConverter APIs (no longer thrown) 3. Update q.out files since queries with UNIONTYPE can now use CBO  The plan changes in existing tests are trivial and expected. * join_thrift: Extra Select Operator; minor perf impact and does not alter result * union21/unionDistinct_3: COUNT(1) becomes COUNT() but both are equivalent in terms of semantics/performance * udf_isnull_isnotnull: Select expressions simplify to true; valid simplification based on the filter predicates  Close apache/hive#5497
apache,hive,673ca384fe7f4fe63b58fc2cb5eae99a3f1790cc,https://github.com/apache/hive/commit/673ca384fe7f4fe63b58fc2cb5eae99a3f1790cc,HIVE-28428: Performance regression in map-hash aggregation (Ryu Kobayashi  reviewed by Denys Kuzmenko)  Closes #5380
JSQLParser,JSqlParser,517fe72c55cdddeca2745d1456a34d42ad152596,https://github.com/JSQLParser/JSqlParser/commit/517fe72c55cdddeca2745d1456a34d42ad152596,chore: minor clean-ups around the performance optimisations  Signed-off-by: Andreas Reichel <andreas@manticore-projects.com>
JSQLParser,JSqlParser,93ca6610425fc8ac99095ba74161c69095682f06,https://github.com/JSQLParser/JSqlParser/commit/93ca6610425fc8ac99095ba74161c69095682f06,Exasol support (#2046)  * feat: Support REGEXP_LIKE as LikeExpression  Implement support of REGEXP_LIKE as LikeExpression as described here: https://docs.exasol.com/db/latest/sql_references/predicates/not_regexp_like.htm  * feat: Support sub select as part of function parameters  Allow unparenthesesed sub selects being part of multiple function parameters  * fix: Readd mistakenly removed K_TEXT_LITERAL  * revert: Revert code formatting changes  * fix: Fix choice conflicts  * refactor: Rename test methods  * refactor: Apply  on changed files  * feat: Introduce allowUnparenthesizedSubSelects feature  * refactor: Apply formatApply  * test: add test for the standard grammar  expected to fail  * test: make the performance tests more robust regarding the time-outs  * style: reformat code  Signed-off-by: Andreas Reichel <andreas@manticore-projects.com>  * fix: Revert default keyword changes  * fix: Revert default keyword changes  ---------  Signed-off-by: Andreas Reichel <andreas@manticore-projects.com> Co-authored-by: Stefan Steinhauser <stefan.steinhauser@arz.at> Co-authored-by: Andreas Reichel <andreas@manticore-projects.com>
JSQLParser,JSqlParser,82470e55aaf915778e07cb39f091fc2fb1c08192,https://github.com/JSQLParser/JSqlParser/commit/82470e55aaf915778e07cb39f091fc2fb1c08192,feature/fix: parsing inserts/updates/delete within CTEs (#2055)  * feature parsing inserts/updates/delete within CTEs  * removing System lines  * fixing codacy issues  * reducing the looping in NestedBracketsPerformanceTest to just 6  * formatting fixes via spotlessApply
redis,lettuce,d3123e8c4b93d399899d362534defd77e8b56992,https://github.com/redis/lettuce/commit/d3123e8c4b93d399899d362534defd77e8b56992,Improve the performance of obtaining write connections through double-check locks. (#3228)  * Improves the performance of obtaining write connections.  Signed-off-by: c00887447 <c00887447@huawei2.com>  * Polishing  * Formatter carelessly forgotten  ---------  Signed-off-by: c00887447 <c00887447@huawei2.com> Co-authored-by: c00887447 <c00887447@huawei2.com> Co-authored-by: Tihomir Mateev <tihomir.mateev@gmail.com>
redis,lettuce,50878081f7f735b2fb6b98c16ef5350ed7dd6cc4,https://github.com/redis/lettuce/commit/50878081f7f735b2fb6b98c16ef5350ed7dd6cc4,Optimize string concatenation in getNodeDescription() (#3262)  Replace String.join with Collectors.joining to avoid intermediate collection creation  improving performance by eliminating the unnecessary List creation step.
alibaba,jetcache,984aa2a5fafadbcbbc61eafc22c9904f1d3f6dde,https://github.com/alibaba/jetcache/commit/984aa2a5fafadbcbbc61eafc22c9904f1d3f6dde,perf: improve performance
alibaba,jetcache,26b9139c586850d8ae2c8c43e650153646178bba,https://github.com/alibaba/jetcache/commit/26b9139c586850d8ae2c8c43e650153646178bba,perf: fix encode/decode performance issue with lettuce #908
spring-projects,spring-ai,faa8778b6a1b4198998f253d066f9a7950367c3f,https://github.com/spring-projects/spring-ai/commit/faa8778b6a1b4198998f253d066f9a7950367c3f,Make ChatClient and Advisor APIs more robust - Part 2  * ChatClient observations now include the full prompt content instead of just the userText and systemText. Furthermore  they include consistent telemetry for the tools passed via the ChatClient and a first-class conversation ID when using memory advisors. Incomplete or unsafe attributes have been deprecated. * Adopted the new robust Advisor APIs for BaseAdvisor and RetrievalAugmentationAdvisor. * Improved the prompt augmentation facilities in ChatClientRequest and Prompt for performance and immutability. * Fixed integration test racing condition. * Updated the documentation for ChatClient and Observability accordingly. * Documented changes in upgrade notes. * Introduced `prompt.augmentUserMessage(String text)` to directly replace the user message content. * Added `prompt.augmentUserMessage(Function<UserMessage  UserMessage> augmenter)` for more granular updates using the `userMessage.mutate()` pattern  allowing modification of text  media  and metadata.  Relates to gh-2655  Signed-off-by: Thomas Vitale <ThomasVitale@users.noreply.github.com>
spring-projects,spring-ai,e128a39ec34831915501183f246bac55fd435c69,https://github.com/spring-projects/spring-ai/commit/e128a39ec34831915501183f246bac55fd435c69,Updates the `SearchRequest` class to be non-final and adds a `MilvusSearchRequest` subclass that includes Milvus-specific fields for native expressions and search parameters.  - Updated `SearchRequest.java` to make the class non-final. - Added `MilvusSearchRequest` with specific Milvus parameters such as `nativeExpression` and `searchParamsJson`. - Modified `doSimilaritySearch` method in `MilvusVectorStore` to handle these new fields from `MilvusSearchRequest`.  Add unit tests for MilvusVectorStore and MilvusSearchRequest  Introduce comprehensive unit tests to validate the functionality of MilvusVectorStore and MilvusSearchRequest  including scenarios for native and filter expressions. Refactor MilvusVectorStore to improve filter expression handling by introducing a helper method for converted expressions.  Add detailed documentation for MilvusSearchRequest usage  Introduced sections explaining MilvusSearchRequest's parameters  `nativeExpression`  and `searchParamsJson`  with examples for enhanced clarity. This update provides guidance on leveraging Milvus-specific features for precise filtering and optimal search performance.  Signed-off-by: waileong <wai_leong1015@hotmail.com>
spring-projects,spring-ai,b525309e8c6b1868f51fa5a79cf8c334258e464b,https://github.com/spring-projects/spring-ai/commit/b525309e8c6b1868f51fa5a79cf8c334258e464b,Introduce PII marker for logging sensitive data  Introduce a utility class `LoggingMarkers` providing an SLF4J marker for tagging log entries with Personally Identifiable Information (PII). Update `BeanOutputConverter` to use the `PII_MARKER` in error logs for invalid JSON conversions. Enhance tests to verify PII marker usage in logging.  - Set Java version dynamically and configure Kotlin compiler  - Updated Maven configurations to dynamically reference the Java version using `${java.version}`. Added Kotlin compiler settings  including `jvmTarget` alignment with Java version and enabling `javaParameters`. This ensures consistency and better compatibility across builds.  - Fix log assertion in BeanOutputConverterTest to use Java 17  - Updated the test to assert log size explicitly before accessing the first log entry. This ensures the test is more robust and avoids potential issues with accessing logs unexpectedly.  - Use placeholders in logger.error to prevent string concatenation.  - Replaced string concatenation with a placeholder in the logger.error call to improve performance and maintain consistency with logging best practices. This helps avoid unnecessary overhead when logging is disabled.  - Update logging markers and improve data classification  - Replaced `PII_MARKER` with `SENSITIVE_DATA_MARKER`. Introduced `RESTRICTED_DATA_MARKER`  `REGULATED_DATA_MARKER` and `PUBLIC_DATA_MARKER`  - Updated associated logging logic and tests to reflect these changes.  - Fix punctuation in Javadoc comments for LoggingMarkers.  - Added missing periods to improve consistency and clarity in the Javadoc comments. This change ensures proper formatting and adheres to standard writing conventions.  Signed-off-by: Konstantin Pavlov <{ID}+{username}@users.noreply.github.com>
spring-projects,spring-ai,0c5455e4cdb95df2e290bdd7cac57694956cb34f,https://github.com/spring-projects/spring-ai/commit/0c5455e4cdb95df2e290bdd7cac57694956cb34f,Improve context formatting in QuestionAnswerAdvisor  - Add line breaks and clarify context boundaries in user text advice. This improve the performance of a Llama3.x - Update corresponding test to reflect new formatting
spring-projects,spring-ai,c205c7d5cad5d6278be9c56d3babe6ed9af87ae9,https://github.com/spring-projects/spring-ai/commit/c205c7d5cad5d6278be9c56d3babe6ed9af87ae9,Fix interleaved output in JsonReader's parseJsonNode method  Replace parallelStream with stream to prevent thread-unsafe appends to the shared StringBuilder. This fixes the issue of intermingled key-value pairs in the generated Document content. Also  replace StringBuffer with StringBuilder for better performance in single-threaded context.  The change ensures correct ordering of extracted JSON keys and their values in the resulting Document  improving the reliability and readability of the parsed output.
spring-projects,spring-ai,2ecffc10c73404e7d1512c12d7fdaa30443280b5,https://github.com/spring-projects/spring-ai/commit/2ecffc10c73404e7d1512c12d7fdaa30443280b5,Refactor data filtering in RelevancyEvaluator  Replace Objects::nonNull and instanceof checks with StringUtils::hasText for more efficient and cleaner content filtering. This change simplifies the stream operation in the getContent method  improving readability and potentially performance.
spring-projects,spring-ai,202148d45bf9c226a04768f7ff9836a89e0bee9c,https://github.com/spring-projects/spring-ai/commit/202148d45bf9c226a04768f7ff9836a89e0bee9c,Prevent timeouts with configurable batching for PgVectorStore inserts  Resolves https://github.com/spring-projects/spring-ai/issues/1199  - Implement configurable maxDocumentBatchSize to prevent insert timeouts when adding large numbers of documents - Update PgVectorStore to process document inserts in controlled batches - Add maxDocumentBatchSize property to PgVectorStoreProperties - Update PgVectorStoreAutoConfiguration to use the new batching property - Add tests to verify batching behavior and performance  This change addresses the issue of PgVectorStore inserts timing out due to large document volumes. By introducing configurable batching  users can now control the insert process to avoid timeouts while maintaining performance and reducing memory overhead for large-scale document additions.
spring-projects,spring-ai,8d31a57698f86842535fdad0c97901c43e728993,https://github.com/spring-projects/spring-ai/commit/8d31a57698f86842535fdad0c97901c43e728993,Add metadataFieldsToFilter property for MongoDB store  Introduce a new property for the MongoDB vector store: spring.ai.vectorstore.mongodb.metadata-fields-to-filter  This property accepts comma-separated values specifying which metadata fields can be used for filtering when querying the vector store. It ensures that metadata indexes are created if they don't already exist.  This addition enhances query performance and flexibility by allowing users to define filterable fields in advance.  Co-authored-by: Eddú Meléndez <eddu.melendez@gmail.com>
spring-projects,spring-ai,0927bd197db7ed4f8ac7fa7d99c9cbfdc24b3c27,https://github.com/spring-projects/spring-ai/commit/0927bd197db7ed4f8ac7fa7d99c9cbfdc24b3c27,Fix MiniMax model function call implementation  Implement function call capability for MiniMax model and add unit tests based on new tool classes. Address most scenarios  but note limitations in complex English contexts with multiple function calls. Weather query example: may stop prematurely when querying multiple locations due to single-location parameter limit. This behavior stems from model performance constraints.  Streaming function calling is not passing tests  will be address seperately.  Resolves #1077  Implement function call capability for the Moonshot model. Include unit tests to verify the new functionality. This feature addresses the requirements outlined in issue #1058. fix: MiniMax function call  review
spring-projects,spring-ai,036093a42b719d2506b93bec9dfea3cc977d730a,https://github.com/spring-projects/spring-ai/commit/036093a42b719d2506b93bec9dfea3cc977d730a,Enhance vector store observability support  * Consolidate usage of “db.collection.name” attribute to track table name  collection name  index name  document name  or whatever concept a vector database uses to store data. Removed “db.index” that was use sometimes instead of “db.collection.name”. This usage is in line with the OpenTelemetry Semantic Conventions. * Configure query response content to be included as a “span event” instead of a “span attribute” if the backend system supports that  similar to how we do for the model observations. * Structure vector store observation attributes in dedicated enums  including one for the Spring AI Kinds to avoid hard-coding the same value in a lot of places. This follows the OpenTelemetry Semantic Conventions as much as possible. Also  adopt Spring usual non-null-by-default strategy as much as possible. * Align vector store conventions to the chat model ones  and follow alphabetical order for values. This is particularly useful for the convention classes  for which the Micrometer performance of exporting telemetry data improves when key values are added already sorted to the context. * Fix flaky test in Mistral AI. * Improve Qdrant integration tests.  Signed-off-by: Thomas Vitale <ThomasVitale@users.noreply.github.com>
apache,hbase,53e3aa902d234b17076a751c73a215464669e102,https://github.com/apache/hbase/commit/53e3aa902d234b17076a751c73a215464669e102,HBASE-28984 Refactor cache update logic to use new PermissionCache instances (#6485)  - This PR replaced the clear operation on existing PermissionCache with logic to create a new PermissionCache instance during updates. - This change ensures that readers have consistent and uninterrupted access to the cache data  minimizing the risk of race conditions. - By creating a new cache instance for each update instead of clearing the existing one  we improve data integrity and avoid potential issues related to concurrent access. - This approach aligns with the Copy-On-Write pattern  optimizing read performance by isolating update operations.  Co-authored-by: Wu Peiming[ 呉培銘 ] <peiming.wu@linecorp.com> Signed-off-by: Duo Zhang <zhangduo@apache.org>
apache,hbase,670deaa1d6020b1f603819504f95e016c1e06dbb,https://github.com/apache/hbase/commit/670deaa1d6020b1f603819504f95e016c1e06dbb,HBASE-29040 Fix description of "sampleRate" of PerformanceEvaluation (#6558)  Signed-off-by: Duo Zhang <zhangduo@apache.org>
apache,hbase,6ebd48e477ff0af5900b4149a3cecc43c8449bc6,https://github.com/apache/hbase/commit/6ebd48e477ff0af5900b4149a3cecc43c8449bc6,HBASE-29013 Make PerformanceEvaluation support larger data sets (#6509)  Use 8-byte long integers in the code to prevent integer overflows.  Signed-off-by: Duo Zhang <zhangduo@apache.org> Reviewed-by: Peng Lu <lupeng_nwpu@qq.com>
apache,hbase,84f4fb3b4210f9d7d50533eed5471bbab381d8c9,https://github.com/apache/hbase/commit/84f4fb3b4210f9d7d50533eed5471bbab381d8c9,HBASE-28432 Refactor tools which are under test packaging to a new module hbase-diagnostics (#6258)  - Move PerformanceEvaluation  LoadTestTool  HFilePerformanceEvaluation  ScanPerformanceEvaluation  LoadBalancerPerformanceEvaluation  and WALPerformanceEvaluation to a new module: hbase-diagnostics  Signed-off-by: Istvan Toth <stoty@apache.org> Signed-off-by: Nick Dimiduk <ndimiduk@apache.org>
apache,nifi,c1403db4f04a47b601073049a1efbddac91274e9,https://github.com/apache/nifi/commit/c1403db4f04a47b601073049a1efbddac91274e9,NIFI-14586  NIFI-14587: Expose Processors' Performance Metrics in the UI as part of the Status History; in doing so  I discovered a bug in which the GC time was not being tracked properly and fixed it. Sorted counter values lexicographically.  Signed-off-by: Pierre Villard <pierre.villard.fr@gmail.com>  This closes #9963.
apache,nifi,7c6a741e6bb8a19ff6520b542fbf84f0bd48d348,https://github.com/apache/nifi/commit/7c6a741e6bb8a19ff6520b542fbf84f0bd48d348,NIFI-14375 Refactored Kafka Components to Improve Partitioning  Performance  and Tests (#9807)  Co-authored-by: Pierre Villard <pierre.villard.fr@gmail.com> Co-authored-by: Jordan Sammut <jordan.sammut@gig.com> Signed-off-by: David Handermann <exceptionfactory@apache.org>
apache,nifi,e8e53c244cc6b7470c2486bea7d0a21b76850267,https://github.com/apache/nifi/commit/e8e53c244cc6b7470c2486bea7d0a21b76850267,NIFI-14077 Added Processing Performance Gauges to Flow Metrics (#9577)  Signed-off-by: David Handermann <exceptionfactory@apache.org>
apache,nifi,6bc0e384f40cdbb6e0b1e9155a6907a90d6fd5e1,https://github.com/apache/nifi/commit/6bc0e384f40cdbb6e0b1e9155a6907a90d6fd5e1,NIFI-13978 Improved Performance of Record Date Time Parsing  - Replaced DateTimeFormatter.parseBest() with custom TemporalQuery that checks for a ZoneId  This closes #9497.  Signed-off-by: Peter Turcsanyi <turcsanyi@apache.org>
apache,nifi,f262e741704d1ab8bd314772445f5cbac609d533,https://github.com/apache/nifi/commit/f262e741704d1ab8bd314772445f5cbac609d533,NIFI-13439 Added Performance Status to Group Status responses  - Added ProcessingPerformanceStatus to nifi-api - Added Performance Status to Process Group and Processor Sources for Query NiFi Reporting Task  This closes #9014  Signed-off-by: David Handermann <exceptionfactory@apache.org>
201206030,novel,cd3a7206a99e482d43a1c61ddc4bc2c849845dab,https://github.com/201206030/novel/commit/cd3a7206a99e482d43a1c61ddc4bc2c849845dab,perf: instanceof 智能转型
201206030,novel,9d8709ed2d97d920345ba2a128db43c098c7a310,https://github.com/201206030/novel/commit/9d8709ed2d97d920345ba2a128db43c098c7a310,perf: 提前创建数据库连接池 Spring Boot 新版本默认会在第一次请求数据库时创建连接池
201206030,novel,60488258f5d36215c23327bacc699badeddce6c3,https://github.com/201206030/novel/commit/60488258f5d36215c23327bacc699badeddce6c3,perf: 提高接口第一次访问速度  Spring Boot 新版本默认会在第一次访问数据库时才创建连接池
00-Evan,shattered-pixel-dungeon,cc3c434d35369b9a48eda7e9da1ca4adcd44284d,https://github.com/00-Evan/shattered-pixel-dungeon/commit/cc3c434d35369b9a48eda7e9da1ca4adcd44284d,v2.5.0: improved text input performance regarding copy/paste
mik3y,usb-serial-for-android,9911e141a74d64ac587d320eed284c5aca08e54e,https://github.com/mik3y/usb-serial-for-android/commit/9911e141a74d64ac587d320eed284c5aca08e54e,01. Refactored SerialInputOutputManager (#615)  Used separate threads for reading and writing  enhancing concurrency and performance.  Note: before was possible to start `SerialInputOutputManager` with `Executors.newSingleThreadExecutor().submit(ioManager)`. Now you have to use `ioManager.start()`
CaffeineMC,sodium,c83c3fb30df0a6529c482c98f8ceff0501018644,https://github.com/CaffeineMC/sodium/commit/c83c3fb30df0a6529c482c98f8ceff0501018644,Combine draw commands to improve rendering performance (#2421)
CaffeineMC,sodium,0d4ab847519dfa7be84c562d6fe4e24761c96e7d,https://github.com/CaffeineMC/sodium/commit/0d4ab847519dfa7be84c562d6fe4e24761c96e7d,Do not cache ambient brightness at initialization  The world may not be assigned to the renderer at initialization  which is the case for non-terrain rendering (i.e. block entities.)  Likely  there is no performance benefit to caching this data in the first place  so the easiest solution is to just remove the code.
CaffeineMC,sodium,5e7c8ac4d78d7085041f3c8d477774480dba7dd4,https://github.com/CaffeineMC/sodium/commit/5e7c8ac4d78d7085041f3c8d477774480dba7dd4,Use alternative workaround for NVIDIA drivers  The NVIDIA driver enables a driver feature called "Threaded Optimizations" when it finds Minecraft  which causes severe performance issues and sometimes even crashes.  Newer versions of the driver seem to use a slightly different heuristic which our workaround doesn't address.  So  instead  use an alternative method that enables GL_DEBUG_OUTPUT_SYNCHRONOUS. This seems to reliably disable the functionality *even if* the user has configured it otherwise in their driver settings.  Additionally  on Windows  we now always indicate to the driver that Minecraft is running  so that users with hybrid graphics don't see regressed performance.
CaffeineMC,sodium,e7ea6f7dd5207c8fd0dac584a1134a16025a85de,https://github.com/CaffeineMC/sodium/commit/e7ea6f7dd5207c8fd0dac584a1134a16025a85de,Block the Overwolf Overlay due to graphical corruption  The overlay does not correctly restore the texture unit state in OpenGL  which causes problems when Minecraft thinks a texture has already been bound to a slot.  Since disabling the OpenGL state cache globally is not an acceptable solution (it would severely hurt performance) and their software doesn't give us any method to detect the problematic version  we block all versions.  gep_minecraft.dll is the payload they actually inject  which has no version information or description.  Fixes #2862
CaffeineMC,sodium,0939130bef947b78f0e2d6e62126026f39aad504,https://github.com/CaffeineMC/sodium/commit/0939130bef947b78f0e2d6e62126026f39aad504,Combine the vertex position attributes (#2753)  This improves terrain rendering performance significantly on Intel Xe-LP graphics under Linux.
bitcoinj,bitcoinj,90963ee7302f0b9af196824cbc86d5461088d81e,https://github.com/bitcoinj/bitcoinj/commit/90963ee7302f0b9af196824cbc86d5461088d81e,MemoryFullPrunedBlockStore: remove type parameters from class `TransactionalMultiKeyHashMap`  Also  rename it to `TransactionalFullBlockMap` and update the JavaDoc.  Removing the type parameters and "de-genericizing" will make the code easier to read  enable further improvements  and likely performance optimizations that are specific to the "full block map" use case.
reactor,reactor-core,ebded61a33055f7ca9731b54285fcd17b436829c,https://github.com/reactor/reactor-core/commit/ebded61a33055f7ca9731b54285fcd17b436829c,Skip Automatic Context Propagation in special operators (#3845)  This change should improve performance of Automatic Context Propagation in certain cases when doOnDiscard  onErrorContinue  and onErrorStop are used.  The context-propagation integration requires contextWrite and tap operators to be barriers for restoring ThreadLocal values. Some internal usage of contextWrite does not require us to treat the operators the same way and we can skip the ceremony of restoring ThreadLocal state as we know that no ThreadLocalAccessor can be registered for them. Therefore  a private variant is introduced to avoid unnecessary overhead when not required.  Related #3840
osmandapp,OsmAnd,6b69d68f3551ff3fdde165ce39dd1a1f960ec4cd,https://github.com/osmandapp/OsmAnd/commit/6b69d68f3551ff3fdde165ce39dd1a1f960ec4cd,Add debug option to show tile performance metrics (#22574)
osmandapp,OsmAnd,a4cfda0ae68d92eaf6a20c7a5efef4f9774013b6,https://github.com/osmandapp/OsmAnd/commit/a4cfda0ae68d92eaf6a20c7a5efef4f9774013b6,Merge pull request #20062 from osmandapp/Performance_fix_19804  Performance fix 19804
osmandapp,OsmAnd,2b761dfadf85e1d545105810bd898fee7c34bc11,https://github.com/osmandapp/OsmAnd/commit/2b761dfadf85e1d545105810bd898fee7c34bc11,Revert for performance reasons
liquibase,liquibase,f8bb5963cb4e45e70192f348e135eac3c815e933,https://github.com/liquibase/liquibase/commit/f8bb5963cb4e45e70192f348e135eac3c815e933,feat: Add PostgreSQL Index Function Support via USING Clause (#6901)  * feat: Add PostgreSQL Index Type Support via USING Clause  This commit adds support for defining PostgreSQL index types through the USING clause. The implementation:  1. Adds a `using` property to `CreateIndexChange` and `Index` classes 2. Enhances JDBC snapshot capabilities to capture index types from PostgreSQL 3. Updates `CreateIndexGeneratorPostgres` to include the USING clause in SQL generation 4. Extends schema definitions by adding the `using` attribute to createIndex in XML 5. Updates related generators and statement classes to support the new property  This enhancement allows specifying index types like btree  gin  gist  etc. when creating/snapshoting indexes in PostgreSQL  providing better control over index behavior and performance.  * feat: add unit and integration tests  * chore: rename column to match metadata  * chore: rename column to match metadata  * Update liquibase-standard/src/main/java/liquibase/sqlgenerator/core/CreateIndexGenerator.java  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>  * fix copilot fix  ---------  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com> Co-authored-by: rberezen <ruslan.berezenskyi@gmail.com> Co-authored-by: Alex <abrackx@gmail.com>
liquibase,liquibase,d534727e88c6ecd17678ee92fe27276deb5c865a,https://github.com/liquibase/liquibase/commit/d534727e88c6ecd17678ee92fe27276deb5c865a,Fixes #6685 Performance issue of v4.30.0 (#6686)  Fixes #6685 Performance issue of an incorrect extract-variable refactoring of commit 89a3c3bc0bd211c8c5862af54d7fb9a22a5577ca  -- The original author extracted replacedSnapshotControl as a local variable but then failed passing it down the line. Instead the original snapshotControl object got passed which seems to operate on the entire catalog instead of just the example type in question.
liquibase,liquibase,946f4e47be62f710458e1adf29745301e0e7c95a,https://github.com/liquibase/liquibase/commit/946f4e47be62f710458e1adf29745301e0e7c95a,improve startup performance (DAT-18327) (#6366)
liquibase,liquibase,da5dfed11ad4b7ea64c8f4fcaf427b7dd60425b1,https://github.com/liquibase/liquibase/commit/da5dfed11ad4b7ea64c8f4fcaf427b7dd60425b1,update-to-tag fixes when provided tag doesn't exist or there is not tagDatabaseChange in the changelog (#6169)  * - Introduce STRICT usage for updateToTag. - Integration tests added.  * Moved tagDatabase check fields inside of STRICT if block.  * Disable update-to-tag warning message when using STRICT mode.  * Apply review comment and move tagChange validation from StatusChangeLogIterator to UpdateToTagCommandStep.  * UpdateToTag code refactoring done for Strict validations added.  * chore: switching to h2 for performance improvement  * chore: switching to h2 for performance improvement; changed tests order (!?); fxied exception message  * Move Strict tests to a new test class.  ---------  Co-authored-by: filipe <flautert@liquibase.org>
liquibase,liquibase,a964357fcffa050356947093e20a5ce6e23b83a6,https://github.com/liquibase/liquibase/commit/a964357fcffa050356947093e20a5ce6e23b83a6,Fixed performance issue with valueBlobFile on Postgres databases introduced in v4.18.0 (#6097)  Fixed performance issue with valueBlobFile on Postgres databases
GeyserMC,Geyser,6452d11d951aa93b2a449f9e0bffd9820f0a7ab1,https://github.com/GeyserMC/Geyser/commit/6452d11d951aa93b2a449f9e0bffd9820f0a7ab1,Merge remote-tracking branch 'origin/fix-fabric-world-manager-performance' into feature/1.21.2
GeyserMC,Geyser,fb634e8528a1d9644c981151e89702447c508a33,https://github.com/GeyserMC/Geyser/commit/fb634e8528a1d9644c981151e89702447c508a33,Merge branch 'master' into fix-fabric-world-manager-performance
GeyserMC,Geyser,8122983765e85a108e80c85d18bb953d25dca273,https://github.com/GeyserMC/Geyser/commit/8122983765e85a108e80c85d18bb953d25dca273,Merge branch 'master' into fix-fabric-world-manager-performance
GeyserMC,Geyser,4c3ea83e20e4876788f6105d3c88d2459f4abb48,https://github.com/GeyserMC/Geyser/commit/4c3ea83e20e4876788f6105d3c88d2459f4abb48,Merge branch 'master' into fix-fabric-world-manager-performance
GeyserMC,Geyser,73f7259b6dd508680582418bbb7963ad0b460907,https://github.com/GeyserMC/Geyser/commit/73f7259b6dd508680582418bbb7963ad0b460907,Add proper text component parsing from NBT (#5029)  * Attempt creating a simple NBT text component parser  * Fix style merging  * Rename TextDecoration to ChatDecoration  use better style deserialization in ChatDecoration  * Remove unused code  * containsKey optimisations  update documentation  improve getStyleFromNbtMap performance slightly  more slight tweaks  * Remove unnecessary deserializeStyle method
GeyserMC,Geyser,0834c70b3ff1e5cb334f2ca58b0c93e7f5902329,https://github.com/GeyserMC/Geyser/commit/0834c70b3ff1e5cb334f2ca58b0c93e7f5902329,Merge branch 'master' into fix-fabric-world-manager-performance
GeyserMC,Geyser,f18a163eaa4a8a3eda3cff199ac04ad1c68d5315,https://github.com/GeyserMC/Geyser/commit/f18a163eaa4a8a3eda3cff199ac04ad1c68d5315,Merge branch 'master' into fix-fabric-world-manager-performance
GeyserMC,Geyser,293bad55bc5aa6e3a583663868f33766fb7ead18,https://github.com/GeyserMC/Geyser/commit/293bad55bc5aa6e3a583663868f33766fb7ead18,Merge branch 'master' into fix-fabric-world-manager-performance
cinit,QAuxiliary,4298e2203a403924c73d52d18ac6f2335b4c8503,https://github.com/cinit/QAuxiliary/commit/4298e2203a403924c73d52d18ac6f2335b4c8503,chore: override findClass instead of loadClass for better performance
runelite,runelite,210c0c3d8fa1ac5a9d2f04e6f3d09a47cfa03bba,https://github.com/runelite/runelite/commit/210c0c3d8fa1ac5a9d2f04e6f3d09a47cfa03bba,entityhider: selectively hide party members  I placed party members above friends  since it is a logically more restrictive group than your entire friends list. It is likely to only contain the people you are actively playing with  and so you might want to hide everyone else  including friends.  PartyService#getMemberByDisplayName performs jagex name sanitization  so it may be desirable to use a cached form of this list if performance is a concern. Let me know and I can add it. I didn't notice any performance impact even at busy places like the GE.
line,armeria,65629276366b5eb7c1094bb492759789094f437a,https://github.com/line/armeria/commit/65629276366b5eb7c1094bb492759789094f437a,Exclude attributes from `Endpoint.toString()` (#6061)  Motivation:  We observed `OutOfMemoryError` in internal CI tests when a new `Endpoint` is created. ``` com.linecorp.armeria.xds.client.endpoint.EndpointsPool$$Lambda$754/1708334230@3a58f44e java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOfRange(Arrays.java:3664) at java.lang.String.<init>(String.java:207) at java.lang.StringBuilder.toString(StringBuilder.java:412) at com.google.protobuf.TextFormat$Printer.printToString(TextFormat.java:593) at com.google.protobuf.AbstractMessage.toString(AbstractMessage.java:87) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:136) at com.linecorp.armeria.internal.shaded.guava.collect.AbstractMapEntry.toString(AbstractMapEntry.java:70) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:136) at com.linecorp.armeria.internal.shaded.guava.collect.Iterators.toString(Iterators.java:294) at com.linecorp.armeria.common.ImmutableAttributes.toString(ImmutableAttributes.java:183) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:136) at com.linecorp.armeria.client.Endpoint.generateToString(Endpoint.java:304) at com.linecorp.armeria.client.Endpoint.<init>(Endpoint.java:268) at com.linecorp.armeria.client.Endpoint.replaceAttrs(Endpoint.java:747) at com.linecorp.armeria.client.Endpoint.withAttr(Endpoint.java:707) at com.linecorp.armeria.xds.client.endpoint.EndpointsPool.withTimestamp(EndpointsPool.java:103) at com.linecorp.armeria.xds.client.endpoint.EndpointsPool.cacheAttributesAndDelegate(EndpointsPool.java:71) at com.linecorp.armeria.xds.client.endpoint.EndpointsPool.lambda$updateClusterSnapshot$1(EndpointsPool.java:62) at com.linecorp.armeria.xds.client.endpoint.EndpointsPool$$Lambda$754/1708334230.run(Unknown Source) ``` When an `Endpoint` is initicated  it pre-generates a string representation for caching and reuse. In our tests  it contained many attributes related to xDS for service discovery. It might be a good idea to remove attributes whose size is hard to predict from `toString()` and prevent unintended OOM.  In addition  the result size of `toString()` gets small  so I doubt that the pre-generated cache is useful for performance.  Modifications:  - Remove attributes from `toString()`. - Lazily build the string representation when `toString()` is called and don't cache the result.  Result:  `Endpoint` no longer includes attributes in `.toString()`.
line,armeria,79112f537eb851aa2aba3b534777a608f2c71e0d,https://github.com/line/armeria/commit/79112f537eb851aa2aba3b534777a608f2c71e0d,Fix race condition where `log.whenComplete` may not complete (#5986)  Motivation:  It has been reported that `log.whenComplete` is not completing in some cases in #5981. The cause seems to be a race condition between `DefaultRequestLog#endRequest` and `DefaultRequestLog#requestContent`. Completion of `log.whenComplete` is important because 1) it is semantically bound to an HTTP request 2) users (including us) have clean up logic using `log.whenComplete`.  I propose that simply `endRequest` only sets `name` if content is not deferred  and `requestContent` sets `name` if content is deferred. The logic is easier to reason about  and there are minimal performance implications since a lock is held anyways.  Modifications:  - `#endRequest` sets `name` if `requestContent` isn't deferred - `#requestContent` sets name if `requestContent` is deferred  Result:  - Closes #5981  <!-- Visit this URL to learn more about how to write a pull request description:  https://armeria.dev/community/developer-guide#how-to-write-pull-request-description -->
line,armeria,93088a30891ce0c1ce2503d51bc7e3f73158d30a,https://github.com/line/armeria/commit/93088a30891ce0c1ce2503d51bc7e3f73158d30a,Avoid redundant fromObject(value) calls in addObject methods (#5962)  Motivation:  In the addObject and addObjectAndNotify methods  the fromObject(value) function is being called redundantly. This results in unnecessary computations  which could affect performance and reduce code readability. By removing the redundant calls to fromObject(value)  we can improve the efficiency and clarity of the code.  Modifications:  - Modified the addObject method to pass the original value directly to the addObjectAndNotify method without calling fromObject(value). - Updated the addObjectAndNotify method to perform the fromObject(value) call once  ensuring it is not called multiple times for the same value.  Result:  - Improved code readability and maintainability. - Eliminated the redundant fromObject(value) calls  enhancing performance.  <!-- Visit this URL to learn more about how to write a pull request description:  https://armeria.dev/community/developer-guide#how-to-write-pull-request-description -->
line,armeria,c1d547518c8e14ddd5e723d02de7bb2a63f9c003,https://github.com/line/armeria/commit/c1d547518c8e14ddd5e723d02de7bb2a63f9c003,Fix a leak in `HttpEncodedResponse` (#5858)  Motivation:  An `HttpData` produced in `HttpEncodedResponse.beforeComplete()` is not collected by `CollectingSubscriberAndSubscription` but is leaked.  ```java Hint: {10B  pooled  <unknown>} com.linecorp.armeria.common.HttpData.wrap(HttpData.java:110) com.linecorp.armeria.server.encoding.HttpEncodedResponse.beforeComplete(HttpEncodedResponse.java:163) com.linecorp.armeria.common.stream.FilteredStreamMessage.lambda$collect$0(FilteredStreamMessage.java:201) java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934) java.base/java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:950) java.base/java.util.concurrent.CompletableFuture.handle(CompletableFuture.java:2340) com.linecorp.armeria.common.stream.FilteredStreamMessage.collect(FilteredStreamMessage.java:142) ```  `CollectingSubscriberAndSubscription` was designed to only apply `filter()` to the `upstream.collect()`. I didn't consider that an object could be published via `onNext()` in `beforeComplete()`. The purpose of `CollectingSubscriberAndSubscription` was to provide an optimized code path for unary calls. it didn't seem the code provides a trivial performance improvement but the implementation was complex and error-prone.  I was able to fix the code not to leak the data but I didn't want to additional complexity to it. It might be cleaner to use the Reactive Streams API instead of keeping the custom `collect()` implementation. There will be no change in performance for normal message sizes.  Modifications:  - Remove the custom `collect()` implemtation in `FilteredStreamMessage`.  Result:  Fix a potential leak when sending compressed responses.
apache,ignite,10ea05c9885be94f73cd67981f7c791b8de6a0d7,https://github.com/apache/ignite/commit/10ea05c9885be94f73cd67981f7c791b8de6a0d7,IGNITE-24284 SQL Calcite: Fix flaky SqlDiagnosticIntegrationTest.testPerformanceStatistics - Fixes #12039.  Signed-off-by: Aleksey Plekhanov <plehanov.alex@gmail.com>
apache,ignite,6b67d839cdd54299d938b4317fa8c027925a5d44,https://github.com/apache/ignite/commit/6b67d839cdd54299d938b4317fa8c027925a5d44,IGNITE-24777 Fix reading performance statistics report due to miss-cached strings (#11949)  Thank you for submitting the pull request to the Apache Ignite.  In order to streamline the review of the contribution we ask you to ensure the following steps have been taken:  ### The Contribution Checklist - [ ] There is a single JIRA ticket related to the pull request. - [x] The web-link to the pull request is attached to the JIRA ticket. - [ ] The JIRA ticket has the _Patch Available_ state. - [x] The pull request body describes changes that have been made. The description explains _WHAT_ and _WHY_ was made instead of _HOW_. - [x] The pull request title is treated as the final commit message. The following pattern must be used: `IGNITE-XXXX Change summary` where `XXXX` - number of JIRA issue. - [ ] A reviewer has been mentioned through the JIRA comments (see [the Maintainers list](https://cwiki.apache.org/confluence/display/IGNITE/How+to+Contribute#HowtoContribute-ReviewProcessandMaintainers)) - [x] The pull request has been checked by the Teamcity Bot and the `green visa` attached to the JIRA ticket (see [TC.Bot: Check PR](https://mtcga.gridgain.com/prs.html))  ### Notes - [How to Contribute](https://cwiki.apache.org/confluence/display/IGNITE/How+to+Contribute) - [Coding abbreviation rules](https://cwiki.apache.org/confluence/display/IGNITE/Abbreviation+Rules) - [Coding Guidelines](https://cwiki.apache.org/confluence/display/IGNITE/Coding+Guidelines) - [Apache Ignite Teamcity Bot](https://cwiki.apache.org/confluence/display/IGNITE/Apache+Ignite+Teamcity+Bot)  If you need any help  please email dev@ignite.apache.org or ask anу advice on http://asf.slack.com _#ignite_ channel.
apache,ignite,86d85f9c191ee8a11f18504c17aabdbd186b83f3,https://github.com/apache/ignite/commit/86d85f9c191ee8a11f18504c17aabdbd186b83f3,IGNITE-23901 Added performance statistics for putAllConflict  removeAllConflict operations (#11793)
apache,ignite,fab69a781090291d7b08a02f29356959171a6a8e,https://github.com/apache/ignite/commit/fab69a781090291d7b08a02f29356959171a6a8e,IGNITE-24385 Fixed performance drop introduced by SQL plan history system view - Fixes #11848.  Signed-off-by: Aleksey Plekhanov <plehanov.alex@gmail.com>
apache,ignite,8ee63d7fc2c6a4965c992a27c5660972bd9921e0,https://github.com/apache/ignite/commit/8ee63d7fc2c6a4965c992a27c5660972bd9921e0,IGNITE-24168 Fixed performance drop caused by IGNITE-22375 (#11797)
apache,calcite,2d6e9a7dda70313bbfcf0e39f4cffc1404f1f521,https://github.com/apache/calcite/commit/2d6e9a7dda70313bbfcf0e39f4cffc1404f1f521,[CALCITE-6471] Improve performance of SqlToRelConverter by preventing unconditional conversion of sqlNodes to string for null-check messages
SPLWare,esProc,bd2221985d446368e25118b19ec7cb67b460f33a,https://github.com/SPLWare/esProc/commit/bd2221985d446368e25118b19ec7cb67b460f33a,Do performance optimization.
apache,maven,f2bc813529b112f19526b70b5063464bc98c7eb7,https://github.com/apache/maven/commit/f2bc813529b112f19526b70b5063464bc98c7eb7,[MNG-8575] Replace a list with O(N²) performance by O(N) at least during iteration. (#2092)  * Replace a list with O(N²) performance by O(N) at least during iteration. * Remove a comment which is not true anymore. * Replace `CopyOnWriteArrayList` by `LinkedHashSet` for avoiding to iterate over all previous values every time that a new value is added. * Short-circuit for `List.isEmpty()`: stop at the first element found  without iterating over all elements.
micrometer-metrics,micrometer,86bb7508199e7364a8b84819c785b54bb1daa652,https://github.com/micrometer-metrics/micrometer/commit/86bb7508199e7364a8b84819c785b54bb1daa652,Improve average performance of LongTaskTimer for out-of-order stopping (#5591)  * Improve average performance of LongTaskTimer for out-of-order stopping  * Polish DefaultLongTaskTimer and its tests  ---------  Co-authored-by: Jonatan Ivanov <jonatan.ivanov@gmail.com>
micrometer-metrics,micrometer,d77b7be4484cc31bd85475c3c9d7ca8d33e82ab2,https://github.com/micrometer-metrics/micrometer/commit/d77b7be4484cc31bd85475c3c9d7ca8d33e82ab2,Fix concurrency issue with Exponential Histogram (#5749)  Synchronize writes to exponential histogram. An alternative would be to use explicit locks  but this made the code more complex and did not yield significant performance improvements over using a synchronized method. Also adds a concurrency test that reproduced the reported issue and verifies this fixes it.  Fixes gh-5740
micrometer-metrics,micrometer,3625a9eae0a0275eded5237468be078e300c7936,https://github.com/micrometer-metrics/micrometer/commit/3625a9eae0a0275eded5237468be078e300c7936,Fix performance regression in MeterRegistry#remove (#5750)  Adds a reverse look-up for the pre-filter meter ID for use when removing a Meter. This avoids the need to iterate over the meters in the cache (preFilterIdMeterMap)  which scales linearly with the number of meters  and is problematic because it does this while holding the meterMap lock needed to add new meters. Also adds benchmarks for measuring the performance of the remove method with different numbers of meters registered.  Resolves gh-5466
micrometer-metrics,micrometer,3c252c25477c5d875d7e78d29f375bb9a8cfb3a6,https://github.com/micrometer-metrics/micrometer/commit/3c252c25477c5d875d7e78d29f375bb9a8cfb3a6,Add benchmarks for DefaultLTT start/stop (#5595)  * Add benchmarks for DefaultLTT start/stop  Start and stop are called on the critical path. We should have benchmarks for them to evaluate changes that may affect their performance.  * Stop a random sample instead of middle sample  We should get a better idea of average performance with a random sample rather than a sample in a fixed position in the active tasks collection.  * AverageTime BenchmarkMode instead of single-shot  Using the invocation-level setup method  we can use average-time rather than single shot.
micrometer-metrics,micrometer,1d498f6b6266a547862c8727060cf36122bb6257,https://github.com/micrometer-metrics/micrometer/commit/1d498f6b6266a547862c8727060cf36122bb6257,Apply performance improvement from Tags to KeyValues  See gh-4959.  Resolves gh-5140
micrometer-metrics,micrometer,1482cdf46babc40273d24ebf572b0b15aec7bfd2,https://github.com/micrometer-metrics/micrometer/commit/1482cdf46babc40273d24ebf572b0b15aec7bfd2,Tags merge optimization (#4959)  Improves the performance of merging two Tags instances by taking advantage of the fact that their internal representation of tags is always sorted and deduplicated. Therefore  they can be merged more efficiently than a collection of tags that may not be sorted and deduplicated. Added benchmark to measure Tags.and(Tags) operation.  See gh-5140
micrometer-metrics,micrometer,bb2ff45562a79f810bb1aafe24d3f55cb1df551a,https://github.com/micrometer-metrics/micrometer/commit/bb2ff45562a79f810bb1aafe24d3f55cb1df551a,Support for ExponentialHistogram in OTLP Registry (#3959)  Adds support for https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/metrics/data-model.md#exponentialhistogram. Math used for index calculation is re-used from the OTEL specification which lays down the formula/techniques to be considered for index calculation also keeping performance in mind.  Some new configuration options are added to `OtlpConfig` for controlling the behavior. A `histogramFlavor` method controls whether the existing explicit bucket histograms will be used or exponential histograms. The max scale and max bucket count of the exponential histograms can also be configured for the registry via the corresponding methods added to `OtlpConfig`.  Resolves gh-3861
nextcloud,android,efd437c5901f6a9069274d22cd623ed8e528ce11,https://github.com/nextcloud/android/commit/efd437c5901f6a9069274d22cd623ed8e528ce11,Merge pull request #14829 from nextcloud/performance  Fix Performance Warnings
nextcloud,android,59de186cdad59b5986db3e10ee0485cc39883418,https://github.com/nextcloud/android/commit/59de186cdad59b5986db3e10ee0485cc39883418,fix performance issues  Signed-off-by: Alex Knop <alexknoptech@protonmail.com>
nextcloud,android,385c424567966a7aa3f5a3b5cc8e24b9ff1fe438,https://github.com/nextcloud/android/commit/385c424567966a7aa3f5a3b5cc8e24b9ff1fe438,fix performance issues  Signed-off-by: Alex Knop <alexknoptech@protonmail.com>
nextcloud,android,7a7b31072e52b572254cf85790bc2147a2477e3e,https://github.com/nextcloud/android/commit/7a7b31072e52b572254cf85790bc2147a2477e3e,fix performance issues  Signed-off-by: Alex Knop <alexknoptech@protonmail.com>
nextcloud,android,a7433351e56d39e658ed4192c5ee60b037ab7f5d,https://github.com/nextcloud/android/commit/a7433351e56d39e658ed4192c5ee60b037ab7f5d,Merge pull request #13700 from Onkar755/improve-bitmaputils  Improve BitmapUtils: Bitmap Handling and Image Processing for Modern APIs and Performance Enhancements
nextcloud,android,ffc0528605019fafe1faced38d19144cd1c7899f,https://github.com/nextcloud/android/commit/ffc0528605019fafe1faced38d19144cd1c7899f,refactor(ui): coroutine based user status retrieval.  Replaced ancient async task with a coroutine based alternative. This comes with numerous advantages.  - Improved performance due to the nature of coroutines and the reduced overhead associated with AsyncTasks and threads. Approx a %50 performance increase was measured during testing. - Passing references to fragment lifecycle context to an AsyncTask is generally a bad idea.
xinnan-tech,xiaozhi-esp32-server,7c5f2d95a40fed2c0b2a5de7fb50678ffe80fc4d,https://github.com/xinnan-tech/xiaozhi-esp32-server/commit/7c5f2d95a40fed2c0b2a5de7fb50678ffe80fc4d,🎈 perf: 更新文档+优雅的退出 (#292)  * 🎈 perf: 更新文档+优雅的退出  * update:gsv代码可以使用gpt_sovits_v2适配器代码拓展，不需要单独。如果需要定义模型名称，可以在gpt_sovits_v2代码中添加扩展  ---------  Co-authored-by: hrz <1710360675@qq.com>
deepjavalibrary,djl,85d09ba667c021610edd3965965ffcbeff3af8e5,https://github.com/deepjavalibrary/djl/commit/85d09ba667c021610edd3965965ffcbeff3af8e5,[api] Adds FUSE based repository support (#3695)  * [api] Adds FUSE based repository support  1. No cloud SDK dependencies 2. No need to download the entire bucket 3. High performance  * Update api/src/test/java/ai/djl/repository/FuseRepositoryTest.java  Co-authored-by: Frank Liu <frankfliu2000@gmail.com>  ---------  Co-authored-by: Frank Liu <frankfliu2000@gmail.com>
deepjavalibrary,djl,41f75681aab8708c375e94f0a99ad7673a74f7ae,https://github.com/deepjavalibrary/djl/commit/41f75681aab8708c375e94f0a99ad7673a74f7ae,[api] Improve listModel performance (#3641)
deepjavalibrary,djl,f91a696fdbab2037e86aa1d91741dcd973c7ef62,https://github.com/deepjavalibrary/djl/commit/f91a696fdbab2037e86aa1d91741dcd973c7ef62,[api] Optimized text embedding post processing performance (#3459)
deepjavalibrary,djl,c895909f47595b4f544acd67fd18b963d9d4c872,https://github.com/deepjavalibrary/djl/commit/c895909f47595b4f544acd67fd18b963d9d4c872,[enhancement] Optimize memory copy overhead to enhance performance. (#3289)
pytorch,serve,c7bbf2cf74b2cf34870f2ddee959dbcee38725bb,https://github.com/pytorch/serve/commit/c7bbf2cf74b2cf34870f2ddee959dbcee38725bb,Torchserve support for Intel GPUs (#3132)  * Merge changes from intel-sandbox/serve  * ipex_gpu_enable - New config in config.properties  * Instructions for IPEX GPU support  * Final Commits 1  * Style: Refactor code formatting  * Readme Updated  * Code Refactoring  * Code Refactoring  * Final Commit  * self.device mapping to XPU  * Code Refactoring  * Mulitple GPU device engagement enabled  * Remove unused changes  * Revert "Remove unused changes"  This reverts commit 9188deb4911633a1d2a3f7f97e80ff7343fbc492.  * Add performance gain info for GPU  * Update README.md  * Add units to table  * Update metric reading configuration.  * Update system metrics script path.  * Update ConfigManager.java  * Reformat ConfigManager.java  * Fix spelling issues  * Fix lint changed file.  ---------  Co-authored-by: root <root@DUT4039DG2FRD.fm.intel.com> Co-authored-by: Kanya-Mo <167922169+Kanya-Mo@users.noreply.github.com> Co-authored-by: Mo  Kanya <kanya.mo@intel.com> Co-authored-by: Anup Renikunta <anup.renikunta@intel.com> Co-authored-by: Ankith Gunapal <agunapal@ischool.Berkeley.edu>
gsantner,markor,ccb2c6a69a8e4101ea0500e003ef969ad76f8c55,https://github.com/gsantner/markor/commit/ccb2c6a69a8e4101ea0500e003ef969ad76f8c55,Improve text rendering performance  closes #2478  closes #2515 (#2509)  * Invisible instead of gone  * Fixes  * tweaks  * Fixed states  * Tweak to wrap state  * Fixed scroll to cursor  * File search perf and all files  * Window options  * Fixes  using compat  * Reverted changes to file search  * Undo file loads  * Improving layout further  * Fix dialog title  * Switching to -1  pulling fixes  * Fixed issues with last line  * reformat code  ---------  Co-authored-by: Gregor Santner <gsantner@mailbox.org>
gsantner,markor,f19f8b2969cbebf26c7a046458d867ad75467fe2,https://github.com/gsantner/markor/commit/f19f8b2969cbebf26c7a046458d867ad75467fe2,Keyboard TAB key handling  syntax highlighting performance  filebrowser navigation (PR #2487 closes #2469 #2484)  * Keypress handler * Restored option  fixed duplicate calls to loadFolder on resume * Made some small changes for highlighting performance ---------  Co-authored-by: Gregor Santner <gsantner@mailbox.org>
crate,crate,5ce8a91e21cd41dc374b846daf4fe277d6f46204,https://github.com/crate/crate/commit/5ce8a91e21cd41dc374b846daf4fe277d6f46204,Always load singleton string primary key from binary dv  If a table has a single primary key of type STRING  Crate will now store it in a BinaryDocValues field. These are more efficient than SortedDocValues for sequential access  and should both improve the performance of joins on primary keys and allow the removal of the Crate DocValuesFormat fork  which will re-enable compression on lower-cardinality string columns and open up the possibility of using sparse indexes in future.  This change shows significant speedups when streaming large numbers of primary key values:  Q: select count(t1."sourceIP") from uservisits_large t1 inner join uservisits_small t2 on t1."sourceIP" = t2."sourceIP" C: 1 | Version |         Mean ±    Stdev |        Min |     Median |         Q3 |        Max | |   V1    |    25393.936 ±  339.451 |  24746.715 |  25338.120 |  25668.752 |  26095.559 | |   V2    |    11375.290 ±  378.957 |  10383.943 |  11328.199 |  11476.776 |  12147.357 | ├---------┴-------------------------┴------------┴------------┴------------┴------------┘ |               -  76.25%                           -  76.42% There is a 100.00% probability that the observed difference is not random  and the best estimate of that difference is 76.25% The test has statistical significance  Q: select count(t1."sourceIP") from uservisits_large t1 inner join uservisits_small t2 on t1."sourceIP" = t2."sourceIP" C: 5 | Version |         Mean ±    Stdev |        Min |     Median |         Q3 |        Max | |   V1    |    27992.866 ±  348.264 |  27397.459 |  27871.027 |  28083.560 |  28950.416 | |   V2    |    14367.342 ±  428.816 |  14025.410 |  14237.366 |  14344.758 |  15561.470 | ├---------┴-------------------------┴------------┴------------┴------------┴------------┘ |               -  64.33%                           -  64.76% There is a 100.00% probability that the observed difference is not random  and the best estimate of that difference is 64.33% The test has statistical significance
crate,crate,326971a6e9d61300977090e55d162305f291b85d,https://github.com/crate/crate/commit/326971a6e9d61300977090e55d162305f291b85d,Use bigger buffer size for OpenDAL's OperatorOutputStream  Helps to avoid BlockCountExceedsLimit and improves upload performance
crate,crate,2f9c95e884496199096a05f5e63a7f212635668e,https://github.com/crate/crate/commit/2f9c95e884496199096a05f5e63a7f212635668e,Avoid BooleanQuery at NotPredicate if not needed  This seems not to have a performance impact but at least improves readability of query string representation.
crate,crate,104ef15f984b477cfa9faf1463e0c24eac71febd,https://github.com/crate/crate/commit/104ef15f984b477cfa9faf1463e0c24eac71febd,Avoid double negation when negating a lucene query by `Queries.not`  Relates to queries which negates inner queries twice like e.g. `IS NOT DISTINCT` -> not(not(eqQuery). This seems not to have a performance impact but at least improves readability of query string representation.  Relates to a discussion at https://github.com/crate/crate/pull/16621
crate,crate,e78efc5c1922a26282a50f0c811fe34df5a1cfa6,https://github.com/crate/crate/commit/e78efc5c1922a26282a50f0c811fe34df5a1cfa6,Add lucene query generation for `IS DISTINCT` queries when possible  This should improve performance a lot when using `IS (NOT)? DISTINCT` inside the ``WHERE`` clause as it will utilize lucene indexes or docvalues.  Closes #16537.
crate,crate,6c63ebf5feee7ef8db1a6a3158e416d98053166d,https://github.com/crate/crate/commit/6c63ebf5feee7ef8db1a6a3158e416d98053166d,Fix array_length function for input arrays containing nulls  Introduce '_array_length_' field to index array columns' lengths for performance improvements  see the PR for details
camunda,camunda-bpm-platform,3151c40896c5447ed14c378b1d9ee3b2dd02d354,https://github.com/camunda/camunda-bpm-platform/commit/3151c40896c5447ed14c378b1d9ee3b2dd02d354,fix(mssql): replace or with union in external task to improve performance
bepass-org,oblivion,e686c693784ba10897d5ce69e7d00d2c31c27902,https://github.com/bepass-org/oblivion/commit/e686c693784ba10897d5ce69e7d00d2c31c27902,Ready to release V5 (#376)  Fixed Hot issue bugs related to Android API. Add enhanced logging logic. Add support IPV4/6 endpoint. Add support multi-endpoint. Improved performance. Add Turkish Language.
bepass-org,oblivion,9fd7a400129b845ce83a79b05d9b3e4b9f8c9d4b,https://github.com/bepass-org/oblivion/commit/9fd7a400129b845ce83a79b05d9b3e4b9f8c9d4b,Fixed battery issue  improved VpnService performance
201206030,novel-plus,415bf8a64c4d75b019381bacba7d22c3d340e7b0,https://github.com/201206030/novel-plus/commit/415bf8a64c4d75b019381bacba7d22c3d340e7b0,perf: 设置小说推荐缓存时间
201206030,novel-plus,d4fa0abc4e30b9ef8ccfd38065bff9a78a46412e,https://github.com/201206030/novel-plus/commit/d4fa0abc4e30b9ef8ccfd38065bff9a78a46412e,perf: 使用流式响应处理AI生成文本，提高用户体验
201206030,novel-plus,4b1507b2d1b17b1d40b60e9190a2a796dc0c2b34,https://github.com/201206030/novel-plus/commit/4b1507b2d1b17b1d40b60e9190a2a796dc0c2b34,perf: 连接池统一创建
201206030,novel-plus,82658f3b5f5ddac4a4fa4e5180823f4a34af38af,https://github.com/201206030/novel-plus/commit/82658f3b5f5ddac4a4fa4e5180823f4a34af38af,perf: 兼容其它数据源
201206030,novel-plus,acf9c76757c194b37498f5b3442985e41fc35cc3,https://github.com/201206030/novel-plus/commit/acf9c76757c194b37498f5b3442985e41fc35cc3,perf: 提前创建数据库连接池 Spring Boot 新版本默认会在第一次请求数据库时创建连接池
201206030,novel-plus,4b00ea68a9127086d9b57a5ed92316db7abb0e11,https://github.com/201206030/novel-plus/commit/4b00ea68a9127086d9b57a5ed92316db7abb0e11,perf: 提高第一次登录速度
201206030,novel-plus,85b64bbc10e3188d8fa0b28ea2ae9a7a49081fde,https://github.com/201206030/novel-plus/commit/85b64bbc10e3188d8fa0b28ea2ae9a7a49081fde,perf: 爬虫采集流程优化
201206030,novel-plus,6d0ab337579c10e90f8b61f50c578a33dc349427,https://github.com/201206030/novel-plus/commit/6d0ab337579c10e90f8b61f50c578a33dc349427,perf: 爬虫分类规则优化
alibaba,fastjson2,c12ef42a7e4eded00188ff16eed828c60b0c7acb,https://github.com/alibaba/fastjson2/commit/c12ef42a7e4eded00188ff16eed828c60b0c7acb,refactor(JSONWriterUTF8): Optimize string encoding methods  Refactored the string encoding logic by removing redundant variable assignments and modifying method visibility for better performance and maintainability.
alibaba,fastjson2,a632e14b37cf01fa78202b0dbbbcc4a886140620,https://github.com/alibaba/fastjson2/commit/a632e14b37cf01fa78202b0dbbbcc4a886140620,fix(JSONReader): Add reference detection disable flag  Add support for disabling reference detection via a new feature flag (MASK_DISABLE_REFERENCE_DETECT). This improves performance in cases where reference tracking is unnecessary.
alibaba,fastjson2,3016e80649b0788adeedde510427518279b1f411,https://github.com/alibaba/fastjson2/commit/3016e80649b0788adeedde510427518279b1f411,Improve some code  fix some bugs (#3320)  * Unified setting file encoding to UTF-8  * Fix issue#3283  * Aligning with new features of JDK8  * Simply some redundant code  * Improve Map handle performance  * Fix some potential bugs  * Improve some String processing performance  * Remove unnecessary protected modifier from final classes  because they are NOT inheritable  * Aligning with new features of JDK8  * Optimize FastJsonJsonView  * Fix conditional branch duplication  * Revert "Unified setting file encoding to UTF-8"  This reverts commit 66f1238da8371886a38473dbe8d5374e87e85381.
alibaba,fastjson2,0a0014b5fc7c8e7cfdd928dbf7cab6810e14097a,https://github.com/alibaba/fastjson2/commit/0a0014b5fc7c8e7cfdd928dbf7cab6810e14097a,Improve some code  fix some bugs (#3317)  * Unified setting file encoding to UTF-8  * Fix issue#3283  * Aligning with new features of JDK8  * Simply some redundant code  * Improve Map handle performance  * Fix some potential bugs  * Improve some String processing performance  * Remove unnecessary protected modifier from final classes  because they are NOT inheritable  * Aligning with new features of JDK8  * Optimize FastJsonJsonView  * Fix conditional branch duplication
geoserver,geoserver,8bdc0ff167782b6682a1336e6ca4e09c462b6502,https://github.com/geoserver/geoserver/commit/8bdc0ff167782b6682a1336e6ca4e09c462b6502,[GEOS-11786] Longitudinal profile process: general performance improvements
geoserver,geoserver,d8f10b65f10643149b5f8774ee27ff72504bf116,https://github.com/geoserver/geoserver/commit/d8f10b65f10643149b5f8774ee27ff72504bf116,[GEOS-11284] Promote community module "datadir catalog loader" to core  Promote the "datadir catalog loader" community module to the GeoServer core  improving startup performance for deployments with large data directories.  * Make the new loader log objects added to the catalog the same way DefaultGeoServerLoader does  and enhance GeoServerLoader's logging of timing with the count of objects added as the new loader does. * Add upgrade docs for optimized data directory loader Added a section to the upgrade guide explaining improvements to the configuration loading process. Included considerations for compatibility  along with configuration details on how to disable the new loader if needed. * Add more data directory loader tests and multi-instance improvements * Explicitly declares bean dependencies to ensure proper initialization order by making GeoServerLoaderProxy depend on essential components (extensions  dataDirectory  securityManager  configLock). * Guard sanitizing writes with GeoServerConfigurationLock (e.g. when adding default styles or estblishing the default workspace). This ensures consistency when multiple instances start off a shared data diretory such as in the case of GeoServer Cloud.
geoserver,geoserver,f1eb501c1b045925c7f3870ad32f7582ebb40be2,https://github.com/geoserver/geoserver/commit/f1eb501c1b045925c7f3870ad32f7582ebb40be2,[GEOS-11768] Reduce thread contention in Catalog operations  Improve performance by reducing thread contention in catalog operations.  Main improvements include: - Using fine-grained locking instead of broad synchronization blocks - Separating read/write locking patterns to allow concurrent reads - Only synchronizing when actually necessary for specialized operations  These changes should improve performance in multi-threaded environments while maintaining thread safety.
geoserver,geoserver,8372e1118ae8250b2f3dcf4b265c9f26c880d63b,https://github.com/geoserver/geoserver/commit/8372e1118ae8250b2f3dcf4b265c9f26c880d63b,[GEOS-11757] Optimize ConfigurationPasswordEncryptionHelper to Cache Encrypted Fields by Store Type  The `ConfigurationPasswordEncryptionHelper` class has been updated to improve performance by caching encrypted fields based on the StoreInfo type when the class cache is hit.  Previously  when a DataStoreInfo object lacked a type property  the method would repeatedly invoke `getCatalog().getResourcePool().getDataStoreFactory(info)`  causing unnecessary factory lookups and expensive serialization/deserialization operations within `ResourcePool`.  `decode(StoreInfo info)` has been updated to only call `securityManager.loadPasswordEncoders()` only when necessary. This makes a significant difference performance improvement when loading a large data directory  about 10% with the new DataDirectoryGeoServerLoader  and about 3% with DefaultGeoServerLoader.
geoserver,geoserver,1f86e8853e983479f47511dd403432b5462a820b,https://github.com/geoserver/geoserver/commit/1f86e8853e983479f47511dd403432b5462a820b,[GEOS-11758] datadir-catalog-loader: Improve reliability and code quality  Refactors the datadir-catalog-loader module in preparation for moving it to core:  - Avoid possible deadlocks in the main thread during startup due to spring holding a lock on the singletons registry. Force parsing Stores  Layers  and LayerGroups in the calling thread while loading the file contents in the ForkJoinPool. It is rather unpredictable when something deep inside XStreamReader will end up calling GeoServerExtensions.bean()/extensions(). - Make configuration parameters consistent with GeoServer naming conventions - Extend DefaultGeoServerLoader tapping into readCatalog() and readConfiguration()  hence fully sharing the loading workflow of the super classes. - Move from catalog to config package to follow gs-main's GeoServerLoader package naming - Use ForkJoinPool.ManagedBlocker to load file contents for better I/O parallelism - Replace test faker library with direct object creation - Replace weak reflection by the com.github.stefanbirkner:system-rules test dependency for environment variables related tests - Fix catalog resolution in StyleInfo  StoreInfo and ResourceInfo instances - Increase test coverage to +90%  This improves startup performance significantly for large catalogs  especially when using network filesystems like NFS.
geoserver,geoserver,1caac1c2a400938e56c88b91e1a8a1f80700cbb5,https://github.com/geoserver/geoserver/commit/1caac1c2a400938e56c88b91e1a8a1f80700cbb5,[GEOS-11580] Improve embedded GWC meta-tiling performance: review  QA  docs  GUI
geoserver,geoserver,2fbb244fbd8d5c6e7bc9fe06af41576e023c4cd6,https://github.com/geoserver/geoserver/commit/2fbb244fbd8d5c6e7bc9fe06af41576e023c4cd6,[GEOS-11580] Improve embedded GWC meta-tiling performance
knowm,XChange,3336c5434c3c97eafbd8b9a9b22a4b60770ed3b0,https://github.com/knowm/XChange/commit/3336c5434c3c97eafbd8b9a9b22a4b60770ed3b0,Merge pull request #4999 from ivelkov/develop  [Kraken] - Improve performance of checksum computation
knowm,XChange,efce27c9319fa22854e0b69e8477635faeba5e29,https://github.com/knowm/XChange/commit/efce27c9319fa22854e0b69e8477635faeba5e29,[Kraken] - Improve performance of checksum computation
jMonkeyEngine,jmonkeyengine,d080f5411d5fdcb3ecca80c025a2ed33ea06ef69,https://github.com/jMonkeyEngine/jmonkeyengine/commit/d080f5411d5fdcb3ecca80c025a2ed33ea06ef69,Refactor: Remove redundant checkAlError call for performance  removes the repeated call to the `checkAlError` method. The repeated string creation and the associated overhead of the error checking can negatively impact performance  especially in a frequently executed game loop. By removing this redundant check  we aim to improve overall performance and reduce garbage collection pressure.
JabRef,jabref,fae38967f663324083a3b907e3b0d087f32f1168,https://github.com/JabRef/jabref/commit/fae38967f663324083a3b907e3b0d087f32f1168,Fixing performance issues when bulk deleting (#12926)  * Fixing performance issues  when bulk deleting  * Removing comments  * Update CHANGELOG.md  * Delete benchmark  Rmoved a benchmark that I was using to test the performance that was accidently pushed.  * Fixing JMH Config  * Checkstyle Fix
JabRef,jabref,de81430c1472920304c8348207ee2af9ec525822,https://github.com/JabRef/jabref/commit/de81430c1472920304c8348207ee2af9ec525822,[OO] Major performance improvements & fixes (#12221)  * Transform \n-><p>  reduce LibreOffice API calls  improve documentation  * Remove unnecessary regex transform  improve trailing regex match  * Fix tests  * Minor fix - comment
JabRef,jabref,538f0eef4ba08410a403f0f30f8020b5f519404a,https://github.com/JabRef/jabref/commit/538f0eef4ba08410a403f0f30f8020b5f519404a,Improve performance with duplicate check on paste (#11843)  * Improve performance with duplicate check on paste  * changelog  * refine changelopg
JabRef,jabref,6af91b963dce2a4c9d0320ed8d0e1423b80f8a54,https://github.com/JabRef/jabref/commit/6af91b963dce2a4c9d0320ed8d0e1423b80f8a54,Lucene search (#11542)  * Use pattern matching for cast  Co-authored-by: Christoph <siedlerkiller@gmail.com>  * Fix pattern matching  * Fix merge  * Speed up switches between sorting/filtering modes  * Fixed merge errors  * Fixed small issues  * Removed obsolete tests  fixed some tests  * Fixed merge error in CHANGELOG.md  * Fixed checkstyle  * Fixed more tests  * Removed obsolete tests  * Fixes "Fixed merge error in CHANGELOG.md" by removing duplicate entries  This reverts commit 536ecfaaedf9b14b93e3d50b1a63166f4440661e.  * WiP on tests  * Checkstyle  * Checkstyle  * Update Java version  * Refine logging  * Fix compile error  * Add LuceneTest  * Update CHANGELOG.md  * Move search classes to pdf package  * Move search classes to search package  * rewriteRun  * Remove bibEntry from DocumentReader  * Rewrite LuceneIndexer  * Remove IndexingTaskManager  * Separate Bib fields index and LinkedFiles index  * Fix null LuceneManager in ExternalFilesEntryLinker  * Save as action  * Clear linkedFiles indexer when fullText indexing is disabled in preferences  * remove comments  * get indexed files on update  * Add LUCENE_MANAGERS map for accessing managers by databaseContext.getUid  * Move LuceneManager from search.indexing package to search  * Fix wrong order for import  * Move SearchQuery to model package  * Fix issue with opening multiple unsaved libraries  * Pass LuceneManager down to the entry editor  * Improve searching performance  * Change SearchFieldConstants to enum  * More performance improvements for searching  - Read document only one time - getHighlighterFragments only when the search results tab is opened  * Update FulltextSearchResultsTab.java  * Fix group union  intersection  * Fix backgroundtask  * Fix subscriptions  * Remove lastSearchQueryLogic  * Fix possible NPE  * Fix searchTask check  * Remove sort by score flag  * Fix score column sorting  * Fix modifier buttons listener  * Add search rank column  In floating mode entries will be ranked and sorted by it. Rank: (1= entry matches group and search  2= matches group but not search  3= matches search but not group  4= matches nothing)  * hide search rank column from preferences  * Add search_rank column to sort order by default  * Update CHANGELOG.md  * fix typo  * Change the order of the rank  1= entry matches group and search  2= matches search but not group  3= matches group but not search  4= matches nothing  * Use NGramAnalyzer for indexing  * Resolve conflicts  * update search matches with lucene  * PreviewViewer highlighting with Lucene  * Delete IndexingTaskManager.java  * SourceTab highlighting with Lucene  * Fix non-ASCII characters  * Extract query terms from search query  * Highlight regex queries  * return js highlight function  * Fix invalid search query throw exception  * Refactor Lucene indexer classes  * Refactor linked files indexer  * Update search matches when entries are added or updated  * Remove preferences from ActionHelper  * checkstyle  * comment out search tests  * OpenRewrite  * Fix Groups Parser/Serializer  * Localization  * Search groups  * Release `IndexSearcher` after completing search task  * Checkstyle  * Correct typo  * Remove GroupSearchQuery  * Remove EventBus from LuceneManager and use BibDatabase eventBus  * Fix number of matched entries in groups  * Fix search groups  * Localization  * Remove bib fields highlighter  * Pass LuceneManager to search groups  * Fix performance issues by caching matched entries  * Update GroupDialogViewModelTest.java  * Update main table matches  * Fix groups icon  * Restore Search.g4 and GrammarBasedSearchRule  * First version of search group migration  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Add groups field to the index  * Remove search rules  * Localization  * Add test cases  * Fix names  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Add some more functionality  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Always add "all" prefix  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Add comment for alternative implementation  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Mark library tab changed after migration  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Add another test for regular expression  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Small fixes  * Fix markBaseChanged  * Fix adding new entries did not update MatchCategory  * Fix searching for Non-ASCII characters  * Fix escaping special characters  Use WhitespaceTokenizer instead of StandardTokenizer https://stackoverflow.com/a/6119584/21694752  * Fix tests  Co-Authored-By: Oliver Kopp <kopp.dev@gmail.com>  * Add first draft of LatexToUnicodeFoldingFilter  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Fix LatexToUnicodeFoldingFilter  Co-Authored-By: Oliver Kopp <kopp.dev@gmail.com>  * Remove LatexToUnicode from SearchQuery  * Localization  * AllowedToUseLogic  * Update CHANGELOG.md  * Use sentence case for search result heading  * Add CHANGELOG for change in JabRefFrameViewModel  * Add more changes to CHANGELOG.md  * Add ADR-0038  * Rename "SCORE" to "MATCH_SCORE"  * Add link to ADR-0038  * Add another CHANGELOG.md entry  * Add CHANGELOG.md entry  * Revert change of filename  * Add JavaDoc comment  * Trying to find better names  * Discard changes to src/main/resources/tinylog.properties  * Remove commented out code  * Remove obsolete testing class  * Remove obsolete test  * Discard changes to src/test/resources/tinylog-test.properties  * Remove completely disabled code  * Rename "all" to "any"  * Catch thrown exception  Invalid regex queries throws an exception  * Remove groups field from the default field  https://github.com/JabRef/jabref/issues/7996  * Remove SearchGroupsListener  * Update Benchmarks.java  * Update module-info.java  * Fixes from code review on LibraryTab  * Remove regex button from search bar  * Use BibEntry.getId instead of System.identityHashCode  * Add BibEntry index map  * Readd option  * Add `@ADR` annotation  * Add some comment  * One more annotation  * Add CHANGELOG.md entry  * One more annotation  * Add CHANGELOG.md entry  * Revert "Add BibEntry index map"  This reverts commit 27ed10599d826af18995c0de61ff480c1ff90274.  * Use binary search to find the index of the entry  * openrewrite  * Tests for LinkedFilesIndexer  * Fix DatabaseSearcher  * LocalizationConsistencyTest  * DatabaseSearcherWithBibFilesTest  * Fix typo in CHANGELOG.md  * Fix typo  * Use parameterized test for DatabaseSearcherTest  * Fix DatabaseSearcherWithBibFiles tests  * Fix exportMatches test  * Remove regex check box from search groups dialog  * JavaDoc  * Fix SearchGroups test  * Remove closeAndWait methods and use CurrentThreadTaskExecutor  * Fix architecture test  * Allow to use logic  * Add debug logging for search  * Add more logging  * Assert with containsInAnyOrder  * Fix DatabaseSearcher test  * Global search dialog  * Rename method  * Improve code quality  - Maintain a map of BibEntryId to BibEntry. - Store search results within SearchQuery instead of using the map in StateManager. - Remove LuceneManager from SearchGroups. - Use a different Analyzer for PDFs.  * Use non-static preferences variables  * Update CHANGELOG.md  * Delete SearchGroupTest.java  * fix typo  * fix indentation  * Update matchedEntries on the UI thread  matchedEntries should be updated on the UI thread because the size binding of matchedEntries will be reflected in the UI.  * Discard changes to src/main/java/org/jabref/gui/importer/actions/GUIPostOpenAction.java  * Fix LoayGhreeb#12  * Sync search flags between search bar and global search bar  * Move VERSION_6_0_ALPHA const to SearchGroupsMigrationAction  * Refactor LuceneSearcher  * Use linked files analyzer for highlighting full-text results  * Fix line break  * Fix tests  * Use EnglishAnalyzer for indexing/searching linked files  https://github.com/apache/lucene/blob/68cc8734ca28a9db800e4192a636d3b490cfd41a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java#L101-L110  * Ask to wait for linked files indexing on shutdown  When closing JabRef  only ask users to wait for the linked files indexer to finish. The bib fields indexer is recalculated on startup  so it doesn't need to be completed before shutdown.  * Use EdgeNGram instead of NGram  * Return comment  * Update CHANGELOG.md  ---------  Co-authored-by: Benedikt Tutzer <btut@users.noreply.github.com> Co-authored-by: Christoph <siedlerkiller@gmail.com> Co-authored-by: Carl Christian Snethlage <50491877+calixtus@users.noreply.github.com> Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>
JabRef,jabref,db9f83cf77b1a5215e729d3c336371b4edef04a4,https://github.com/JabRef/jabref/commit/db9f83cf77b1a5215e729d3c336371b4edef04a4,Search floating mode (#11510)  * Search/Groups floating mode  * Hide search rank column  * Update query listener from global to library-specific  * Update selected groups listener from global to library-specific  * Move table-row CSS classes to Base.css  * Adapt tests  * Update JabRef_en.properties  * CHANGELOG  * Fix jumpToSearchKey  * Add shortcut to scroll to the next/prev rank  Left  right arrows  * Localization  * Fix scroll shortcut to handle rank gaps  * OpenRewrite  * Add temporary MappedBackedList implementation  * Use constants for rank values  * Update rank colors  * Add CustomFilteredList  * Improve group switching and search performance  - Update matches in the background - Prevent unnecessary search query rechecks when switching groups  * OpenRewrite  * Fix NPE  * Fix NPE  * Create a list of observables  * refilter the list after updateVisibility  * Add onUpdateCallback to the CustomFilteredList  * iterate over updated range  * Rename onUpdateCallback to onUpdate  * Register events to the row  * Update matches in the background  * Delete MappedBackedList.java  * Update matches in the background for global search  * Pass properties to MainTableDataModel instead of LibraryTab  * Remove search rank column from the preferences  * Add SearchRank enum  * EnumSet constructor  * Remove int value from SearchRank enum  * Move SearchRank enum to search package  * Remove FILTERING_SEARCH from search flags  * Remove KEEP_SEARCH_STRING from search flags  * Fix SearchPreferences constructor  * Move comment up  * Rename SearchRank to MatchCategory  * Update src/main/java/org/jabref/gui/util/CustomFilteredList.java  Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>  * Update src/main/java/org/jabref/gui/util/CustomFilteredList.java  Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>  * Update CustomFilteredList.java  * Update PreferencesMigrations.java  * Replace CustomFilteredList.java with reflection  Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>  * Minor stylistic fixes  * Fix reflection  Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>  * Correct typo  * Fix typo  * Fix unit tests  * CHANGELOG  ---------  Co-authored-by: Carl Christian Snethlage <50491877+calixtus@users.noreply.github.com> Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>
JFormDesigner,FlatLaf,0ad3180b10341623ce3cf41a481e024b3f96469f,https://github.com/JFormDesigner/FlatLaf/commit/0ad3180b10341623ce3cf41a481e024b3f96469f,FileChooser: improved performance when navigating to large directories with thousands of files (issue #953)
tencentmusic,supersonic,9a1fac5d4cef14cf5c673778d06ed0909bb8dcea,https://github.com/tencentmusic/supersonic/commit/9a1fac5d4cef14cf5c673778d06ed0909bb8dcea,(improvement)(chat) Reduce frequent loading of embedding models to improve loading performance. (#1478)
tencentmusic,supersonic,ae34c15c9595e1c7f8662b5e4badaa96c6b511d3,https://github.com/tencentmusic/supersonic/commit/ae34c15c9595e1c7f8662b5e4badaa96c6b511d3,(improvement)(chat) Improve vector recall performance. (#1458)
apache,kylin,56a189a5856cf979135065b1385760ea414db6b8,https://github.com/apache/kylin/commit/56a189a5856cf979135065b1385760ea414db6b8,KYLIN-6009 fix api performance
apache,kylin,220a2dffd639af5a333b21e933f841791af5fc1e,https://github.com/apache/kylin/commit/220a2dffd639af5a333b21e933f841791af5fc1e,KYLIN-5952 Support recommendation for kylin5  **SQL-Based Model and Index Creation**  1. **Quick Model Creation:** Upload SQL files directly through the front-end interface to rapidly create models and generate corresponding indexes.  2. **Index Optimization Recommendations:** Leverage existing models to recommend and optimize indexes  improving query performance.  3. **Historical Query Analysis:** Generate index optimization suggestions based on query history. Select the most profitable recommendations through statistical analysis  which users can review and approve via the front-end interface.  4. **Unnecessary Index Elimination:** Use statistical insights to identify and recommend the removal of redundant indexes.  5. **Enhanced Left Join Models:** For left join models  add new dimension tables by configuring `kylin.smart.conf.model-opt-rule=append`.  6. **Optimized Aggregation Indexes:** For excluded columns  configure `kylin.smart.conf.propose-all-foreign-keys=false` to maximize the aggregation of recommended indexes.  7. **Further Configuration:** For additional recommended settings  refer to the full range of SmartConfig configurations and specific settings within the `KylinConfigBase` class.  --------- Co-authored-by: Yifei.Wu <vafuler945@gmail.com> Co-authored-by: Yifan Zhang <event.dimlas@gmail.com> Co-authored-by: Zhiting Guo <35057824+frearb@users.noreply.github.com> Co-authored-by: huangfeng1993 <715187657@qq.com> Co-authored-by: Junqing Cai <caicai121723@163.com> Co-authored-by: Jiale He <965374246@qq.com> Co-authored-by: Liang.Hua <36814772+jacobhua@users.noreply.github.com> Co-authored-by: lixiang <447399170@qq.com> Co-authored-by: Ruixuan Zhang <ruixuan.zhang@kyligence.io> Co-authored-by: lionelcao <whucaolu@gmail.com> Co-authored-by: Xuecheng Shan <shanxuecheng@gmail.com> Co-authored-by: Zhimin Wu <596361258@qq.com> Co-authored-by: feng.zhu <fishcus@outlook.com> Co-authored-by: Cheng Hao <chenghao2262@users.noreply.github.com> Co-authored-by: xinbei <xinbei.fu.gr@dartmouth.edu> Co-authored-by: bingfeng.guo <546745169@qq.com> Co-authored-by: lxian2shell <lxian2shell@gmail.com>
apache,kylin,205a100e174a0481b9ae1404adaac838295713be,https://github.com/apache/kylin/commit/205a100e174a0481b9ae1404adaac838295713be,KYLIN-5899 Optimization of job table and transaction  1. Add index of project and model_id for job table. 2. Ops booter add async-profiler-lib. 3. transparent transaction should skip fetch memory lock. 4. Optimize the performance of accessing the job_info table.  --------- Co-authored-by: Xuecheng Shan <xuecheng.shan@kyligence.io> Co-authored-by: sibing.zhang <sibing.zhang@qq.com>
spotbugs,spotbugs,1b42dde322dc43e039a45208d1a689203fb4badb,https://github.com/spotbugs/spotbugs/commit/1b42dde322dc43e039a45208d1a689203fb4badb,fix: buffer ViewCFG's output to improve performance (#3407)
spotbugs,spotbugs,9ce74c0793af7725a9f183b98ae60bf4b25dfc5e,https://github.com/spotbugs/spotbugs/commit/9ce74c0793af7725a9f183b98ae60bf4b25dfc5e,perf: avoid calling File.getCanonicalPath twice to improve performance
camunda,camunda,e2fa252130abad8887c72b72bd021ce1b6b4e7a6,https://github.com/camunda/camunda/commit/e2fa252130abad8887c72b72bd021ce1b6b4e7a6,feat: make IncidentUpdateTask async (#28255)  ## Description The purpose of this PR is refactor the IncidentUpdateTask in a way that it does not block any threads with `.join()` calls and try to parallelize as much as possible. In order to perform causal ordering when processing incidents  only one incident is processed at a given time. A couple of tasks have been parallelized and are mostly about fetching necessary data before processing the incidents: - `searchForInstances > checkDataAndCollectParentTreePaths` : data is collected in parallel - `processIncident > createFlowNodeInstanceUpdates`: flow node instances are fetched in parallel if necessary  All async operation like `thenComposeAsync` are done in the provided executor  which is the executor created in `BackgroundManager`. The size of the executor has been kept the same even if the task is not blocking anymore  to avoid starving other threads. Considering that some tasks may be spawned in parallel  this task could use more than 1 thread  but in general it should be non-blocking so it should not use 1 thread in average.  A simple mechanism for trying to gracefully shutdown this tasks have been added: before rescheduling any task check if it's been cancelled and it if it  it will not reschedule itself. The executor is then terminated "gracefully" with `shutdown` first  giving some time for the tasks to stop gracefully before running `shutdownNow`  if they are not yet terminated. This mechanism can be removed if not deemed necessary  ### Comments The code is much more hard to follow and understand after this change and the parallelism that we gain is really just limited to a couple of places. So I think it would be ok if we decide to keep using a synchronous style except for those two places  where we can actually gain some performance.  ## Related issues closes #27894
camunda,camunda,94d32e7299406d8b2458124b7af0e0cfc153bb7f,https://github.com/camunda/camunda/commit/94d32e7299406d8b2458124b7af0e0cfc153bb7f,31055 add dedicated call hierarchy endpoint to v2 api (#31219)  ## Description  <!-- Describe the goal and purpose of this PR. --> This PR implements the process instance call hierarchy feature for the V2 API by addressing both the API layer for the ES/OS and RDBMS backends.  ### 🔍 What’s included: - Implements [issue #31055](https://github.com/camunda/camunda/issues/31055): Adds a dedicated REST API endpoint to retrieve the call hierarchy for a given process instance (using ES/OS data store). - Implements [issue #31061](https://github.com/camunda/camunda/issues/31061): Adds logic in the RDBMS exporter to generate and persist the treePath  enabling the call hierarchy data to be stored and queried.  The call hierarchy is essential for supporting features in Operate such as breadcrumb navigation and root instance cancellation. This functionality is intentionally separated from the main /process-instances/{key} endpoint to avoid performance degradation during standard queries.  ---  ### Note on Code Duplication  To integrate this feature with the RDBMS backend in a self-contained and milestone-friendly manner  I mirrored logic previously written for ES/OS. The following classes and methods were adapted and copied into the RDBMS module: - CachedProcessEntity - ExporterEntityCache interface - ProcessCacheUtil - createTreePath(...) from ProcessInstanceExporterHandler  While not ideal  this duplication was accepted for pragmatic reasons. A follow-up task is planned ([#31218](https://github.com/camunda/camunda/issues/31218)) to extract this shared logic into a reusable module (camunda-domain-common) to ensure long-term maintainability.  ### Circular Dependency Handling  Originally  these changes were split across two separate PRs (#31219 and #31291). However  due to mutual dependencies between the API contract and the exporter logic  they have now been merged into this single PR to enable a clean review and integration.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria) - [ ] enable backports [when recommended](https://github.com/camunda/camunda/wiki/CI-&-Automation#when-to-backport-ci-changes)  ## Related issues  closes #31055  #31061
camunda,camunda,69e5c593fc71c566d5bd76202f0d23550cce9551,https://github.com/camunda/camunda/commit/69e5c593fc71c566d5bd76202f0d23550cce9551,refactor: retrieve `userTaskKey` from element instance  Changed the approach for retrieving the user task key during `CREATING` task listener job completion. Instead of extracting the key from the job's custom headers  it is now retrieved directly from the associated `ElementInstance` in the state.  Accessing the element instance state might cause an additional RocksDB lookup  but it may be cached internally and is unlikely to cause a noticeable performance impact.  If the element instance is missing (which should not happen in normal flow) or the user task key is invalid or uninitialized (<= 0)  an `IllegalStateException` is thrown to indicate an unexpected internal issue and to provide additional context for debugging.
camunda,camunda,558aecaece644de507bdf0fe80a7425322e59131,https://github.com/camunda/camunda/commit/558aecaece644de507bdf0fe80a7425322e59131,[Backport main] add verbose flag to operate backup list API to improve performance if `verbose=false` (#30849)  # Description Backport of #30730 to `main`.  > [!WARNING] Because of the changes in the webapps backup module the backport has been mostly manual  - Added verbose flag to OpenAPI backup spec - Added tests in BackupRestoreIT checking that the backups are completed with both `verbose=false` & `verbose=true`  relates to #30695
camunda,camunda,008e80952483bde5f823db7e17cfa26b8af2c647,https://github.com/camunda/camunda/commit/008e80952483bde5f823db7e17cfa26b8af2c647,perf: use an `EnumMap`
camunda,camunda,50a967afff8c67739e8c0437ca194e351bf5046d,https://github.com/camunda/camunda/commit/50a967afff8c67739e8c0437ca194e351bf5046d,feat: add verbose flag to operate backup list API to improve performance  Adding verbose to the request sent to ES/OS speeds up the query by a lot  but we don't get startTime and metadata fields populated. startTime will be null  while metadata can be extracted from the snapshot name if metadata field is missing. closes #30695  (cherry picked from commit fe1e6e30eab1fb124d87d4a0a4811d0e24428f44)
camunda,camunda,464b6174dacd942d8493e070c9223adbbc85357f,https://github.com/camunda/camunda/commit/464b6174dacd942d8493e070c9223adbbc85357f,feat: remove a limiter on appends based on avg append latency (#30353)  ## Description The condition `(now() - (appendLatency.mean() / maxAppendsPerMember) >= appendTimeStart)` may be superfluous as we already limit the number of inflight requests to maxAppendsPerMember.  Looking into the implementation of `DescriptiveStatistics` class it's not been implemented with performance in mind  as it create a lot of temporary `double[] ` instead of using a fixed size circular buffer.  A better solution would be to implement #30379  but it's outside the scope of this PR  FYI @ChrisKujawa
camunda,camunda,819c0b70ac6356f1a49e0f7f3fb6ee33a8feee4c),[solution](https://github.com/camunda/camunda/commit/819c0b70ac6356f1a49e0f7f3fb6ee33a8feee4c),Improve archiving performance (#30684)  ## Description  It was [highlighted](https://camunda.slack.com/archives/C08D74J6HUG/p1740412731557659) that the archiving is too slow to catch up with exporting/processing. We were able to complete a lot of process instances  and also to export them to ES. But they haven't been moved to the dated indices quickly enough  causing to fill up ES at some point.  We can see in the following metric that we complete per partition ~50 process instances (on the medic benchmarks)  but only ~5% are actually moved to the dated indices  where they can be deleted by ILM later.  ![archiver-lagging-behind](https://github.com/user-attachments/assets/354d052e-abc1-42b1-8526-4d43b74c0e58)  We haven't had not much observability around this topic and missed some visualization (previously we added some panels via #30679). On top of this  the PR adds some more panels to better visualize the archiving  and how many instances are actually found to be archived.  This allowed better to pinpoint where the bottleneck was  querying the completed process instances. Especially that it is not easy to fine-tune related `rolloverBatchSize` as it would simply stop finding new results  ![2025-04-07_14-46](https://github.com/user-attachments/assets/5106cf19-621f-4711-b7e4-946bd963ed48)  It turned out to be an issue with the used aggregation  also highlighted by @lenaschoenburg [previously](https://camunda.slack.com/archives/C08D74J6HUG/p1744033725997889?thread_ts=1744030148.764179&cid=C08D74J6HUG).  With the new metric and the
camunda,camunda,95b11c1aa811793e64a4a3d468aea7efc1c653a3,https://github.com/camunda/camunda/commit/95b11c1aa811793e64a4a3d468aea7efc1c653a3,feat: invalidate caches instead of updating them to avoid upfront performance cost
camunda,camunda,c34066fc76ca0056378a929f7b1f1a9ed78f48cf,https://github.com/camunda/camunda/commit/c34066fc76ca0056378a929f7b1f1a9ed78f48cf,feat: remove a limiter on appends based on avg append latency  The condition (now() - (appendLatency.mean() / maxAppendsPerMember) >= appendTimeStart) may be superfluous as we already limit the number of inflight requests to maxAppendsPerMember.  Looking into the implementation of DescriptiveStatistics class it's not been implemented with performance in mind  as it create a lot of temporary double[] instead of using a fixed size circular buffer.
camunda,camunda,8dcb9d7f21e0010e2c9ff688d05bb667ab63b3de,https://github.com/camunda/camunda/commit/8dcb9d7f21e0010e2c9ff688d05bb667ab63b3de,feat: introducing `deleteIntermediateStateIfExists` method  Reason: Allow deletion of intermediate state without additional checks in order to benefit performance.
camunda,camunda,14000162ae35335ba3fbd9156dea48806b4a4fa3,https://github.com/camunda/camunda/commit/14000162ae35335ba3fbd9156dea48806b4a4fa3,perf: avoid singleton set when caching a single key
camunda,camunda,15b0d5ac6ce7036849a477c26ae4d6dd5993887f,https://github.com/camunda/camunda/commit/15b0d5ac6ce7036849a477c26ae4d6dd5993887f,perf: rollback only locks cache once
camunda,camunda,c9eca7ec17c7fcb13dcecd0b53a49e595fae9408,https://github.com/camunda/camunda/commit/c9eca7ec17c7fcb13dcecd0b53a49e595fae9408,Raise incident at specific called process depth (#22735)  ## Description  <!-- Describe the goal and purpose of this PR. -->  This is the effort we made during our mob programming session. We added a test case with a failsafe to ensure the process instance doesn't overload the test engine. We also added a simple implementation to check the called process depth.  Still to do: - actually increment the depth - make the depth configurable - consider how to deal with existing child instances - configure the depth in the test for performance  ## Related issues  closes #16410
camunda,camunda,18a73e46433806806a942db06c4a2d114a3c3900,https://github.com/camunda/camunda/commit/18a73e46433806806a942db06c4a2d114a3c3900,perf: correctly declare number of properties  This helps avoid unnecessary allocations and memory copies for the declared properties.  Co-authored-by: Dmitriy Melnychuk <dmitriy.melnychuk@capgemini.com> Co-authored-by: Remco <remco@westerhoud.nl> Co-authored-by: Stephan Epping <stephan.epping@camunda.com> Co-authored-by: berkaycanbc <berkay.can@camunda.com>
camunda,camunda,9be05f33b7dc214568f8785be35c43da874bd363,https://github.com/camunda/camunda/commit/9be05f33b7dc214568f8785be35c43da874bd363,fix: start calledProcessDepth at 0  There's a reason to keep it at -1  if you want to differentiate between the calledProcessDepth not set  and it just being set by default.  The -1 case could've been useful to ensure we set the calledProcessDepth correctly for child process instance created after 8.7 that are part of root process instance created prior to 8.7. However  this comes at the cost of performance  as a lookup is needed for each called level.  Instead  we simply set the default to 0  to ensure that all element instances created prior to 8.7  have it set to 0.  This has the added benefit that we do not have to explicitly set the value in other places (like test code).  Co-authored-by: Dmitriy Melnychuk <dmitriy.melnychuk@capgemini.com> Co-authored-by: Remco <remco@westerhoud.nl> Co-authored-by: Stephan Epping <stephan.epping@camunda.com> Co-authored-by: berkaycanbc <berkay.can@camunda.com>
camunda,camunda,ed17ba8adfdce47a4a3431dd33d7ed82f40bd23f,https://github.com/camunda/camunda/commit/ed17ba8adfdce47a4a3431dd33d7ed82f40bd23f,feat: check for depth  This is the effort we made during our mob programming session. We added a test case with a failsafe to ensure the process instance doesn't overload the test engine. We also added a simple implementation to check the called process depth.  Still to do: - actually increment the depth - make the depth configurable - consider how to deal with existing child instances - configure the depth in the test for performance  Co-authored-by: berkaycanbc <berkay.can@camunda.com> Co-authored-by: ana.vinogradova <ana.vinogradova@camunda.com> Co-authored-by: Remco Westerhoud <remcowesterhoud@msn.com>
camunda,camunda,acd48487191a817c2572ee23ba87d4a5cd7ccd76,https://github.com/camunda/camunda/commit/acd48487191a817c2572ee23ba87d4a5cd7ccd76,deps: Update spring boot to v3.4.1 (main) (#25062)  This PR contains the following updates:  | Package | Change | Age | Adoption | Passing | Confidence | |---|---|---|---|---|---| | [org.springframework.boot:spring-boot-dependencies](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.5` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-dependencies/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-dependencies/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-dependencies/3.3.5/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-dependencies/3.3.5/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-maven-plugin](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-maven-plugin/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-maven-plugin/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-maven-plugin/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-maven-plugin/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter-webflux](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter-webflux/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter-webflux/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter-webflux/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter-webflux/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter-test](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter-test/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter-test/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter-test/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter-test/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-dependencies](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-dependencies/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-dependencies/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-dependencies/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-dependencies/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter-json](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter-json/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter-json/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter-json/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter-json/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter-actuator](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter-actuator/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter-actuator/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter-actuator/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter-actuator/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-configuration-processor](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-configuration-processor/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-configuration-processor/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-configuration-processor/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-configuration-processor/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-test](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-test/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-test/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-test/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-test/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-actuator-autoconfigure](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-actuator-autoconfigure/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-actuator-autoconfigure/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-actuator-autoconfigure/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-actuator-autoconfigure/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-autoconfigure](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-autoconfigure/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-autoconfigure/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-autoconfigure/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-autoconfigure/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-actuator](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-actuator/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-actuator/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-actuator/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-actuator/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter-jersey](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter-jersey/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter-jersey/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter-jersey/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter-jersey/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) |  ---  > [!WARNING] > Some dependencies could not be looked up. Check the Dependency Dashboard for more information.  ---  ### Release Notes  <details> <summary>spring-projects/spring-boot (org.springframework.boot:spring-boot-dependencies)</summary>  ### [`v3.4.1`](https://redirect.github.com/spring-projects/spring-boot/compare/v3.4.0...v3.4.1)  ### [`v3.4.0`](https://redirect.github.com/spring-projects/spring-boot/releases/tag/v3.4.0)  ##### :star: New Features  - Add withDefaultRequestConfigCustomizer method to HttpComponentsClientHttpRequestFactoryBuilder [#&#8203;43139](https://redirect.github.com/spring-projects/spring-boot/issues/43139) - Fail JsonWriter if duplicate names are detected [#&#8203;43041](https://redirect.github.com/spring-projects/spring-boot/issues/43041) - Add JsonObjectDeserializer.nullSafeValue method that accepts a mapper Function [#&#8203;42972](https://redirect.github.com/spring-projects/spring-boot/issues/42972) - Support timeout property for GraphQL over SSE [#&#8203;42966](https://redirect.github.com/spring-projects/spring-boot/pull/42966) - Improve performance of ConfigurationPropertiesBinder by storing bind handlers on first access [#&#8203;42950](https://redirect.github.com/spring-projects/spring-boot/issues/42950) - Improve performance of ConcurrentReferenceCachingMetadataReaderFactory [#&#8203;42949](https://redirect.github.com/spring-projects/spring-boot/issues/42949) - Log warning in HikariCheckpointRestoreLifecycle if pool suspension isn't configured [#&#8203;42937](https://redirect.github.com/spring-projects/spring-boot/pull/42937) - Remove spring-boot-starter-aop dependency from spring-boot-starter-data-jpa and spring-boot-starter-integration [#&#8203;42934](https://redirect.github.com/spring-projects/spring-boot/issues/42934)  ##### :lady_beetle: Bug Fixes  - Jersey body handling is inconsistent with Spring Webflux and Spring MVC [#&#8203;43209](https://redirect.github.com/spring-projects/spring-boot/issues/43209) - Classes are accidentally named "structure logging" instead of "structured logging" [#&#8203;43203](https://redirect.github.com/spring-projects/spring-boot/pull/43203) - StructuredLoggingJsonProperties customizer should be a Class reference rather than a String [#&#8203;43202](https://redirect.github.com/spring-projects/spring-boot/issues/43202) - Cannot package OCI image when 'docker.io/paketobuildpacks/new-relic' is provided as a buildpack [#&#8203;43171](https://redirect.github.com/spring-projects/spring-boot/issues/43171) - Incorrect Type for 'management.endpoints.access.default' defined in additional-spring-configuration-metadata.json [#&#8203;43154](https://redirect.github.com/spring-projects/spring-boot/issues/43154) - WebServerPortFileWriter fails when using a portfile without extension [#&#8203;43117](https://redirect.github.com/spring-projects/spring-boot/issues/43117) - SslOptions.isSpecified() only returns true if ciphers and enabled protocols are set [#&#8203;43084](https://redirect.github.com/spring-projects/spring-boot/issues/43084) - SslHealthIndicator throws NullPointerException when using SslBundle with SslStoreBundle.NONE [#&#8203;43078](https://redirect.github.com/spring-projects/spring-boot/issues/43078) - JdkClientHttpRequestFactoryBuilder and JettyClientHttpRequestFactoryBuilder do not set Ciphers or Enabled Protocols [#&#8203;43077](https://redirect.github.com/spring-projects/spring-boot/issues/43077) - Root cause of errors is hidden when loading images from archive [#&#8203;43070](https://redirect.github.com/spring-projects/spring-boot/issues/43070) - mvn spring-boot:run fails on Windows with "Could Not Find or Load Main Class" when path contains non-ASCII characters [#&#8203;43062](https://redirect.github.com/spring-projects/spring-boot/issues/43062) - A `@SpyBean` on the output of a FactoryBean is not reset [#&#8203;43053](https://redirect.github.com/spring-projects/spring-boot/issues/43053) - Logback logging system does not process URLs with paths not ending in .xml [#&#8203;42990](https://redirect.github.com/spring-projects/spring-boot/issues/42990) - Bean-based conditions do not consider factory beans correctly when determining if they are a candidate [#&#8203;42970](https://redirect.github.com/spring-projects/spring-boot/issues/42970) - NPE in bootBuildImage when setting DOCKER_CONTEXT=default [#&#8203;42960](https://redirect.github.com/spring-projects/spring-boot/issues/42960) - Warning due to duplicate MockResolver extensions [#&#8203;42957](https://redirect.github.com/spring-projects/spring-boot/issues/42957) - HttpHostConnectException is thrown when using buildpacks with Gradle or Maven on Windows [#&#8203;42952](https://redirect.github.com/spring-projects/spring-boot/issues/42952) - build-info doesn't support seconds since the epoch from project.build.outputTimestamp [#&#8203;42936](https://redirect.github.com/spring-projects/spring-boot/issues/42936) - NPE in OnClassCondition.resolveOutcomesThreaded following thread interruption because firstHalf is null [#&#8203;42926](https://redirect.github.com/spring-projects/spring-boot/issues/42926) - Default WebSocketMessageBrokerConfigurer is always overriding custom channel executor [#&#8203;42924](https://redirect.github.com/spring-projects/spring-boot/issues/42924) - X-Registry-Auth header sent to Docker Engine API contains field "authHeader" [#&#8203;42915](https://redirect.github.com/spring-projects/spring-boot/issues/42915) - ApplicationContextRunner has inconsistent behaviour with duplicate auto-configuration class names [#&#8203;17963](https://redirect.github.com/spring-projects/spring-boot/issues/17963)  ##### :notebook_with_decorative_cover: Documentation  - Migrate class references to full javadoc links [#&#8203;43239](https://redirect.github.com/spring-projects/spring-boot/issues/43239) - Documentation for 'spring.datasource.type' is misleading [#&#8203;43199](https://redirect.github.com/spring-projects/spring-boot/issues/43199) - Update "Upgrading From" section to use "2.x" [#&#8203;43160](https://redirect.github.com/spring-projects/spring-boot/issues/43160) - Include spring-boot-loader in API documentation [#&#8203;43153](https://redirect.github.com/spring-projects/spring-boot/issues/43153) - Document how and where to add custom GraalVM configuration files [#&#8203;43074](https://redirect.github.com/spring-projects/spring-boot/issues/43074) - Rework DataSource configuration examples to separate defining an additional DataSource and defining a DataSource of a different type [#&#8203;43059](https://redirect.github.com/spring-projects/spring-boot/issues/43059) - Location of the layers schema is incorrect in the Maven Plugin's examples [#&#8203;43033](https://redirect.github.com/spring-projects/spring-boot/issues/43033) - Link to Eclipse setup instructions [#&#8203;42954](https://redirect.github.com/spring-projects/spring-boot/issues/42954) - Fix link to Checkpoint and Restore status page [#&#8203;42939](https://redirect.github.com/spring-projects/spring-boot/issues/42939)  ##### :hammer: Dependency Upgrades  - Upgrade to ActiveMQ 6.1.4 [#&#8203;43128](https://redirect.github.com/spring-projects/spring-boot/issues/43128) - Upgrade to Byte Buddy 1.15.10 [#&#8203;43097](https://redirect.github.com/spring-projects/spring-boot/issues/43097) - Upgrade to Couchbase Client 3.7.5 [#&#8203;43098](https://redirect.github.com/spring-projects/spring-boot/issues/43098) - Upgrade to Elasticsearch Client 8.15.4 [#&#8203;43129](https://redirect.github.com/spring-projects/spring-boot/issues/43129) - Upgrade to Flyway 10.20.1 [#&#8203;43130](https://redirect.github.com/spring-projects/spring-boot/issues/43130) - Upgrade to Groovy 4.0.24 [#&#8203;43099](https://redirect.github.com/spring-projects/spring-boot/issues/43099) - Upgrade to Hibernate 6.6.2.Final [#&#8203;43100](https://redirect.github.com/spring-projects/spring-boot/issues/43100) - Upgrade to HttpClient5 5.4.1 [#&#8203;43102](https://redirect.github.com/spring-projects/spring-boot/issues/43102) - Upgrade to Infinispan 15.0.11.Final [#&#8203;43131](https://redirect.github.com/spring-projects/spring-boot/issues/43131) - Upgrade to Jackson Bom 2.18.1 [#&#8203;43103](https://redirect.github.com/spring-projects/spring-boot/issues/43103) - Upgrade to Jetty 12.0.15 [#&#8203;43104](https://redirect.github.com/spring-projects/spring-boot/issues/43104) - Upgrade to jOOQ 3.19.15 [#&#8203;43105](https://redirect.github.com/spring-projects/spring-boot/issues/43105) - Upgrade to Kafka 3.8.1 [#&#8203;43106](https://redirect.github.com/spring-projects/spring-boot/issues/43106) - Upgrade to Lettuce 6.4.1.RELEASE [#&#8203;43185](https://redirect.github.com/spring-projects/spring-boot/issues/43185) - Upgrade to Logback 1.5.12 [#&#8203;43107](https://redirect.github.com/spring-projects/spring-boot/issues/43107) - Upgrade to Lombok 1.18.36 [#&#8203;43186](https://redirect.github.com/spring-projects/spring-boot/issues/43186) - Upgrade to Maven Dependency Plugin 3.8.1 [#&#8203;43108](https://redirect.github.com/spring-projects/spring-boot/issues/43108) - Upgrade to Maven Failsafe Plugin 3.5.2 [#&#8203;43109](https://redirect.github.com/spring-projects/spring-boot/issues/43109) - Upgrade to Maven Surefire Plugin 3.5.2 [#&#8203;43110](https://redirect.github.com/spring-projects/spring-boot/issues/43110) - Upgrade to Micrometer 1.14.1 [#&#8203;43187](https://redirect.github.com/spring-projects/spring-boot/issues/43187) - Upgrade to Micrometer Tracing 1.4.0 [#&#8203;43120](https://redirect.github.com/spring-projects/spring-boot/issues/43120) - Upgrade to MongoDB 5.2.1 [#&#8203;43111](https://redirect.github.com/spring-projects/spring-boot/issues/43111) - Upgrade to Netty 4.1.115.Final [#&#8203;43133](https://redirect.github.com/spring-projects/spring-boot/issues/43133) - Upgrade to Prometheus Client 1.3.3 [#&#8203;43112](https://redirect.github.com/spring-projects/spring-boot/issues/43112) - Upgrade to Pulsar Reactive 0.5.9 [#&#8203;43188](https://redirect.github.com/spring-projects/spring-boot/issues/43188) - Upgrade to Reactor Bom 2024.0.0 [#&#8203;43015](https://redirect.github.com/spring-projects/spring-boot/issues/43015) - Upgrade to Spring AMQP 3.2.0 [#&#8203;43016](https://redirect.github.com/spring-projects/spring-boot/issues/43016) - Upgrade to Spring Authorization Server 1.4.0 [#&#8203;43017](https://redirect.github.com/spring-projects/spring-boot/issues/43017) - Upgrade to Spring Batch 5.2.0 [#&#8203;43018](https://redirect.github.com/spring-projects/spring-boot/issues/43018) - Upgrade to Spring Data Bom 2024.1.0 [#&#8203;43019](https://redirect.github.com/spring-projects/spring-boot/issues/43019) - Upgrade to Spring Framework 6.2.0 [#&#8203;43020](https://redirect.github.com/spring-projects/spring-boot/issues/43020) - Upgrade to Spring HATEOAS 2.4.0 [#&#8203;43021](https://redirect.github.com/spring-projects/spring-boot/issues/43021) - Upgrade to Spring Integration 6.4.0 [#&#8203;43022](https://redirect.github.com/spring-projects/spring-boot/issues/43022) - Upgrade to Spring Kafka 3.3.0 [#&#8203;43023](https://redirect.github.com/spring-projects/spring-boot/issues/43023) - Upgrade to Spring LDAP 3.2.8 [#&#8203;43189](https://redirect.github.com/spring-projects/spring-boot/issues/43189) - Upgrade to Spring Pulsar 1.2.0 [#&#8203;43024](https://redirect.github.com/spring-projects/spring-boot/issues/43024) - Upgrade to Spring RESTDocs 3.0.3 [#&#8203;43025](https://redirect.github.com/spring-projects/spring-boot/issues/43025) - Upgrade to Spring Security 6.4.1 [#&#8203;43232](https://redirect.github.com/spring-projects/spring-boot/issues/43232) - Upgrade to Spring Session 3.4.0 [#&#8203;43027](https://redirect.github.com/spring-projects/spring-boot/issues/43027) - Upgrade to Testcontainers 1.20.4 [#&#8203;43243](https://redirect.github.com/spring-projects/spring-boot/issues/43243) - Upgrade to Tomcat 10.1.33 [#&#8203;43134](https://redirect.github.com/spring-projects/spring-boot/issues/43134) - Upgrade to Undertow 2.3.18.Final [#&#8203;43166](https://redirect.github.com/spring-projects/spring-boot/issues/43166) - Upgrade to WebJars Locator Lite 1.0.1 [#&#8203;43135](https://redirect.github.com/spring-projects/spring-boot/issues/43135)  ##### :heart: Contributors  Thank you to all the contributors who worked on this release:  [@&#8203;ahoehma](https://redirect.github.com/ahoehma)  [@&#8203;deki](https://redirect.github.com/deki)  [@&#8203;izeye](https://redirect.github.com/izeye)  [@&#8203;ngocnhan-tran1996](https://redirect.github.com/ngocnhan-tran1996)  [@&#8203;nosan](https://redirect.github.com/nosan)  [@&#8203;quaff](https://redirect.github.com/quaff)  and [@&#8203;wickdynex](https://redirect.github.com/wickdynex)  ### [`v3.3.7`](https://redirect.github.com/spring-projects/spring-boot/compare/v3.3.6...v3.3.7)  ### [`v3.3.6`](https://redirect.github.com/spring-projects/spring-boot/releases/tag/v3.3.6)  [Compare Source](https://redirect.github.com/spring-projects/spring-boot/compare/v3.3.5...v3.3.6)  ##### :warning: Noteworthy  - This release upgrades to OpenTelemetry 1.38.0  see [this issue comment](https://redirect.github.com/spring-projects/spring-boot/issues/43200#issuecomment-2486198324) for more details.  ##### :lady_beetle: Bug Fixes  - Spring Boot 3.3.x dependencies do not converge for Micrometer Tracing and OpenTelemetry [#&#8203;43200](https://redirect.github.com/spring-projects/spring-boot/issues/43200) - Cannot package OCI image when 'docker.io/paketobuildpacks/new-relic' is provided as a buildpack [#&#8203;43170](https://redirect.github.com/spring-projects/spring-boot/issues/43170) - WebServerPortFileWriter fails when using a portfile without extension [#&#8203;43116](https://redirect.github.com/spring-projects/spring-boot/issues/43116) - SslOptions.isSpecified() only returns true if ciphers and enabled protocols are set [#&#8203;43083](https://redirect.github.com/spring-projects/spring-boot/issues/43083) - Root cause of errors is hidden when loading images from archive [#&#8203;43069](https://redirect.github.com/spring-projects/spring-boot/issues/43069) - mvn spring-boot:run fails on Windows with "Could Not Find or Load Main Class" when path contains non-ASCII characters [#&#8203;43051](https://redirect.github.com/spring-projects/spring-boot/issues/43051) - Logback logging system does not process URLs with paths not ending in .xml [#&#8203;42989](https://redirect.github.com/spring-projects/spring-boot/issues/42989) - NPE in bootBuildImage when setting DOCKER_CONTEXT=default [#&#8203;42959](https://redirect.github.com/spring-projects/spring-boot/issues/42959) - build-info doesn't support seconds since the epoch from project.build.outputTimestamp [#&#8203;42935](https://redirect.github.com/spring-projects/spring-boot/issues/42935) - NPE in OnClassCondition.resolveOutcomesThreaded following thread interruption because firstHalf is null [#&#8203;42925](https://redirect.github.com/spring-projects/spring-boot/issues/42925) - X-Registry-Auth header sent to Docker Engine API contains field "authHeader" [#&#8203;42914](https://redirect.github.com/spring-projects/spring-boot/issues/42914) - A `@SpyBean` on the output of a FactoryBean is not reset [#&#8203;31204](https://redirect.github.com/spring-projects/spring-boot/issues/31204)  ##### :notebook_with_decorative_cover: Documentation  - Documentation for 'spring.datasource.type' is misleading [#&#8203;43198](https://redirect.github.com/spring-projects/spring-boot/issues/43198) - Update "Upgrading From" section to use "2.x" [#&#8203;43159](https://redirect.github.com/spring-projects/spring-boot/issues/43159) - Include spring-boot-loader in API documentation [#&#8203;43151](https://redirect.github.com/spring-projects/spring-boot/issues/43151) - Document how and where to add custom GraalVM configuration files [#&#8203;43073](https://redirect.github.com/spring-projects/spring-boot/issues/43073) - Rework DataSource configuration examples to separate defining an additional DataSource and defining a DataSource of a different type [#&#8203;43058](https://redirect.github.com/spring-projects/spring-boot/issues/43058) - Location of the layers schema is incorrect in the Maven Plugin's examples [#&#8203;43032](https://redirect.github.com/spring-projects/spring-boot/issues/43032) - Link to Eclipse setup instructions [#&#8203;42953](https://redirect.github.com/spring-projects/spring-boot/issues/42953) - Fix link to Checkpoint and Restore status page [#&#8203;42938](https://redirect.github.com/spring-projects/spring-boot/issues/42938) - Update HttpWebServiceMessageSenderBuilder javadoc [#&#8203;42893](https://redirect.github.com/spring-projects/spring-boot/issues/42893) - Move default value descriptions to "description" in logging property metadata [#&#8203;42881](https://redirect.github.com/spring-projects/spring-boot/issues/42881)  ##### :hammer: Dependency Upgrades  - Upgrade to ActiveMQ 6.1.4 [#&#8203;43146](https://redirect.github.com/spring-projects/spring-boot/issues/43146) - Upgrade to Groovy 4.0.24 [#&#8203;43095](https://redirect.github.com/spring-projects/spring-boot/issues/43095) - Upgrade to Infinispan 15.0.11.Final [#&#8203;43147](https://redirect.github.com/spring-projects/spring-boot/issues/43147) - Upgrade to Jackson Bom 2.17.3 [#&#8203;43036](https://redirect.github.com/spring-projects/spring-boot/issues/43036) - Upgrade to Jetty 12.0.15 [#&#8203;43093](https://redirect.github.com/spring-projects/spring-boot/issues/43093) - Upgrade to jOOQ 3.19.15 [#&#8203;43037](https://redirect.github.com/spring-projects/spring-boot/issues/43037) - Upgrade to Logback 1.5.12 [#&#8203;43038](https://redirect.github.com/spring-projects/spring-boot/issues/43038) - Upgrade to Lombok 1.18.36 [#&#8203;43181](https://redirect.github.com/spring-projects/spring-boot/issues/43181) - Upgrade to Micrometer 1.13.8 [#&#8203;43182](https://redirect.github.com/spring-projects/spring-boot/issues/43182) - Upgrade to Micrometer Tracing 1.3.6 [#&#8203;43000](https://redirect.github.com/spring-projects/spring-boot/issues/43000) - Upgrade to Netty 4.1.115.Final [#&#8203;43148](https://redirect.github.com/spring-projects/spring-boot/issues/43148) - Upgrade to Pulsar Reactive 0.5.9 [#&#8203;43183](https://redirect.github.com/spring-projects/spring-boot/issues/43183) - Upgrade to Reactor Bom 2023.0.12 [#&#8203;43002](https://redirect.github.com/spring-projects/spring-boot/issues/43002) - Upgrade to Spring AMQP 3.1.8 [#&#8203;43004](https://redirect.github.com/spring-projects/spring-boot/issues/43004) - Upgrade to Spring Data Bom 2024.0.6 [#&#8203;43006](https://redirect.github.com/spring-projects/spring-boot/issues/43006) - Upgrade to Spring Framework 6.1.15 [#&#8203;43008](https://redirect.github.com/spring-projects/spring-boot/issues/43008) - Upgrade to Spring Integration 6.3.6 [#&#8203;43010](https://redirect.github.com/spring-projects/spring-boot/issues/43010) - Upgrade to Spring Kafka 3.2.5 [#&#8203;43011](https://redirect.github.com/spring-projects/spring-boot/issues/43011) - Upgrade to Spring LDAP 3.2.8 [#&#8203;43184](https://redirect.github.com/spring-projects/spring-boot/issues/43184) - Upgrade to Spring Pulsar 1.1.6 [#&#8203;43012](https://redirect.github.com/spring-projects/spring-boot/issues/43012) - Upgrade to Spring RESTDocs 3.0.3 [#&#8203;43014](https://redirect.github.com/spring-projects/spring-boot/issues/43014) - Upgrade to Spring Security 6.3.5 [#&#8203;43013](https://redirect.github.com/spring-projects/spring-boot/issues/43013) - Upgrade to Tomcat 10.1.33 [#&#8203;43149](https://redirect.github.com/spring-projects/spring-boot/issues/43149)  ##### :heart: Contributors  Thank you to all the contributors who worked on this release:  [@&#8203;ahoehma](https://redirect.github.com/ahoehma)  [@&#8203;izeye](https://redirect.github.com/izeye)  [@&#8203;ngocnhan-tran1996](https://redirect.github.com/ngocnhan-tran1996)  [@&#8203;nosan](https://redirect.github.com/nosan)  [@&#8203;quaff](https://redirect.github.com/quaff)  and [@&#8203;wickdynex](https://redirect.github.com/wickdynex)  </details>  ---  ### Configuration  📅 **Schedule**: Branch creation - At any time (no schedule defined)  Automerge - At any time (no schedule defined).  🚦 **Automerge**: Enabled.  ♻ **Rebasing**: Whenever PR is behind base branch  or you tick the rebase/retry checkbox.  🔕 **Ignore**: Close this PR and you won't be reminded about these updates again.  ---  - [ ] <!-- rebase-check -->If you want to rebase/retry this PR  check this box  ---  This PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/camunda/camunda).  <!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS4xOS4wIiwidXBkYXRlZEluVmVyIjoiMzkuNzIuNSIsInRhcmdldEJyYW5jaCI6Im1haW4iLCJsYWJlbHMiOlsiYXV0b21lcmdlIl19-->
camunda,camunda,52429c0ad2a889f96db3b3e42cd8cf8e238474c1,https://github.com/camunda/camunda/commit/52429c0ad2a889f96db3b3e42cd8cf8e238474c1,upsert query. This should improve performance.  ## Related issues  closes #26490
camunda,camunda,3d34648460a533b30288c9142b49a0d2be15201a,https://github.com/camunda/camunda/commit/3d34648460a533b30288c9142b49a0d2be15201a,fix: improve performance of range queries (#26647)  ## Description  Created one range query for range operations for all applicable data types.  ## Related issues  closes https://github.com/camunda/camunda/issues/24195
camunda,camunda,66117bde8fe951f552f516c40ca01d89e38d3ddc,https://github.com/camunda/camunda/commit/66117bde8fe951f552f516c40ca01d89e38d3ddc,fix: improve performance of range queries
camunda,camunda,5dcd9da63a76839862c0ae627e62c540889c6fb2,https://github.com/camunda/camunda/commit/5dcd9da63a76839862c0ae627e62c540889c6fb2,refactor: write migrated event for sequence flow  We decided to write `ELEMENT_MIGRATED` event for the sequence flow instead of the joining gateway. A joining gateway does not have an instance in the state until all incoming sequence flows are taken. Sequence flows do not have an instance in the state. But  on `SEQUENCE_FLOW_TAKEN` event  all data related to the sequence flow is created. Additionally  the joining gateway will not have a key yet while we are migrating the instance. Additionally  in the applier  we are actually updating the same column family as we do while applying `SEQUENCE_FLOW_TAKEN` event.  In short  while migrating joining gateways  sequence flow data exist in the state but the joining gateway instance is not. Therefore  we chose to write migrated event for sequence flows.  refactor: add integrity check for active sequence flow ids  There is a warning on the `getActiveSequenceFlowIds()` method about usage: Warning  this method should not be used for process instances created before 8.6. It may provide incorrect information for such process instances.  To make sure we have complete active sequence flow ids  we need to make sure active sequence flow ids count matches with the active sequence flow count that exists before 8.6. Additionally  `size()` method is added to `ArrayProperty` to improve the performance of the count query.  refactor: use copyFrom  refactor: use record for sequence flow instead of Map.Entry  refactor: group related methods  refactor: simplify test  Revert "refactor: add integrity check for active sequence flow ids"  This reverts commit 5d476e421fafd320b28d9449c3aa80d9b9d16c10.  refactor: use visitTakenSequenceFlows to retrieve sequence flow ids  Previously we were using getSequenceFlowIds over the elementInstance but the result of that method could be invalid for process instances created before 8.6. Instead  we can use your newly created method ElementInstanceState.visitTakenSequenceFlows to visit them. This provides all the needed info reliably as it is build around the numberOfTakenSequenceFlowsColumnFamily which has been the state tracking this for the joining parallel gateway since "forever". This would also mean we don't need a safety check.  Note: `getSequenceFlowsToMigrate()` method will be simplified along with validations PR.  refactor: use correct taken sequence flows count  To detect if there are any gateways with taken sequence flows  we need to use the count on the column family `numberOfTakenSequenceFlowsColumnFamily`.
camunda,camunda,d70f6ed15f66b5cdcffacb842c355fe75be345b4,https://github.com/camunda/camunda/commit/d70f6ed15f66b5cdcffacb842c355fe75be345b4,perf: wrap the GroupRecord instead of making a full copy  Wrapping is more performant than copying. In this place we don't need a full copy as we immediately insert it in our ColumnFamily.
camunda,camunda,97bce18840217d6c297f78cdd32825dac3b65569,https://github.com/camunda/camunda/commit/97bce18840217d6c297f78cdd32825dac3b65569,feat: verify permissions of groups (#25634)  Includes the user's group authorizations when checking for permissions. Tests in `AuthorizationCheckBehaviorTest` are extended.  I've also included a minor performance improvement and a small refactoring to prefer functional composition but feel free to oppose them  they are optional and can be dropped.  Closes #24403
camunda,camunda,b03c09a48fe3a8908e2371f042f2dbca9dc695f6,https://github.com/camunda/camunda/commit/b03c09a48fe3a8908e2371f042f2dbca9dc695f6,perf: avoid collecting authorizations in temporary sets  When checking for permissions  authorizations were collected to a set just to be turned into a stream again. We can avoid the intermediate set and just work on a stream of authorizations directly.
camunda,camunda,d4324810141ad5596f0d54d1bebe10eb3cd6ad91,https://github.com/camunda/camunda/commit/d4324810141ad5596f0d54d1bebe10eb3cd6ad91,Support multi handler flush (#24859)  ## Description ### Context  > Current state: > - In the ExporterBatchWriter we pass records to all handlers that accept the value type and want to handle the given record > - The results of each handler are stored in a cache (with the corresponding handler) > - Results are identified by id and entity type > - On flush  we call for each cached entity the corresponding handler to flush > > **Expectations (previous):** > > - The export handlers have a one-to-one mapping to an entity. > - This means per entity we have one specific handler  and vice-versa > - It could happen that the same handler is called multiple times during a batch because we see multiple events related to the same entity (ACTIVIATED-COMPLETED  etc.) > > Problem: > > - We have implemented multiple handlers that reuse entity types  even if they set only a subset of properties. > - Consequence: > - As they use the same type  and in some cases even generate the same id only the last handler wins. Flush is only called on the last one  which consumed the record. >     - Only partial data is updated. > - On creation (missing document) this is not an issue because we use mostly upserts  which uses the complete entity object. Here all data is written. > > https://github.com/camunda/camunda/issues/24736#issuecomment-2485444919  We were approaching this by caching all handlers related to the entity.  > [!Important] > > We are aware that this will cause more upserts per entity  and this might have a potential performance impact. But  we have no actual data to prove that this is for sure the case. > > - As we want to fix this critical bug ASAP we **[accept](https://camunda.slack.com/archives/C06F0GLJNFM/p1732020688807309?thread_ts=1731426489.455309&cid=C06F0GLJNFM)** this risk. > - If we see performance issues/regressions we can reconsider the solution and solve it differently. > - As an additional task  we will extend our metric support  ### In this PR  * Added new tests (first failing) that verify multiple registered handlers on the BatchWriter are called and flushing entities * Store more handlers per entity  before we only stored one * Call all handlers for an entity to flush all related data  <!-- Describe the goal and purpose of this PR. -->  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  For context https://camunda.slack.com/archives/C06F0GLJNFM/p1732015209561459?thread_ts=1731426489.455309&cid=C06F0GLJNFM  closes https://github.com/camunda/camunda/issues/24736
camunda,camunda,ded1c39ddca01aa1516311211f628a1fd41e5d11,https://github.com/camunda/camunda/commit/ded1c39ddca01aa1516311211f628a1fd41e5d11,Communicate when importing done (#24671)  ## Description  Early iteration PR for importing done communication on the operate side for ES. Will implement for tasklist and OS if the approach is approved.  **Approach** Since records are stored sequentially in the log  when the importer receives an 8.7.x record then all the 8.6.x records have been written to the zeebe indices and we can mark that partition as "finished" importing. Then if a record reader for that partition (which is finished) gets five empty batch requests we say that record reader is done importing.  five empty batch requests are required to avoid a race condition.If records are written to zeebe indices and before a refresh  the record reader pulls the import batch  it is empty so it then says that the record reader is done when it is not as there are still records to process. To avoid this we can either - refresh before each import batch  this should be avoided as there is a heavy performance cost with manual refreshes. - when retrieving batches add the `refresh=wait_for` query parameter so the request will block and not return until after a scheduled refresh has occurred  this should also be avoided as it would add time to the already long delay. - Use two empty batches as this should catch the zeebe indices after a refresh  ## Related issues  closes #22953
camunda,camunda,5ce2cc83643d5bff4f26f78d6b111a39d37d226c,https://github.com/camunda/camunda/commit/5ce2cc83643d5bff4f26f78d6b111a39d37d226c,feat: store metadata of newly created deployments (#24692)  This creates a new applier for the Deployment/Created event that actually stores the record in the state. The records only contains the resource metadata.  Because we don't want to overwrite deployments  the processor now checks whether a deployment already exists before handling a distributed Deployment/Create command. If it does  we simply write a rejection. This also means that we no longer re-create the resources needlessly  so that's a small performance win too.  :thought_balloon: I wasn't sure what to do about the old deployment handling where we remove the deployment from the state once fully distributed. This mechanism is no longer used since we switched to generalized command distribution but the code for it is still there  so there's a chance that we can still remove deployments. Could we remove the old processors/appliers now?  closes #24683
camunda,camunda,9e646e0dbd5a804b5ac7597b549b6e25b09a1647,https://github.com/camunda/camunda/commit/9e646e0dbd5a804b5ac7597b549b6e25b09a1647,fix: only serialize relevant fields  We don't want to serialize a couple of fields  like the empty  encodedLength  and length fields. This is common amongst UnpackedObjects. We also don't want to serialize the DirectBuffer methods  which are only there for cases where we need the best performance.
camunda,camunda,bfd1690dda7aee0b71213ba8af4ecb936cbb5f54,https://github.com/camunda/camunda/commit/bfd1690dda7aee0b71213ba8af4ecb936cbb5f54,perf: don't read entire deployment record to check for existence
camunda,camunda,4e96bfbfbad19031a2f146fb97be92976b3150f0,https://github.com/camunda/camunda/commit/4e96bfbfbad19031a2f146fb97be92976b3150f0,perf: remove redundant synchronized (#24621)  ## Description  This was originally removed with bcc0ee4  but got reintroduced accidentally to 8.6.0 on main with f69acb4.  See https://camunda.slack.com/archives/C037W9NMATG/p1731082662618709?thread_ts=1731053567.928969&cid=C037W9NMATG
camunda,camunda,9eff46f708fde00edd508faa5320efafcffa4e9c,https://github.com/camunda/camunda/commit/9eff46f708fde00edd508faa5320efafcffa4e9c,perf: remove redundant synchronized  This was originally removed with bcc0ee4  but got reintroduced accidentally to main with f69acb4.
camunda,camunda,baea357beee75c78a53570b603b4773cbcdbd58c,https://github.com/camunda/camunda/commit/baea357beee75c78a53570b603b4773cbcdbd58c,perf: return early  We should not look up the banned instance state for every command  but only for those where they are process instance related.
camunda,camunda,94b07cbb206f4d6619b22b4843bb66d70aa80c4a,https://github.com/camunda/camunda/commit/94b07cbb206f4d6619b22b4843bb66d70aa80c4a,perf: Add Backoff Mechanism to Handle ElasticSearch Unavailability in Tasklist Importer
camunda,camunda,276f528929e2cd28b254460b3275e286cfaf6fa8,https://github.com/camunda/camunda/commit/276f528929e2cd28b254460b3275e286cfaf6fa8,Metrics for Camunda exporter (#23090)  ## Description  Micrometer metrics for Camunda exporter  currently tracks - Failed flushes - Flush duration - Flush latency (time from first record in batch to flush command) - Bulk size (number of entities) - Bulk size memory  Currently there is no implementation of bulk size memory as the exporter cannot currently track the memory usage of records currently as not implemented.  Screenshots of grafana metrics at https://github.com/camunda/camunda/issues/22867#issuecomment-2397375347  **Success Criteria:**  - [x] Identify relevant metrics that helps us with performance tuning and debugging - [x] Add the metrics to the exporter - [x] Make the new metrics available in grafana dashboard  ## Related issues  closes #22867
camunda,camunda,97f8910b8eb352f4e259117191ea56899016448c,https://github.com/camunda/camunda/commit/97f8910b8eb352f4e259117191ea56899016448c,feat: Decision instance get by key - API implementaton (#22811)  ## Description  <!-- Describe the goal and purpose of this PR. --> Implement Decision instance get by key API `evaluatedInputs` and `matchedRules` are returned in the response  in opposite to the search service for which these fields are omitted for performance reasons  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  relates to [#22656](https://github.com/camunda/camunda/issues/22656)
camunda,camunda,37707bd33d9d8e65ea8bdf7c6e441d1a57f6cb2f,https://github.com/camunda/camunda/commit/37707bd33d9d8e65ea8bdf7c6e441d1a57f6cb2f,feat: implement user task query by variable (#22293)  ## Description  Implementation of `UserTaskVariableFilter` as an array that will be converted to `VariableFilterValue` (with `eq` operator)  The decision to use array by name/value is based on the future implementations: - Being able to apply advanced query in name and value fields - User the array as the possibility to add `or` operator for 2 or more variables conditions - We do not support `and` for multiple variables  once `Elasticsearch/OpenSearch` does not allow `a= "something"` AND `a="another thing"`  like the situation of `name`. This would imply on multiple queries  and it is not the goal  once it could lead performance issues (This can be rediscussed later)  The query logic is provided for support: - Search Task Variables - Search Process Variables - When the TaskVariable contains the ProcessVariable as input/output - The Task is returned by the current value of the Process Variable inside the Task  For subprocess: - Only Task Variables are supported (Input)  ## Related issues  closes #https://github.com/camunda/camunda/issues/21846 closes #https://github.com/camunda/camunda/issues/21849
camunda,camunda,357c972c7736cfaaa1e1a57a6753f467d4b7a90d,https://github.com/camunda/camunda/commit/357c972c7736cfaaa1e1a57a6753f467d4b7a90d,refactor: record value and metadata length is always non-zero (#21311)  This is a performance improvement to avoid calculating the length of record value and metadata for every entry that's being appended.  Record values inherit from `ObjectValue` which always has non-zero length due to the map header. Record metadata is serialized via SBE which also requires a header and thus always has non-zero length.  closes #19225
camunda,camunda,17e57f88b7be1c36642ad44b40a058e79843a46a,https://github.com/camunda/camunda/commit/17e57f88b7be1c36642ad44b40a058e79843a46a,refactor: record value and metadata length is always non-zero  This is a performance improvement to avoid calculating the length of record value and metadata for every entry that's being appended.  Record values inherit from `ObjectValue` which always has non-zero length due to the map header. Record metadata is serialized via SBE which also requires a header and thus always has non-zero length.
camunda,camunda,4a10b2e98a7512de5e7d158fd4fbc7246e910e63,https://github.com/camunda/camunda/commit/4a10b2e98a7512de5e7d158fd4fbc7246e910e63,Add mapping functionality to Actor futures (#18990)  ## Description  This PR adds mapping functionality similar to `CompletionStage#thenApply`  allowing you to chain futures and map the result only on success  but short-circuiting the chain if any error occurs at any point.  Additionally refactors one place as an example of where/how to use `thenApply`.  I've keep the warning about performance  but honestly I'd like to challenge it. Creating an intermediate future is pretty much required if you're gonna chain things _and_ allow changing the type of the result. The other option is essentially building your own stream-like pipeline  but wouldn't allow for branching  just pipelining  the results. So there's definite advantages to intermediate futures  and I'm not sure it's sure a performance hit.  As for executing callbacks on the given executor  is that so terrible? :thinking:
camunda,camunda,0c34eba36322cf6acb5acb2a0292ff1353248abb,https://github.com/camunda/camunda/commit/0c34eba36322cf6acb5acb2a0292ff1353248abb,build: merge optimize in (#19381)  This PR places Optimize in the `camunda/camunda` repo  together with every other application of the Camunda stack.  **What's in scope**  * placing Optimize in the monorepo `camunda/camunda` * making sure that the repo is healthy after the introduction of camunda  **What's not in scope**  * making Optimize as part of the single JAR  Optimize is being introduced in `camunda/camunda` as a submodule  declared as such in the main `pom.xml` file  but it's **not** using the parent pom from `parent/pom.xml`. This will be changed later  in the [context of becoming part of the same single JAR](https://github.com/camunda/product-hub/issues/2289).  **Workflow statuses**  * The workflows that are marked as "skipped" have not been checked * At the time of marking this PR as ready for review  all of the intended workflows are healthy * The workflow `Commitlint / lint-commits (pull_request)` currently fails  because the git history brought in by`camunda/camunda-optimize` had less strict rules than `camunda/camunda` * There is a number of workflows that are failing. The failures are due to a known issue: https://github.com/camunda/camunda/issues/17926  **Review**  As this PR is introducing several thousands of commits in the repo  it's difficult to review it using the diff page provided by github. If a given file `$file` shall be reviewed  a visually-good way to do it would be to use the terminal as follows:  ```bash $ git checkout main $ git pull $ git checkout merge-optimize-in $ git pull $ git merge main $ file="zeebe/sample-file.md" $ git diff main -- "$file" ```  The content of this PR can be logically categorized as follows:  * files that introduce Optimize in the repo  under the `optimize/` directory * project changes in the various `pom.xml`s * changes in the CI files that were already present in the repo * introduction of new CI files that are related to Optimize * changes to other files that are not related to Optimize (currently only 1 test)  The first part is practically a mere copy of Optimize. It's under the directory `optimize/`. One note is that as `camunda/camunda-optimize` had different coding style rules  currently this not directory is not compliant with the rules of `camunda/camunda`. Nevertheless  as the parent `parent/pom.xml` is not being used by Optimize  this doesn't currently represent a problem.  The `pom.xml`s files that have been changed are identifiable as follows: ```bash $ git diff --name-only main | grep 'pom.xml' | grep -v 'optimize' dist/pom.xml parent/pom.xml pom.xml testing/camunda-process-test-java/pom.xml ``` From the result  the entry `testing/camunda-process-test-java/pom.xml` can be neglected.  The changes to the CI files that are not related to Optimize  can be found as follows: ```bash $ git diff --name-only main | grep '.github/workflows' | grep -v 'optimize' .github/workflows/ci.yml .github/workflows/operate-a11y.yml .github/workflows/operate-ci-test-reusable.yml .github/workflows/operate-docker-tests.yml .github/workflows/operate-e2e-tests.yml .github/workflows/operate-frontend.yml .github/workflows/operate-playwright.yml .github/workflows/tasklist-docker-tests.yml .github/workflows/tasklist-e2e-tests.yml .github/workflows/zeebe-ci.yml ``` Other changes were mostly related to timeouts and to runner changes  as there was the need for a more powerful machine to complete the build (Optimize will address the problem of having a time-consuming build in a later moment).  The CI files introduced by Optimize can be found as follows: ```bash $ git diff --name-only main | grep 'workflows/optimize' .github/workflows/optimize-backend-linting.yml .github/workflows/optimize-check-c4.yml .github/workflows/optimize-check-renovate-config.yml .github/workflows/optimize-ci.yml .github/workflows/optimize-code-scanning-java.yml .github/workflows/optimize-code-scanning-javascript.yml .github/workflows/optimize-command-assign.yml .github/workflows/optimize-command-eng.yml .github/workflows/optimize-command-help.yml .github/workflows/optimize-command-pm.yml .github/workflows/optimize-command-qa.yml .github/workflows/optimize-commands-dispatch.yml .github/workflows/optimize-create-release-branch.yml .github/workflows/optimize-deploy-artifacts.yml .github/workflows/optimize-e2e-test-cloud.yml .github/workflows/optimize-e2e-tests-sm.yml .github/workflows/optimize-engine-compatibility.yml .github/workflows/optimize-es-compatibility.yml .github/workflows/optimize-generate-cambpm-test-datasets.yml .github/workflows/optimize-generate-changelog.yml .github/workflows/optimize-generate-upgrade-plan.yml .github/workflows/optimize-hadolint.yml .github/workflows/optimize-health-status-report.yml .github/workflows/optimize-helm-integration.yml .github/workflows/optimize-import-static-data-performance.yml .github/workflows/optimize-issue-add-labels.yml .github/workflows/optimize-opened-issues-automation.yml .github/workflows/optimize-os-compatibility.yml .github/workflows/optimize-pmd.yml .github/workflows/optimize-preview-env-clean.yml .github/workflows/optimize-preview-env-deploy.yml .github/workflows/optimize-preview-env-teardown.yml .github/workflows/optimize-publish-c4.yml .github/workflows/optimize-release-optimize-c8-only.yml .github/workflows/optimize-run-java-checks.yml .github/workflows/optimize-sync-issues.yml .github/workflows/optimize-unit-tests.yml .github/workflows/optimize-zeebe-compatibility.yml ```  In addition to this  the following file has been changed for a test to pass: ``` zeebe/exporters/elasticsearch-exporter/src/test/java/io/camunda/zeebe/exporter/ElasticsearchExporterTest.java ```
camunda,camunda,2af4f3d8c411f09c28d7d6bfd2cbd5d197535af1,https://github.com/camunda/camunda/commit/2af4f3d8c411f09c28d7d6bfd2cbd5d197535af1,feat: improve import performance for treePathCache (#20623)  ## Description  * Cache `DatabaseType` in `DatabaseInfo` * Avoid double lookup for treePath * This PR does not contain `treePathCache` per partition. That raises test issues  therefore this comes in another PR.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  Related to #20031   #20027   #20076
camunda,camunda,4aefde01b582bebcab50619571d90e78667384ac,https://github.com/camunda/camunda/commit/4aefde01b582bebcab50619571d90e78667384ac,feat: implement building calling element path  For now  the calling element path is constructed using call activity element Ids  This is not performant or memory efficient since element ids have infinite length  Working on a way to encode it to optimize performance and memory usage is going to follow
camunda,camunda,8bc849a735404a61846156ee464c3e7baa89f5d7,https://github.com/camunda/camunda/commit/8bc849a735404a61846156ee464c3e7baa89f5d7,fix: fix concurrency mode performance (#19909)  ## Description  Perform concurrency mode check once per batch instead of once per every imported record.  ## Related issues  closes #19840
camunda,camunda,3b8f8983c1b2bf5cab936c9c46ef41995a8f7fbc,https://github.com/camunda/camunda/commit/3b8f8983c1b2bf5cab936c9c46ef41995a8f7fbc,fix: only fetch credentials once at a time  By removing the synchronized previously  we broke the test case: `OAuthCredentialsProviderTest.shouldCallOauthServerOnlyOnceInMultithreadMode`  This says that we should only make a single Oauth request at a time. Previously  this was controlled by using synchronized on all cache methods. But we only need a synchronized on the `computeIfMissingOrInvalid` method.  This works  because it calls `OAuthCredentialsProvider.fetchCredentials` as the `zeebeClientCredentialsConsumer`. By blocking on the entire cache  we prevent the credentials to be fetched multiple times.  This is a crude solution  as different requests to fetch credentials are likely still needed when different endpoint are used  but the entire method is locked regardless of the endpoint.  As it's unclear what the performance gain would be from locking per endpoint  and this previously hasn't been any issue  I'll revert back to using synchronized on the `computeIfMissingOrInvalid` method.  (cherry picked from commit af15bf2a300356fb024c67b44ab9a31223a948a0)
camunda,camunda,8090103fb7d8d8e5148fbd5482d502cbe5d28ecd,https://github.com/camunda/camunda/commit/8090103fb7d8d8e5148fbd5482d502cbe5d28ecd,perf: remove redundant synchronized  In the previous commit we made this class threadsafe by using an atomic reference and replacing its reference locklessly.  (cherry picked from commit bcc0ee46c017d37437aafe1035c4c2f734c27d19)
camunda,camunda,0eafbaa16e0309c697ccfb5fea0e590da3c72e6f,https://github.com/camunda/camunda/commit/0eafbaa16e0309c697ccfb5fea0e590da3c72e6f,feat: implement user task search transformers (#19273)  ## Description  Create the User Task Filter/Query on Camunda Service  Unit tests available on this Pull Request: https://github.com/camunda/camunda/pull/19434  Note: This filter will only contemplate User Task  to filter user task variables  this logic will be handled on the endpoint business logic for the performance reasons  only we will have the variable service  and we can divide the search by the tasks that are running vs. tasks that are completed.  The variable service is available on this Pull Request: https://github.com/camunda/camunda/pull/19385  ## How to test?  Example of Java Application ``` public class TestApplication {  public static void main(final String[] args) { final var serverUrl = "http://localhost:9200"; final var restClient = RestClient.builder(HttpHost.create(serverUrl)).build(); final var transport = new RestClientTransport(restClient  new JacksonJsonpMapper()); final var esClient = new ElasticsearchClient(transport);  final var dataStoreClient = new ElasticsearchSearchClient(esClient); final var camundaServices = new CamundaServices(dataStoreClient);  final SearchQueryResult<UserTaskEntity> result = camundaServices .userTaskServices() .search( (q) -> q.page((p) -> p.size(100)) .filter((p) -> p.taskStates("COMPLETED")) .sort((s) -> s.startDate().desc()));  result.items().stream().forEach(System.out::println);  System.exit(-1); } } ```  ## Related issues  closes #https://github.com/camunda/camunda/issues/19210
camunda,camunda,5790f7502ff8122032d5b88ff2aeec17ce80ea1f,https://github.com/camunda/camunda/commit/5790f7502ff8122032d5b88ff2aeec17ce80ea1f,Ensure that stream processor can be closed while writes are rejected (#19278)  ## Description  This fixes a subtle bug that only recently occurred for the first time because we had a [performance regression](https://github.com/camunda/camunda/pull/19233) that made it more likely that writes were rejected  sometimes permanently.  What would happen is that the async scheduled task actor would try to write results from a task but get rejected by the sequencer. The retry strategy used for the _async_ variant had no proper abort condition because we assumed that we could always just close the actor. This is not correct because the retry strategy constantly submits jobs to the fast lane  starving the queue of externally submitted jobs such as the job initiating the actor closing.  So in effect  the stream processor would fail to close properly because it waits indefinitely on the closing of the async scheduled task actor. This would show up as partition transitions not making progress and Zeebe partitions no longer matching their raft role.  ## Related issues  closes #19219
camunda,camunda,6b72c094887e199bb3784eec44cdc700843d44bc,https://github.com/camunda/camunda/commit/6b72c094887e199bb3784eec44cdc700843d44bc,fix: don't adjust append limit when request limit is exhausted (#19233)  Whenever a user request is rejected because the request limit is exhausted  we want to revert the claimed spot in the append limit and not signal that the append was dropped.  This most likely caused weird performance issues where every rejected user command would also lower the append limit  eventually leading to too many rejections and increased workload due to timeouts and retries.
camunda,camunda,abf37203d2b229280fda9b6b817afce3fb5232ed,https://github.com/camunda/camunda/commit/abf37203d2b229280fda9b6b817afce3fb5232ed,fix: fix failing upgrade performance tests
camunda,camunda,791f9a8a38db804070074d6808c1b0bde945a5bc,https://github.com/camunda/camunda/commit/791f9a8a38db804070074d6808c1b0bde945a5bc,feat: Return failed/successful operations count together with batch operation (#18171)  ## Description Added feature to return the failed and successfully completed count of operations for batch operations to display them in the UI.  ## Notes for reviewer I decided to fetch the additional data from the Operation index individually instead of for all currently active operations (as suggested in the issue)  since the batch operations are loaded progressively while scrolling. Performance could probably be improved by requesting information for all the operations that are being loaded in one scroll at once. I'd like to hear opinions on that (and on whether it should be done before we merge the feature or if we can improve that later).  ## Related issues  closes #https://github.com/camunda/operate/issues/6294
camunda,camunda,4f3df7ff2e8c206d69dc1ddff3f7a5d6c90b6c84,https://github.com/camunda/camunda/commit/4f3df7ff2e8c206d69dc1ddff3f7a5d6c90b6c84,feat: improved request performance and refactored batch operations count  -Moved data transformation/aggregation to new class DataTransformer -Changed search requests and interfaces to use nested aggregations rather than multiple ID requests to elasticsearch/opensearch  # Conflicts: #	operate/webapp/src/main/java/io/camunda/operate/webapp/opensearch/reader/OpensearchOperationReader.java
yacy,yacy_search_server,71a6074cc570df0f9df95bcd24b4325ea01aedf1,https://github.com/yacy/yacy_search_server/commit/71a6074cc570df0f9df95bcd24b4325ea01aedf1,added setting of cache configuration for solr according to recommendation from https://community.searchlab.eu/t/yacy-support-gpt-chatgpt-assistant/1622 However it is not clear if this configuration actually works (has an effect at all) or is the solution for performance issues.
alibaba,spring-ai-alibaba,16b4b257c5de7e03966b22f96426abc990a47379,https://github.com/alibaba/spring-ai-alibaba/commit/16b4b257c5de7e03966b22f96426abc990a47379,Merge pull request #465 from qnnn/performance  reduce redundant getProperty calls to improve performance
alibaba,spring-ai-alibaba,648f8d3009bf0282ce4155268cd52e967efdd8a2,https://github.com/alibaba/spring-ai-alibaba/commit/648f8d3009bf0282ce4155268cd52e967efdd8a2,reduce redundant getProperty calls to improve performance.
tchiotludo,akhq,f4ae29dfb198555f7392d90edc8c1b384700d3e6,https://github.com/tchiotludo/akhq/commit/f4ae29dfb198555f7392d90edc8c1b384700d3e6,fix(masking): Various performance  functional  and readability fixes for JSON-based masking
apache,logging-log4j2,3709962553ddc27774163eb77845b0a47a7b9684,https://github.com/apache/logging-log4j2/commit/3709962553ddc27774163eb77845b0a47a7b9684,Improve performance and avoid memory consumption if logging primitive arrays as parameters (#3645)  Current implementation: Method ParameterFormatter.appendArray() delegats to java.util.Arrays.toString() which then allocates a new StringBuilder to return a String which is then added to the existing StringBuilder.  Improved implementation: For all primitive types  a method like ParameterFormatter.appendArray(int[]  StringBuilder) has been added which is called by ParameterFormatter.appendArray() and avoids the unnecessary object creation.  * review comments
apache,logging-log4j2,70f058daaa64379cb62e4bf473ed1fd3b66c31db,https://github.com/apache/logging-log4j2/commit/70f058daaa64379cb62e4bf473ed1fd3b66c31db,Fix extended stack trace (i.e.  `%xEx`) rendering performance regression (#3123)  Co-authored-by: Volkan Yazıcı <volkan@yazi.ci>
apache,logging-log4j2,151f7a6a902eef57a30cac6822387daf7498d6b1,https://github.com/apache/logging-log4j2/commit/151f7a6a902eef57a30cac6822387daf7498d6b1,Revamp the `Lookups` page  We revamp the `Lookups` page by:  * Adding information on different evaluation contexts supported by lookups. * Adding a cheatsheet on which lookup is available in which evaluation context. E.g.  using `$${sys:something}` on a per-event level is a major performance waster. * Removing the description of the broken `jvmrunargs` lookup (see #2726). * Improving the description of the `main` lookup by providing a code example that does not fail if the user switches logging implementations. * Shortening and improving the description of other lookups.
apache,logging-log4j2,91a8178568301c3ed5ab66c7f3603e995cc69fa9,https://github.com/apache/logging-log4j2/commit/91a8178568301c3ed5ab66c7f3603e995cc69fa9,New ScopedContextMap implementation for better performance
Creators-of-Create,Create,46ad8ebbd59ae16e38996947f137dcb8312cfac0,https://github.com/Creators-of-Create/Create/commit/46ad8ebbd59ae16e38996947f137dcb8312cfac0,Conveyable performance  - Make chain conveyor visual render packages and guards - Chain rendering is already pretty efficient - Tick box visuals in the ChainConveyorVisual if flywheel is enabled
Creators-of-Create,Create,1038b76d38f2bb4534eb86a7355306aed0b77d4c,https://github.com/Creators-of-Create/Create/commit/1038b76d38f2bb4534eb86a7355306aed0b77d4c,Performance improvements in several tick methods (#6697)  * Performance improvements in several tick methods  Avoid capturing lambdas  streams  and Set#removeAll  * Update ServerSchematicLoader to not modify activeUploads while iterating in tick  * Replace iterator with enhanced for loop
Creators-of-Create,Create,9dfbe061def61b99b03b5bbf66a349323709fd91,https://github.com/Creators-of-Create/Create/commit/9dfbe061def61b99b03b5bbf66a349323709fd91,Preserving BakedModel and switching to ItemRenderer.render instead of ItemRenderer.renderStatic for performance
AxonFramework,AxonFramework,a902cdc63b00c039ec881da823435164d3504010,https://github.com/AxonFramework/AxonFramework/commit/a902cdc63b00c039ec881da823435164d3504010,Fixes subscription query update permits issue  The permits were based on the configuration of Query permits  rather than the updateBufferSize which can be provided by clients based on their expectations and performance characteristics.  This commit ensures that the updateBufferSize is respected.
AxonFramework,AxonFramework,be29adf40afe5d3ffac5c57a28d0395a1cfa8361,https://github.com/AxonFramework/AxonFramework/commit/be29adf40afe5d3ffac5c57a28d0395a1cfa8361,Merge pull request #3163 from AxonFramework/enhancement/add-reentrant-lock  Wrap `SinksManyWrapper#performWithBusyWaitSpin` in `ReentrantLock` to improve performance of Subscription Query Updates
AxonFramework,AxonFramework,c68358e9c4f41e09a8bc5c8908d618767fc9dcb1,https://github.com/AxonFramework/AxonFramework/commit/c68358e9c4f41e09a8bc5c8908d618767fc9dcb1,Code optimizations  Implemented things which were mentioned in the pull request: * deactive the unit test which tests the performance * add debug log if an accessor method returns void * updated the copyright from 2010-2018 to 2010-2024 in one test
springdoc,springdoc-openapi,189151fae2d05c2f023c8f99d014b9ed8832acfa,https://github.com/springdoc/springdoc-openapi/commit/189151fae2d05c2f023c8f99d014b9ed8832acfa,Merge branch 'ML-Marco-performance-controller-advice'
springdoc,springdoc-openapi,9ea87b0dcbb566f126262b34de0c8fa5471b97b6,https://github.com/springdoc/springdoc-openapi/commit/9ea87b0dcbb566f126262b34de0c8fa5471b97b6,Merge branch 'ML-Marco-performance-controller-advice'
springdoc,springdoc-openapi,520f514fe149e29312c3a388574ef506d1c92ef4,https://github.com/springdoc/springdoc-openapi/commit/520f514fe149e29312c3a388574ef506d1c92ef4,Merge branch 'performance-controller-advice' of https://github.com/ML-Marco/springdoc-openapi into ML-Marco-performance-controller-advice
springdoc,springdoc-openapi,93af2e60bab7f701013a8571fce58382729383c8,https://github.com/springdoc/springdoc-openapi/commit/93af2e60bab7f701013a8571fce58382729383c8,Merge branch 'performance-controller-advice' of https://github.com/ML-Marco/springdoc-openapi into ML-Marco-performance-controller-advice
springdoc,springdoc-openapi,02ed05f9ba62275334f4d3609f22ad4acc67bc4a,https://github.com/springdoc/springdoc-openapi/commit/02ed05f9ba62275334f4d3609f22ad4acc67bc4a,Improve performance of getGenericMapResponse
springdoc,springdoc-openapi,47ca1e3679260b06ddb08893bd7853e890802e77,https://github.com/springdoc/springdoc-openapi/commit/47ca1e3679260b06ddb08893bd7853e890802e77,Improve performance of getGenericMapResponse
Netflix,concurrency-limits,53eee348ab55bf1f0c2b6a8a7782642f741a1f0e,https://github.com/Netflix/concurrency-limits/commit/53eee348ab55bf1f0c2b6a8a7782642f741a1f0e,perf: WindowedLimit allow non-updating threads to proceed when updating (#205)
TNG,ArchUnit,b5e080a407dafb5eadda1c1a4914835c3e56f7d3,https://github.com/TNG/ArchUnit/commit/b5e080a407dafb5eadda1c1a4914835c3e56f7d3,Improve performance for transitive dependency checks (#1381)  `TransitiveDependencyCondition` internally calls `contains()` recursively on the collection of all objects to be tested. If this collection is a large list and there are enough recursive calls to `getDirectDependencyTargetsOutsideOfAnalyzedClasses()` this results in a heavy performance impact. On a reasonable large project a single test using that condition may take minutes to complete.  Here is a 30 seconds FlameGraph taken while an transitive check was running for > 2 minutes:   ![FlameGraph_30s](https://github.com/user-attachments/assets/fe4096b9-c6f7-4d2f-a448-1bf17f5802b5)  Based on the samples  the CPU hangs in [this filter lamdba](https://github.com/TNG/ArchUnit/blob/main/archunit/src/main/java/com/tngtech/archunit/lang/conditions/TransitiveDependencyCondition.java#L91) for > 86% of the time:  - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (47 247 323 samples  0.03%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (44 447 761 samples  0.03%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (10 837 882 731 samples  7.32%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (13 262 127 759 samples  8.96%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (12 668 650 362 samples  8.56%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (15 368 403 186 samples  10.38%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (46 224 364 samples  0.03%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (47 314 101 samples  0.03%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (18 048 208 277 samples  12.19%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (13 405 921 387 samples  9.06%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (93 152 524 samples  0.06%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (3 244 023 882 samples  2.19%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (919 865 902 samples  0.62%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (6 438 577 874 samples  4.35%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (6 760 263 856 samples  4.57%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (7 031 313 250 samples  4.75%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (7 723 048 585 samples  5.22%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (11 918 050 716 samples  8.05%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (46 026 822 samples  0.03%)  So  converting the given list to a Set with much better `contains()` performance fixes this issue.
TNG,ArchUnit,9c6adcb00d25ae8a2a23ed40edbff27ef163a9d9,https://github.com/TNG/ArchUnit/commit/9c6adcb00d25ae8a2a23ed40edbff27ef163a9d9,Improve performance for transitive dependency checks  `TransitiveDependencyCondition` internally calls `contains()` recursively on the collection of all objects to be tested. If this collection is a large list and there are enough recursive calls to `getDirectDependencyTargetsOutsideOfAnalyzedClasses()` this results in a heavy performance impact. On a reasonable large project a single test using that condition may take minutes to complete. Converting the given list to a Set with much better `contains()` performance fixes this issue.  on-behalf-of: @e-solutions-GmbH <info@esolutions.de> Signed-off-by: To6i <To6i@users.noreply.github.com>
eclipse-openj9,openj9,7789c85a4006c1291e37ed012ebf3a3e45338a8d,https://github.com/eclipse-openj9/openj9/commit/7789c85a4006c1291e37ed012ebf3a3e45338a8d,Disable use of method handle for core reflection in JDK21  In JEP 416  core reflection was re-implemented with Method Handles with the purpose of simplifying adding new language features. While analyzing performance of workload based on large enterprise based application  I observed that this was causing visible performance degradation. Disabling the use of direct method handles for reflection calls while we are working on improving the performance with direct method handles.  Signed-off-by: Rahil Shah <rahil@ca.ibm.com>
apache,fury,50d3cb64b6473d33f4050c9bae5337d29dc14a7c,https://github.com/apache/fury/commit/50d3cb64b6473d33f4050c9bae5337d29dc14a7c,feat(java): row encoder supports custom types and collections (#2243)  ## What does this PR do?  Extend Java Row Format to allow registering custom datatypes (e.g. UUID as Int128) and collection factories (e.g. `SortedSet<UUID>` as `new TreeSet<UUID>(customComparator)` ) Additionally supports arrays of custom types e.g. `UUID[]`  Since the type inference is in `fury-core` but I wanted to keep new features scoped to `fury-format`  I had to add a small plugin interface to core so that format can add types dynamically without affecting existing core behavior.  ## Related issues  https://github.com/apache/fury/issues/2208  ## Does this PR introduce any user-facing change?  - [x] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  The `Encoders` class has new `registerCustomCodec` and `registerCustomCollectionFactory` methods. All custom types are written with the existing protocol as embedded memory buffers just like any other field  but with a custom byte representation  so there should be no wire compatibility concerns.  ## Benchmark  There should be no change to performance in existing use cases. The code is carefully written to have no runtime impact if not used. Custom types are invoked via static methods or instance method on static final fields  which should be easily inlined by jit for minimum overhead.  Here is example generated code to help show this:  https://gist.github.com/stevenschlansker/ed7dae863e78d3c87e30bdea39fa8dea
apache,fury,6613de0fb387de7446c1e6cd52d62b9b776be821,https://github.com/apache/fury/commit/6613de0fb387de7446c1e6cd52d62b9b776be821,fix(java): use serialization binding (#2241)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,d4380ea1db5e1beb0beb868c4d03f65cb8711e05,https://github.com/apache/fury/commit/d4380ea1db5e1beb0beb868c4d03f65cb8711e05,feat(java): Support furyField nullable in codeGen pattern (#2191)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues #2169 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,ea979a1b850a825e0171efe70e58d04f8a736497,https://github.com/apache/fury/commit/ea979a1b850a825e0171efe70e58d04f8a736497,feat(java): add DescriptorBuilder for easy build and copying Descriptor (#2229)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues #2222 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,16459abba585850a0399be4d82987da7f2acb2e6,https://github.com/apache/fury/commit/16459abba585850a0399be4d82987da7f2acb2e6,feat(java): support trackingRef in furyField (#2168)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues #2167 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,b9b22b2aa7d6f6b85341e89d46251cba70e3b180,https://github.com/apache/fury/commit/b9b22b2aa7d6f6b85341e89d46251cba70e3b180,feat(java): type meta encoding for xlang in java (#2197)  ## What does this PR do?  This PR implements type meta encoding for xlang in java and refined the tyoe meta for java based new spec in #2216  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,541f11e3afa58dd956fc679a6ef266d6aa57fd30,https://github.com/apache/fury/commit/541f11e3afa58dd956fc679a6ef266d6aa57fd30,fix(java): fix field super class missing in compatible mode (#2214)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #2210  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,77896a044f3dcb9c5842174a5c23e6c7016979c3,https://github.com/apache/fury/commit/77896a044f3dcb9c5842174a5c23e6c7016979c3,feat(java): add protobuf serializer for message and byte string (#2213)  ## What does this PR do?  add protobuf serializer for message and byte string  ## Related issues  #1945  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a2dab1d9411aafda2bb28a5eb8fb1bbb77271083,https://github.com/apache/fury/commit/a2dab1d9411aafda2bb28a5eb8fb1bbb77271083,fix(java): ensure FuryObjectInputStream.read never returns 0 when length>0 #2204 (#2205)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  Ensure FuryObjectInputStream.read never returns 0 when length>0  so that when outer logic attempts to read in a while loop  it won't get stuck or throw exception.  ## Related Issues #2204  ## Does this PR introduce any user-facing change?  No.  ## Benchmark  No measurable performance impact is expected.  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,dd3edef18cf986d225600e0669d1cebd2fb0c8d7,https://github.com/apache/fury/commit/dd3edef18cf986d225600e0669d1cebd2fb0c8d7,feat: add Dart to Language enums across all implementations (#2187)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on Fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of PR titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md). - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  This PR adds Dart as a recognized language across all Fury language implementations. It modifies the Language enums/constants in Java  Python  Go  JavaScript  Rust  and Dart codebases to include Dart as a peer language.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach benchmark data here. -->
apache,fury,7c2eac1e99362df555e69c8d4382037423b7b3ca,https://github.com/apache/fury/commit/7c2eac1e99362df555e69c8d4382037423b7b3ca,fix(java): ensure readVarUint36Small reads full bits regardless of remaining buffer size (#2179)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of PR titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  This PR fixes the inconsistent behavior of `readVarUint36Small()` when reading from buffers of different remaining sizes. - Aligns the fast path and slow path so both read the full 36 bits. - Updates the bit‐extraction in the slow path (`readVarUint36Slow()`) to match the fast path’s handling of the high‐order bits.  ## Related issues  - #2110  ## Does this PR introduce any user‐facing change?  No user‐facing changes are introduced. - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  No measurable performance impact is expected  as the fix only adjusts bit extraction logic without altering code paths. Benchmark
apache,fury,3a8d478f21291e6abd15bc8b67d3e8275c359850,https://github.com/apache/fury/commit/3a8d478f21291e6abd15bc8b67d3e8275c359850,fix(java): fix nested map chunk serialization codegen (#2172)  ## What does this PR do?  fix nested map chunk serialization codegen ## Related issues  Closes #2170  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,6376e5275524a0254772d52e305b3a1556aef2ca,https://github.com/apache/fury/commit/6376e5275524a0254772d52e305b3a1556aef2ca,feat(java): FuryField annotation hints for struct serialization (#2036)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues #1956 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,a3da498344cfb85ab79b8cd518671406e093b857,https://github.com/apache/fury/commit/a3da498344cfb85ab79b8cd518671406e093b857,feat(java): support enum/time/array final types in xlang serialization (#2164)  ## What does this PR do?  support more final types in xlang serialization: - enum - timestamp/date - primitive array  ## Related issues  Closes #2163  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,2a75704f7dd9f4d9547d5dbcc624b099313fee63,https://github.com/apache/fury/commit/2a75704f7dd9f4d9547d5dbcc624b099313fee63,fix(java): fix xlang container field deserialization type error (#2161)  ## What does this PR do?  fix xlang container field deserialization type error  ## Related issues  Closes #2105  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,b3196e39a46c26b1e49f144283b1f22a398e2b7c,https://github.com/apache/fury/commit/b3196e39a46c26b1e49f144283b1f22a398e2b7c,feat(java): unify java and xlang object serialization (#2146)  ## What does this PR do?  This PR unify the java and xlang object serialization in java: - Remove StructSerializer - Unify struct hash compute between java and python - Align fields sort between java and python - unify the java and xlang object serialization in java  ## Related issues    ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,8ce3953f99ceea71b5b21ad9992e00422d72d810,https://github.com/apache/fury/commit/8ce3953f99ceea71b5b21ad9992e00422d72d810,fix(java): fix fury logger log exception (#2153)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #2152  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,30935d019bf7221677f0e39a7733903a774d9330,https://github.com/apache/fury/commit/30935d019bf7221677f0e39a7733903a774d9330,chore(java): Update the content that needs to be corrected when reading the code. (#2143)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Signed-off-by: Liangliang Sui <coolsui.coding@gmail.com>
apache,fury,3907c0651304efcacb633cdadc7227f86e14881f,https://github.com/apache/fury/commit/3907c0651304efcacb633cdadc7227f86e14881f,chore(java): use the SHA256_HASH field value directly. (#2144)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. --> 1. Directly use the `DisallowedList#DISALLOWED_LIST_TXT_PATH` value in `DisallowedListTest#testCalculateSHA256`. 2. Directly use the `DisallowedList#SHA256_HASH` value in `DisallowedListTest#testCalculateSHA256`.  Reduce manual maintenance costs :smile:  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Signed-off-by: Liangliang Sui <coolsui.coding@gmail.com>
apache,fury,c5ab2a2f1d934312bebb253c8b5b698aacb94f25,https://github.com/apache/fury/commit/c5ab2a2f1d934312bebb253c8b5b698aacb94f25,fix(java): fix DisallowedList calculate hash in Windows (#2142)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do? It appears that [PR #2128](https://github.com/apache/fury/pull/2128) did not truly resolve this issue  as it only modified the test case code without addressing the actual runtime logic. This PR re-fixes the problem. <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,c19bafd4c95fb57896da56c12367a8104819f31e,https://github.com/apache/fury/commit/c19bafd4c95fb57896da56c12367a8104819f31e,refactor(java): move methods from object serializer to abstract object serializer (#2140)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,13cfe6cedc9cfb41f86e11d3fe2790678fdeb5d2,https://github.com/apache/fury/commit/13cfe6cedc9cfb41f86e11d3fe2790678fdeb5d2,refactor(java): refactor object serializer for unifying xlang/java serialization in java (#2139)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,e2ca88e5f4e078758c89334efaee41b2b87a0252,https://github.com/apache/fury/commit/e2ca88e5f4e078758c89334efaee41b2b87a0252,fix(java): fix nested chunk map serialization error when generics exists (#2136)  ## What does this PR do?  fix nested chunk map serialization error when generics exists ## Related issues  Closes #2135  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a37cc5513dd60c1995faa0998ac8c185d8337ce6,https://github.com/apache/fury/commit/a37cc5513dd60c1995faa0998ac8c185d8337ce6,feat: xlang homogeneous collection serialization between java/python (#2130)  ## What does this PR do?  This PR implements homogeneous xlang collection serialization between java/python. Changes include:  - Xlang homogeneous  collection serialization in cython/python - Xlang chunk map serialization in pure python for debug - Use  homogeneous  collection serialization in java - homogeneous  collection serialization between java and python  ## Related issues  Closes #2131 Closes #2132  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,1236559d508e9cd0b4028582c29a5ff71f8611b1,https://github.com/apache/fury/commit/1236559d508e9cd0b4028582c29a5ff71f8611b1,fix(java): fix disallowed.txt check in windows (#2128)  ## What does this PR do? - add windows ci for java21 - Fix sha256 check error on windows:  ![Image](https://github.com/user-attachments/assets/b082fc7a-c929-42fc-a910-707f604251f4) ## Related issues  Closes #2100  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,681a308831e928227e04dcca86b295121f6674ec,https://github.com/apache/fury/commit/681a308831e928227e04dcca86b295121f6674ec,feat: xlang map chunk serialization between java/python (#2127)  ## What does this PR do?  This PR supports xlang map chunk serialization between java/python  ## Related issues  Closes #2125 Closes #2126  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,3aeccf0aed8d382deb767fb96984b40680ac3520,https://github.com/apache/fury/commit/3aeccf0aed8d382deb767fb96984b40680ac3520,feat(java): support inconsistent registration by name/id (#2120)  ## What does this PR do?   ## Related issues Closes #2119 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,d3691fcde21e412642062c2c3895899273d5aa51,https://github.com/apache/fury/commit/d3691fcde21e412642062c2c3895899273d5aa51,feat(java): support nested bean in array/collection/map for row format (#2116)  ## What does this PR do?  support nested bean in array/collection/map for row format  ## Related issues  Closes #2106  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,fcdf77875395dad17b5328c7f721e4adf1384e99,https://github.com/apache/fury/commit/fcdf77875395dad17b5328c7f721e4adf1384e99,fix(java): use registered id to sort fields (#2115)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #2093  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,22020323dfa1cff8ae29ca236c90ff7e78b2b667,https://github.com/apache/fury/commit/22020323dfa1cff8ae29ca236c90ff7e78b2b667,fix(java): fix not null value flag (#2114)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #2089  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,8df1c7f4d1f8ddb413f2f0a8d6c62996550a2950,https://github.com/apache/fury/commit/8df1c7f4d1f8ddb413f2f0a8d6c62996550a2950,feat(java): support passed tracking ref meta when building serializers (#2113)  ## What does this PR do?  This pr supports passed tracking ref meta when building serializers. The meta can be pssed by Type Annotation in #2036 or by classdef encoded into binary.  ## Related issues   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,594278ef23c46c50063e124033fce563a46ea099,https://github.com/apache/fury/commit/594278ef23c46c50063e124033fce563a46ea099,fix(java): fix serialization npe of collection with all null elems (#2111)  ## What does this PR do?  ## Related issues  Closes #2109  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,bc6a0b586aa2ab99e3a0059ee645c7b9901e92a3,https://github.com/apache/fury/commit/bc6a0b586aa2ab99e3a0059ee645c7b9901e92a3,perf(java): Refactor field sorting in StructSerializer to cache transformed field names and avoid redundant computation (#2091)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  This PR refactors the field sorting in `StructSerializer.java` by caching transformed field names to avoid redundant computations during sorting. The `lowerCamelToLowerUnderscore` transformation is now applied once per field instead of multiple times.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,fcdbebad94c7dd76dd66cd1888c06fe29f26fd53,https://github.com/apache/fury/commit/fcdbebad94c7dd76dd66cd1888c06fe29f26fd53,feat(java): use sha256 to check disallowed.txt tamper (#2102)  ## What does this PR do?  use sha256 to check disallowed.txt tamper ## Related issues  Closes #2100  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,676c5742b94ec30137cf282b7ebfd69186d9f8b6,https://github.com/apache/fury/commit/676c5742b94ec30137cf282b7ebfd69186d9f8b6,perf(java): Improve performance by using System.arraycopy to copy between byte arrays (#2101)  ## What does this PR do?  Try to use `System.arraycopy` when copying between byte arrays. This significantly increases performance  in our use case serialization is twice as fast (serializing/deserializing 1M complex strucutures using (de)serializeJavaObject in parallel). When using Platform.copyMemory  the JDK is doing a lot of type checking to see if it's a byte array  causing a visible slowdown. `Unsafe.memoryCopy` implementation was changed in Java 9 from a native method to a method calling the internal unsafe method containing the reflection checks.  There are a few places where System.arraycopy is already used  referring to it being faster on certain JDK implementations. Tested our use case on a macbook with arm64 cpu against zulu 21 and open-jdk 23.   ![image](https://github.com/user-attachments/assets/ce4a2c9d-e7b7-49ae-867d-68eadbe8a140)  ## Does this PR introduce any user-facing change?  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark I tried to reproduce the performance impact through one on existing benchmarks `UserTypeSerializeSuite.fury_serialize`  however I didn't see a difference here.  Signed-off-by: Seppe Volkaerts <seppevolkaerts@hotmail.com>
apache,fury,8ab006c3ea02dc2fd9dd2e57867f6c073a81bff2,https://github.com/apache/fury/commit/8ab006c3ea02dc2fd9dd2e57867f6c073a81bff2,perf(java): Refactor ThreadPoolFury to improve performance (#2092)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. --> After testing  it was observed that ​ThreadPoolFury​ experiences prolonged blocking during cold starts under high-concurrency scenarios. Analysis revealed that improper usage of locks in ​ClassLoaderFuryPooled​ was the root cause. This PR refactors the implementation of ​ClassLoaderFuryPooled​ by significantly reducing the granularity of locks  thereby drastically minimizing blocking time during cold starts.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 --> [](url)https://github.com/apache/fury/issues/2087 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. --> CPU:9950X Class:org.apache.fury.benchmark.ThreadPoolFurySuite.java old: Percentiles  ms/op: p(0.0000) =      0.001 ms/op p(50.0000) =      0.001 ms/op p(90.0000) =   1587.388 ms/op p(95.0000) =   1587.388 ms/op p(99.0000) =   1587.388 ms/op p(99.9000) =   1587.388 ms/op p(99.9900) =   1587.388 ms/op p(99.9990) =   1587.388 ms/op p(99.9999) =   1587.388 ms/op p(100.0000) =   1587.388 ms/op  new: Percentiles  ms/op: p(0.0000) =      0.001 ms/op p(50.0000) =      0.001 ms/op p(90.0000) =     62.746 ms/op p(95.0000) =     62.746 ms/op p(99.0000) =     62.746 ms/op p(99.9000) =     62.746 ms/op p(99.9900) =     62.746 ms/op p(99.9990) =     62.746 ms/op p(99.9999) =     62.746 ms/op p(100.0000) =     62.746 ms/op
apache,fury,ecc3347066c099438d8d5cd5abbe4f3687c3f830,https://github.com/apache/fury/commit/ecc3347066c099438d8d5cd5abbe4f3687c3f830,fix(java): Modify some mistake (#2086)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,6e5179230496f90cf75c1f6cfb728168cac405b7,https://github.com/apache/fury/commit/6e5179230496f90cf75c1f6cfb728168cac405b7,fix(java): fix ImmutableCollections$SubList duplicate registration (#2074)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do? fix ImmutableCollections$SubList duplicate registration  java.util.ImmutableCollections$SubList has been registered twice  resulting in different numbers of classIdGenerator across different JDK versions  which ultimately leads to serialization failures when crossing JDK versions. <!-- Describe the purpose of this PR. -->  ## Related issues Closes #2070 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: 吴怡帆 <wuyifan3@xiaomi.com>
apache,fury,670588f8564d425ab98f5eaf7bdf46374f01e945,https://github.com/apache/fury/commit/670588f8564d425ab98f5eaf7bdf46374f01e945,fix(java): java.util.Date and its subclasses are mutable (#2076)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  java.util.Date and its subclasses are mutable but in version <= 0.10.0 all TimeSerializers extends from ImmutableSerializer. This pr reorganized the inheritance relationships of TimeSerializers.Moved and added test case to adapte new version.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 --> [issus:](url)https://github.com/apache/fury/issues/2071  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,c531d84b5b6303c0b602b56ade7cc947a94d0ea2,https://github.com/apache/fury/commit/c531d84b5b6303c0b602b56ade7cc947a94d0ea2,perf(java): Optimize Computational Efficiency of MetaStringEncoder::encodeGeneric (#2072)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  In this update  the bit manipulation logic within the MetaStringEncoder::encodeGeneric method has been optimized for better performance. The original implementation processed the bits one at a time in each loop iteration  updating only a single bit per cycle. The new approach improves efficiency by processing multiple bits (up to one byte) per loop iteration  reducing the number of iterations required and minimizing the overhead of bit-level operations.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  | Test # | encodeGeneric (ms) | new encodeGeneric (ms) | |--------|--------------------|---------------------| | 1      | 102                | 27                  | | 2      | 85                 | 29                  | | 3      | 86                 | 29                  | | 4      | 81                 | 26                  | | 5      | 94                 | 33                  | | **Average** | **89.6**             | **28.8**             |   here is the simple testing code: ```java package org.example;  public class Main { private static int charToValueLowerSpecial(char c) { return 127; }  private static int charToValueLowerUpperDigitSpecial(char c) { return 127; }  static private byte[] encodeGeneric2(char[] chars  int bitsPerChar) { int totalBits = chars.length * bitsPerChar + 1; int byteLength = (totalBits + 7) / 8; byte[] bytes = new byte[byteLength]; int byteInd = 0; int bitInd = 1;  // Start from the second bit (the first is reserved for the flag) int charInd = 0; int charBitRemain = bitsPerChar;  // Remaining bits to process for the current character int mask; while (charInd < chars.length) { int charVal = (bitsPerChar == 5) ? charToValueLowerSpecial(chars[charInd]) : charToValueLowerUpperDigitSpecial(chars[charInd]); // Calculate how many bits are remaining in the current byte int nowByteRemain = 8 - bitInd; if (nowByteRemain >= charBitRemain) { // If the remaining bits in the current byte can fit the whole character value mask = (1 << charBitRemain) - 1;  // Create a mask for the bits of the character bytes[byteInd] |= (byte) ((charVal & mask) << (nowByteRemain - charBitRemain));  // Place the character bits into the byte bitInd += charBitRemain; if (bitInd == 8) { // Move to the next byte if the current byte is filled ++byteInd; bitInd = 0; } // Character has been fully placed in the current byte  move to the next character ++charInd; charBitRemain = bitsPerChar;  // Reset the remaining bits for the next character } else { // If the remaining bits in the current byte are not enough to hold the whole character mask = (1 << nowByteRemain) - 1;  // Create a mask for the current available bits in the byte bytes[byteInd] |= (byte) ((charVal >> (charBitRemain - nowByteRemain)) & mask);  // Place part of the character bits into the byte ++byteInd;  // Move to the next byte bitInd = 0;  // Reset bit index for the new byte charBitRemain -= nowByteRemain;  // Decrease the remaining bits for the character } }  boolean stripLastChar = bytes.length * 8 >= totalBits + bitsPerChar; if (stripLastChar) { // Mark the first byte as indicating a stripped character bytes[0] = (byte) (bytes[0] | 0x80); } return bytes; }  static private byte[] encodeGeneric(char[] chars  int bitsPerChar) { int totalBits = chars.length * bitsPerChar + 1; int byteLength = (totalBits + 7) / 8; // Calculate number of needed bytes byte[] bytes = new byte[byteLength]; int currentBit = 1; for (char c : chars) { int value = (bitsPerChar == 5) ? charToValueLowerSpecial(c) : charToValueLowerUpperDigitSpecial(c); // Encode the value in bitsPerChar bits for (int i = bitsPerChar - 1; i >= 0; i--) { if ((value & (1 << i)) != 0) { // Set the bit in the byte array int bytePos = currentBit / 8; int bitPos = currentBit % 8; bytes[bytePos] |= (byte) (1 << (7 - bitPos)); } currentBit++; } } boolean stripLastChar = bytes.length * 8 >= totalBits + bitsPerChar; if (stripLastChar) { bytes[0] = (byte) (bytes[0] | 0x80); } return bytes; }  static private boolean bytesEqual(byte[] bytes1  byte[] bytes2) { if (bytes1.length != bytes2.length) { return false; } for (int i = 0; i < bytes1.length; ++i) { if (bytes1[i] != bytes2[i]) { return false; } } return true; }  public static void main(String[] args) { test(); }  // test performance comparison encodeGeneric2 vs encodeGeneric public static void test() { char[] chars = new char[10000000]; for (int i = 0; i < chars.length; i++) { // random chars[i] = (char) (Math.random() * 127); } long start = System.currentTimeMillis(); byte[] bytes = encodeGeneric(chars  5); System.out.println("encodeGeneric: " + (System.currentTimeMillis() - start) + "ms"); start = System.currentTimeMillis(); byte[] bytes2 = encodeGeneric2(chars  5); System.out.println("encodeGeneric2: " + (System.currentTimeMillis() - start) + "ms"); assert bytes.length == bytes2.length && bytesEqual(bytes  bytes2); } } ```
apache,fury,23299daa7eb52a4fe6c5bcebcc2705673ce4d5e6,https://github.com/apache/fury/commit/23299daa7eb52a4fe6c5bcebcc2705673ce4d5e6,fix(java): fix read primitives error on fill buffer bound (#2064)  ## What does this PR do? fix read primitives error on fill buffer bound <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #2060 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,e33a1f71d32d767fb62e5b7da576ccada702116b,https://github.com/apache/fury/commit/e33a1f71d32d767fb62e5b7da576ccada702116b,fix(java): Fix error with `MemoryBuffer::readBytesAsInt64` when not in LITTLE_ENDIAN mode #2068 (#2069)    ## What does this PR do?  Fix the issue with `MemoryBuffer::readBytesAsInt64` when the system is not in LITTLE_ENDIAN mode.  ## Related issues - Fix #2068  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,bda04fe7a5ff763f000b7c3db1ca0e3cdd6dc833,https://github.com/apache/fury/commit/bda04fe7a5ff763f000b7c3db1ca0e3cdd6dc833,fix(java): fix read null chunk out of bound (#2065)  ## What does this PR do?  fix read null chunk out of bound  ## Related issues  Closes #2062  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,f23f71781fb7b83b20265118eadf6c7c91835c0a,https://github.com/apache/fury/commit/f23f71781fb7b83b20265118eadf6c7c91835c0a,feat(java): Add fastpath for collection/map serialize and deserialize (#2050)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  Add fastpath serialize and deserialize for hashmap and arraylist type  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,9e8316eb106444f04c1f9b11b61bc88466ff1ccb,https://github.com/apache/fury/commit/9e8316eb106444f04c1f9b11b61bc88466ff1ccb,feat(spec): remove polymorphic from type id (#2054)  ## What does this PR do?  polymorphic info is only used when serializing fields of a struct. This is not a generic type information  we should not include it in type id.    ## Related issues   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,e412cd46640e935715bcf196ff4ff1048837d845,https://github.com/apache/fury/commit/e412cd46640e935715bcf196ff4ff1048837d845,feat(java): support register type by name in java (#2053)  ## What does this PR do?  support register type by name in java   ## Related issues  Closes https://github.com/apache/fury/discussions/1969 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a80140aca047d6c3ced2b7f0ff090ed154429f68,https://github.com/apache/fury/commit/a80140aca047d6c3ced2b7f0ff090ed154429f68,feat(java): zstd meta compressor (#2042)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do? create zstd metacompressor as an option. let zstd access the src arr instead of copy to new array.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,b288a23e44d8d84dcf2eb9f884cdc679a552a8af,https://github.com/apache/fury/commit/b288a23e44d8d84dcf2eb9f884cdc679a552a8af,fix(java): fix duplicate entry write at max chunk size bound (#2040)  ## What does this PR do?  fix duplicate entry write at max chunk size bound ## Related issues  #2025 #2027  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,888920373463877bab4f4114fc90a780fbc175fe,https://github.com/apache/fury/commit/888920373463877bab4f4114fc90a780fbc175fe,feat(java): deserialize one pojo into another type (#2012)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do? replace class def if target class is different type with the actual serialized one  so it can be deserialized to another type #1998 <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: chaokunyang <shawn.ck.yang@gmail.com>
apache,fury,e952b63a2dadd4c903019ecca1deabac63a14a6e,https://github.com/apache/fury/commit/e952b63a2dadd4c903019ecca1deabac63a14a6e,feat(java): jit support for chunk based map serialization (#2027)  ## What does this PR do? This PR added jit support for chunk based map serialization  it supports all kinds of map serializaiton by generated code: -  final map key and value field type - polymorphic map key and value field type - nested map key and value type  This PR also removed the old map serialization protocol code.   The new chunk based protocol improve serialized size by **2.3X** at most.  data: ``` stringMap: {"k1": "v1"  "k2": "v2  ...  "k10": "v10" } intMap: {1:2  2:4  3: 6  ...  10: 20} ```  new protocol: ``` stringMapBytes 68 stringKVStructBytes 69 intMapBytes 28 intKVStructBytes 29 ```  old protocol: ``` stringMapBytes 104 stringKVStructBytes 87 intMapBytes 64 intKVStructBytes 47 ```  And improve performance by 20%   ## Related issues  Closes #925  #2025  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark   [chunk-jmh-result.csv](https://github.com/user-attachments/files/18575900/chunk-jmh-result.csv)  [nochunk-jmh-result.csv](https://github.com/user-attachments/files/18575901/nochunk-jmh-result.csv)   ![image](https://github.com/user-attachments/assets/754f8e48-b45e-489b-adf5-cca1c5d03f1e)
apache,fury,a907a9a3c70123de5ced40bc2e841c8ba346df8e,https://github.com/apache/fury/commit/a907a9a3c70123de5ced40bc2e841c8ba346df8e,fix(java): chunk map serialize an error (#2030)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  fix some error  new implement of map chunk can not pass all of the unit test in the org.apache.fury.serializer.collection.MapSerializersTest   ![image](https://github.com/user-attachments/assets/2af5978d-2ebc-4ad9-9ac2-f90511ed5c59)  ![image](https://github.com/user-attachments/assets/edc08cc4-495f-4cbe-9f48-0b8e5bcef10e) I found an error in line 298 of the AbstractMapSequencer file ## Related issues  <!-- Is there any related issue? Please attach here. none - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change? none <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark none <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,2faede237f3362e5c9221c5a272e0cd927bd8f46,https://github.com/apache/fury/commit/2faede237f3362e5c9221c5a272e0cd927bd8f46,feat(java): support streaming encode/decode to/from buffer for row format (#2024)  ## What does this PR do?  support streaming encode/decode to/from buffer for row format  ## Related issues  Closes #2019  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,7fd582a587b6ffbe79629b944f300057884b216c,https://github.com/apache/fury/commit/7fd582a587b6ffbe79629b944f300057884b216c,feat(java): Chunk by chunk predictive map serialization protocol (#1722)  ## What does this PR do?  Implement chunk based map serialization in #925. This pr doesn't provide JIT support  it will be implemented in later PR.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #925  -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com> Co-authored-by: chaokunyang <shawn.ck.yang@gmail.com>
apache,fury,b952bf1e067bdd0821a9c3c88a8f04924c486186,https://github.com/apache/fury/commit/b952bf1e067bdd0821a9c3c88a8f04924c486186,feat(java): make 4 bytes utf16 size header optional for utf8 encoding (#2010)  ## What does this PR do?  Currently fury serialize utf8 string in java will write num bytes of utf16 first  so that the deserializaiton can save one copy. But C++ and golang does not need this information. This PR makes the 4 bytes utf16 size header optional for utf8 encoding  so theat the xlang serialiation can use the standard fury string serialization spec  and align to other languages.  For performance consideration  this PR introduce `writeNumUtf16BytesForUtf8Encoding` which can perserve current behaviour.  ## Related issues  #1890  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  This PR will introduce an extra copy for deserialization since we can't know the size of utf16 in advance before decoding utf8 string.
apache,fury,880c6e50e93889971eb9426515c19a68f78e9324,https://github.com/apache/fury/commit/880c6e50e93889971eb9426515c19a68f78e9324,feat(python): support latin1/utf16 string encoding in python (#1997)  ## What does this PR do?  Support support latin1/utf16 string encoding in python. For utf16  since python doesn't use surrogate pairs  this pr also added a vectorized surrogate pairs check function.  Note: - Python UCS-2 doesn't contains surrogate pairs  we must check utf16 first before contruct string from the binary. This is different from java/nodejs  ## Related issues  Closes #1967  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,d7ddd904f8805948c8dd25d8b469c9f720d7cc0c,https://github.com/apache/fury/commit/d7ddd904f8805948c8dd25d8b469c9f720d7cc0c,fix(java): Compatible mode on de/serialize api failed to deserialize (#1996)  ## What does this PR do? Read and write class data on COMPATIBLE mode for de/serializeJavaObject api.  When COMPATIBLE mode is on and need to serialize and deserialize different POJO  users are required to register classes those are going to be serialized or deserialized.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,3c1df17a1edd8b64196475c8c6a7db5acf2e502a,https://github.com/apache/fury/commit/3c1df17a1edd8b64196475c8c6a7db5acf2e502a,fix(java): Fix the issue caused by not using readCompressedBytesString during deserialization when string compression is enabled. (#1991)  Fix the issue caused by not using readCompressedBytesString during deserialization when string compression is enabled.  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  fix issue [#1984 ](https://github.com/apache/fury/issues/1984) <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,98efd72c2c740167edafd23d9d9c8c157095b780,https://github.com/apache/fury/commit/98efd72c2c740167edafd23d9d9c8c157095b780,feat(java/python): new xlang type system spec implementation (#1690)  ## What does this PR do?  This PR implements a new [type system](https://fury.apache.org/docs/specification/fury_xlang_serialization_spec/#type-systems) for xlang serialization between java and python.  The changes includes: - Refine type system spec: added new types: - named_enum: an enum whose value will be serialized as the registered name. - struct: a morphic(final) type serialized by Fury Struct serializer. - polymorphic_struct: a type which is not morphic(not final). i.e. it don't have subclasses. Suppose we're deserializing `List<SomeClass>`  we can save dynamic serializer dispatch if `SomeClass` is morphic(final). - compatible_struct: a morphic(final) type serialized by Fury compatible Struct serializer. - polymorphic_compatible_struct: a non-morphic(non-final) type serialized by Fury compatible Struct serializer. - named_struct: a `struct` whose type mapping will be encoded as a name. - named_polymorphic_struct: a `polymorphic_struct` whose type mapping will be encoded as a name. - named_compatible_struct: a `compatible_struct` whose type mapping will be encoded as a name. - named_polymorphic_compatible_struct: a `polymorphic_compatible_struct` whose type mapping will be encoded as a name. - ext: a type which will be serialized by a customized serializer. - polymorphic_ext: an `ext` type which is not morphic(not final). - named_ext: an `ext` type whose type mapping will be encoded as a name. - named_polymorphic_ext: an `polymorphic_ext` type whose type mapping will be encoded as a name. - Added a new XtypeResolver in java to resolve xlang types - Support register class mapping by id. Before this PR  we only support register class by name  which is more expensive at space/performance cost. - Support pass type into to resolve type ambiguation such as `ArrayList/Object[]` in java. Users can `serialize(List.of(1  2   3))` and deserialize it into array by `deserialize(bytes  Integer[].class)` - Refactor pyfury serialization by moving type resolver into python code from cython  this will make debug more easy and reduce code duplciation  it also speed serialization performance. - golang xtype serialization test are disabled  it will be reenabled after new type system is implemented in golang  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,1515f94c0013276f6cbb2f6b704b525fca036c2d,https://github.com/apache/fury/commit/1515f94c0013276f6cbb2f6b704b525fca036c2d,fix(java): only print warn message if scopedMetaShareEnabled is true … (#1985)  …and not in CompatibleMode  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  - https://github.com/quarkiverse/quarkus-fury/issues/51  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,b3f531cee934256c4b1f4fb929548d013b57e2e2,https://github.com/apache/fury/commit/b3f531cee934256c4b1f4fb929548d013b57e2e2,feat(java): configurable buffer size limit (#1963)  ## What does this PR do?  This PR introduces a new configuration option `bufferSizeLimitBytes` that replaces the hard-coded default of 128kb.  ## Related issues  #1950  ## Does this PR introduce any user-facing change?  The PR introduces a new configuration option `bufferSizeLimitBytes`.  - [x] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Discussion  This PR solves my problem  but I'm not sure if it is the right way to move forward. This is quite a low-level configuration option  but a potentially very important one. Every user whose average payload size is >=128kb  will need to increase this value for maximum performance. Maybe the default limit should be increased to something less conservative like 1MB  so fewer users will need to adjust this setting?
apache,fury,54b62fb6ab5d7e557131efe07c7402c885f6e7c4,https://github.com/apache/fury/commit/54b62fb6ab5d7e557131efe07c7402c885f6e7c4,feat(java): use varint for jdk compatible serializers (#1960)  ## What does this PR do?  use varint for jdk compatible serializers to reduce serialized size  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a3a22381a67451327f1b000cad780f730db951d7,https://github.com/apache/fury/commit/a3a22381a67451327f1b000cad780f730db951d7,fix(java): fix find constructor error in generated serializer class caused by duplicated class classloading for Fury (#1948)  ## What does this PR do?  fix duplicate classloading in parent classloader.  Some classloader such as flink classloader can load class from children classloader. If fury is located in children classloader  but we are serializing a class in such parent class  the parent class will load Fury class again  which caused two Fury clases loaded.  ## Related issues Closes #1947  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,8add13c7967735f2fbe45ad879f4e071db0faa55,https://github.com/apache/fury/commit/8add13c7967735f2fbe45ad879f4e071db0faa55,fix(java): ClassLoaderFuryPooled#setFactoryCallback cannot effect old Fury (#1946)    ## What does this PR do?  ClassLoaderFuryPooled#setFactoryCallback cannot effect old fury. so if `org.apache.fury.pool.FuryPooledObjectFactory#classLoaderFuryPooledCache` expired  new classLoaderFuryPooled can not effected by custom factoryCallback  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Co-authored-by: shuchang.li <shuchang.li@qunar.com>
apache,fury,5b22ccd035ea80f453a7483b407ced6a5401a738,https://github.com/apache/fury/commit/5b22ccd035ea80f453a7483b407ced6a5401a738,fix(java): Move schema caching to unsafe trait to avoid issues when using non-inferred schema. (#1944)    ## What does this PR do?  This PR removes the java specific `ExtField` class from the schema and moves the extData mechanism to the internal UnsafeTrait class. This is necessary because `ExtField` is only created internally from `inferSchema` method used by java  and we would potentially import or derive schemas from other sources (e.g XLANG  handwritten schema  arrow-native source) -- those schemas will be incompatible when we attempt to retrieve the cached schema. Removing schema-caching will fix the issue  but create allocations  so after some discussion  we decided to move the mechanism to the internal UnsafeTrait class.  This implementation makes changes internal API: - Derived classes from `UnsafeTrait` need to initialize the `extData` cache and define the number of extData slots needed. - The internal `getStruct` method needs to define which slot we use to retrieve `extData`.  Other:  - REVERTED: pom.xml for fury-format will automatically run tests with appropriate --add-opens flag for arrow  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change? N/A  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,2ed6adc2492d99e437d804474c1ffa3cb6c4ab59,https://github.com/apache/fury/commit/2ed6adc2492d99e437d804474c1ffa3cb6c4ab59,feat(java): ReplaceResolveSerializer deep copy (#1925)    ## What does this PR do? Adjusting the deep copy of ReplaceResolveSerializer.  obj -> writeReplace -> copy -> readResolve -> newObj  <!-- Describe the purpose of this PR. -->  ## Related issues https://github.com/apache/fury/issues/1849 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,785572daf60d166d7f9c884ec57c79b11bb4cad0,https://github.com/apache/fury/commit/785572daf60d166d7f9c884ec57c79b11bb4cad0,feat(scala): support scala native image build (#1922)  ## What does this PR do?  This PR supports scala native image build and fix quarkus graalvm build for java in https://github.com/quarkiverse/quarkus-fury/issues/7:  ``` Error: Class initialization of org.apache.fury.type.ScalaTypes failed. Use the option  '--initialize-at-run-time=org.apache.fury.type.ScalaTypes'  to explicitly request initialization of this class at run time. com.oracle.svm.core.util.UserError$UserException: Class initialization of org.apache.fury.type.ScalaTypes failed. Use the option  '--initialize-at-run-time=org.apache.fury.type.ScalaTypes'  to explicitly request initialization of this class at run time. at org.graalvm.nativeimage.builder/com.oracle.svm.core.util.UserError.abort(UserError.java:85) at org.graalvm.nativeimage.builder/com.oracle.svm.hosted.classinitialization.ClassInitializationSupport.ensureClassInitialized(ClassInitializationSupport.java:195) at .................. org.graalvm.nativeimage.builder/com.oracle.svm.hosted.classinitialization.ClassInitializationSupport.ensureClassInitialized(ClassInitializationSupport.java:177) ... 43 more Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: scala.collection.Iterable at org.apache.fury.reflect.ReflectionUtils.loadClass(ReflectionUtils.java:649) at org.apache.fury.type.ScalaTypes.<clinit>(ScalaTypes.java:40) ... 46 more Caused by: java.lang.ClassNotFoundException: scala.collection.Iterable at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526) at org.graalvm.nativeimage.builder/com.oracle.svm.hosted.NativeImageClassLoader.loadClass(NativeImageClassLoader.java:637) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526) at org.apache.fury.reflect.ReflectionUtils.loadClass(ReflectionUtils.java:646) ... 47 more ```  ## Related issues  Closes https://github.com/quarkiverse/quarkus-fury/issues/7  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,0201ade88eefc0660ddce813d66b46ecffe4d402,https://github.com/apache/fury/commit/0201ade88eefc0660ddce813d66b46ecffe4d402,feat(java): Improve error message on architecture not using little-endian format (#1918)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  previously there was this kind of error stacktrace: ``` Caused by: java.lang.IllegalArgumentException: false at org.apache.fury.util.Preconditions.checkArgument(Preconditions.java:52) at org.apache.fury.Fury.deserialize(Fury.java:765) at org.apache.fury.Fury.deserialize(Fury.java:815) at org.apache.fury.Fury.deserialize(Fury.java:808) at org.apache.camel.component.fury.FuryDataFormat.unmarshal(FuryDataFormat.java:79) ```  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Signed-off-by: Aurélien Pupier <apupier@redhat.com>
apache,fury,e08748177a1217faf8f9e84957ad7bef67817857,https://github.com/apache/fury/commit/e08748177a1217faf8f9e84957ad7bef67817857,fix(java): Fix incorrect results of utf16 to utf8 conversion for latin1 but not ascii characters (#1914)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  Fix incorrect results of utf16 to utf8 conversion for latin1 but not ascii characters  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,b222660278d75e4ef0779d96667692d8c55de032,https://github.com/apache/fury/commit/b222660278d75e4ef0779d96667692d8c55de032,fix(java): child container deep copy (#1911)    ## What does this PR do? fix child container bug   <!-- Describe the purpose of this PR. -->  ## Related issues https://github.com/apache/fury/issues/1855 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a45886efe81bf223f6c33574c69e7769c8ab0aba,https://github.com/apache/fury/commit/a45886efe81bf223f6c33574c69e7769c8ab0aba,fix(java): ThreadLocalFury and ThreadPoolFury prioritize using the user classloader (#1907)    ## What does this PR do? ThreadLocalFury and ThreadPoolFury prioritize using the user-specified ClassLoader   <!-- Describe the purpose of this PR. -->  ## Related issues [1884](https://github.com/apache/fury/issues/1884) [1878](https://github.com/apache/fury/issues/1878) <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,39b919e04c5646d470e67681b31272368e8be91d,https://github.com/apache/fury/commit/39b919e04c5646d470e67681b31272368e8be91d,fix(java): NonExistentEnum on mode serializeEnumByName (#1904)    ## What does this PR do? Handle NonExistentEnum on mode serializeEnumByName by returning UNKNOWN. since there are no relevancy anymore by using enum ordinal.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,5bd3de5fb0e87274879e19a795bebc1148949af8,https://github.com/apache/fury/commit/5bd3de5fb0e87274879e19a795bebc1148949af8,feat(java): add option to treat enum as string (#1892)  ## What does this PR do? ### Current implementation For now fury read/write enum with ordinal approach by default. So when serializer serialize as 1  deserializer will return the 2nd enum (index 1)  #### For example : non updated library on serializer: ``` enum SearchMode { CHEAPEST  EARLIEST } ```  updated library on deserializer:  ``` enum SearchMode { RECOMMENDED  CHEAPEST  EARLIEST } ``` - if serializer serialize CHEAPEST  deserializer will return RECOMMENDED.  ___ ### New Option This PR will allow fury to treat enum as string so it can have more flexibility like JSON serializer. #### For example: non updated library on serializer: ``` enum SearchMode { CHEAPEST  EARLIEST } ``` updated library on deserializer: ``` enum SearchMode { RECOMMENDED  CHEAPEST  EARLIEST } ``` updated library with different class on deserializer: ``` enum SearchModeV2 { RECOMMENDED  CHEAPEST  EARLIEST } ``` - if serializer serialize CHEAPEST  deserializer will return CHEAPEST. - if serializer serialize CHEAPEST  deserializer for different class will still return CHEAPEST.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [x] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,d6698b0bab04288349626a4d4098edeb90992868,https://github.com/apache/fury/commit/d6698b0bab04288349626a4d4098edeb90992868,feat(scala): add scala range serializer (#1899)  ## What does this PR do?  add scala range serializer  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,14bad4225a45182895df711c721cd1c52db72f32,https://github.com/apache/fury/commit/14bad4225a45182895df711c721cd1c52db72f32,feat(java): support thread safe register callback for scala kotlin (#1895)  ## What does this PR do? support thread safe register callback for scala kotlin <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #1894  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,09abde8afe115f0691062998ad37fef5721ff14b,https://github.com/apache/fury/commit/09abde8afe115f0691062998ad37fef5721ff14b,feat(java): Refactor String serialization and deserialization (#1890)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #1868 Closes #1754  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: chaokunyang <shawn.ck.yang@gmail.com>
apache,fury,750a511d38df604c858f17b274b7843d5121a1ff,https://github.com/apache/fury/commit/750a511d38df604c858f17b274b7843d5121a1ff,feat(kotlin): Add unsigned array support and tests for arrays and strings (#1891)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  This PR adds tests for serializing/deserializing: - Strings (same as Java) - Primitive arrays in Kotlin (same as Java) - Array<T> in kotlin (same as Java) - Unsigned arrays in kotlin - `UByteArray`  `UIntArray`  `UShortArray`  `ULongArray`  Unsigned arrays in kotlin are currently marked experimental  and are subject to API changes (hence the annotations needed to suppress those warnings).  These types are implemented as a view over the signed arrays e.g. UByteArray is a view over ByteArray with contents reinterpreted as UByte  so serializers. The current implementation delegate to existing serializers for corresponding signed types.  The xlang type id is set to LIST for unsigned types.  ## Related issues  #683  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  Yes. Unsigned primitives no longer need to be registered for `fury-kotlin`.  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark N/A
apache,fury,027ddaadf9fe6160d9f01deb304afc7f6817bd41,https://github.com/apache/fury/commit/027ddaadf9fe6160d9f01deb304afc7f6817bd41,fix(java): fix add fury thread safety issue (#1889)  ## What does this PR do? fix add fury thread safety issue <!-- Describe the purpose of this PR. -->  ## Related issues #1840  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,d44e27dc0277da8350b6d3c03cbfb689673844ec,https://github.com/apache/fury/commit/d44e27dc0277da8350b6d3c03cbfb689673844ec,feat(kotlin): Add Unsigned Primitive Support (#1886)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  This PR adds unsigned primitive support to Kotlin Fury. It also adds tests for the standard kotlin primitives(supported by fury Java)  nullable primitive tests  boundary tests for unsigned serializers.  ## Related issues #683  ## Does this PR introduce any user-facing change?  Yes it adds Unsigned support for Kotlin. There's documentation new issue (should add something to document Kotlin!)  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark N/A  ---------  Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,c8698b65f976987edc189a537ca66bf42b9cfcdc,https://github.com/apache/fury/commit/c8698b65f976987edc189a537ca66bf42b9cfcdc,fix(java): fix async compilation switch for non-public nested class (#1883)  ## What does this PR do?  - fix async compilation switch for non-public nested class - fix install sbt for scala - fix install python  ## Related issues  Closes #1879  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,da57b79c2fa6fea93046dfda591b36ce579ae10b,https://github.com/apache/fury/commit/da57b79c2fa6fea93046dfda591b36ce579ae10b,feat(java): use SubListViewSerializer only when tracking ref (#1858)  ## What does this PR do?  use SubListViewSerializer only when tracking ref  ## Related issues Closes #1198  #1856  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,5c82798d252a1e23e2efbc92c8b0f993a5a64bf9,https://github.com/apache/fury/commit/5c82798d252a1e23e2efbc92c8b0f993a5a64bf9,feat(java): implement sublist serializers (#1856)  ## What does this PR do?  Since so many users serialize sublist  but we don't allow this type for serialization  this introduce some confustion. So I added sublist serializers in this PR.  ## Related issues Closes https://github.com/apache/fury/issues/281  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,12a6c83ad3705289a41c35767dbd523fff81278e,https://github.com/apache/fury/commit/12a6c83ad3705289a41c35767dbd523fff81278e,fix(java): fix jdk proxy serialization when proxy writePlace method (#1857)  ## What does this PR do?  fix jdk proxy serialization when proxy writePlace method ## Related issues  Closes #1854  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,3d559292233ae6734fef103703e6972eb39b7840,https://github.com/apache/fury/commit/3d559292233ae6734fef103703e6972eb39b7840,feat(scala): optimize scala class serialization (#1853)  ## What does this PR do?  - Optimize scala Iterable type serialization - Reduce type name writing for msot scala collection and factory types - Add serializers for ToFactory type ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,df5bd78b39149f4ee4ed0a7ee8f56a8342dd0b91,https://github.com/apache/fury/commit/df5bd78b39149f4ee4ed0a7ee8f56a8342dd0b91,perf(java): optimize read classdef perf (#1852)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,902594715659345cb85e92855506e6dabba6724d,https://github.com/apache/fury/commit/902594715659345cb85e92855506e6dabba6724d,perf(java): inline same element invoke in jit (#1851)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,89a8d012901c10981e9fe0b29ba06bfad6321716,https://github.com/apache/fury/commit/89a8d012901c10981e9fe0b29ba06bfad6321716,chore(java): simplify generated codec name (#1850)  ## What does this PR do?  simplify generated codec name  before:  ![image](https://github.com/user-attachments/assets/bbb51fe7-23d1-405a-89c4-e4d0c4c8ab60)  after:  ![image](https://github.com/user-attachments/assets/f3540884-89e7-4a81-9d04-d3dff66ff379)  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,1f1528fd832f9a346e3573c9cf7bed2fbd2be09a,https://github.com/apache/fury/commit/1f1528fd832f9a346e3573c9cf7bed2fbd2be09a,feat(java): support graalvm 17/21/22 (#1845)  ## What does this PR do?  - Support  graalvm 17/21/22 - Add ci for Fury graalvm 17/22 support  ## Related issues  https://github.com/apache/fury/pull/1813   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,8c45d959d1d11fca0b4534f8497c98cc1902fb7b,https://github.com/apache/fury/commit/8c45d959d1d11fca0b4534f8497c98cc1902fb7b,fix(java): Fix for maximum size of java arrays (#1843)  ## What does this PR do?  Fixes the maximum size of Java arrays using Integer.MAX_VALUE when it should be Integer.MAX_VALUE - 8. See this https://github.com/openjdk/jdk14u/blob/84917a040a81af2863fddc6eace3dda3e31bf4b5/src/java.base/share/classes/jdk/internal/util/ArraysSupport.java#L577 or https://www.baeldung.com/java-arrays-max-size  ## Related issues   - #1842  ## Does this PR introduce any user-facing change?  No  - [ ] Does this PR introduce any public API change? No - [ ] Does this PR introduce any binary protocol compatibility change? No  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Co-authored-by: Arthur Finkelstein <arthur.finkelstein@instant-system.com>
apache,fury,8bbd35effac7e864bb7aa8fc7d61fc35699db2dd,https://github.com/apache/fury/commit/8bbd35effac7e864bb7aa8fc7d61fc35699db2dd,fix(java): fix serializer factory getSerializerClass (#1836)  ## What does this PR do? fix serializer factory getSerializerClass <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,bcc01d7a0f2d8b14b5a6018491613369c18d0a3b,https://github.com/apache/fury/commit/bcc01d7a0f2d8b14b5a6018491613369c18d0a3b,fix(java): fix long type name meta string encoding (#1837)  ## What does this PR do? fix long type name meta string encoding <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #1835 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,d648c2840fd414e74ea597874ea61c0b32a6497e,https://github.com/apache/fury/commit/d648c2840fd414e74ea597874ea61c0b32a6497e,fix(java): fix collection view serialization (#1833)  ## What does this PR do? fix collection view serialization <!-- Describe the purpose of this PR. -->  ## Related issues Closes #1831 Closes #1832  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,3cef53c340efc30dd8e8499e29128eb81212f144,https://github.com/apache/fury/commit/3cef53c340efc30dd8e8499e29128eb81212f144,fix(java): fix nested map field value serialization by private map serializer (#1820)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues Closes #1816  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,2f64ade0944193d108fda7fee6fe23a7fe308968,https://github.com/apache/fury/commit/2f64ade0944193d108fda7fee6fe23a7fe308968,fix(java): fix reserved keyword conflict (#1819)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues Closes #1818 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,8d5f8f3da1e8217edcdc448ab83ff7b7efe6b073,https://github.com/apache/fury/commit/8d5f8f3da1e8217edcdc448ab83ff7b7efe6b073,fix(java): Fix replace resolver serializaiton (#1812)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #1805 Closes #1804   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,fd4ba2e2cbb7da1d0c6752de20752290b9594cee,https://github.com/apache/fury/commit/fd4ba2e2cbb7da1d0c6752de20752290b9594cee,fix(scala): fix nested type serialization in scala object type (#1809)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->   ## Related issues  Closes #1801   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,9a39cb3fc7a92e2c9546b094d60af62fc6d2e603,https://github.com/apache/fury/commit/9a39cb3fc7a92e2c9546b094d60af62fc6d2e603,feat(java): Support copy capabilities for some classes without no-argument constructors (#1794)  ## What does this PR do? Some classes with no-argument constructors will report an error when calling `copy()`.  This pr: - implement the copy method for the no-argument constructor serializer - add test case   ## Related issues https://github.com/apache/fury/issues/1777 https://github.com/apache/fury/issues/1679  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,497fe0af970c4a40d5b20e4323e9f9736171fdb4,https://github.com/apache/fury/commit/497fe0af970c4a40d5b20e4323e9f9736171fdb4,fix(java): fix classloader get npe (#1792)  ## What does this PR do?  fix classloader get npe  ## Related issues  Closes #1763   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,6aa768665ebf06d5b374f64f7097a890bbf27f1a,https://github.com/apache/fury/commit/6aa768665ebf06d5b374f64f7097a890bbf27f1a,chore(java): Disallow writing meta classdef when obj is null (#1686)    ## What does this PR do?  <!-- Describe the purpose of this PR. --> When obj is null  if shareMeta is enabled  no need to write meta classdef  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Signed-off-by: LiangliangSui <coolsui.coding@gmail.com> Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,407b65820fa3c1395b9643557faee30e39451f58,https://github.com/apache/fury/commit/407b65820fa3c1395b9643557faee30e39451f58,feat(java): ThreadSafeFury add getClassResolver method (#1780)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. --> Because using ThreadSafeFury cannot get ClassResolver  so I submitted this pr to increase some checklistener and other behavior  ## Related issues    ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,fd3976076a27f6f906c79bedb7c0c23a34a3abca,https://github.com/apache/fury/commit/fd3976076a27f6f906c79bedb7c0c23a34a3abca,feat(java): support deep ref copy (#1771)  ## What does this PR do?  support deep ref copy  ## Related issues  Closes #1747 https://github.com/apache/fury/issues/1679   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,40697ed5855830542d9c259cd91af90ff529c5a3,https://github.com/apache/fury/commit/40697ed5855830542d9c259cd91af90ff529c5a3,fix(java): fix enum abstract field serialization (#1765)  ## What does this PR do?  fix enum abstract field serialization  ## Related issues  Closes #1764  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,4f24bd4d196080cf8b85b3042cdd001b723a87d7,https://github.com/apache/fury/commit/4f24bd4d196080cf8b85b3042cdd001b723a87d7,fix(java): fix fury logger npe (#1762)  ## What does this PR do?   ## Related issues Closes #1761   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,6e4d8a0a746a8d471895afbdbda51caa4ea0e633,https://github.com/apache/fury/commit/6e4d8a0a746a8d471895afbdbda51caa4ea0e633,fix(java): fix big buffer streaming MetaShared read offset (#1760)  ## What does this PR do?  fix big buffer streaming MetaShared read by using relative offset  ## Related issues  Closes #1759  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,7b6e9ed5b5472f37bfcaf92ac517ade81c6d7a82,https://github.com/apache/fury/commit/7b6e9ed5b5472f37bfcaf92ac517ade81c6d7a82,chore(java): rename copyTrackingRef to copyRef (#1748)  ## What does this PR do?  rename copyTrackingRef to copyRef  ## Related issues #1679 #1747   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,1e2a52843ad2540eb0bd444d970f8cfa3cc8cee3,https://github.com/apache/fury/commit/1e2a52843ad2540eb0bd444d970f8cfa3cc8cee3,fix(java): fix streaming classdef read (#1758)  ## What does this PR do?  fix streaming classdef read  ## Related issues  Closes #1757  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,1a5c35788fdfcffbb518b44c77eb6c7cf7a3533d,https://github.com/apache/fury/commit/1a5c35788fdfcffbb518b44c77eb6c7cf7a3533d,feat(java): support Ignore inconsistent types deserialize (#1737)    ## What does this PR do?  <!-- Describe the purpose of this PR. -->   ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [x] Does this PR introduce any public API change? - [x] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: weijiang.wj <weijiang.wj@alibaba-inc.com> Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,eee65280990b10a4e875b6a17da8112cddea9c9e,https://github.com/apache/fury/commit/eee65280990b10a4e875b6a17da8112cddea9c9e,feat(java): support jdk17+ record copy (#1741)  ## What does this PR do?  This PR supports jdk17+ record copy  ## Related issues  #1739 #1701  Closes #1740  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,5e0b8a9c535c954132baad1cd0bf580824f7226d,https://github.com/apache/fury/commit/5e0b8a9c535c954132baad1cd0bf580824f7226d,perf(java): optimize pojo copy performance (#1739)  ## What does this PR do? optimize pojo copy performance by 1~4X and add copy benchmark  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  Before: ```java Benchmark                (bufferType)   (objectType)  (references)   Mode  Cnt        Score         Error  Units CopyBenchmark.fury_copy         array  MEDIA_CONTENT         false  thrpt    3  1294614.644 ± 2103796.392  ops/s CopyBenchmark.fury_copy         array         SAMPLE         false  thrpt    3  1909071.799 ± 2343118.356  ops/s CopyBenchmark.fury_copy         array         STRUCT         false  thrpt    3  1220680.635 ± 1019806.837  ops/s CopyBenchmark.fury_copy         array        STRUCT2         false  thrpt    3   584429.541 ±  111229.502  ops/s CopyBenchmark.kryo_copy         array  MEDIA_CONTENT         false  thrpt    3  1008490.635 ±  309047.316  ops/s CopyBenchmark.kryo_copy         array         SAMPLE         false  thrpt    3   921863.274 ± 1082442.180  ops/s CopyBenchmark.kryo_copy         array         STRUCT         false  thrpt    3  1336939.990 ±  795836.830  ops/s CopyBenchmark.kryo_copy         array        STRUCT2         false  thrpt    3   168367.000 ±  236966.711  ops/s ```  Java ```java Benchmark                (bufferType)   (objectType)  (references)   Mode  Cnt        Score         Error  Units CopyBenchmark.fury_copy         array  MEDIA_CONTENT         false  thrpt    3  2201830.808 ± 4640532.805  ops/s CopyBenchmark.fury_copy         array         SAMPLE         false  thrpt    3  4945272.027 ± 5429361.187  ops/s CopyBenchmark.fury_copy         array         STRUCT         false  thrpt    3  4809373.970 ± 6803285.896  ops/s CopyBenchmark.fury_copy         array        STRUCT2         false  thrpt    3  2577391.052 ± 6682601.210  ops/s CopyBenchmark.kryo_copy         array  MEDIA_CONTENT         false  thrpt    3   830059.189 ± 2509547.599  ops/s CopyBenchmark.kryo_copy         array         SAMPLE         false  thrpt    3   696901.072 ±  525070.309  ops/s CopyBenchmark.kryo_copy         array         STRUCT         false  thrpt    3   980635.311 ± 2495689.418  ops/s CopyBenchmark.kryo_copy         array        STRUCT2         false  thrpt    3   141996.627 ±  343339.930  ops/s ```
apache,fury,a8a140b41c744467239461d0d9742254ee035233,https://github.com/apache/fury/commit/a8a140b41c744467239461d0d9742254ee035233,perf(java): add struct benchmark with pb (#1736)  ## What does this PR do?  add struct benchmark with pb:  Perf: ``` Benchmark                       Mode  Cnt      Score      Error  Units fury_deserialize                thrpt   30  49667.900 ± 3004.061  ops/s fury_kv_compatible_deserialize  thrpt   30  33014.595 ± 3716.199  ops/s fury_kv_compatible_serialize    thrpt   30  23915.260 ± 3968.119  ops/s fury_serialize                  thrpt   30  63146.826 ± 2930.505  ops/s protobuf_deserialize            thrpt   30  14156.610 ±  685.272  ops/s protobuf_serialize              thrpt   30  10060.293 ±  706.064  ops/s ```   Lib | Size -- | -- fury | 8077 furystrict | 8009 furykv | 48028 protobuf | 18000  <br class="Apple-interchange-newline">    ![image](https://github.com/user-attachments/assets/f46a7e66-ae50-44ca-972c-4b176b38146c)  ![image](https://github.com/user-attachments/assets/74b59d30-028d-47a2-8499-0962ab44e20c)   ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a5fc14246a3d15d4742f2e5bcb920b2106d8d639,https://github.com/apache/fury/commit/a5fc14246a3d15d4742f2e5bcb920b2106d8d639,perf(java): optimize scoped meta share mode perf (#1734)  ## What does this PR do?  This PR optimizes scoped meta share mode writing perf by about 30%: - Replace ArrayList by ObjectArray  which can save `clear` cost - Speed up copy performance when writing classdefs  ## Related issues  #1733  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,c9705d1bd976f33a8e5f49431dc796853b069668,https://github.com/apache/fury/commit/c9705d1bd976f33a8e5f49431dc796853b069668,feat(java): enable scoped meta share for compatible mode by default (#1733)  ## What does this PR do?  This PR enable scoped meta share for compatible mode by default: - Enable scoped meta share for compatible mode by default - Extensive tests for scoped meta share mode - Support for graalvm  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,46d48c312b8f414498a6b9dd4b395d7017da5fc1,https://github.com/apache/fury/commit/46d48c312b8f414498a6b9dd4b395d7017da5fc1,chore(java): merge reflect.Types into TypeRef (#1731)  ## What does this PR do?  This PR moves `org.apache.fury.reflect.Types` into TypeRef. Types is used in Fury type system  by merge `org.apache.fury.reflect.Types`  we can reduce ambiguation in fury type system  ## Related issues #1553 #1690  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,0c9dcc2056b332e73584d2f7cddf3b412d0699c7,https://github.com/apache/fury/commit/0c9dcc2056b332e73584d2f7cddf3b412d0699c7,fix(java): fix fastjson object serialization (#1717)  ## What does this PR do?  Closes #1716  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a7c45f344bf65bea73e2f0cb9aeb17a3698bec1f,https://github.com/apache/fury/commit/a7c45f344bf65bea73e2f0cb9aeb17a3698bec1f,fix(java): fix nested map serialization codegen (#1713)  ## What does this PR do?  Closes #1700  ## Related issues     ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,09fda94ab7f476da4dc3b6752825b7249bda6ac2,https://github.com/apache/fury/commit/09fda94ab7f476da4dc3b6752825b7249bda6ac2,refactor(java): move latin language checker method from string serializer to string util (#1708)    ## What does this PR do?  <!-- Describe the purpose of this PR. --> This PR decouples and moves the `isLatin([])` method from `StringSerializer` class to `StringUtils`.   ## Related issues  <!-- Is there any related issue? Please attach here.  - #1703 - #xxxx1 - #xxxx2 --> #1703   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,f1651d6ed8fa389dd19ad766021e74869bf59d04,https://github.com/apache/fury/commit/f1651d6ed8fa389dd19ad766021e74869bf59d04,fix(java): return fury to pooled which get from (#1697)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  incase of pooledCache expired from cache  and fury will return to another pooledCache. so we save pooledCache instead of get from cache.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,31d37f9cf2021899f3dc2f1b06d5b45e79099251,https://github.com/apache/fury/commit/31d37f9cf2021899f3dc2f1b06d5b45e79099251,perf(java): Add ClassInfo ClassBytes generation conditions. (#1667)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/incubator-fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/incubator-fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. --> `ClassInfo#classNameBytes` and `ClassInfo#packageNameBytes` are only used when `classInfo.classId == NO_CLASS_ID && !metaContextShareEnabled`  ```java public void writeClass(MemoryBuffer buffer  ClassInfo classInfo) { if (classInfo.classId == NO_CLASS_ID) { // no class id provided. // use classname if (metaContextShareEnabled) { buffer.writeByte(USE_CLASS_VALUE_FLAG); // FIXME(chaokunyang) Register class but not register serializer can't be used with //  meta share mode  because no class def are sent to peer. writeClassWithMetaShare(buffer  classInfo); } else { // if it's null  it's a bug. assert classInfo.packageNameBytes != null; metaStringResolver.writeMetaStringBytesWithFlag(buffer  classInfo.packageNameBytes); assert classInfo.classNameBytes != null; metaStringResolver.writeMetaStringBytes(buffer  classInfo.classNameBytes); } } else { // use classId buffer.writeVarUint32(classInfo.classId << 1); } } ```   ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/incubator-fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Signed-off-by: LiangliangSui <coolsui.coding@gmail.com>
apache,fury,a2515a936b439129d93eb22acc5c63a23285f23b,https://github.com/apache/fury/commit/a2515a936b439129d93eb22acc5c63a23285f23b,fix(java): Fix header offset issue in MetaStringBytes hashcode (#1668)    ## What does this PR do?  <!-- Describe the purpose of this PR. --> MetaStringBytes `hashcode & 0xff`  that is  header  represents the encoding  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/incubator-fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Signed-off-by: LiangliangSui <coolsui.coding@gmail.com>
apache,fury,da5f8473818bc2768b4f3b5a42c39a2c7fff8120,https://github.com/apache/fury/commit/da5f8473818bc2768b4f3b5a42c39a2c7fff8120,feat(java): support meta compression by Deflater (#1663)  ## What does this PR do?  This PR support meta compression and add Deflater as default compressor.  In our test  it can compress meta by reduce size of **243** without introducing any performance cost:  ``` before: Fury | STRUCT | false | array | 1227 |  after STRUCT | false | array | 984 |  ```  ## Related issues #1660   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/incubator-fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,3f5cf31461cfed1f2c1d0f5b620529fe4abb6f40,https://github.com/apache/fury/commit/3f5cf31461cfed1f2c1d0f5b620529fe4abb6f40,fix(java): fix scala object type codegen (#1659)  ## What does this PR do?  fix scala object type codegen  ## Related issues  Closes #1658  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/incubator-fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,3a0e410cb83756d3b139a9fbed01a9a64dbb2970,https://github.com/apache/fury/commit/3a0e410cb83756d3b139a9fbed01a9a64dbb2970,feat(java): support nonexistent class deserialization in meta share mode (#1646)  ## What does this PR do?  support nonexistent class deserialization in meta share mode  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/incubator-fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
aeron-io,simple-binary-encoding,ec42a79fab4eadc5fc248c06fef453a39258573d,https://github.com/aeron-io/simple-binary-encoding/commit/ec42a79fab4eadc5fc248c06fef453a39258573d,Std span (#1038)  * [C++] Integrate std::span support for flyweight API  The impetus was a bug that we ran into when writing a string-literal to a fixed-width char field:  ```c++ flyweight.putFixedChar("hello"); ```  This is unsafe: - If the field size is less than 6  we overrun the buffer and corrupt it. - If the field size is more than 6  we don't zero pad the rest of it.  Instead  we build on support for the std::string_view getters and setters  which do length checking. std::span generalizes this to fixed-width fields of all types. Notably  if the size of the std::span is knowable at compile time  we pay no runtime cost for the length checking  and we should get similar performance to the existing API which takes a raw pointer.  Further  we add a sbetool option to disable accepting arrays by raw pointer  which should prevent memcpy operation without bounds checking. This is off by default to avoid a breaking change.  * [C++] hide USE_SPAN behind ENABLE_SPAN  * [C++] add macro guards  ---------  Co-authored-by: Matt Stern <stern@pdtpartners.com>
aeron-io,simple-binary-encoding,96b70326c33c1f1756aca7e5f5746022d8aee72c,https://github.com/aeron-io/simple-binary-encoding/commit/96b70326c33c1f1756aca7e5f5746022d8aee72c,[C++] Generate DTOs for non-perf-sensitive usecases.  In some applications performance is not cricital. Some users would like to use SBE across their whole "estate"  but don't want the "sharp edges" associated with using flyweight codecs  e.g.  accidental escape.  In this commit  I've added a first cut of DTO generation for C++ and a simple test based on the Car Example.  The DTOs support encoding and decoding via the generated codecs using `DtoT::encode(CodecT& codec  const DtoT& dto)` and `DtoT::decode(CodecT& codec  Dto& dto)` methods.  Generation can be enabled specifying the target code generator class  `uk.co.real_logic.sbe.generation.cpp.CppDtos`  or by passing a system property `-Dsbe.cpp.generate.dtos=true`.
aeron-io,simple-binary-encoding,5389910a15df61c0c0f1f27810d2c4cfb47d0a21,https://github.com/aeron-io/simple-binary-encoding/commit/5389910a15df61c0c0f1f27810d2c4cfb47d0a21,[C#] Generate DTOs from SBE IR for non-perf-sensitive usecases.  In some applications performance is not cricital. Some users would like to use SBE across their whole "estate"  but don't want the "sharp edges" associated with using flyweight codecs  e.g.  accidental escape.  In this commit  I've added a first cut of DTO generation for C# and a simple test based on the Car Example.  The DTOs support encoding and decoding via the generated codecs using `EncodeInto(CodecT codec)` and `DecodeFrom(CodecT codec)` methods.  Currently there is no support for equality/comparison or read-only views over the data; although  these have been requested.  Here are some points that we may or may not wish to change in the future:  1. Non-present (due to the encoded version) string/array data and repeating groups are represented as `null` rather than empty.  2. Non-present primitive values are represented as their associated null value rather than using nullable types.  3. Non-present bitsets are represented as `0`.  4. DTOs are generated via a separate `CodeGenerator` rather than a flag to the existing C# `CodeGenerator`.
portfolio-performance,portfolio,46f36c346b2529b119f9b583a1a59ad35c5c2995,https://github.com/portfolio-performance/portfolio/commit/46f36c346b2529b119f9b583a1a59ad35c5c2995,Modify Swissquote PDF-Importer to support new trasnactions (#4751)  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/52
portfolio-performance,portfolio,9231f9dd6d540bbf0a922b7757f0bd0739775766,https://github.com/portfolio-performance/portfolio/commit/9231f9dd6d540bbf0a922b7757f0bd0739775766,Modify KBC Group NV PDF-Importer to support new transaction (#4750)  https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/34 https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/35 https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/36
portfolio-performance,portfolio,6245079836a1075304ff42bf0520cff76e5661aa,https://github.com/portfolio-performance/portfolio/commit/6245079836a1075304ff42bf0520cff76e5661aa,Modify Saxo Bank PDF-Importer to support new transactions (#4745)  Closes #4711 Closes #4712 Closes #4713 Closes #4714  https://forum.portfolio-performance.info/t/pdf-import-from-saxo-bank/21481/33
portfolio-performance,portfolio,ec2fe497c88011008617f7d905589f677c77d1aa,https://github.com/portfolio-performance/portfolio/commit/ec2fe497c88011008617f7d905589f677c77d1aa,Modify Baader Bank PDF-Importe to support new transaction (#4738)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/447
portfolio-performance,portfolio,f1d75fbc98f6f3b8f72351af824317289456e7af,https://github.com/portfolio-performance/portfolio/commit/f1d75fbc98f6f3b8f72351af824317289456e7af,Fixed calculation of moving average in trades for currencies other than EUR  Issue: https://forum.portfolio-performance.info/t/error-in-trades-view-after-adding-column-profit-loss-ma-and-entry-value-ma/33132
portfolio-performance,portfolio,cee2fc02b44649e44c0d336998e4687df396f7de,https://github.com/portfolio-performance/portfolio/commit/cee2fc02b44649e44c0d336998e4687df396f7de,Modify Scalable Capital PDF-Importer to support new transaction (#4727)  https://forum.portfolio-performance.info/t/pdf-import-von-scalable-capital/33088/17
portfolio-performance,portfolio,f0df65976425abfedd12742c35bbe517a253696b,https://github.com/portfolio-performance/portfolio/commit/f0df65976425abfedd12742c35bbe517a253696b,Modify Scalable Capital PDF-Importer to support new transaction (#4725)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/436 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/441 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/442
portfolio-performance,portfolio,4e70bd5182429d3fb5566202223db007c18f7ecc,https://github.com/portfolio-performance/portfolio/commit/4e70bd5182429d3fb5566202223db007c18f7ecc,Modify N26 PDF-Importer to support new transaction (#4723)  https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/16
portfolio-performance,portfolio,c1607a5549656cafc40dbbc829e18dc58b513177,https://github.com/portfolio-performance/portfolio/commit/c1607a5549656cafc40dbbc829e18dc58b513177,Modify Scalable Capital PDF-Importer to support new trasnaction (#4722)  https://forum.portfolio-performance.info/t/pdf-import-von-scalable-capital/33088
portfolio-performance,portfolio,5c37029f6d30cdea6e0759b31da4db62bcfa00b0,https://github.com/portfolio-performance/portfolio/commit/5c37029f6d30cdea6e0759b31da4db62bcfa00b0,Modify Saxo Bank PDF-Importer to support new transaction (#4684)  https://forum.portfolio-performance.info/t/pdf-import-from-saxo-bank/21481/23
portfolio-performance,portfolio,bcaf07c55a98e8ddc25b0ce137e365795a23eb7d,https://github.com/portfolio-performance/portfolio/commit/bcaf07c55a98e8ddc25b0ce137e365795a23eb7d,Troubleshooting and menu improvement  Closes #3591  * The entries in the help menu are sorted by intuitive. * Some URLs are incorrect  incomplete  or link to the wrong website. * Add the menu manual * Add the menu Report an issue * Add in bundle.properties command.openbrowser.{...}.url * Revision in bundle.properties command.openbrowser.{...}.name * Revision in bundle.properties command.openbrowser.{...}.tooltip * Moved OpenBrowserHandler from name.abuchen.portfolio.ui.handlers to name.abuchen.portfolio.bootstrap.handlers  Some command.openbrowser.{...}.tooltips were removed  because they are either identical with command.openbrowser.{...}.name or not necessary. There would therefore be duplicate translations  only that the parameter is different.  As an example: %command.openbrowser.changelog.name is equal to %command.openbrowser.changelog.tooltip ...  therefore tooltip is removed and %command.openbrowser.changelog.name is used as tooltip.  Portfolio Performance main menu:  New & noteworthy: Link not correct (e.g. ...tooltip in Polish) Default: "https://forum.portfolio-performance.info/t/new-noteworthy/17945/last/" German: "https://forum.portfolio-performance.info/t/sunny-neues-nennenswertes/23/last/"  Manual: Newly added Default: "https://help.portfolio-performance.info/en/" German: "https://help.portfolio-performance.info/de/"  Report an issue: Newly added Default: "https://forum.portfolio-performance.info/t/what-should-i-bear-in-mind-when-reporting-a-problem-or-error/32558/" German: "https://forum.portfolio-performance.info/t/was-sollte-ich-beim-melden-eines-problems-oder-fehlers-beachten/463/"  Forum: Default: "https://forum.portfolio-performance.info/c/english/" German: "https://forum.portfolio-performance.info/c/deutsch/"  GitHub: a. o. Portuguese is linked incorrectly Default: "https://github.com/portfolio-performance/portfolio/"  How-Tos: The link "https://forum.portfolio-performans.info/c/how-to" does not exist. Links assigned incorrectly. Default: "https://forum.portfolio-performance.info/c/english/how-to/18/" German: "https://forum.portfolio-performance.info/c/deutsch/how-to/5/" Rename: in bundle rename "how-tos" to "howTo"  FAQ: The FAQ link (https://forum.portfolio-performance.info/c/faq) redirects to the How-Tos section  therefore it is duplicated by the How-Tos. Removed: FAQ menu link  Changelog: The links are wrong  incorrect and mixed up - https://github.com/buchen/portfolio/releases/ - https://github.com/portfolio-performance/portfolio/releases  Default: "https://github.com/portfolio-performance/portfolio/releases/" German: "https://www.portfolio-performance.info/portfolio/versions.html"  ---  Welcome page: Removed: FAQ removed. The same as in the main menu. Added: Portfolio Performance Manual  Issue: #4646
portfolio-performance,portfolio,94b36a83ba7f5768b5fcbc7e040f80d0e329cd69,https://github.com/portfolio-performance/portfolio/commit/94b36a83ba7f5768b5fcbc7e040f80d0e329cd69,Improve PortfolioPerformanceSearchProvider  Convert the security type using the SecuritySearchProvider instance Remove OpenFIGI.java  Issue: #4672 Signed-off-by: Alexander Ott <webmaster@nirus-online.de> [added missing type to SecuritySearchProvider; keep original capitalization if type is unknown] Signed-off-by: Andreas Buchen <andreas.buchen@gmail.com>
portfolio-performance,portfolio,2575926221e49f0ae52157b28dbefa2647b5de65,https://github.com/portfolio-performance/portfolio/commit/2575926221e49f0ae52157b28dbefa2647b5de65,Modify Commerzbank PDF-Importer to support new transaction (#4682)  https://github.com/portfolio-performance/portfolio/pull/4651#issuecomment-2832306299
portfolio-performance,portfolio,9f48cca1bcefcdb67329dfa0f5f3a04b35805b04,https://github.com/portfolio-performance/portfolio/commit/9f48cca1bcefcdb67329dfa0f5f3a04b35805b04,Modify KBC Group NV PDF-Importer to support new transaction (#4679)  https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/28 https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/30
portfolio-performance,portfolio,1c0c56727a834c872561f9f17cd62581cc4c52e5,https://github.com/portfolio-performance/portfolio/commit/1c0c56727a834c872561f9f17cd62581cc4c52e5,Modify Trade Republic PDF-Importer to support new transaction (#4678)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/766 Closes #4642
portfolio-performance,portfolio,5902183ecda7f879dc9b878ffb0dd238b87cddf4,https://github.com/portfolio-performance/portfolio/commit/5902183ecda7f879dc9b878ffb0dd238b87cddf4,Refactor tree map view: option to color by performance  Additionally adds the option to show a heading for the categories to increase readability (but could decrease the comparability of the rectangle sizes).
portfolio-performance,portfolio,36a4791e06392ea4a5bddfb342bc2c35edefbc14,https://github.com/portfolio-performance/portfolio/commit/36a4791e06392ea4a5bddfb342bc2c35edefbc14,Modify Consorbank PDF-Importer to support new transaction (#4677)  https://forum.portfolio-performance.info/t/pdf-import-von-consorsbank/2697/313
portfolio-performance,portfolio,a5d061da48db05875fcec889438e3bae70859808,https://github.com/portfolio-performance/portfolio/commit/a5d061da48db05875fcec889438e3bae70859808,Modify Modena Estonia PDF-Importer to support new transaction (#4675)  https://forum.portfolio-performance.info/t/pdf-import-von-modena-estonia-ou/32378/8
portfolio-performance,portfolio,125abb2f43a4d0bf8dc63ebf7831a9e31de5e7a9,https://github.com/portfolio-performance/portfolio/commit/125abb2f43a4d0bf8dc63ebf7831a9e31de5e7a9,Update edit dialog for Portfolio Performance feed
portfolio-performance,portfolio,5459519ea20fccfbdd57336d56db9716eb19683c,https://github.com/portfolio-performance/portfolio/commit/5459519ea20fccfbdd57336d56db9716eb19683c,Modify Commerzbank PDF-Importer (#4659)  Merge #4651 Closes #4621 https://forum.portfolio-performance.info/t/pdf-import-von-commerzbank/9865/46 https://forum.portfolio-performance.info/t/pdf-import-von-commerzbank/9865/52 https://forum.portfolio-performance.info/t/pdf-import-von-commerzbank/9865/58
portfolio-performance,portfolio,3403b59ab06d2bde618c180e678052850abd782c,https://github.com/portfolio-performance/portfolio/commit/3403b59ab06d2bde618c180e678052850abd782c,Modify ComDirect PDF-Importer to support new transaction (#4656)  https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/413 https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/418 https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/419
portfolio-performance,portfolio,b076195087c2ea7dce0e00adc74dbe0e006c355a,https://github.com/portfolio-performance/portfolio/commit/b076195087c2ea7dce0e00adc74dbe0e006c355a,Use lazy security performance record in SecuritiesChart
portfolio-performance,portfolio,5467825377c92643acd958b86b35017d5a4c61b0,https://github.com/portfolio-performance/portfolio/commit/5467825377c92643acd958b86b35017d5a4c61b0,Modify Saxo Bank PDF-Importer to support new transaction (#4644)  https://forum.portfolio-performance.info/t/pdf-importer-fur-saxo-bank/30548/31
portfolio-performance,portfolio,5dd6ca0552458d6db4322af0ba7c96843a0b0bd4,https://github.com/portfolio-performance/portfolio/commit/5dd6ca0552458d6db4322af0ba7c96843a0b0bd4,Modfiy BSDEX PDF-Importer to support new transaction (#4643)  https://forum.portfolio-performance.info/t/pdf-import-von-bsdex-borse-stuttgart-digital-exchange/20155/17
portfolio-performance,portfolio,4337463f1d44ffa9d6d7ed6844eb156178951c31,https://github.com/portfolio-performance/portfolio/commit/4337463f1d44ffa9d6d7ed6844eb156178951c31,Fix NumberFormatException with moving average gains and outbound delivery of zero  Issue: https://forum.portfolio-performance.info/t/v-0-75-0-bugs-neue-wertpapier-suchmaske/32532/7?u=andreasb Issue: https://forum.portfolio-performance.info/t/0-75-0-interner-zinsfuss-fehlermeldung/32545
portfolio-performance,portfolio,0bea413de223c73dbbcae3e76939a12860f5d901,https://github.com/portfolio-performance/portfolio/commit/0bea413de223c73dbbcae3e76939a12860f5d901,Modify Bison PDF-Importer to support new transaction (#4633)  https://forum.portfolio-performance.info/t/pdf-import-von-bison/18929/11
portfolio-performance,portfolio,a73d34ba23e4bf1eb921217052bab255665f7e6f,https://github.com/portfolio-performance/portfolio/commit/a73d34ba23e4bf1eb921217052bab255665f7e6f,Add test case to Baader Bank PDF-Importer (#4632)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/428
portfolio-performance,portfolio,2c0b42fcac8de55757cfc8154d2bb1b1b5982e6b,https://github.com/portfolio-performance/portfolio/commit/2c0b42fcac8de55757cfc8154d2bb1b1b5982e6b,Modify Estateguru PDF-Importer to support new transactions (#4631)  https://forum.portfolio-performance.info/t/pdf-import-von-estateguru/21304/9
portfolio-performance,portfolio,17b645cd1f8e0b97cbc91be8c960c33c8d04822d,https://github.com/portfolio-performance/portfolio/commit/17b645cd1f8e0b97cbc91be8c960c33c8d04822d,Modify Swissquote PDF-Importer to support new transaction (#4626)  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/47
portfolio-performance,portfolio,690f070f62524b0289a1780d806762fe39460e80,https://github.com/portfolio-performance/portfolio/commit/690f070f62524b0289a1780d806762fe39460e80,Modify Trade Republic PDF-Importer to support new transaction (#4625)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/104
portfolio-performance,portfolio,4ca9fdbc61f599fec2c0cda42f6c201cccb9313a,https://github.com/portfolio-performance/portfolio/commit/4ca9fdbc61f599fec2c0cda42f6c201cccb9313a,Modify FlatEx PDF-Importer and implement getTickerSymbolForCrypto  https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/277 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/278 ------------------ Enhancement: Implement getTickerSymbolForCrypto Method Description  This enhancement introduces the getTickerSymbolForCrypto method  which maps cryptocurrency names to their respective ticker symbols. The method accepts a cryptocurrency name as input. It normalizes the input (trims whitespace and converts it to uppercase). A predefined mapping (cryptoTickerMap) returns the corresponding ticker symbol. If the name is missing or not found  an empty string ("") is returned.
portfolio-performance,portfolio,8e58e17a2e0cb1f66bd766359930d7f2a5d0a1d9,https://github.com/portfolio-performance/portfolio/commit/8e58e17a2e0cb1f66bd766359930d7f2a5d0a1d9,Modify Arkéa Direct Bank PDF-Importer to support new transaction (#4619)  Close #4614 https://forum.portfolio-performance.info/t/pdf-import-from-arkea-direct-bank-fortuneo-banque-france/28131/24
portfolio-performance,portfolio,b7bf6d26388b8b5c303ab1eb1fcff3e98e09d591,https://github.com/portfolio-performance/portfolio/commit/b7bf6d26388b8b5c303ab1eb1fcff3e98e09d591,Add new Bourse Direct PDF-Importer (#4609)  https://forum.portfolio-performance.info/t/pdf-import-from-boursedirect/31291
portfolio-performance,portfolio,d70103a845f2011b07f273bc5c44f6e0df214465,https://github.com/portfolio-performance/portfolio/commit/d70103a845f2011b07f273bc5c44f6e0df214465,Modify BaaderBank PDF-Importer to support new transactions (#4608)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/418 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/424
portfolio-performance,portfolio,52e30af9093df9da482d5d6e6c7ca1fe48132f3b,https://github.com/portfolio-performance/portfolio/commit/52e30af9093df9da482d5d6e6c7ca1fe48132f3b,Add new Modena Estonia OÜ PDF-Importer (#4607)  https://forum.portfolio-performance.info/t/pdf-import-von-modena-estonia-ou/32378
portfolio-performance,portfolio,a4a0c6f9d06bbf4a645e80811ca50d456599c925,https://github.com/portfolio-performance/portfolio/commit/a4a0c6f9d06bbf4a645e80811ca50d456599c925,Add new Crédit Mutuel Alliance Fédérale PDF-Importer (#4606)  https://forum.portfolio-performance.info/t/pdf-import-from-credit-mutuel-alliance-federale-suravenir/31859
portfolio-performance,portfolio,e7d575a8ff629066d91c9438c2b79a65f14a5e77,https://github.com/portfolio-performance/portfolio/commit/e7d575a8ff629066d91c9438c2b79a65f14a5e77,Add test case to Comdirect PDF-Importer (#4603)  https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/395
portfolio-performance,portfolio,fca87b06377db0efc082737ebe12715c51507ee5,https://github.com/portfolio-performance/portfolio/commit/fca87b06377db0efc082737ebe12715c51507ee5,Modify Easybank PDF-Importer to support new transaction (#4601)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/75
portfolio-performance,portfolio,963139350a8caa81c258b4253673627a73c3a243,https://github.com/portfolio-performance/portfolio/commit/963139350a8caa81c258b4253673627a73c3a243,Fix Swissquote PDF-Importer (#4594)  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/39
portfolio-performance,portfolio,04b011078e50a8735b65def10ed59ac49b1c3192,https://github.com/portfolio-performance/portfolio/commit/04b011078e50a8735b65def10ed59ac49b1c3192,Modify Postbank PDF-Importer to support new transaction (#4592)  https://forum.portfolio-performance.info/t/pdf-import-von-postbank/7234/74
portfolio-performance,portfolio,0a8934769f014261fffd84314ff838a99002a201,https://github.com/portfolio-performance/portfolio/commit/0a8934769f014261fffd84314ff838a99002a201,Modify ING Diba PDF-Importer to support new transaction (#4591)  https://forum.portfolio-performance.info/t/pdf-import-von-ing/7114/176
portfolio-performance,portfolio,acacde0da63ebbc21621b4b43e57b3738365a793,https://github.com/portfolio-performance/portfolio/commit/acacde0da63ebbc21621b4b43e57b3738365a793,Calculate capital gains based costs using the moving average method  * Added CapitalGainsCalculationMovingAverage class to calculate gains * Calculate forex gains based on average exchange rate * Extend performance view  security performance view  dashboard widgets with an option to pick the cost calculation method  Issue: #4546 Signed-off-by: mierin12 <mattmailspoubelle+GIT@gmail.com> [squashed commits; updated calculation of forex gains; added test case; renamed variables for consistency; rebased to master] Signed-off-by: Andreas Buchen <andreas.buchen@gmail.com>
portfolio-performance,portfolio,81c13917161a6c59913362cbfb2b2a912b14dd0c,https://github.com/portfolio-performance/portfolio/commit/81c13917161a6c59913362cbfb2b2a912b14dd0c,Modify Trade Republic PDF-Importer to support new transactions (#4587)  Closes #4584 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/757 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/760
portfolio-performance,portfolio,48c27d00c4f86bf9830d5fe0150f13dd19e18f5a,https://github.com/portfolio-performance/portfolio/commit/48c27d00c4f86bf9830d5fe0150f13dd19e18f5a,Separate filter drop down in SecuritiesPerformanceView
portfolio-performance,portfolio,c5afe6ea9eeb1ca60207c41e38488176ba7e8348,https://github.com/portfolio-performance/portfolio/commit/c5afe6ea9eeb1ca60207c41e38488176ba7e8348,Fix NullPointerException in TextUtil.compare  https://forum.portfolio-performance.info/t/fehlermeldung-internal-error/32110
portfolio-performance,portfolio,2bbb8bc46522cea4a5ece7182f0098778f0b70d8,https://github.com/portfolio-performance/portfolio/commit/2bbb8bc46522cea4a5ece7182f0098778f0b70d8,Modify Swissquote PDF-Importer to support new transaction (#4579)  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/37
portfolio-performance,portfolio,5e7db84e30e8c219b38ca050fc3f116f92f90ef2,https://github.com/portfolio-performance/portfolio/commit/5e7db84e30e8c219b38ca050fc3f116f92f90ef2,Modify Trade Republic PDF.Importer to support new transaction (#4573)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/753
portfolio-performance,portfolio,124794fbd724d78415eb4ff50f7a2ea37638affd,https://github.com/portfolio-performance/portfolio/commit/124794fbd724d78415eb4ff50f7a2ea37638affd,Modify Trade Republic PDF-Importer to support new transaction (#4567)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/745 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/746 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/747 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/749 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/751
portfolio-performance,portfolio,d28c9d46940c01104c79d229734ad7255a3af67f,https://github.com/portfolio-performance/portfolio/commit/d28c9d46940c01104c79d229734ad7255a3af67f,Modify N26 PDF-Importer to support new transaction (#4564)  https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/14
portfolio-performance,portfolio,a067e0910e404adf8e4b7090e116e75da9bafe9f,https://github.com/portfolio-performance/portfolio/commit/a067e0910e404adf8e4b7090e116e75da9bafe9f,Modify Trade Republic PDF-Importer to support new transaction (#4556)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/99
portfolio-performance,portfolio,d3d7cd3ed983740b5f3544a70c57a16b39df9881,https://github.com/portfolio-performance/portfolio/commit/d3d7cd3ed983740b5f3544a70c57a16b39df9881,Modify Saxo PDF-Importer to support new transaction (#4553)  https://forum.portfolio-performance.info/t/pdf-importer-fur-saxo-bank/30548/24
portfolio-performance,portfolio,8760a90034f87b91f7b3d2a7f12fddf46a13811f,https://github.com/portfolio-performance/portfolio/commit/8760a90034f87b91f7b3d2a7f12fddf46a13811f,Modify Trade Republic PDF-Importer to support new transaction (#4552)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/96
portfolio-performance,portfolio,86cb5e23340d7a228fa6d188de0fb9581e2790e3,https://github.com/portfolio-performance/portfolio/commit/86cb5e23340d7a228fa6d188de0fb9581e2790e3,Modify Tradegate AG PDF-Importer to support new transactions (#4550)  https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/13 https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/14 https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/15
portfolio-performance,portfolio,964dfbec93980e35d8584eb4309c8d80a7c02c2d,https://github.com/portfolio-performance/portfolio/commit/964dfbec93980e35d8584eb4309c8d80a7c02c2d,Modify ING DiBa PDF-Importer to support new transaction (#4549)  https://forum.portfolio-performance.info/t/pdf-import-von-ing/7114/174
portfolio-performance,portfolio,0bc6e6e8cda5296415563a9067b3bb6b8694c067,https://github.com/portfolio-performance/portfolio/commit/0bc6e6e8cda5296415563a9067b3bb6b8694c067,Fixed tax withholding calculation in the Targo Bank PDF importer (#4547)  https://forum.portfolio-performance.info/t/pdf-import-von-targobank/5537/60  1.) No tax burden: The losses offset the income so that no withholding tax is payable. 2.) Offsetting only with tax liability: Withholding tax can only be offset if capital gains tax is levied in Germany. 3.) Loss offsetting: The offsetting reduces the tax assessment basis to zero  which also eliminates offsetting. 4.) Savers' lump sum/exemption order: If used  this could also reduce the tax burden to zero.  See test case of taxes treatment transaction ...Dividende05 vs. ...Dividende08
portfolio-performance,portfolio,6acc517477caf125653b25fed50166b1511befa9,https://github.com/portfolio-performance/portfolio/commit/6acc517477caf125653b25fed50166b1511befa9,Enhancement of Arkea Direct Bank pdf import to support more cases (#4534) (#4535)  * Enhancement of Arkea Direct Bank pdf import to support more cases (#4534)  - Allows duplicate ')' at the end of ISIN - Quantity and date can be on the same line - Get security currency from amount instead of earlier from a data which is not always present  Closes #4534  * Improve Arkéa Direct Bank PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-from-arkea-direct-bank-fortuneo-banque-france/28131/13  ---------  Co-authored-by: Alexander Ott <webmaster@nirus-online.de>
portfolio-performance,portfolio,7a8bc4306bc567e9269b93e1a3c876a573be47f6,https://github.com/portfolio-performance/portfolio/commit/7a8bc4306bc567e9269b93e1a3c876a573be47f6,Modify Targo Bank PDF-Importer to support new transaction (#4537)  https://forum.portfolio-performance.info/t/pdf-import-von-targobank/5537/47
portfolio-performance,portfolio,6b3000757488c8855e9efa64cf1e0c6c6cde97dd,https://github.com/portfolio-performance/portfolio/commit/6b3000757488c8855e9efa64cf1e0c6c6cde97dd, Modify FlatEx PDF-Importer to support new transaction (#4533)  https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/265
portfolio-performance,portfolio,aa15fd64eb2b84bd8ae1715f333968f0c2dbb1cd,https://github.com/portfolio-performance/portfolio/commit/aa15fd64eb2b84bd8ae1715f333968f0c2dbb1cd, Modify C24 Bank GmbH PDF-Importer to support new transaction (#4532)  https://forum.portfolio-performance.info/t/pdf-import-von-c24-bank-gmbh/28636/7
portfolio-performance,portfolio,b0e65bc1155cd9f5a6a6810a3bab55751834195e,https://github.com/portfolio-performance/portfolio/commit/b0e65bc1155cd9f5a6a6810a3bab55751834195e,Modify Baader Bank PDF-Importer to support new transaction (#4531)  https://forum.portfolio-performance.info/t/pdf-import-von-traders-place/25739/8 https://forum.portfolio-performance.info/t/pdf-import-von-traders-place/25739/13 https://forum.portfolio-performance.info/t/pdf-import-von-traders-place/25739/14
portfolio-performance,portfolio,224e5c3c411fb83835b24fe30b59d0649ef298c0,https://github.com/portfolio-performance/portfolio/commit/224e5c3c411fb83835b24fe30b59d0649ef298c0,Modify FlatEx PDF-Importer to support new transaction (#4530)  https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/261 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/267 https://forum.portfolio-performance.info/t/import-von-flatex/31261
portfolio-performance,portfolio,df36da793b0cbc6ea948803eddb2f1c04c43fdcb,https://github.com/portfolio-performance/portfolio/commit/df36da793b0cbc6ea948803eddb2f1c04c43fdcb,Modify Trade Republic PDF-Importer to support new transactions (#4524)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/83 https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/88 https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/90
portfolio-performance,portfolio,a4c37c28506b2f72fab04716cc4323f051a3c6a3,https://github.com/portfolio-performance/portfolio/commit/a4c37c28506b2f72fab04716cc4323f051a3c6a3,Modify Scalable Capital  PDF-Importer to support new transaction (#4523)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/416
portfolio-performance,portfolio,1d6930cd7be435410bf33a080d3ddee2600c3dd9,https://github.com/portfolio-performance/portfolio/commit/1d6930cd7be435410bf33a080d3ddee2600c3dd9,Modify DKB PDF-Importer to support new transaction (#4522)  https://forum.portfolio-performance.info/t/pdf-import-von-dkb/4449/125
portfolio-performance,portfolio,b3a39ff07af5f913556a5730b06ada2da572a6cc,https://github.com/portfolio-performance/portfolio/commit/b3a39ff07af5f913556a5730b06ada2da572a6cc,Modify Solarisbank AG PDF-Importer to support new transaction (#4485)  https://forum.portfolio-performance.info/t/pdf-import-von-solarisbank-ag/22410/2
portfolio-performance,portfolio,c1cceacde12b3f6fe5270dd3a762255eef264bf2,https://github.com/portfolio-performance/portfolio/commit/c1cceacde12b3f6fe5270dd3a762255eef264bf2,Modify Comdirect PDF-Importer to support new transaction (#4484)  https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/392
portfolio-performance,portfolio,e96d3f4d675a79f3d46b9c00c5485604227428bf,https://github.com/portfolio-performance/portfolio/commit/e96d3f4d675a79f3d46b9c00c5485604227428bf,Modify BoursoBank PDF-Importer to support new transaction (#4483)  https://forum.portfolio-performance.info/t/pdf-import-from-boursobank-ex-boursorama-banque/28876/7
portfolio-performance,portfolio,4c21d8b954bb19cf83a3831ff63061d1716b748a,https://github.com/portfolio-performance/portfolio/commit/4c21d8b954bb19cf83a3831ff63061d1716b748a,Modify Easybank PDF-Importer to support new transaction (#4482)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/71
portfolio-performance,portfolio,aea6e0899314dd7194058ea66f6cff69306b0080,https://github.com/portfolio-performance/portfolio/commit/aea6e0899314dd7194058ea66f6cff69306b0080,fix NullPointerException  https://forum.portfolio-performance.info/t/tried-to-read-6-bytes-but-only-got-1/29723/18
portfolio-performance,portfolio,6d209b2c266ba5a3321241cad2958f76825eaefd,https://github.com/portfolio-performance/portfolio/commit/6d209b2c266ba5a3321241cad2958f76825eaefd,Fix NullPointerException for DekaBank PDF-Importer (#4444)  If the transactions for the latest ISIN in a quarterly report span several PDF pages  they have not been processed correctly. Issue: https://forum.portfolio-performance.info/t/pdf-import-von-dekabank/18048/61 Co-authored-by: Christian <c.klossek@group-apo.com>
portfolio-performance,portfolio,3202aa6cd74128e2e0987481fb4822afa71d2015,https://github.com/portfolio-performance/portfolio/commit/3202aa6cd74128e2e0987481fb4822afa71d2015,Modify Saxo PDF-Importer to support new format (#4442)  https://forum.portfolio-performance.info/t/pdf-importer-fur-saxo-bank/30548/18
portfolio-performance,portfolio,5e49a736887309eb26a87ae25af0bda21348fe0f,https://github.com/portfolio-performance/portfolio/commit/5e49a736887309eb26a87ae25af0bda21348fe0f,Modify Trade Republic PDF-Importer to support new transaction (#4440)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/78
portfolio-performance,portfolio,4e7e746c3ea82255926d4643b7ab5b695455612c,https://github.com/portfolio-performance/portfolio/commit/4e7e746c3ea82255926d4643b7ab5b695455612c,Modify Easybank PDF-Importer to support new transaction (#4439)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/69
portfolio-performance,portfolio,68bff777018ce53bcc817ebe91b6b452f2b2c129,https://github.com/portfolio-performance/portfolio/commit/68bff777018ce53bcc817ebe91b6b452f2b2c129,Modify Scalable Capita PDF-Importer to support new transaction (#4438)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/414
portfolio-performance,portfolio,6025f87c0f2d73d11587dd5e8b1c1e835cb283d3,https://github.com/portfolio-performance/portfolio/commit/6025f87c0f2d73d11587dd5e8b1c1e835cb283d3,Rework Targo Bank PDF-Importer (#4428)  Complete rework of Targo Bank PDF-Importer https://forum.portfolio-performance.info/t/pdf-import-von-targobank/5537/38 https://forum.portfolio-performance.info/t/pdf-import-von-targobank/5537/39 https://forum.portfolio-performance.info/t/pdf-import-von-targobank/5537/44
portfolio-performance,portfolio,924037cb40cf3fe476cf6f226b86b96f4c8f9d04,https://github.com/portfolio-performance/portfolio/commit/924037cb40cf3fe476cf6f226b86b96f4c8f9d04,Modify Hypothekarbank PDF-Importer to support new transaction (#4426)  https://forum.portfolio-performance.info/t/pdf-import-von-hypothekarbank-lenzburg-ag/27716/26
portfolio-performance,portfolio,0143cc4b5636e014b1044b1bf95b6e7caca4d1bd,https://github.com/portfolio-performance/portfolio/commit/0143cc4b5636e014b1044b1bf95b6e7caca4d1bd,Modify PostFinance PDF-Importer to support new transaction (#4422)  https://forum.portfolio-performance.info/t/pdf-import-von-postfinance-ag/19186/16
portfolio-performance,portfolio,49d08f6455d5eebc3938378c7fdf363b4666e8c7,https://github.com/portfolio-performance/portfolio/commit/49d08f6455d5eebc3938378c7fdf363b4666e8c7,Modify Baader Bank PDF-Importer to support new transaction (#4417)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/411
portfolio-performance,portfolio,870fab98164de17e1fa2096fa65b8e5b239570f8,https://github.com/portfolio-performance/portfolio/commit/870fab98164de17e1fa2096fa65b8e5b239570f8,Modify Baader Bank PDF-Importer to support new transaction (#4416)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/407 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/409
portfolio-performance,portfolio,cd20daf536ef15f93e12b47f2e38ad26ae3528ac,https://github.com/portfolio-performance/portfolio/commit/cd20daf536ef15f93e12b47f2e38ad26ae3528ac,Modify Commerzbank PDF-Importer to support new transaction (#4413)  https://forum.portfolio-performance.info/t/pdf-import-von-commerzbank/9865/38 https://forum.portfolio-performance.info/t/pdf-import-von-commerzbank/9865/39
portfolio-performance,portfolio,f541d2544fe2d18290cd3a541618e01e090b60ae,https://github.com/portfolio-performance/portfolio/commit/f541d2544fe2d18290cd3a541618e01e090b60ae,Added purchase price with taxes and fees in assets and performance views  Closes #792 Issue: #4310 Signed-off-by: mierin12 <mattmailspoubelle+GIT@gmail.com> [renamed to column.heading; flipped label to 'taxes and fees'] Signed-off-by: Andreas Buchen <andreas.buchen@gmail.com>
portfolio-performance,portfolio,0ad66baf39084d6f6e86c03eff1bf4570b3c3321,https://github.com/portfolio-performance/portfolio/commit/0ad66baf39084d6f6e86c03eff1bf4570b3c3321,Fix range out of bounds in ComDirect PDF-Importer (#4409)  https://forum.portfolio-performance.info/t/fehlermeldung-bei-comdirect-import-von-gemeinschaftsdepot/30910
portfolio-performance,portfolio,821e29e5274db520a165833363b0fe1be313574c,https://github.com/portfolio-performance/portfolio/commit/821e29e5274db520a165833363b0fe1be313574c,Modify Saxo Bank PDF-Importer to support new transactions (#4402)  https://forum.portfolio-performance.info/t/pdf-importer-fur-saxo-bank/30548/12
portfolio-performance,portfolio,2dd534e15b03547a508432803ebb6b7d1110724a,https://github.com/portfolio-performance/portfolio/commit/2dd534e15b03547a508432803ebb6b7d1110724a,Modify Consorbank PDF-Importer to support new transaction (#4386)  https://forum.portfolio-performance.info/t/consors-kauf-eurex-pdf/30661/3
portfolio-performance,portfolio,e773fdcd69dfd0bbdcb86bbb5b82f9888a53d921,https://github.com/portfolio-performance/portfolio/commit/e773fdcd69dfd0bbdcb86bbb5b82f9888a53d921,Modify Baader Bank PDF-Importer to support new transaction (#4378)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/395
portfolio-performance,portfolio,b9083e55df9f82de4866855d463fc0ed1b00656f,https://github.com/portfolio-performance/portfolio/commit/b9083e55df9f82de4866855d463fc0ed1b00656f,Modify DADAT PDF-Importer to support new transaction (#4377)  https://forum.portfolio-performance.info/t/pdf-import-von-dadat/15684/63
portfolio-performance,portfolio,93051e1b1f277b607966b6a069d3a979572fd679,https://github.com/portfolio-performance/portfolio/commit/93051e1b1f277b607966b6a069d3a979572fd679,Fix FlatEx PDF-Importer with Steuerkorrektur transaction (#4375)  https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/259
portfolio-performance,portfolio,8d6222cabf6a864c798a01fef839e6e5a6ceae4d,https://github.com/portfolio-performance/portfolio/commit/8d6222cabf6a864c798a01fef839e6e5a6ceae4d,Modify Deutsche Bank PDF-Importer to support new transaction (#4373)  https://forum.portfolio-performance.info/t/pdf-import-von-deutsche-bank/2973/61
portfolio-performance,portfolio,12fb0e837db824ef5bfdd5606d4a8b81963599e5,https://github.com/portfolio-performance/portfolio/commit/12fb0e837db824ef5bfdd5606d4a8b81963599e5,fix italic and small font for "alt hint" in chart tooltip  Fix https://github.com/portfolio-performance/portfolio/issues/4318
portfolio-performance,portfolio,26c0aba3ded0c31e22c5c6d7c2434183d67873c3,https://github.com/portfolio-performance/portfolio/commit/26c0aba3ded0c31e22c5c6d7c2434183d67873c3,Add new Saxo bank PDF-Importer (#4370)  https://forum.portfolio-performance.info/t/pdf-importer-fur-saxo-bank/30548
portfolio-performance,portfolio,a55d72f917661ea5c1eaeb3ec760cdcb61e0e2e3,https://github.com/portfolio-performance/portfolio/commit/a55d72f917661ea5c1eaeb3ec760cdcb61e0e2e3,Add test case to Baader Bank PDF-Importer (#4368)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/385
portfolio-performance,portfolio,62df9543e240feefd435e4b8198228e9c2b66473,https://github.com/portfolio-performance/portfolio/commit/62df9543e240feefd435e4b8198228e9c2b66473,Modify Quirin Privatbank AG PDF-Importer to support new transaction (#4363)  https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/74 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/75 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/77 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/79 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/81
portfolio-performance,portfolio,c4804772822812439702f0a01bcd09722c48934a,https://github.com/portfolio-performance/portfolio/commit/c4804772822812439702f0a01bcd09722c48934a,Modify 1822direkt PDF-Importer to support new transaction (#4360)  https://forum.portfolio-performance.info/t/pdf-import-von-1822direkt/984/44
portfolio-performance,portfolio,e98bef3d19da7c19f2297414445d5e1cc3eef79b,https://github.com/portfolio-performance/portfolio/commit/e98bef3d19da7c19f2297414445d5e1cc3eef79b,Modify Raiffeisenbank PDF-Importer to support new transaction (#4354)  https://forum.portfolio-performance.info/t/pdf-import-von-raiffeisen-schweiz/30442
portfolio-performance,portfolio,6972c65bd5fe000bf82d0ecd0b6b1288915fe05b,https://github.com/portfolio-performance/portfolio/commit/6972c65bd5fe000bf82d0ecd0b6b1288915fe05b,Modify Easybank PDF-Importer to support taxes lost adjustment (#4353)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/63 https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/64
portfolio-performance,portfolio,e723fd4f71edaadb171f9aee5dfc894b0daeb8be,https://github.com/portfolio-performance/portfolio/commit/e723fd4f71edaadb171f9aee5dfc894b0daeb8be,Modify Trade Republic PDF-Importer to support new transaction (#4352)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/77
portfolio-performance,portfolio,545d1c277f501d1421c98a19f2c4071fd7a38fa8,https://github.com/portfolio-performance/portfolio/commit/545d1c277f501d1421c98a19f2c4071fd7a38fa8,Modify Easybank PDF-Importer to support new transaction (#4350)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/61
portfolio-performance,portfolio,3a230232b9f2d7cdb7e11ae2945a121a87ab7058,https://github.com/portfolio-performance/portfolio/commit/3a230232b9f2d7cdb7e11ae2945a121a87ab7058,Modify Trade Republic PDF-Importer to support new transaction (#4349)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/697 https://forum.portfolio-performance.info/t/trade-republic-import/30322/2
portfolio-performance,portfolio,ef02b13e1fc714e009179df6446a45d66700fe4a,https://github.com/portfolio-performance/portfolio/commit/ef02b13e1fc714e009179df6446a45d66700fe4a,Make column config dialog in CSV import bigger and resizable  Issue: https://forum.portfolio-performance.info/t/csv-import-improvements/29924
portfolio-performance,portfolio,5558b94b836b99d9a53b221df7aeed2bf67bc457,https://github.com/portfolio-performance/portfolio/commit/5558b94b836b99d9a53b221df7aeed2bf67bc457,sanitize account and portfolio names for bulk CSV export  Closes https://github.com/portfolio-performance/portfolio/issues/4231
portfolio-performance,portfolio,b6e46c39490c86e17456709df8989d0600d2eb6c,https://github.com/portfolio-performance/portfolio/commit/b6e46c39490c86e17456709df8989d0600d2eb6c,Modify 1822direct PDF-Importer to support new transaction (#4337)  https://forum.portfolio-performance.info/t/pdf-import-von-1822direkt/984/42
portfolio-performance,portfolio,f10dbc7da65c1de741eb29653e91e0a4094937d7,https://github.com/portfolio-performance/portfolio/commit/f10dbc7da65c1de741eb29653e91e0a4094937d7,Modify Audi Bank PDF-Importer to support new transaction (#4334)  https://forum.portfolio-performance.info/t/pdf-import-von-audi-bank/30187/10
portfolio-performance,portfolio,0f6aa4f47c84b5ba6001e5e5dcee907d9352f4ef,https://github.com/portfolio-performance/portfolio/commit/0f6aa4f47c84b5ba6001e5e5dcee907d9352f4ef,Modify Audi Bank PDF-Importer to support new transaction (#4333)  https://forum.portfolio-performance.info/t/pdf-import-von-audi-bank/30187/7
portfolio-performance,portfolio,9859649512714c52eca27cdec0c2e1b9ec5ba7a4,https://github.com/portfolio-performance/portfolio/commit/9859649512714c52eca27cdec0c2e1b9ec5ba7a4, Modify Trade Republic PDF-Importer to support new transaction (#4332)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/690
portfolio-performance,portfolio,ccd521eb8a6f5fd9ea3e8ecf2ebba819414d0705,https://github.com/portfolio-performance/portfolio/commit/ccd521eb8a6f5fd9ea3e8ecf2ebba819414d0705,Add new Audi Bank PDF-Importer (#4324)  https://forum.portfolio-performance.info/t/pdf-import-von-audi-bank/30187/4
portfolio-performance,portfolio,f807a885c2105896e1dc5dd37711c591985e17a9,https://github.com/portfolio-performance/portfolio/commit/f807a885c2105896e1dc5dd37711c591985e17a9,Modify Oldenburgische Landesbank AG PDF-Importer to support new transaction (#4317)  https://forum.portfolio-performance.info/t/pdf-import-fuer-ebase/7204/94 https://forum.portfolio-performance.info/t/pdf-import-fuer-ebase/7204/95
portfolio-performance,portfolio,f06ce514c5bc95e718ea0f52096dccfb11e3f2b5,https://github.com/portfolio-performance/portfolio/commit/f06ce514c5bc95e718ea0f52096dccfb11e3f2b5,Modify FNZ Bank AG PDF-Importer to support new transaction (#4316)  https://forum.portfolio-performance.info/t/pdf-import-von-fnz-bank-vorher-ebase/25223/10 https://forum.portfolio-performance.info/t/pdf-import-von-fnz-bank-vorher-ebase/25223/14
portfolio-performance,portfolio,93f220f58e592207af6dc23cc6f1b0331b3a56e4,https://github.com/portfolio-performance/portfolio/commit/93f220f58e592207af6dc23cc6f1b0331b3a56e4,Modify Trade Republic PDF-Importer to support new transaction (#4315)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/681
portfolio-performance,portfolio,c595ebbd7f52c784aee0a43cdacf0b3cf103a23e,https://github.com/portfolio-performance/portfolio/commit/c595ebbd7f52c784aee0a43cdacf0b3cf103a23e,Modify Hypothekarbank Lenzburg AG PDF-Importer to support new transaction (#4307)  https://forum.portfolio-performance.info/t/pdf-import-von-hypothekarbank-lenzburg-ag/27716/21
portfolio-performance,portfolio,71b3c10f049618451a5eb9a633e5b6a1be6a0478,https://github.com/portfolio-performance/portfolio/commit/71b3c10f049618451a5eb9a633e5b6a1be6a0478,Modify PostFinance AG PDF-Importer to support new transaction (#4305)  https://forum.portfolio-performance.info/t/pdf-import-von-postfinance-ag/19186/14
portfolio-performance,portfolio,faae7a5b466349809933438f437f6a8238acd8a1,https://github.com/portfolio-performance/portfolio/commit/faae7a5b466349809933438f437f6a8238acd8a1,Modify kbc group nv pdf importer to support (#4294)  https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/10 https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/11
portfolio-performance,portfolio,d86f22e8a41e1cf61a9174efd65bb2bf14307956,https://github.com/portfolio-performance/portfolio/commit/d86f22e8a41e1cf61a9174efd65bb2bf14307956,Modify Hypothekarbank Lenzburg AG PDF-Importer to support new transaction (#4292)  https://forum.portfolio-performance.info/t/pdf-import-von-hypothekarbank-lenzburg-ag/27716/19
portfolio-performance,portfolio,498b95f99d6d2c05e37c5d869ea23c536ea4f7ef,https://github.com/portfolio-performance/portfolio/commit/498b95f99d6d2c05e37c5d869ea23c536ea4f7ef,Add test case to sBroker PDF-Importer (#4289)  https://forum.portfolio-performance.info/t/pdf-import-von-s-broker-sparkasse/5195/130
portfolio-performance,portfolio,fdc5013666344f15b563216bc58f7bfbce15c6b4,https://github.com/portfolio-performance/portfolio/commit/fdc5013666344f15b563216bc58f7bfbce15c6b4,Modify sBroker PDF-Importer to support new transaction (#4288)  https://forum.portfolio-performance.info/t/pdf-import-von-s-broker-sparkasse/5195/128
portfolio-performance,portfolio,f2e3834ffed50e5e2d5ac9dd7f0147346ad8f718,https://github.com/portfolio-performance/portfolio/commit/f2e3834ffed50e5e2d5ac9dd7f0147346ad8f718,Modify Trade Republic PDF-Importer to support new transactions (#4286)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/71 https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/72
portfolio-performance,portfolio,deb12732001b71186083e7af2280f4a58ad86051,https://github.com/portfolio-performance/portfolio/commit/deb12732001b71186083e7af2280f4a58ad86051,Modify Trade Republic PDF-Importer to support new transaction (#4285)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/676
portfolio-performance,portfolio,89ba6a62d880a09fb924573c519e254c74da659b,https://github.com/portfolio-performance/portfolio/commit/89ba6a62d880a09fb924573c519e254c74da659b,Add test case to OnVista PDF-Importer (#4282)  https://forum.portfolio-performance.info/t/pdf-import-von-onvista-bank/2076/154
portfolio-performance,portfolio,2a0ab43a4acb2e034500dc4380718f60afb3584c,https://github.com/portfolio-performance/portfolio/commit/2a0ab43a4acb2e034500dc4380718f60afb3584c,Modify FlaxEx PDF-Importer to support new transaction (#4275)  https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/254 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/253 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/244 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/239 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/237 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/231 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/195 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/193
portfolio-performance,portfolio,ce8fb86d30c0ccdadca4bc4e07089ba6c2297789,https://github.com/portfolio-performance/portfolio/commit/ce8fb86d30c0ccdadca4bc4e07089ba6c2297789,Modify EstateGuru PDF-Importer to support new transaction (#4273)  https://forum.portfolio-performance.info/t/pdf-import-von-estateguru/21304/7
portfolio-performance,portfolio,fee2e032ac480b8ba78eb523b7c767a64958597d,https://github.com/portfolio-performance/portfolio/commit/fee2e032ac480b8ba78eb523b7c767a64958597d,Fix WitheBox PDF-Importer (#4272)  https://forum.portfolio-performance.info/t/pdf-import-von-whitebox-gmbh/29756/6 Recalculation for fees after TeamViewer  Meeting
portfolio-performance,portfolio,531a5e79838e41c85e17f4f41fa73c3ee1c75acb,https://github.com/portfolio-performance/portfolio/commit/531a5e79838e41c85e17f4f41fa73c3ee1c75acb,Modify Estateguru PDF-Importer to support new transactions (#4271)  https://forum.portfolio-performance.info/t/pdf-import-von-estateguru/21304/5
portfolio-performance,portfolio,d3d268322eab86c9b6fb05d5d40bc296e8db39e9,https://github.com/portfolio-performance/portfolio/commit/d3d268322eab86c9b6fb05d5d40bc296e8db39e9,Modify ING Diba PDF-Importer to support new transaction (#4269)  https://forum.portfolio-performance.info/t/pdf-import-von-ing/7114/166
portfolio-performance,portfolio,953560993eb55d78da63cdc433352799d9c5883e,https://github.com/portfolio-performance/portfolio/commit/953560993eb55d78da63cdc433352799d9c5883e,Modify Barclay PDF-Importer to support new transaction (#4267)  https://forum.portfolio-performance.info/t/pdf-import-von-barclays-bank/27762/3?u=nirus
portfolio-performance,portfolio,d186edf6828b3b6c3a3fe3ea1f92463d05d772f3,https://github.com/portfolio-performance/portfolio/commit/d186edf6828b3b6c3a3fe3ea1f92463d05d772f3,Fixed exception while collecting header information from request  Issue: https://forum.portfolio-performance.info/t/kurse-von-ariva-werden-nicht-aktualisiert/29744/54?u=andreasb
portfolio-performance,portfolio,cad4b38461b7a9ec5954f92e0c127e1acf25eb16,https://github.com/portfolio-performance/portfolio/commit/cad4b38461b7a9ec5954f92e0c127e1acf25eb16,Modify Trade Republic PDF-Importer to support new transaction (#4266)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/657
portfolio-performance,portfolio,efb50e6bcd61c1b614958e91a0f3ba81d4423e39,https://github.com/portfolio-performance/portfolio/commit/efb50e6bcd61c1b614958e91a0f3ba81d4423e39,Change "absolute deviation" in the "delta percentage indicator"  `node.getRoot()` leaded to an "absolute deviation" calculated in relation to the total value of *all* (filtered) assets. The filtering worked  but the "root" TaxonomyNode includes the "Without Classification" class  which we don't want.  Issue: https://forum.portfolio-performance.info/t/ohne-klassifizierungen-nicht-auswerten-filter-vergessen-ihre-einstellungen/655/9 Issue: #4260
portfolio-performance,portfolio,23ea30327d4a6ddca6c864742a1a91d127780fcd,https://github.com/portfolio-performance/portfolio/commit/23ea30327d4a6ddca6c864742a1a91d127780fcd,Modify Baader Bank PDF-Importer to support new transaction (#4262)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/360 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/363 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/370 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/373
portfolio-performance,portfolio,712eea7f4e19a5075dd90a65f92cb437b85e1f76,https://github.com/portfolio-performance/portfolio/commit/712eea7f4e19a5075dd90a65f92cb437b85e1f76,Add new Estateguru PDF-Importer (#4259)  https://forum.portfolio-performance.info/t/pdf-import-von-estateguru/21304/3
portfolio-performance,portfolio,0bbb11c4e1b8f6d8953c16ca87cbe84d9d49e5da,https://github.com/portfolio-performance/portfolio/commit/0bbb11c4e1b8f6d8953c16ca87cbe84d9d49e5da,Add new Alpac Capital PDF-Importer (#4257)  https://forum.portfolio-performance.info/t/pdf-import-from-alpac-capital/29082 https://forum.portfolio-performance.info/t/pdf-import-from-alpac-capital/29082/3
portfolio-performance,portfolio,e13cfc95bb27fe32d8930d56f2978f63394839ca,https://github.com/portfolio-performance/portfolio/commit/e13cfc95bb27fe32d8930d56f2978f63394839ca,Modify Tradegate AG PDF-Importer to support new transaction (#4255)  https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/9
portfolio-performance,portfolio,047a0f7b8441060133f0460577ab2f670899e470,https://github.com/portfolio-performance/portfolio/commit/047a0f7b8441060133f0460577ab2f670899e470,Modify Trade Republic PDF-Importer to support new transaction (#4254)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/651
portfolio-performance,portfolio,15c1ebcb6b2c93833322aa6202a04f2f6090ff6e,https://github.com/portfolio-performance/portfolio/commit/15c1ebcb6b2c93833322aa6202a04f2f6090ff6e,Modify Easybank PDF-Importer to support new transaction (#4253)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/58
portfolio-performance,portfolio,ddc569f751a949e8220981e2495bd161c91289c8,https://github.com/portfolio-performance/portfolio/commit/ddc569f751a949e8220981e2495bd161c91289c8,Modify Akf Bank PDF-Importer to support new transaction (#4251)  https://forum.portfolio-performance.info/t/pdf-import-von-akf-bank/27325/6
portfolio-performance,portfolio,641cb331e3f03412f72d4e8343ee89739d1df5b1,https://github.com/portfolio-performance/portfolio/commit/641cb331e3f03412f72d4e8343ee89739d1df5b1,Add new WitheBox GmbH PDF-Importer (#4247)  https://forum.portfolio-performance.info/t/pdf-import-von-whitebox-gmbh/29756
portfolio-performance,portfolio,65882becb775334356b3490b60b9169e1f409b58,https://github.com/portfolio-performance/portfolio/commit/65882becb775334356b3490b60b9169e1f409b58,Modify Tiger Broker PDF-Importer to support new transaction (#4246)  https://forum.portfolio-performance.info/t/pdf-import-from-tiger-brokers/20484/26 https://forum.portfolio-performance.info/t/pdf-import-from-tiger-brokers/20484/34
portfolio-performance,portfolio,a6e386a428a9306960fb7b96813726c0e4b0c8df,https://github.com/portfolio-performance/portfolio/commit/a6e386a428a9306960fb7b96813726c0e4b0c8df,Modify KBC Bank NV PDF-Importer to support new transactions (#4244)  https://forum.portfolio-performance.info/t/pdf-import-from-bolero/29582
portfolio-performance,portfolio,15ab2a2ca6a8de20ecd36b2036cc7d2e1f9dc6ba,https://github.com/portfolio-performance/portfolio/commit/15ab2a2ca6a8de20ecd36b2036cc7d2e1f9dc6ba,Fixed 'widget diposed' error when remove the active sort column  Issue: https://forum.portfolio-performance.info/t/spalte-div-jahr-wird-nur-einmal-berechnet-danach-ist-die-spalte-leer/29706
portfolio-performance,portfolio,1fb005be8f9ecfbaa5193f5934254e056ced5f93,https://github.com/portfolio-performance/portfolio/commit/1fb005be8f9ecfbaa5193f5934254e056ced5f93,Fixed lazy dividend calculation requiring the moving averages from cost calculation  Issue: https://forum.portfolio-performance.info/t/spalte-div-jahr-wird-nur-einmal-berechnet-danach-ist-die-spalte-leer/29706
portfolio-performance,portfolio,49197f2afaf772c1e3727133fe52d534787fb2a1,https://github.com/portfolio-performance/portfolio/commit/49197f2afaf772c1e3727133fe52d534787fb2a1,Labels summary row consistently with 'Sum'  The view 'statement of assets'  'payments' and 'security performance' now use all the same label for the summary row.
portfolio-performance,portfolio,27b936fc4722f240a0ad6b62db451acd8d7e048f,https://github.com/portfolio-performance/portfolio/commit/27b936fc4722f240a0ad6b62db451acd8d7e048f,Modify AKF Bank PDF-Importer to support new transaction (#4242)  https://forum.portfolio-performance.info/t/pdf-import-von-akf-bank/27325/3
portfolio-performance,portfolio,60d92eb6451789deca9598460966afe8ac67d310,https://github.com/portfolio-performance/portfolio/commit/60d92eb6451789deca9598460966afe8ac67d310,Modify OnVista PDF-Importer to support new transactions (#4241)  https://forum.portfolio-performance.info/t/pdf-import-von-onvista-bank/2076/149 Merge parts of #4147  Co-authored-by: ZfT2 <16590801+zft2@users.noreply.github.com>
portfolio-performance,portfolio,f7a2f397c4498a833d3f08c6aa91da87c01e1001,https://github.com/portfolio-performance/portfolio/commit/f7a2f397c4498a833d3f08c6aa91da87c01e1001,Modify N26 PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/7
portfolio-performance,portfolio,ce6b4fbe436ea21c11efa3df31f6b6d048fdf086,https://github.com/portfolio-performance/portfolio/commit/ce6b4fbe436ea21c11efa3df31f6b6d048fdf086,ModModify Hypothekarbank Lenzburg AG PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-hypothekarbank-lenzburg-ag/27716/17
portfolio-performance,portfolio,04e0bea2379664141a4d572217004c44f3f14fee,https://github.com/portfolio-performance/portfolio/commit/04e0bea2379664141a4d572217004c44f3f14fee,Added aggregate rows to the security performance view
portfolio-performance,portfolio,876ea7315e4d1093655a779524bb873ec71eb181,https://github.com/portfolio-performance/portfolio/commit/876ea7315e4d1093655a779524bb873ec71eb181,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/61 https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/66 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/634 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/638
portfolio-performance,portfolio,2ad60fbbb58f1e6a2864792eda905ecd636f3cfe,https://github.com/portfolio-performance/portfolio/commit/2ad60fbbb58f1e6a2864792eda905ecd636f3cfe,SecurityEvent: Move "source" field to the parent SecurityEvent class  Previously  "source" field was available only in the DividendEvent subclass. But it makes sense to have it on the parent class  to streamline integration with external tools to manage events. Thus  "source" is a generic machine-readable field to record source and/or "ownership" of the event information. A typical workflow would be:  1. An external tool TOOL1 creates an event and sets its source attribute to "TOOL1" to designate this event is "owned" by the tool  which it's free to modify or even delete later. 2. This attribute eventually gets rendered as <source> element within <event> element in PortfolioPerformance XML. 3. PP preserves the value of the source attributes across load/save cycle of the portfolio. 4. On the next run  TOOL1 can identify events originally create by itself  and update them as needed. This is important  because event information provided by external source if often tentative and subject to change.  As mentioned above  the "source" field is intended to be machine-readable  and as such  it's currently now shown anywhere in the PP UI. It might be useful to address that at later time  if the need for that is found.  WARNING: This change doesn't include steps which would be required to migrate Protobuf binary format. It's expected that developers familiar with this format would step in to help with the migration.
portfolio-performance,portfolio,7c4b3fa2da55f948378cf5d838026969e93253cb,https://github.com/portfolio-performance/portfolio/commit/7c4b3fa2da55f948378cf5d838026969e93253cb,Modify FFB PDF-Importer to support new transactions  https://forum.portfolio-performance.info/t/pdf-import-von-frankfurter-fondsbank-ffb/4751/79
portfolio-performance,portfolio,11a976084d919bab5ce0feded5e7d7e20aff1d9a,https://github.com/portfolio-performance/portfolio/commit/11a976084d919bab5ce0feded5e7d7e20aff1d9a,Modify C24 Bank GmbH PDF-Importer to support new transaction (#4186)  https://forum.portfolio-performance.info/t/pdf-import-von-c24-bank-gmbh/28636/3
portfolio-performance,portfolio,a10214344ec3b98b024d1c56584f76e84d99f604,https://github.com/portfolio-performance/portfolio/commit/a10214344ec3b98b024d1c56584f76e84d99f604,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/57
portfolio-performance,portfolio,09d57e1522421feb473c44081b3ce81cc19b13c5,https://github.com/portfolio-performance/portfolio/commit/09d57e1522421feb473c44081b3ce81cc19b13c5,Removed immutable type setting for latest security price  For whatever reasons  there are actually some referenced latest security prices in files out there in the wild.  Issue: #4117 Issue: https://forum.portfolio-performance.info/t/fehlermeldungen-nach-update-auf-version-70/29289 Issue: https://forum.portfolio-performance.info/t/fehler-beim-offnen-xml-kann-nicht-geparst-werden/21859/18 Issue: https://www.wertpapier-forum.de/topic/38306-portfolio-performance-mein-neues-programm/?do=findComment&comment=1728577
portfolio-performance,portfolio,22c93ba43eb30034abf51be069ea7042e95d2cea,https://github.com/portfolio-performance/portfolio/commit/22c93ba43eb30034abf51be069ea7042e95d2cea,Added filtered columns to the security performance view
portfolio-performance,portfolio,18dd67ed4640e54998aa19a1c73f7119b92ec1ff,https://github.com/portfolio-performance/portfolio/commit/18dd67ed4640e54998aa19a1c73f7119b92ec1ff,Modify Trade Repbulic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/613 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/617 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/625 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/629
portfolio-performance,portfolio,21f8fabcc1561660c544f80236331ad158e5f759,https://github.com/portfolio-performance/portfolio/commit/21f8fabcc1561660c544f80236331ad158e5f759,ClientFactory: Autodetect reference format of plain XML files  When opening XML file  do detection whether it uses XPath for references (older PortfolioPerformance way) or "id" attributes. Sadly  the serialization library  XStream  doesn't do such detection on its own. We do detection by reading a few initial bytes of file and seeing if top-level <client> tag has "id" attribute (just using string matching). This is done only on plain-text XML files (not compressed  not encrypted). This should be adequate trade-off  as the whole idea of switching to "id" encoding is to simplify interoperability with 3rd-party tools  which requires plain-text XML anyway.  Issue: #4117
portfolio-performance,portfolio,10cf095424c137063643fe8871727d55940ae06a,https://github.com/portfolio-performance/portfolio/commit/10cf095424c137063643fe8871727d55940ae06a,Allow '.' from numpad as French decimal separator  When entering a number of a French keyboard  the dot from the numpad is usually automatically converted into the French decimal separator ' '.  We do not have to change the NumberVerifyListener. It is not triggered because the KeyListener is directly inserting the decimal. However  that is not a problem because we know it is a valid character for numbers.  Issue: #4143 Issue: #3780 Issue: https://forum.portfolio-performance.info/t/decimal-separator-on-french-keyboard/28939 Signed-off-by: mierin12 <mattmailspoubelle+GIT@gmail.com> [squashed commits; refactored code into utility class; rebased to master] Signed-off-by: Andreas Buchen <andreas.buchen@gmail.com>
portfolio-performance,portfolio,878494f6967c28488d3cff02a8dbedc87c301d85,https://github.com/portfolio-performance/portfolio/commit/878494f6967c28488d3cff02a8dbedc87c301d85,Add new VZ Depotbank AG PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-vz-depotbank-ag/29123/6
portfolio-performance,portfolio,5aa37c5b6715ca3065fa4ecfc4b480f79c240dd3,https://github.com/portfolio-performance/portfolio/commit/5aa37c5b6715ca3065fa4ecfc4b480f79c240dd3,Modify KeyTrade PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-keytrade/19238/4 https://forum.portfolio-performance.info/t/pdf-import-from-keytrade/19238/5 https://forum.portfolio-performance.info/t/pdf-import-from-keytrade/19238/6 https://forum.portfolio-performance.info/t/pdf-import-from-keytrade/19238/8
portfolio-performance,portfolio,17f303a8b4a5acd385abee4545b5bdb3316a2d18,https://github.com/portfolio-performance/portfolio/commit/17f303a8b4a5acd385abee4545b5bdb3316a2d18,Fixed NPE when linking securities to portfolio report after creation from PDF  Issue: https://forum.portfolio-performance.info/t/wertpapieranlage-aus-pdf-import-fuhrt-zu-fehler/29130
portfolio-performance,portfolio,a3bdaaac40504e3173b7a6b6ee9bbedd2969c266,https://github.com/portfolio-performance/portfolio/commit/a3bdaaac40504e3173b7a6b6ee9bbedd2969c266,Modify Deutsche Bank PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-deutsche-bank/2973/58
portfolio-performance,portfolio,be536a5743f602a82c639f1fd0cf15a046bca84b,https://github.com/portfolio-performance/portfolio/commit/be536a5743f602a82c639f1fd0cf15a046bca84b,Fix N26 PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/5
portfolio-performance,portfolio,f4ea0c295038694f572456d7b5d327df8599d7f9,https://github.com/portfolio-performance/portfolio/commit/f4ea0c295038694f572456d7b5d327df8599d7f9,Modify Direkt1822 PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-1822direkt/984/40
portfolio-performance,portfolio,1bd7ecfeeb49cc967b0f96d3326de3416f79bcec,https://github.com/portfolio-performance/portfolio/commit/1bd7ecfeeb49cc967b0f96d3326de3416f79bcec,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/55
portfolio-performance,portfolio,4e15abcca6744839fe4aee05edea147db193a6db,https://github.com/portfolio-performance/portfolio/commit/4e15abcca6744839fe4aee05edea147db193a6db,Add new N26 PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/2 https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/3
portfolio-performance,portfolio,e09822256fba990f8ca96141f5bb8e91106e33d9,https://github.com/portfolio-performance/portfolio/commit/e09822256fba990f8ca96141f5bb8e91106e33d9,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/52
portfolio-performance,portfolio,2b2869441bf72ec23faa843eed6a77f9d20ee147,https://github.com/portfolio-performance/portfolio/commit/2b2869441bf72ec23faa843eed6a77f9d20ee147,Modify Consorbank PDF-Importer to support new transaction (#4151)  https://forum.portfolio-performance.info/t/pdf-import-von-consorsbank/2697/292
portfolio-performance,portfolio,32490aab0b138c0c6367820585595bb9d79884f7,https://github.com/portfolio-performance/portfolio/commit/32490aab0b138c0c6367820585595bb9d79884f7,Modify Trade Republic PDF-Importer to support new transaction (#4150)  Closes #4137 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/571 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/586 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/587 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/588 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/597
portfolio-performance,portfolio,4335a52a43ba4c0847a1ed1fc6aeb7bb6820d9c4,https://github.com/portfolio-performance/portfolio/commit/4335a52a43ba4c0847a1ed1fc6aeb7bb6820d9c4,Modify DKB PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-dkb/4449/121
portfolio-performance,portfolio,3ded76152a8524617b1a4e8a80e38d2f479f7337,https://github.com/portfolio-performance/portfolio/commit/3ded76152a8524617b1a4e8a80e38d2f479f7337,Sortability of the status when checking the extracted elements  https://forum.portfolio-performance.info/t/sortierung-nach-status-bei-buchung-aus-pdf-importieren/28819
portfolio-performance,portfolio,7ee0694402fd2de9d46b936e4d2b1d4258e26ffc,https://github.com/portfolio-performance/portfolio/commit/7ee0694402fd2de9d46b936e4d2b1d4258e26ffc,Use lazy security performance record for security performance view
portfolio-performance,portfolio,d018e1c4cded8c1d5ea9cff10c9575a5fc1c30b6,https://github.com/portfolio-performance/portfolio/commit/d018e1c4cded8c1d5ea9cff10c9575a5fc1c30b6,Added a lazy security performance record that computes values on demand
portfolio-performance,portfolio,07de44c5629bc1ffd005201fc29684b92c5c0425,https://github.com/portfolio-performance/portfolio/commit/07de44c5629bc1ffd005201fc29684b92c5c0425,Refactored SecurityPerformanceRecord in order to modularize computations
portfolio-performance,portfolio,f89f0c70a3f5689560f7f7c2fb0b4ddd25139e3c,https://github.com/portfolio-performance/portfolio/commit/f89f0c70a3f5689560f7f7c2fb0b4ddd25139e3c,Performance improvement on TTWROR calculation: incrementally apply transactions
portfolio-performance,portfolio,611ee3e0cc8fbde82d36182bc69f53cc3c277a9e,https://github.com/portfolio-performance/portfolio/commit/611ee3e0cc8fbde82d36182bc69f53cc3c277a9e,Modify Raisin Bank AG PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-raisin-bank-ag/28544/12
portfolio-performance,portfolio,957ba5d9b2f9d28b03dc5974fdad7282345390a8,https://github.com/portfolio-performance/portfolio/commit/957ba5d9b2f9d28b03dc5974fdad7282345390a8,Add new Bourso Bank PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-from-boursobank-ex-boursorama-banque/28876
portfolio-performance,portfolio,5c2b1af0ba691f2887f0656561889798c70568f2,https://github.com/portfolio-performance/portfolio/commit/5c2b1af0ba691f2887f0656561889798c70568f2,Modify Quirin PDF-Importer to support new transaction (#4128)  https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/65 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/67 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/68
portfolio-performance,portfolio,a01b34a49b738b5e510e8fb058409e7c384b4a6d,https://github.com/portfolio-performance/portfolio/commit/a01b34a49b738b5e510e8fb058409e7c384b4a6d,Modify Raisin Bank AG PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-raisin-bank-ag/28544/10
portfolio-performance,portfolio,dd97c0ebd6b93c6a9671d8beef467d625289f890,https://github.com/portfolio-performance/portfolio/commit/dd97c0ebd6b93c6a9671d8beef467d625289f890,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/567
portfolio-performance,portfolio,2412a7fb7c4bfbd963b93baf2b3d6cbd62e95b45,https://github.com/portfolio-performance/portfolio/commit/2412a7fb7c4bfbd963b93baf2b3d6cbd62e95b45,Modify ING DiBa PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-ing-bank-nv-espana/28716 https://forum.portfolio-performance.info/t/pdf-import-from-ing-bank-nv-espana/28716/3
portfolio-performance,portfolio,afc3db74aebe638f602235772cfe282329d29dd1,https://github.com/portfolio-performance/portfolio/commit/afc3db74aebe638f602235772cfe282329d29dd1,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/44
portfolio-performance,portfolio,c79e42c9bda21c0f37aeb219dd1bb6f12229d617,https://github.com/portfolio-performance/portfolio/commit/c79e42c9bda21c0f37aeb219dd1bb6f12229d617,Modify JustTrade PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-justtrade/10853/52
portfolio-performance,portfolio,5c11edde034a0d1e621fd76c30541526a5c05ddb,https://github.com/portfolio-performance/portfolio/commit/5c11edde034a0d1e621fd76c30541526a5c05ddb,Modify Tradegate PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/4
portfolio-performance,portfolio,c17a9927f791d441a9cc9e7cebcf5edfaa8ee107,https://github.com/portfolio-performance/portfolio/commit/c17a9927f791d441a9cc9e7cebcf5edfaa8ee107,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/561
portfolio-performance,portfolio,82a4644caf6fcfcc45c9f265322a4156f805fcc5,https://github.com/portfolio-performance/portfolio/commit/82a4644caf6fcfcc45c9f265322a4156f805fcc5,Modify Raisin Bank AG PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-raisin-bank-ag/28544/6
portfolio-performance,portfolio,2719f201ef8b413b93a61b20e0f9747af7c229f9,https://github.com/portfolio-performance/portfolio/commit/2719f201ef8b413b93a61b20e0f9747af7c229f9,Modify FFB PDF-Importer to support new transactions (#4101)  https://forum.portfolio-performance.info/t/pdf-import-von-frankfurter-fondsbank-ffb/4751/77
portfolio-performance,portfolio,0518dbcd6e6355eadb090d63e389f00c9d79bba0,https://github.com/portfolio-performance/portfolio/commit/0518dbcd6e6355eadb090d63e389f00c9d79bba0,Modify Arkea Direct Bank PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-arkea-direct-bank-fortuneo-banque-france/28131/8  Fixing multiple transactions in one document Renaming of files and tests by language
portfolio-performance,portfolio,8dad1d391c876de7187ccc7d79041ccba93473e9,https://github.com/portfolio-performance/portfolio/commit/8dad1d391c876de7187ccc7d79041ccba93473e9,Modify sBroker PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-s-broker-sparkasse/5195/118
portfolio-performance,portfolio,2ca4664998d2872a4ddfc8c0a4e4c5b89a6ee6ee,https://github.com/portfolio-performance/portfolio/commit/2ca4664998d2872a4ddfc8c0a4e4c5b89a6ee6ee,Modify ING DIBa PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-ing/7114/159
portfolio-performance,portfolio,3681558b96038dccab8ddeaad58e00ad9b37cc18,https://github.com/portfolio-performance/portfolio/commit/3681558b96038dccab8ddeaad58e00ad9b37cc18,Modify Trade Republic PDF-Importer to support new transaction  Closes #4083 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/540 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/543 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/544 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/545 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/546 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/547
portfolio-performance,portfolio,459c8606a8f1b0edc96dd8830eff67b927606f94,https://github.com/portfolio-performance/portfolio/commit/459c8606a8f1b0edc96dd8830eff67b927606f94,Modify Swissquote PDF-Importer to support new transaction (#4080)  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/27  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/28
portfolio-performance,portfolio,fc4bd5b956e69dbace5863db62771b8aed225f7b,https://github.com/portfolio-performance/portfolio/commit/fc4bd5b956e69dbace5863db62771b8aed225f7b,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/43
portfolio-performance,portfolio,68a49aad4c784c425abcf6c8eee16b422c792620,https://github.com/portfolio-performance/portfolio/commit/68a49aad4c784c425abcf6c8eee16b422c792620,Modify Swissquote PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/25
portfolio-performance,portfolio,f45aab26d0160bd2446c74af49c638887abac65a,https://github.com/portfolio-performance/portfolio/commit/f45aab26d0160bd2446c74af49c638887abac65a,Fix Disappearing filter in Transactions Panes  Fixes https://github.com/portfolio-performance/portfolio/pull/3985#issuecomment-2129064338 Due to keeping Filter in Pane's property transactionFilter its widget was not null stoping DropDown::fill method from exectuion
portfolio-performance,portfolio,0bf1d193b1cc767f1fff757e93529d296c7610d9,https://github.com/portfolio-performance/portfolio/commit/0bf1d193b1cc767f1fff757e93529d296c7610d9,Modify Swissquote PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/21
portfolio-performance,portfolio,818b9d2332445310eef560eba7a6b5930992251d,https://github.com/portfolio-performance/portfolio/commit/818b9d2332445310eef560eba7a6b5930992251d,Modify Comsorbank PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-consorsbank/2697/289
portfolio-performance,portfolio,d34b3833d39eb8a7ec7e6efb5de52e73a302d748,https://github.com/portfolio-performance/portfolio/commit/d34b3833d39eb8a7ec7e6efb5de52e73a302d748,Add new Tradegate AG PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606 https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/2
portfolio-performance,portfolio,b147d6ba527befebeadaecbdfb3147e88e31d257,https://github.com/portfolio-performance/portfolio/commit/b147d6ba527befebeadaecbdfb3147e88e31d257,Add new Raisin Bank AG PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-weltsparen-raisin-bank/28544
portfolio-performance,portfolio,b302023ed829ca924266b07b6e9fdfeb32c3402d,https://github.com/portfolio-performance/portfolio/commit/b302023ed829ca924266b07b6e9fdfeb32c3402d,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/521
portfolio-performance,portfolio,de401b9dc5ff7f0e43476ed69f643a91136760f5,https://github.com/portfolio-performance/portfolio/commit/de401b9dc5ff7f0e43476ed69f643a91136760f5,Add new Firstrade Securities Inc. PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-firstrade/28276/4
portfolio-performance,portfolio,6f9997bb8dedec3204e0e7973428fe6aacdc83a8,https://github.com/portfolio-performance/portfolio/commit/6f9997bb8dedec3204e0e7973428fe6aacdc83a8,Modify Comdirect PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/380  Rename VerkaufMitSteuerbehandlung08 in Verkauf04 which is without tax treatment
portfolio-performance,portfolio,cd3dc367bfeaf05f83e827d701c83e28441af6d5,https://github.com/portfolio-performance/portfolio/commit/cd3dc367bfeaf05f83e827d701c83e28441af6d5, Modify Raiffeisenbank PDF-Importer to support new transactions (#4036)  https://forum.portfolio-performance.info/t/pdf-import-von-raiffeisenbank-bankgruppe/12535/135
portfolio-performance,portfolio,a0079a75b95410f5882f1b3a4222fcbbbde1fda6,https://github.com/portfolio-performance/portfolio/commit/a0079a75b95410f5882f1b3a4222fcbbbde1fda6, Modify ING DIBa PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-ing/7114/157
portfolio-performance,portfolio,3276a30fc815f0ce9fc8329f8d552455f43d2df3,https://github.com/portfolio-performance/portfolio/commit/3276a30fc815f0ce9fc8329f8d552455f43d2df3,fix Tooltips of ActualValueStackedChart in Discreet mode  Closes https://github.com/portfolio-performance/portfolio/issues/4020
Helium314,HeliBoard,eec197c32cfc809a82fa88a524fdc317807d8ddc,https://github.com/Helium314/HeliBoard/commit/eec197c32cfc809a82fa88a524fdc317807d8ddc,cache subtype display names for improved performance
Helium314,HeliBoard,d3bd97a1043c98a4504ef8a6ea7753f6ee8d3d1f,https://github.com/Helium314/HeliBoard/commit/d3bd97a1043c98a4504ef8a6ea7753f6ee8d3d1f,reload text on selection updates even if selection is as expected  if composing region was changed e.g. KDE Connect removes composing region after entering a letter  and we should be able to deal with it this is not really a good solution  as it will reload the suggestions  which flashes the underline and has noticeable performance impact  fixes #1141
Helium314,HeliBoard,5b1f40f0f6c415a17e554df217a75df98598da3a,https://github.com/Helium314/HeliBoard/commit/5b1f40f0f6c415a17e554df217a75df98598da3a,do a sanity check when setting composing text on text fields that have suggestions disabled fixes #225 could cause unnecessary text / suggestion reloads or performance regressions  though not found in testing
prometheus,jmx_exporter,0a3437b2e3c5f4ccac1042bab0abf842c3bc7fd2,https://github.com/prometheus/jmx_exporter/commit/0a3437b2e3c5f4ccac1042bab0abf842c3bc7fd2,Redesign cache for better performance (#1163)  Signed-off-by: Rafał Sumisławski <rafal.sumislawski@coralogix.com>
Guardsquare,proguard,03d7effdd2be72db980814a44b745f99bbff4d2d,https://github.com/Guardsquare/proguard/commit/03d7effdd2be72db980814a44b745f99bbff4d2d,Improve DictionaryNameFactory performance
spring-projects,spring-data-jpa,8c690555fc68a3652fd08a0e02296af9151ae686,https://github.com/spring-projects/spring-data-jpa/commit/8c690555fc68a3652fd08a0e02296af9151ae686,Move Benchmarks from `performance` module into `spring-data-jpa`.  Closes #3655
spring-projects,spring-data-jpa,58fe95f1d63477a1c4a00b186d08ace56689f863,https://github.com/spring-projects/spring-data-jpa/commit/58fe95f1d63477a1c4a00b186d08ace56689f863,Optimize entity deletion in SimpleJpaRepository.  This change improves the performance of the delete method by first checking if the entity is already managed by the EntityManager. If so  it removes the entity directly without additional database queries. This optimization can reduce unnecessary database lookups in certain scenarios.  Closes #3564
spring-projects,spring-data-jpa,b8319a07b9e3abf2c914ff09b66decc83430ca09,https://github.com/spring-projects/spring-data-jpa/commit/b8319a07b9e3abf2c914ff09b66decc83430ca09,Add performance module.  Add new module using JMH to benchmark certain aspects of query parsing & rendering.  See: #3309
wildfly,wildfly,de908e0298acff4ab0570447eec5866d1cf84ed1,https://github.com/wildfly/wildfly/commit/de908e0298acff4ab0570447eec5866d1cf84ed1,[WFLY-20521] Revert "[WFLY-19393] Persistence container bytecode enhancement must be enabled by default to ensure better performance"  This reverts commit 2caa7d617b148be19a5f2c70830fc28dc592c3c2.
wildfly,wildfly,2caa7d617b148be19a5f2c70830fc28dc592c3c2,https://github.com/wildfly/wildfly/commit/2caa7d617b148be19a5f2c70830fc28dc592c3c2,[WFLY-19393] Persistence container bytecode enhancement must be enabled by default to ensure better performance
wildfly,wildfly,6fc93af1c284fb2227cc1575ada47f889e439fdf,https://github.com/wildfly/wildfly/commit/6fc93af1c284fb2227cc1575ada47f889e439fdf,Merge pull request #18112 from pferraro/WFLY-19613  WFLY-19613 Immutability performance optimizations
DependencyTrack,dependency-track,04f5ccc1ede03dd26af2cda3e6a9994bd7cf8901,https://github.com/DependencyTrack/dependency-track/commit/04f5ccc1ede03dd26af2cda3e6a9994bd7cf8901,Merge pull request #3869 from nscuro/issue-3811  Improve performance of findings retrieval
DependencyTrack,dependency-track,ba17eb211f421f804d97b9312a358ca0403e6c24,https://github.com/DependencyTrack/dependency-track/commit/ba17eb211f421f804d97b9312a358ca0403e6c24,Improve performance of findings retrieval  The `/v1/finding/{projectUuid}` endpoint has historically been slow to respond (#3811). While the "main" query behind it is somewhat optimized SQL already  it still suffered from various performance killers:  * Filtering of suppressed findings was done in-memory  and required fetching of individual `Analysis` records *for every single finding*. * `Clob` fields were not mapped directly from the SQL query result  but instead by re-fetching `Component` and `Vulnerability` records *for every single finding*  such that the ORM would provide properly `String`-ified field values. * Aliases were fetched *for every single finding* individually. * Latest component versions were fetched *for every single finding* individually.  Performance was improved via the following changes:  1. Filtering of suppressed findings is moved to the main SQL query  voiding the need to fetch individual `Analysis` records later. This also reduces the overall result set that needs to be transferred and mapped. 2. Mapping of `Clob` fields is done within the `Finding` constructor  voiding the need to re-fetch `Vulnerability` records in order to retrieve `String` values for them. 3. Aliases are loaded in bulk  and in a way that avoids redundant queries if the same `Vulnerability` appears multiple times within a list of `Finding`s. 4. Latest component versions are loaded in bulk  and in a way that avoids redundant queries if the same `Component` appears multiple times within a list of `Finding`s.  Because the modified functionality is re-used across the code base  multiple features benefit from this enhancement:  * `/v1/finding/{projectUuid}` endpoint * Corresponds to the *Audit Vulnerabilities* tab in the UI * `/v1/project/{projectUuid}/export` endpoint * CycloneDX exports for *Inventory with Vulnerabilities*  *VDR*  and *VEX* * Fortify  Kenna  and DefectDojo integrations  Signed-off-by: nscuro <nscuro@protonmail.com>
apache,nutch,b02340dfecb26d0599f57358e18edfb12030ff34,https://github.com/apache/nutch/commit/b02340dfecb26d0599f57358e18edfb12030ff34,Merge pull request #827 from sebastian-nagel/NUTCH-3067  NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved
apache,nutch,633fa10d821870e7c96022836285072a4cbb5997,https://github.com/apache/nutch/commit/633fa10d821870e7c96022836285072a4cbb5997,NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved  - fix typo in name of variable
apache,nutch,63da6267f82e49778abae3549a9a2d5c5b2e5cd8,https://github.com/apache/nutch/commit/63da6267f82e49778abae3549a9a2d5c5b2e5cd8,NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved  - more verbose logging when reaching the Fetcher throughput threshold  when emptying fetch queues and when aborting with hung threads - add note that fetcher.throughput.threshold.retries should not exceed the timeout defined by mapreduce.task.timeout and fetcher.threads.timeout.divisor
apache,nutch,bd2fce6ff391ef7f6291cae86e295472d9eb45ac,https://github.com/apache/nutch/commit/bd2fce6ff391ef7f6291cae86e295472d9eb45ac,NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved  - skip empty fetch queues which hold exception counts after the time configured in fetcher.exceptions.per.queue.clear.after has passed in addition to the delay defined by the exponential backoff
apache,nutch,0b06b1bc783ef9a3971f9dd8b7b5162fabd19b89,https://github.com/apache/nutch/commit/0b06b1bc783ef9a3971f9dd8b7b5162fabd19b89,NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved  - purge and block queues which are delayed because of exceptions in case the next fetch would happen after the fetcher timelimit
apache,nutch,e053ed078412c17ff9f0fa2a23848511ca4b052e,https://github.com/apache/nutch/commit/e053ed078412c17ff9f0fa2a23848511ca4b052e,NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved  - reduce memory footprint of FetchItemQueue
aeron-io,agrona,c5ad4a1f7dcb2fbdae27d2df1a94411d07ce15d4,https://github.com/aeron-io/agrona/commit/c5ad4a1f7dcb2fbdae27d2df1a94411d07ce15d4,AtomicBuffer opaque operations (#313)  Added opaque operations to AtomicBuffer. An opaque operation provides atomicity and visibility  but it doesn't provide any ordering guarantees beyond coherence. So it doesn't order loads/stores to different addresses  only to its own address.  Opaque operations are great for performance counters or progress indicators and provide the least amount of overhead on the CPU (all modern CPUs are coherent); especially CPUs with a weak memory model like ARM or RISC-V.
aeron-io,agrona,98f5ebc957e5abd585a334a6d8c5d53d82990376,https://github.com/aeron-io/agrona/commit/98f5ebc957e5abd585a334a6d8c5d53d82990376,Added MarkFile.timestampRelease (#318)  This method is a replacement for the timestampOrdered. The ordered methods use an old naming schema and the release methods use the new naming schema.  There is a slight performance penalty because the timestampOrdered calls the timestampRelease method.
aeron-io,agrona,92e7bc4c9ecd59b23ccc52444ef869e5a60ee9f5,https://github.com/aeron-io/agrona/commit/92e7bc4c9ecd59b23ccc52444ef869e5a60ee9f5,AtomicBuffer acquire/release methods (#314)  * AtomicBuffer acquire/release operations.  The AtomicBuffer has release method for every ordered method. So for e.g. a AtomicBUffer.putLongOrdered  there is a putLongRelease.  The ordered method will call the release version  so there is a sligth performance penalty.  Also acquire get methods have been added that have slightly weaker memory ordering semantics compared to a volatile read. Which could provide more oppertunity for the JIT to do its magic and in theory could give better performance on ISAs with weaker memory models.  * Added JCStressTests for release/acquire  * Added @since  * Fixed checkstyle issues in UnsafeBufferTest
aeron-io,agrona,a49ea6713a0551ba61acb6f8358017b848fd3bd8,https://github.com/aeron-io/agrona/commit/a49ea6713a0551ba61acb6f8358017b848fd3bd8,Improve performance of IntHashSet via avoiding the fill operation by using 0 as MISSING_VALUE.
apache,tika,469cd40c058c5de4269cd5f8ea8d88adb9008922,https://github.com/apache/tika/commit/469cd40c058c5de4269cd5f8ea8d88adb9008922,TIKA-4290 Replaced string concatenation in loop - for every loop previously a StringBuilder was implicitly created. It's performance related (#1900)
apache,lucene,d72021a1a7baf7ae1b11f6a1aa9e96bd90cb1858,https://github.com/apache/lucene/commit/d72021a1a7baf7ae1b11f6a1aa9e96bd90cb1858,Fix leadCost calculation in BooleanScorerSupplier.requiredBulkScorer (#14543)  Fixes #14542 by setting leadCost in BooleanScorerSupplier.requiredBulkScorer to the minimum of both MUST and FILTER clauses' costs. This bug caused performance regressions in some queries. More details are in the original issue.
apache,lucene,686a5b8def51f52c06fc9b5c714262685ac52a45,https://github.com/apache/lucene/commit/686a5b8def51f52c06fc9b5c714262685ac52a45,Add support for determining off-heap memory requirements for KnnVectorsReader (#14426)  This PR adds support to KnnVectorsReader in order to determine the off-heap memory requirements.  The motivation here is to give better insight into the size of off-heap memory that will be needed  so that deployments can be better scaled so that vector search workloads fit in memory  in order to provide best execution performance.
apache,lucene,fefa4e7076fb33eed33fa1af88b5559c007fbaae,https://github.com/apache/lucene/commit/fefa4e7076fb33eed33fa1af88b5559c007fbaae,RegExp: add CASE_INSENSITIVE_RANGE support (#14381)  Add optional flag to support case-insensitive ranges. A minimal DFA is always created. This works with Unicode but may have a performance cost.  Each codepoint in the range must be iterated  and any alternatives added to a set. This can be large if the range spans much of Unicode.  CPU and memory costs are contained within a single function enabled by the optional flag. For example when matching a caseless /[a-z]/  54 codepoints will be accumulated into an int[]  which is then compressed to 4 ranges before adding to the parse tree.  Closes #14378
apache,lucene,88d7709de7e4ac8a7f8fbe97cb2f71e32f611652,https://github.com/apache/lucene/commit/88d7709de7e4ac8a7f8fbe97cb2f71e32f611652,Fix optimization to help inline calls to live docs. (#14294)  While doing benchmarks on indexes with deletions  I found a bug in `ScorerUtil`  which optimizes live docs for the wrong class: `FixedBitSet` instead of `FixedBit`. Another performance bug is that `FixedBits` did not override `Bits#applyMask` with a more efficient implementation.
apache,lucene,2d422afe31025a35c182fceab08303d4d9bcf784,https://github.com/apache/lucene/commit/2d422afe31025a35c182fceab08303d4d9bcf784,Add histogram facet capabilities. (#14204)  This is inspired from a paper by Tencent where the authors describe how they speed up so-called "histogram queries" by sorting the index by timestamp translating ranges of values corresponding to each histogram bucket to ranges of doc IDs. This way  at collection time  they no longer need to look up values and can compute the histogram purely by looking at collected doc IDs.  YU  Muzhi  LIN  Zhaoxiang  SUN  Jinan  et al. TencentCLS: the cloud log service with high query performances. Proceedings of the VLDB Endowment  2022  vol. 15  no 12  p. 3472-3482.  Instead of binary-searching the doc ID space to translate histogram buckets into ranges of doc IDs  the new collector manager uses recently introduced support for sparse indexing. When playing with the geonames dataset  computing a histogram of the elevation field runs ~2-3x faster with this optimization than with the naive implementation.
apache,lucene,f315f53acab338e92a14b073efcadcb5ce8d5435,https://github.com/apache/lucene/commit/f315f53acab338e92a14b073efcadcb5ce8d5435,Optimize ContextQuery with big number of contexts (#14169)  When there are big number of contexts  ContextQuery may take a lot of time because of how context automata are constructed. Instead of the currentt appraoch of repeatedly concatenating and unioning context automata  this PR first constucts all individual context automata and then does one single union at the end.  Thus for the added test with 1000 contexts  the performance improved from 4000 ms to 18 ms.
apache,lucene,e4b85cab57602ddcd9c7e2e6647be9988621ebbe,https://github.com/apache/lucene/commit/e4b85cab57602ddcd9c7e2e6647be9988621ebbe,Implement IntersectVisitor#visit(IntsRef) whenever it makes sense (#14138)  Implement IntersectVisitor#visit(IntsRef) in many of the current implementations and add BulkAdder#add(IntsRef) method. They should provide better performance due to less virtual method calls and more efficient bulk processing.
apache,lucene,e34e0824fdbe200af72add419d43471938d56e5d,https://github.com/apache/lucene/commit/e34e0824fdbe200af72add419d43471938d56e5d,Reduce specialization in `ForUtil` and `ForDeltaUtil`. (#14048)  These classes specialize all bits per value up to 24. But performance of high numbers of bits per value is not very important  because they are used by short postings lists  which are fast to iterate anyway. So this PR only specializes up to 16 bits per value.  For instance  if a postings list uses blocks of 17 bits per value  it means that one can find gaps of 65 536 consecutive doc IDs that do not contain the term. Such rare terms do not drive query performance.
apache,lucene,c88f9334e5c99abbeb4f233f9606873e5037c118,https://github.com/apache/lucene/commit/c88f9334e5c99abbeb4f233f9606873e5037c118,Introduce a BulkScorer for DisjunctionMaxQuery. (#14040)  This introduces a bulk scorer for `DisjunctionMaxQuery` that delegates to the bulk scorers of the query clauses. This helps make the performance of top-level `DisjunctionMaxQuery` better  especially when its clauses have optimized bulk scorers themselves (e.g. disjunctions).
apache,lucene,6c48b404cd4c5a48435350007e1f5f41a0f3d01c,https://github.com/apache/lucene/commit/6c48b404cd4c5a48435350007e1f5f41a0f3d01c,Combine all postings enum impls of the default codec into a single class (#14033)  Recent speedups by making call sites bimorphic made me want to play with combining all postings enums and impacts enums of the default codec into a single class  in order to reduce polymorphism. Unfortunately  it does not yield a speedup since the major polymorphic call sites we have that hurt performance (DefaultBulkScorer  ConjunctionDISI) are still 3-polymorphic or more.  Yet  reduced polymorphism at little performance impact is a good trade-off as it would help make call sites bimorphic for users who don't have as much query diversity as nightly benchmarks  or in the future when we remove other causes of polymorphism.
apache,lucene,07955ff88d6ef900ee471426408ad02e42312309,https://github.com/apache/lucene/commit/07955ff88d6ef900ee471426408ad02e42312309,Avoid performance regression by constructing lazily the PointTree in NumericComparator (#13498) (#13877)
apache,lucene,e37eaea11a523fd31ed6fa320b8b3479c18a7841,https://github.com/apache/lucene/commit/e37eaea11a523fd31ed6fa320b8b3479c18a7841,Revert "Disjunction as CompetitiveIterator for numeric dynamic pruning (#13221)" (#13857) (#13971)  This reverts commit 1ee4f8a1115d1de623f242014681032d87ed2c1e.  We have observed performance regressions that can be linked to #13221. We will need to revise the logic that such change introduced in main and branch_10x. While we do so  I propose that we bake it out of branch_10_0 and we release Lucene 10 without it.  Closes #13856
apache,lucene,fafd6af004e0c39582043b797555d6eeb9aa7638,https://github.com/apache/lucene/commit/fafd6af004e0c39582043b797555d6eeb9aa7638,Add support for intra-segment search concurrency (#13542)  This commit introduces support for optionally creating slices that target leaf reader context partitions  which allow them to be searched concurrently. This is good to maximize resource usage when searching force-merged indices  or indices with rather big segments  by parallelizig search execution across subsets of segments being searched.  Note: this commit does not affect default generation of slices. Segments can be partitioned by overriding the `IndexSearcher#slices(List<LeafReaderContext>)` method to plug in ad-hoc slices creation. Moreover  the existing  `IndexSearcher#slices` static method now creates segment partitions when the additional `allowSegmentsPartitions` argument is set to `true`.  The overall design of this change is based on the existing search concurrency support that is based on `LeafSlice` and `CollectorManager`. A new `LeafReaderContextPartition` abstraction is introduced  that holds a reference to a `LeafReaderContext` and the range of doc ids it targets. A `LeafSlice` noew targets segment partitions  each identified by a `LeafReaderContext` instance and a range of doc ids. It is possible for a partition to target a whole segment  and for partitions of different segments to be combined into the same leaf slices freely  hence searched by the same thread. It is not possible for multiple partitions of the same segment to be added to the same leaf slice.  Segment partitions are searched concurrently leveraging the existing `BulkScorer#score(LeafCollector collector  Bits acceptDocs  int min  int max)` method  that allows to score a specific subset of documents for a provided `LeafCollector`  in place of the `BulkScorer#score(LeafCollector collector  Bits acceptDocs)` that would instead score all documents.  ## Changes that require migration  The migrate guide has the following new clarifying items around the contract and breaking changes required to support intra-segment concurrency: - `Collector#getLeafCollector` may be called multiple times for the same leaf across distinct `Collector` instances created by a `CollectorManager`. Logic that relies on `getLeafCollector` being called once per leaf per search needs updating. - a `Scorer`  `ScorerSupplier` or `BulkScorer` may be requested multiple times for the same leaf - `IndexSearcher#searchLeaf` change of signature to accept the range of doc ids - `BulkScorer#score(LeafCollector  BitSet)` is removed in favour of `BulkScorer#score(LeafCollector  BitSet  int  int)` -  static `IndexSearcher#slices` method changed to take a last boolean argument that optionally enables the creation of segment partitions - `TotalHitCountCollectorManager` now requires that an array of `LeafSlice`s  retrieved via `IndexSearcher#getSlices`  is provided to its constructor   Note: `DrillSideways` is the only component that does not support intra-segment concurrency and needs considerable work to do so  due to its requirement that the entire set of docs in a segment gets scored in one go.  The default searcher slicing is not affected by this PR  but `LuceneTestCase` now randomly leverages intra-segment concurrency. An additional `newSearcher` method is added that takes a `Concurrency` enum as the last argument in place of the `useThreads` boolean flag. This is important to disable intra-segment concurrency for `DrillSideways` related tests that do support inter-segment concurrency but not intra-segment concurrency.  ## Next step  While this change introduces support for intra-segment concurrency  it only sets up the foundations of it. There is still a performance penalty for queries that require segment-level computation ahead of time  such as points/range queries. This is an implementation limitation that we expect to improve in future releases  see #13745.  Additionally  we will need to decide what to do about the lack of support for intra-segment concurrency in `DrillSideways` before we can enable intra-segment slicing by default. See #13753 .  Closes #9721
apache,lucene,b4a8810b7aea2fa6143aa0323f924f85c5cb3329,https://github.com/apache/lucene/commit/b4a8810b7aea2fa6143aa0323f924f85c5cb3329,Inline skip data into postings lists (#13585)  This updates the postings format in order to inline skip data into postings. This format is generally similar to the current `Lucene99PostingsFormat`  e.g. it shares the same block encoding logic  but it has a few differences: - Skip data is inlined into postings to make the access pattern more sequential. - There are only 2 levels of skip data: on every block (128 docs) and every 32 blocks (4 096 docs).  In general  I found that the fact that skip data is inlined may slow down a bit queries that don't need skip data at all (e.g. `CountOrXXX` tasks that never advance of consult impacts) and speed up a bit queries that advance by small intervals. The fact that the greatest level only allows skipping 4096 docs at once means that we're slower at advancing by large intervals  but data suggests that it doesn't significantly hurt performance.
apache,lucene,8d4f7a6e99d2da802b7019247b0f8f305d71c024,https://github.com/apache/lucene/commit/8d4f7a6e99d2da802b7019247b0f8f305d71c024,Bump the window size of disjunction from 2 048 to 4 096. (#13605)  It's been pointed multiple times that a difference between Tantivy and Lucene is the fact that Tantivy uses windows of 4 096 docs when Lucene has a 2x smaller window size of 2 048 docs and that this might explain part of the performance difference. luceneutil suggests that bumping the window size to 4 096 does indeed improve performance for counting queries  but not for top-k queries. I'm still suggesting to bump the window size across the board to keep our disjunction scorer consistent.
apache,lucene,3304b60c9cc2fd23531263ea9d95dc3e92c0b2ce,https://github.com/apache/lucene/commit/3304b60c9cc2fd23531263ea9d95dc3e92c0b2ce,Improve VectorUtil::xorBitCount perf on ARM (#13545)  This commit improves the performance of VectorUtil::xorBitCount on ARM by ~4x.  This change is effectively a workaround for the lack of vectorization of Long::bitCount on ARM.  On x64 there is no issue  the long variant of xorBitCount outperforms the int variant by ~15%.
apache,lucene,2a8d328ab22261d22616370b51da088aa005223f,https://github.com/apache/lucene/commit/2a8d328ab22261d22616370b51da088aa005223f,Replace AtomicLong with LongAdder in HitsThresholdChecker (#13546)  The value for the global count is incremented a lot more than it is read  the space overhead of LongAdder seems irrelevant => lets use LongAdder. The performance gain from using it is the higher the more threads you use  but at 4 threads already very visible in benchmarks.
apache,lucene,512ff4ac9241751d448d75d32ad987ee1a5a91ad,https://github.com/apache/lucene/commit/512ff4ac9241751d448d75d32ad987ee1a5a91ad,MultiTermQuery return null for ScoreSupplier (#13454)  MultiTermQuery return null for ScoreSupplier if there are no terms in an index that match query terms.  With the introduction of PR #12156 we saw degradation in performance of bool queries where one of the mandatory clauses is a TermInSetQuery with query terms not present in the field. Before for such cases TermsInSetQuery returned null for ScoreSupplier which would shortcut the whole bool query.  This PR adds ability for MultiTermQuery to return null for ScoreSupplier if a field doesn't contain any query terms.  Relates to PR #12156
soot-oss,soot,8d201bb94d0d23513d82806137489a4cbd0edcdf,https://github.com/soot-oss/soot/commit/8d201bb94d0d23513d82806137489a4cbd0edcdf,Merge pull request #2138 from MarcMil/mdev  Fix the ConstantValueToInitializerTransformer & Performance optimizations for type resolver
soot-oss,soot,4047aae55472e871d7c4c100639e7e6abce4e424,https://github.com/soot-oss/soot/commit/4047aae55472e871d7c4c100639e7e6abce4e424,Improve type assigner performance  Parallelize minimize typing check and reuse local defs/uses.
soot-oss,soot,21d341fb3e28ef4c3c890ff3154120f873e47ceb,https://github.com/soot-oss/soot/commit/21d341fb3e28ef4c3c890ff3154120f873e47ceb,Merge pull request #2116 from MarcMil/fixes  Improve performance of dex typer & introduce Array Type Cache
soot-oss,soot,8154e49c35e92a3e21ea7734c8914c7f00bbe331,https://github.com/soot-oss/soot/commit/8154e49c35e92a3e21ea7734c8914c7f00bbe331,Separate constant typings for performance reasons
soot-oss,soot,6df988477e5d77d8e4bee3753efd044c11f193bb,https://github.com/soot-oss/soot/commit/6df988477e5d77d8e4bee3753efd044c11f193bb,Improve performance of typer
soot-oss,soot,0069dc36c45498a35758560f24bc4c0cd3e1d5dc,https://github.com/soot-oss/soot/commit/0069dc36c45498a35758560f24bc4c0cd3e1d5dc,Merge pull request #2115 from MarcMil/fixes  Fix a bug with explicit cassts in dex and improve performance of type assigner
soot-oss,soot,987955d1ac6d224fcb29749b316021a558427190,https://github.com/soot-oss/soot/commit/987955d1ac6d224fcb29749b316021a558427190,Performance
soot-oss,soot,b1b2022506a7f73c7f64e536f1c2525bb497a26f,https://github.com/soot-oss/soot/commit/b1b2022506a7f73c7f64e536f1c2525bb497a26f,Fix a bug with explicit cassts in dex and improve performance of typer slightly
igniterealtime,Openfire,14832dacf84faf3cef2f6e29ee0efda52dd9ea20,https://github.com/igniterealtime/Openfire/commit/14832dacf84faf3cef2f6e29ee0efda52dd9ea20,OF-3048: Improve ClientSession comparison performance (clustering)  Comparing ClientSessions typically involves evaluating the 'is anonymous' property of a session.  In a cluster  the property value of remote sessions is retrieved under guard of a distributed lock. As comparison of larger sets frequently evaluates the property value  that lock is obtained very frequently. This dramatically reduces performance.  As the property will never change after authentication  it's safe to cache the value. This improves performance considerably.  Note that RemoteClientSession instances are typically short-lived (as they're typically discarded after use)  which reduces the effectiveness of this change. Still  even without re-use  the performance improvement is considerable: in a cluster with 10 000 sessions  the responsiveness of the session summary page is improved from many minutes to 20 to 30 seconds in my test environment.
igniterealtime,Openfire,6d0de547f5feb4ed36d3d6e1cee2c0e8b76de6a6,https://github.com/igniterealtime/Openfire/commit/6d0de547f5feb4ed36d3d6e1cee2c0e8b76de6a6,OF-3028: Netty threads from 'child' EventLoop should use Netty-default settings  As we're overriding the thread factories used for Netty EventLoops to change the names of threads  we've also changed the default configuration of threads used by Netty.  Netty's default configuration can be expected to be optimized for performance. Openfire should use a similar configuration.  In this commit  the configuration is reverted back to the default configuration that's used by Netty (based on `io.netty.util.concurrent.DefaultThreadFactory`).
igniterealtime,Openfire,741fc5560869663a0fe46bcb1bb565ee92f5c0dd,https://github.com/igniterealtime/Openfire/commit/741fc5560869663a0fe46bcb1bb565ee92f5c0dd,Chore: when debug logging  replace string concat of message  Instead of concatenation  use the `{}` construct will prevent building the string until it's actually logged.  Although it's probably a performance improvement  it's mainly a readability improvement.
igniterealtime,Openfire,ad0c69b5d3b61c8defdd0dfbd4fe778756f62da6,https://github.com/igniterealtime/Openfire/commit/ad0c69b5d3b61c8defdd0dfbd4fe778756f62da6,Using the more generic org.eclipse.jetty.server.Handler in place of the more specific org.eclipse.jetty.server.handler.ContextHandler implementation. According to the [Migration Guide](https://jetty.org/docs/jetty/12/programming-guide/migration/11-to-12.html#api-changes-handler-sequence)  Handler.Sequence replaced HandlerCollection and HandlerList. ContextHandlerCollection is retained for its efficient child ContextHandler selection. Unless performance considerations or the need for a more restrictive implementation justify it  a more generic implementation reduces code complexity and provides greater functional possibilities for plugins. (#2609)  Co-authored-by: “Huy <huy.vu@surevine.com> Co-authored-by: Guus der Kinderen <guus.der.kinderen@gmail.com>
igniterealtime,Openfire,67108eddd2346099eab7cd4081286548c8bf3d70,https://github.com/igniterealtime/Openfire/commit/67108eddd2346099eab7cd4081286548c8bf3d70,OF-2824: Manipulation of client-related caches in RoutingTable using the same lock  RoutingTableImpl has three closely related caches  that are used to represent the state of client session routes: - `usersSessionsCache` - `anonymousUsersCache` - `usersCache`  Each value in the first cache is expected to correspond to a value in one of the other two caches.  Under OF-2824  a bug is described where `usersCache` contains values that are _not_ in `usersSessionsCache`. That shouldn't be possible.  Prior to this commit  manipulation of these caches is performed under a lock obtained from each of the caches. This means that the overall operation of adding an entry to `usersSessionsCache` and one of the other two caches is _not_ guarded by one singular lock (instead  two locks are used  each guarding the operation pertaining to that particular cache). This leaves room for a race-condition.  This commit addresses the race condition by using one singular lock to guard manipulations in all of these caches.  As all caches use a JID (in either bare or full form) as their key value  the singular lock introduced by this commit is based on the bare JID of the key that's being manipulated. This lock is obtained from the `usersSessionsCache`.  This change can lead to more lock contention (as more operations are guarded by the same lock). Simultaneously  less acquiring of locks will take place (as many operations previously required two locks to be acquired  while now  only one is needed. What the effects are on performance is as of yet undetermined.  This commit also introduces some related  minor changes: - Logged messages are made more consistent - Some operations have been moved outside of the protection of a (potentially cluster-wide) lock  to improve performance - Where methods expect to be called with a full JID  exceptions are thrown when a bare JID is used. This fail-fast behavior is intended to uncover any existing or future bugs.
igniterealtime,Openfire,3c7a0c684939ca5738868689360d22cb3583904a,https://github.com/igniterealtime/Openfire/commit/3c7a0c684939ca5738868689360d22cb3583904a,OF-2824: Remove cluster-cache optimization  It is suspected that  under race conditions  the optimization removed by this commit introduces data inconsistency.  The optimization prevents modification of a second cache  as through the output of manipulation of the first cache it can be deduced that the second cache should already be in the expected state. By skipping manipulation of that second cache  a cluster-wide operation is prevented  which improves performance.  The presence-based override for the optimization - one that I frankly do not understand - becomes obsolete by this change  and is also removed.  The change introduced by this commit trades performance for more reliable data consistency. As an added benefit  the code becomes less complex  reducing maintenance costs.  This commit changes the public signature of the `addClientRoute` method of `RoutingTable`: it no longer returns a value. The return value is currently not used by Openfire's own code. It was used for only two days  back in 2007: it was introduced in commit a940eeff4f72e4e9da70fcd0b4a1db1b3c40cd8d where it was used to update a statistic. This statistic got removed two days later  in commit 67f9ab65c36b8a38f5ab7c480415897839da21f0. Given the nature of the code (RoutingTable being _very_ low level)  it is not expected that third-party code uses this method. It contract change should therefor be reasonably safe to do.
apache,pdfbox,ed6def6fe9f5c63dedac9f3958f5b45edc5c8dea,https://github.com/apache/pdfbox/commit/ed6def6fe9f5c63dedac9f3958f5b45edc5c8dea,PDFBOX-5847: Improve performance of FileSystemFontProvider.scanFonts() by introducing an "only headers" mode for the font parsers where each table reads as little information as possible  by Mykola Bohdiuk  git-svn-id: https://svn.apache.org/repos/asf/pdfbox/trunk@1918773 13f79535-47bb-0310-9956-ffa450edef68
konsoletyper,teavm,7d865565f8bf52c24869ebee1e8b1fbdd0b48a4d,https://github.com/konsoletyper/teavm/commit/7d865565f8bf52c24869ebee1e8b1fbdd0b48a4d,wasm gc: improve disassembler performance
konsoletyper,teavm,64ceaf3958d45e958da9ae5dc7dd6ebe0fcabd82,https://github.com/konsoletyper/teavm/commit/64ceaf3958d45e958da9ae5dc7dd6ebe0fcabd82,wasm gc: improve deobfuscator performance
konsoletyper,teavm,39cbcfce66862090cb18d2def4fed7d5067b3a0c,https://github.com/konsoletyper/teavm/commit/39cbcfce66862090cb18d2def4fed7d5067b3a0c,wasm gc: improve performance when copying JS typed arrays to/from Java arrays
konsoletyper,teavm,baf61b1f4eb5d9b544e2779a28e5aa030ba0ff38,https://github.com/konsoletyper/teavm/commit/baf61b1f4eb5d9b544e2779a28e5aa030ba0ff38,classlib: improve performance of time zone compilation
konsoletyper,teavm,ab8fb13415f94b49c7e1e92c3971a44817f4f223,https://github.com/konsoletyper/teavm/commit/ab8fb13415f94b49c7e1e92c3971a44817f4f223,Refactor JUnit test runner and improve performance of running in  browser
konsoletyper,teavm,e14993f50992c2daf1f539b9ab225bc7be812cdc,https://github.com/konsoletyper/teavm/commit/e14993f50992c2daf1f539b9ab225bc7be812cdc,Improve performance of test running by running all tests from single class in same iframe without recompilation
konsoletyper,teavm,8cbbb35d9ba4f1a52ed67789ca0c41b4dbd03e36,https://github.com/konsoletyper/teavm/commit/8cbbb35d9ba4f1a52ed67789ca0c41b4dbd03e36,wasm gc: compiler performance optimization
konsoletyper,teavm,753a028fc9111c93ee87181d5c284e0f6be14704,https://github.com/konsoletyper/teavm/commit/753a028fc9111c93ee87181d5c284e0f6be14704,wasm gc: improve performance of JS interop
spring-projects,spring-batch,5a62de9031923aac0e76fc52b972ee2aafdd00c3,https://github.com/spring-projects/spring-batch/commit/5a62de9031923aac0e76fc52b972ee2aafdd00c3,Move ORDER BY in getLastStepExecution from DB to java  This addresses performance issues with large STEP_EXECUTION table on DB2.  Fixes #4657
apache,netbeans,a1c16923a7b53bd7e3831267090255d57885d9e9,https://github.com/apache/netbeans/commit/a1c16923a7b53bd7e3831267090255d57885d9e9,Merge pull request #8481 from mbien/compute-overrides-performance  ComputeOverrides: move ClasspathInfo creation out of inner loops
apache,netbeans,5780a4ce6007994bd32f8ebcbcebffa9ebb3097a,https://github.com/apache/netbeans/commit/5780a4ce6007994bd32f8ebcbcebffa9ebb3097a,Merge pull request #8437 from mbien/unused-pkgprivate-performance  Cache ClassIndex during UnusedDetector search
apache,netbeans,3c852705e11b30b32e9ae43dfe7a1fba4dbc3f4e,https://github.com/apache/netbeans/commit/3c852705e11b30b32e9ae43dfe7a1fba4dbc3f4e,Merge pull request #8423 from mbien/js-embedder-perf-scaling  Fix performance scaling problem in JS-embedder annotation scanner
apache,netbeans,ba7c010f12458eee1e6c4d7e6aab755caa994b33,https://github.com/apache/netbeans/commit/ba7c010f12458eee1e6c4d7e6aab755caa994b33,Fix performance scaling problem in JS-embedder annotation scanner  Reduced scan time from 102s to 13ms for a synthetic test file containing 10k inner classes with Override annotations (no javascript).
apache,netbeans,93c422f3639fa767b33c850fb5b82e331b09126b,https://github.com/apache/netbeans/commit/93c422f3639fa767b33c850fb5b82e331b09126b,Merge pull request #8417 from mbien/javadoc-hint-performance   Remove source level query from javadoc hint.
apache,netbeans,54231a1eecd01530d4f74ff4096b7be60dfd8a06,https://github.com/apache/netbeans/commit/54231a1eecd01530d4f74ff4096b7be60dfd8a06,Versioning: don't wait for indexer before refresh  - indexing and versioning status updates can run concurrently since they don't interact with each other. - git status refresh typically takes < 1s even in large projects. indexing can take much longer.  fix and performance:  - refresh versioning annotations on project open otherwise they might be missing on re-open until the tree is expanded - lock-free CacheIndex#get(file)
apache,netbeans,5118ff96fc41cc94a49b7674fec834b95eef10cb,https://github.com/apache/netbeans/commit/5118ff96fc41cc94a49b7674fec834b95eef10cb,PHP 8.4 Support: #[\Deprecated] Attribute (Part 1)  - https://github.com/apache/netbeans/issues/8035 - https://wiki.php.net/rfc#php_84 - https://wiki.php.net/rfc/deprecated_attribute - Fix/Improve `SemanticAnalysis` - Get deprecated attributes or phpdoc tags from ASTNodes(Attributes and PHPDoc comments) instead of getting deprecated flags from an index. That way  it should also improve performance. - Add the `IncorrectDeprecatedAttributeHintError` because "Deprecated" attribute cannot target type and field - Add unit tests for the navigator  hints  and semantic analysis
apache,netbeans,397d507ada822e9b483b26fd045a3526527992f4,https://github.com/apache/netbeans/commit/397d507ada822e9b483b26fd045a3526527992f4,Warmup Maven Embedder to improve first-project-creation UX  performance:  - first embedder initialization can take a while  async-profiler showed that a big chunk of it is spent within google guice - this starts the warmup task early  which solves the problem  since this will happen while the user is looking at the wizard - the warmup task is a no-op if it was already initialized  cleanup:  - small jdk 17 renovation
apache,netbeans,7ad885e30e2f5427d6927aaf8d300cc7ec447409,https://github.com/apache/netbeans/commit/7ad885e30e2f5427d6927aaf8d300cc7ec447409,Performance Improvements  CopyFinderBasedBulkSearch:  - implemented matches method - extracted matcher from loop  JavaFixUtilities:  - fast path for JavaFixUtilities can-safely-remove scanners
apache,paimon,abe5b4b496581b0a37a11965d4a72398e1695eb4,https://github.com/apache/paimon/commit/abe5b4b496581b0a37a11965d4a72398e1695eb4,[core] Improve performance of PartialUpdateMergeFunction with sequence group (#5481)
apache,paimon,1fc123e18bf1160e41f7afb70763968fc77d0497,https://github.com/apache/paimon/commit/1fc123e18bf1160e41f7afb70763968fc77d0497,[cdc] Fix database sync performance issue of schema evolution (#5382)
apache,paimon,274ed05699439d008094d10ffe6f49ce2ac523a6,https://github.com/apache/paimon/commit/274ed05699439d008094d10ffe6f49ce2ac523a6,[core] optimize the binlog table read performance (#4773)
apache,paimon,cdd4061db4b43393aab6fc5b2ce2c13ed34c69f3,https://github.com/apache/paimon/commit/cdd4061db4b43393aab6fc5b2ce2c13ed34c69f3,[core] Improve the performance of show tables with hive metastore (#4605)
apache,paimon,139b5a75a5f0b89bc9d9c91f8c06dfb68691c9e0,https://github.com/apache/paimon/commit/139b5a75a5f0b89bc9d9c91f8c06dfb68691c9e0,[core] Improve the performance of show tables (#4592)
apache,paimon,c7170e60f41a263770a3f4ba9d08b53ecdf69e9d,https://github.com/apache/paimon/commit/c7170e60f41a263770a3f4ba9d08b53ecdf69e9d,[flink] Make FileStoreLookupFunction.refreshBlacklist nullable to avoid performance regression
apache,paimon,668e673353f7a02aa6e160b3ac2f84e8c6746aad,https://github.com/apache/paimon/commit/668e673353f7a02aa6e160b3ac2f84e8c6746aad,[core] Improve TruncateSimpleColStatsCollector performance (#4338)
apache,paimon,95c3ce17f6b9cf25fe91c3ab5c573671d697afc2,https://github.com/apache/paimon/commit/95c3ce17f6b9cf25fe91c3ab5c573671d697afc2,[arrow] Remove field vector set row count to avoid performance issue (#4012)
apache,paimon,6da33cb6072ce90ad311450e0143aa9bf069d624,https://github.com/apache/paimon/commit/6da33cb6072ce90ad311450e0143aa9bf069d624,[core] Improve performance of dinstinct collect agg. (#3772)
apache,paimon,fcf30bdecb022f808462aa442aef91e0144b286a,https://github.com/apache/paimon/commit/fcf30bdecb022f808462aa442aef91e0144b286a,[cdc]Fix performance issue in CanalRecordParser (#3572)
apache,paimon,5d394d630c1f4e3e1d7644fbe63a0200b60a9414,https://github.com/apache/paimon/commit/5d394d630c1f4e3e1d7644fbe63a0200b60a9414,[flink] Fix performance issue in CdcActionCommonUtils (#3550)
apache,paimon,22c7c6136619563fe026dd854a7a05c10ee30ea8,https://github.com/apache/paimon/commit/22c7c6136619563fe026dd854a7a05c10ee30ea8,[core] Adjust VectoredReadable to better read performance (#3430)
apache,commons-lang,665f047e552ad71c189582af15a0b697133fff0b,https://github.com/apache/commons-lang/commit/665f047e552ad71c189582af15a0b697133fff0b,[StringUtils::indexOfAnyBut] redesign due to inconsistent/faulty behaviour regarding UTF-16 surrogates (#1327)  * [StringUtils::indexOfAnyBut] redesign due to inconsistent/faulty… …behaviour regarding UTF-16 surrogates  Both signatures of StringUtils::indexOfAnyBut currently behave inconsistently in matching UTF-16 supplementary characters and single UTF-16 surrogate characters (i.e. paired and unpaired surrogates)  since they differ unnecessarily in their algorithmic implementations  use their own incomplete and faulty interpretation of UTF-16 and don't take full advantage of the standard library.  The example cases below show that they may yield contradictory results or correct results for the wrong reasons.  This proposal gives a unified algorithmic implementation of both signatures that a) is much easier to grasp due to a clear mathematical set approach and safe iteration and doesn't become entangled in index arithmetic; stresses the set semantics of the 2nd argument b) fully relies on the standard library for defined UTF-16 handling/interpretation; paired surrogates are merged into one codepoint  unpaired surrogates are left as they are c) scales much better with input sizes and result index position d) can benefit from current and future improvements in the standard library and JVM (streams implementation  parallelization  JIT optimization  JEP 218  ???…)  The algorithm boils down to: find index i of first char in cs such that (cs.codePointAt(i) ∈ {x ∈ codepoints(cs) ∣ x ∉ codepoints(searchChars) })  Examples: ---------  <H>: high-surrogate character <L>: low-surrogate character (<H><L>): valid supplementary character signature 1: StringUtils::indexOfAnyBut(final CharSequence seq  final CharSequence searchChars) signature 2: StringUtils::indexOfAnyBut(final CharSequence cs  final char... searchChars)  Case 1: matching of unpaired high-surrogate ---------seq/cs-------searchChars------exp./new-----sig.1-------sig.2---  1.1     <H>aaaa      <H>abcd          !found       !found      !found sig.2: 'a' happens to follow <H> in searchChars; sig.1: 'a' is somewhere in searchChars  1.2     <H>baaa      <H>abcd          !found       !found      0 sig.1: 'b' is somewhere in searchChars  1.3     <H>aaaa      (<H><L>)abcd     0            !found      0 sig.1: 'a' is somewhere in searchChars  1.4     aaaa<H>      (<H><L>)abcd     4            !found      !found sig.1+2 don't interpret suppl. character  Case 2: matching of unpaired low-surrogate ---------seq/cs-------searchChars------exp./new-----sig.1-------sig.2---  2.1     <L>aaaa      (<H><L>)abcd     0            !found      !found sig.1+2 don't interpret suppl. character  2.2     aaaa<L>      (<H><L>)abcd     4            !found      !found sig.1+2 don't interpret suppl. character  Case 3: matching of supplementary character ---------seq/cs-------------searchChars-----exp./new----sig.1-----sig.2-  3.1     (<H><L>)aaaa       <L>ab<H>cd      0           !found    0 sig.1: <L> is somewhere in searchChars  3.2     (<H><L>)aaaa       abcd            0           1         0 sig.1 always points to low-surrogate of (fully) unmatched suppl. character  3.3     (<H><L>)aaaa       abcd<H>         0           0         1 3.4     (<H><L>)aaaa       abcd<L>         0           !found    0 sig.1: <H> skipped by algorithm  * [StringUtils::indexOfAnyBut] further reduction of algorithm  by simplifying set consideration: find index i of first char in seq such that (seq.codePointAt(i) ∉ { x ∈ codepoints(searchChars) })  * [StringUtils::indexOfAnyBut] simplify input-sequence iteration  by transforming ListIterator loop into index-based loop  advancing by Character.charCount(codepoint); enabling short-circuit processing  avoiding full in-advance processing of input-sequence  * [StringUtils:indexOfAnyBut] parameterization of test functions  providing a single source-of-truth (arguments stream) for the two function variants  * [StringUtils:indexOfAnyBut] remove comment  Set::contains of immutable Set has unclear desastrous performance issues when searching for large values (here: >0xffff) in a set of smaller values (including JDK 23)  ---------  Co-authored-by: IBue <>
apache,commons-lang,55a70c84f3658fd3ad3de72649b511ff21dc7175#r143834333,https://github.com/apache/commons-lang/commit/55a70c84f3658fd3ad3de72649b511ff21dc7175#r143834333,Reimplement RandomStringUtils on top of SecureRandom#getInstanceStrong() (#1235)  * Reimplement RandomStringUtils on top of SecureRandom#getInstanceStrong()  The previous implementation used  ThreadLocalRandom#current()  * Performance optimizations for RandomStringUtils  This commit improves the performance of RandomStringUtils:  * Reduces the number of random bytes generated and the number of calls to the random number generator  by using a cache system `AmortizedRandomBits`. * Optimizes the case of alphanumerical strings  reducing the number of rejections in the rejection sampling.  See comments in code for details.  * Code style and comment improvements  * Fix 2 checkstyle errors. * Apply suggestions from review for garydgregory/commons-lang#2 * Improve comments in new (non-public) class `AmortizedRandomBits` to match comments in other classes.  * Make class final  - Rename package-private class - Whitespace - Add null check - Add serialVersionUID - Remove redunant type cast - Throw IllegalStateException  not RuntimeException - nextBytes() should throw NullPointerException per contract - Javadoc: Use longer lines  * Apply comments by aherbert in
Minestom,Minestom,461c56e7495b472d6c6a5f3d0c7b5abe4866adc9,https://github.com/Minestom/Minestom/commit/461c56e7495b472d6c6a5f3d0c7b5abe4866adc9,Revert "Improve collision performance (#2321)" (#2322)  This reverts commit f917ba1b9f3d7ff1991844b118f1f9a084b60072.
Minestom,Minestom,f917ba1b9f3d7ff1991844b118f1f9a084b60072,https://github.com/Minestom/Minestom/commit/f917ba1b9f3d7ff1991844b118f1f9a084b60072,Improve collision performance (#2321)  * Improve fastPhysics perfomance  * Re-add final to Vec
openrewrite,rewrite,3a8aab51759c4248860b7210ff110fd42fc985c4,https://github.com/openrewrite/rewrite/commit/3a8aab51759c4248860b7210ff110fd42fc985c4,Fix performance issue with `UpgradeDependencyVersion` (#5479)
openrewrite,rewrite,96e17a833e2e300e213de93563ebeb7ec86577f5,https://github.com/openrewrite/rewrite/commit/96e17a833e2e300e213de93563ebeb7ec86577f5,Improve performance of `MethodMatcher#matches(JavaType.Method)`  Won't make a big difference...
openrewrite,rewrite,174d5da9495373a425cf600bf006072a5c30f18a,https://github.com/openrewrite/rewrite/commit/174d5da9495373a425cf600bf006072a5c30f18a,Improve performance of FindAnnotations precondition (#5315)
openrewrite,rewrite,b288a97641b4fc51f7568e6fd464717bf1ef3314,https://github.com/openrewrite/rewrite/commit/b288a97641b4fc51f7568e6fd464717bf1ef3314,Remove metrics from `TreeVisitor#visit()`  As of OpenRewrite 8.14.5 we have the `RecipeRunStats` data table  which should provide enough actionable metrics when investigating performance issues.
openrewrite,rewrite,37dee365b67fe33f546438bf9aec7afd98340405,https://github.com/openrewrite/rewrite/commit/37dee365b67fe33f546438bf9aec7afd98340405,Improve `TypeMatcher` performance  No more `NoViableAltException` get instantiated.
openrewrite,rewrite,d5a44f7b545256f40d810b9b824c5be3da33147b,https://github.com/openrewrite/rewrite/commit/d5a44f7b545256f40d810b9b824c5be3da33147b,Performance improvement for `TypeTable.Reader` (#5093)  * Performance improvement for `TypeTable.Reader`  Only match row GAV against artifact name patterns when different from previous row's GAV.  * Performance improvement for `TypeTable.Reader`  Only match row GAV against artifact name patterns when different from previous row's GAV.  Also  don't accumulate classes over artifact boundaries.
openrewrite,rewrite,777d6e151554d77bbc8d00c7fdd05ad523d7787e,https://github.com/openrewrite/rewrite/commit/777d6e151554d77bbc8d00c7fdd05ad523d7787e,Remove Snappy dependency (#5092)  The `JavaTypeCache` now internally uses the `AdaptiveRadixTree` instead of a `HashMap` with Snappy-compressed keys.  Overall this results in a lower memory footprint:  ``` Retained AdaptiveRadixTree size:    7316248 bytes Retained Snappy size:               8575104 bytes Retained HashMap size:              9105560 bytes ```  Here are the JMH benchmark measurements showing an overall increase in performance (the tests with the `AddOpens` suffix started the JVM using `--add-opens java.base/java.lang=ALL-UNNAMED` to skip some `byte[]` allocations for increased performance):  ``` Benchmark                                                              Mode  Cnt         Score   Error   Units JavaTypeCacheBenchmark.readAdaptiveRadix                              thrpt    2       794.740           ops/s JavaTypeCacheBenchmark.readAdaptiveRadix:gc.alloc.rate                thrpt    2      2239.494          MB/sec JavaTypeCacheBenchmark.readAdaptiveRadix:gc.alloc.rate.norm           thrpt    2   2956386.085            B/op JavaTypeCacheBenchmark.readAdaptiveRadix:gc.count                     thrpt    2       104.000          counts JavaTypeCacheBenchmark.readAdaptiveRadix:gc.time                      thrpt    2        37.000              ms JavaTypeCacheBenchmark.readAdaptiveRadixAddOpens                      thrpt    2       886.947           ops/s JavaTypeCacheBenchmark.readAdaptiveRadixAddOpens:gc.alloc.rate        thrpt    2         0.002          MB/sec JavaTypeCacheBenchmark.readAdaptiveRadixAddOpens:gc.alloc.rate.norm   thrpt    2         1.842            B/op JavaTypeCacheBenchmark.readAdaptiveRadixAddOpens:gc.count             thrpt    2           ≈ 0          counts JavaTypeCacheBenchmark.readSnappy                                     thrpt    2       344.614           ops/s JavaTypeCacheBenchmark.readSnappy:gc.alloc.rate                       thrpt    2      3134.230          MB/sec JavaTypeCacheBenchmark.readSnappy:gc.alloc.rate.norm                  thrpt    2   9544228.824            B/op JavaTypeCacheBenchmark.readSnappy:gc.count                            thrpt    2       144.000          counts JavaTypeCacheBenchmark.readSnappy:gc.time                             thrpt    2        61.000              ms JavaTypeCacheBenchmark.writeAdaptiveRadix                             thrpt    2       650.472           ops/s JavaTypeCacheBenchmark.writeAdaptiveRadix:gc.alloc.rate               thrpt    2      4941.345          MB/sec JavaTypeCacheBenchmark.writeAdaptiveRadix:gc.alloc.rate.norm          thrpt    2   7969122.563            B/op JavaTypeCacheBenchmark.writeAdaptiveRadix:gc.count                    thrpt    2       112.000          counts JavaTypeCacheBenchmark.writeAdaptiveRadix:gc.time                     thrpt    2       101.000              ms JavaTypeCacheBenchmark.writeAdaptiveRadixAddOpens                     thrpt    2       754.388           ops/s JavaTypeCacheBenchmark.writeAdaptiveRadixAddOpens:gc.alloc.rate       thrpt    2      3604.748          MB/sec JavaTypeCacheBenchmark.writeAdaptiveRadixAddOpens:gc.alloc.rate.norm  thrpt    2   5012698.210            B/op JavaTypeCacheBenchmark.writeAdaptiveRadixAddOpens:gc.count            thrpt    2       100.000          counts JavaTypeCacheBenchmark.writeAdaptiveRadixAddOpens:gc.time             thrpt    2        81.000              ms JavaTypeCacheBenchmark.writeSnappy                                    thrpt    2       324.280           ops/s JavaTypeCacheBenchmark.writeSnappy:gc.alloc.rate                      thrpt    2      3355.699          MB/sec JavaTypeCacheBenchmark.writeSnappy:gc.alloc.rate.norm                 thrpt    2  10859397.103            B/op JavaTypeCacheBenchmark.writeSnappy:gc.count                           thrpt    2        77.000          counts JavaTypeCacheBenchmark.writeSnappy:gc.time                            thrpt    2       102.000              ms ```
openrewrite,rewrite,81ba6ec4029d8ebc6a4baa3d108f3ad1a1b907ca,https://github.com/openrewrite/rewrite/commit/81ba6ec4029d8ebc6a4baa3d108f3ad1a1b907ca,Improve performance by caching the handful of new LST elements this recipe needs to generate rather than re-parsing many similar variations.  Shouldn't make any difference if you're using only one copy of the recipe  but in a situation where it is invoked many times in a recipe run the advantages should be quite noticeable.
openrewrite,rewrite,2a8bf2d7d3bad4f90b668e0304f10c68b159bbd6,https://github.com/openrewrite/rewrite/commit/2a8bf2d7d3bad4f90b668e0304f10c68b159bbd6,Add lombok support for java-11 (#4769)  * Add lombok support for java-11  * Handle erroneous nodes in open rewrite (#4412)  * Handle erroneous nodes in a tree  * Add visitErroneous to all java parser visitors  * Override the visitVariable to handle erroneous identifier names set by JavacParser  * retain name and suffix for erroneous varDecl  * override the visitVariable to handle error identifiers in all java parser visitors  * Remove sysout  * Update rewrite-java-test/src/test/java/org/openrewrite/java/JavaParserTest.java  Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>  * Update rewrite-java-test/src/test/java/org/openrewrite/java/JavaParserTest.java  Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>  * handle errors in method params  variable declarations  fix tests  * Add missing license headers  * fix compilation error  * fix compilation error in Java8ParserVisitor  * Apply code suggestions from bot  * fix cases for statementDelim  * fix block statement template generator to handle adding semicolon  * fix ChangeStaticFieldToMethod recipe  * Record compiler errors from erroneous LST nodes  * Adjustments for comments  * Java 17 parser adjustment alos in 8  11 and 21  * Add `FindCompileErrorsTest` & move away from deprecated `print()`  ---------  Co-authored-by: Jonathan Schnéider <jkschneider@gmail.com> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: Tim te Beek <tim@moderne.io> Co-authored-by: aboyko <aboyko@vmware.com>  * Make Groovy Parser correctly handle nested parenthesis (#4801)  * WIP  * Format  * Format  * Move grabbing of whitespace and resetting cursor to where it is actually required  * Extra check is not required  * Use toString  * Add `emptyListLiteralWithParentheses` test  * Add `insideFourParenthesesAndEnters` test  * Move list tests all to ListTest  * Add `emptyMapLiteralWithParentheses`  * Review feedback and fix new testcases  * Add `attributeWithParentheses`  * Improve AttributeTest  * Improve AttributeTest  * Improve AttributeTest  * Improve AttributeTest  * Improve AttributeTest  * Review fix new testcases  * Revert edit to testcase  * Add and fix testcase with newline  * Add JavaDoc and move logic regarding whitespace and resetting cursor  ---------  Co-authored-by: lingenj <jacob.van.lingen@moderne.io>  * suppress javax.json (#4804)  * suppress javax.json  * Update suppressions.xml  * Refactor SpringReference (#4805)  * Separating and clearer naming  * Add license header  * Review feedback  * refactor: Update Gradle wrapper (#4808)  Use this link to re-run the recipe: https://app.moderne.io/recipes/org.openrewrite.gradle.UpdateGradleWrapper?organizationId=T3BlblJld3JpdGU%3D#defaults=W3sibmFtZSI6ImFkZElmTWlzc2luZyIsInZhbHVlIjoiRmFsc2UifV0=  Co-authored-by: Moderne <team@moderne.io>  * Add recipe to remove Gradle Enterprise and Develocity (#4809)  * Add recipe to remove Gradle Enterprise and Develocity  * Remove left over java plugin  * Add a UsesType precondition to ReplaceConstant  * Allow file scheme in `RemoteArchive` to simplify testing (#4791)  * Allow file scheme in `RemoteArchive` to simplify testing  While it might look a bit controversial  the file scheme can also point to a remote (for instance a mounted network share) file. By allowing the `file://` scheme we can use `RemoteArchive` for those files.  As a useful side effect  this makes testing RemoteArchive handling a lot easier.  * fix test  * Update rewrite-core/src/test/java/org/openrewrite/remote/RemoteArchiveTest.java  Co-authored-by: Sam Snyder <sam@moderne.io>  ---------  Co-authored-by: Sam Snyder <sam@moderne.io>  * Try alternative way of determining parenthesis level for `BinaryExpression` when AST doesn't provide `_INSIDE_PARENTHESES_LEVEL` flag (#4807)  * Add a `isClassAvailable` method to the ReflectionUtils (#4810)  * Add a `isClassAvailable` method to the ReflectionUtils  * Add a `isClassAvailable` method to the ReflectionUtils  * Add a `isClassAvailable` method to the ReflectionUtils  * Update rewrite.yml to enforce CompareEnumsWithEqualityOperator  * Correctly map generic return and parameter types in `JavaReflectionTypeMapping` (#4812)  * Polish formatting  * Add more scenarios to JavaTypeGoat for simply typed fields and methods that return exceptions.  * Support mapping of generic thrown exception types (#4813)  * refactor: Enum values should be compared with "==" (#4811)  Use this link to re-run the recipe: https://app.moderne.io/recipes/org.openrewrite.staticanalysis.CompareEnumsWithEqualityOperator?organizationId=T3BlblJld3JpdGU%3D  Co-authored-by: Moderne <team@moderne.io>  * Keep the names of generic type variables defined by methods. (#4814)  * Make the same performance improvement to parameter names allocations that we previously made to Java 17/21 in #3345.  * Fix Java reflection mapping of generic typed fields. (#4815)  * Revert parenthesis changes (#4818)  * Revert "Try alternative way of determining parenthesis level for `BinaryExpression` when AST doesn't provide `_INSIDE_PARENTHESES_LEVEL` flag (#4807)"  This reverts commit e59e48b3a6e6be18ecb779ac329a243ed025da58.  * Revert "Make Groovy Parser correctly handle nested parenthesis (#4801)"  This reverts commit 91a031a3d517be1fe78656eb6b841141b336c085.  * JavaTemplate bug when inserting `final var` into for-each (#4806)  * JavaTemplate bug when inserting `final var` into for-each  * Split variable declarations when they contain stop comment  * Reduce accidental changes between Java 11 and 17 parsers  * Add missing import  ---------  Co-authored-by: Udayani Vaka <79973862+vudayani@users.noreply.github.com> Co-authored-by: Jonathan Schnéider <jkschneider@gmail.com> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: Tim te Beek <tim@moderne.io> Co-authored-by: aboyko <aboyko@vmware.com> Co-authored-by: Laurens Westerlaken <laurens.westerlaken@jdriven.com> Co-authored-by: lingenj <jacob.van.lingen@moderne.io> Co-authored-by: Peter Streef <peter@moderne.io> Co-authored-by: Shannon Pamperl <shanman190@gmail.com> Co-authored-by: Moderne <team@moderne.io> Co-authored-by: Sam Snyder <sam@moderne.io>
openrewrite,rewrite,6dc9d5064071df4dac16c06a0c5a292a5dbcfa72,https://github.com/openrewrite/rewrite/commit/6dc9d5064071df4dac16c06a0c5a292a5dbcfa72,Keep the names of generic type variables defined by methods. (#4814)  * Make the same performance improvement to parameter names allocations that we previously made to Java 17/21 in #3345.
openrewrite,rewrite,8cfcf0c8c26f3a4abb361456ba83c8bf37e51ca5,https://github.com/openrewrite/rewrite/commit/8cfcf0c8c26f3a4abb361456ba83c8bf37e51ca5,Remove `Find#findAllNewLineIndexes()`  Minor performance optimization.
openrewrite,rewrite,d789bcb8418c985982b4ed5ef9a1a4b010ca8bf0,https://github.com/openrewrite/rewrite/commit/d789bcb8418c985982b4ed5ef9a1a4b010ca8bf0,Performance improvements of `Find` recipe (#4758)  * Implement some performance improvements on Find recipe  * Add extra tests  * Modify last test  * Restore linked list  * More performance gains  ---------  Co-authored-by: Tim te Beek <tim@moderne.io>
openrewrite,rewrite,0cf8d79456287f2141c8f8e0428534e2b60e4c17,https://github.com/openrewrite/rewrite/commit/0cf8d79456287f2141c8f8e0428534e2b60e4c17,Yet another YAML `JsonPatchMatcher` performance tweak
openrewrite,rewrite,f30cd55a8de23ff9823dc1cf2a40343ac4b80edc,https://github.com/openrewrite/rewrite/commit/f30cd55a8de23ff9823dc1cf2a40343ac4b80edc,Yet another YAML `JsonPatchMatcher` performance tweak
openrewrite,rewrite,ae9083973ca333639ffc4b2231822bbdf7da4e6e,https://github.com/openrewrite/rewrite/commit/ae9083973ca333639ffc4b2231822bbdf7da4e6e,Improve performance of `JsonPathMatcher`s by only parsing once
openrewrite,rewrite,f71dc0d429184ba7094715a0cf6dd51e17873e2f,https://github.com/openrewrite/rewrite/commit/f71dc0d429184ba7094715a0cf6dd51e17873e2f,Improve performance of YAML `JsonPathMatcher`  Only apply `ReplaceAliasWithAnchorValueVisitor` to `Yaml.Document` elements.  See: #4650
openrewrite,rewrite,0f76203bede5f77c0df557f42c7bd5627f9b8c4b,https://github.com/openrewrite/rewrite/commit/0f76203bede5f77c0df557f42c7bd5627f9b8c4b,Slightly improve performance of `JsonPathMatcher`s  This is especially important for the YAML implementation  which applies the `ReplaceAliasWithAnchorValueVisitor` visitor to all elements.
openrewrite,rewrite,c63ab562c3347b341893d045e991d639bd12debb,https://github.com/openrewrite/rewrite/commit/c63ab562c3347b341893d045e991d639bd12debb,Skip UTF-8 BOM mark in `EncodingDetectingInputStream` and default to UTF-8 in `RewriteTest` (#4546)  * Skip UTF-8 BOM mark in `EncodingDetectingInputStream`  As the `EncodingDetectingInputStream` is only used as input for the parsers  we typically don't want to see any UTF-8 BOM marker. Additionally  platforms like .NET remove the BOM mark as well  so this change brings better compatibility.  * Add test for BOM skipping  * Improve performance of `EncodingDetectingInputStream` by a lot  * Fix `JavadocPrinterTest`  * Use UTF-8 as default encoding in `RewriteTest`  * Fix bug when reading from single byte stream  * Add missing test  * Adjust `CompilationUnitTest` whitespace  * Fix handling of empty files  * Polish one more test case
cucumber,cucumber-jvm,20ec82a9a6edd35641c246463ecd76fa709990e9,https://github.com/cucumber/cucumber-jvm/commit/20ec82a9a6edd35641c246463ecd76fa709990e9,[Core] Improve caching glue performance (#2971)  The `CachingGlue` performance has been improved by caching the following elements:  - parameter types - data table types - docString types - step definitions  The cache is invalidated when a parameter type or a step definition is added/removed  or when the language changes. The cache is not used when scenario-scoped glue is present (e.g. cucumber-java8).   This improves the `CachingGlue` performance.  On a personal project with about 250 step definitions and 1000 test scenarios  the performance is the following:  | Version | Test duration | |---------|----------------| |7.20.1 (original) | 8.2 seconds | |7.21.0-SNAPSHOT (this PR) | 3.4 seconds (2.4x faster) |   ---------  Co-authored-by: M.P. Korstanje <rien.korstanje@gmail.com> Co-authored-by: Julien Kronegg <julien@kronegg.ch>
assertj,assertj,aed17a70ed3998b90f15bf278ea14651350669f4,https://github.com/assertj/assertj/commit/aed17a70ed3998b90f15bf278ea14651350669f4,perf: improve performance on map key assertion - fix #3744
mongodb,mongo-java-driver,01aff5a0789f71b9d0b56190d4996e8ac5436827,https://github.com/mongodb/mongo-java-driver/commit/01aff5a0789f71b9d0b56190d4996e8ac5436827,Implement Client Side Operation Timeout (CSOT) feature (#1215)  Introduce a unified Client Side Operation Timeout (CSOT) as an Alpha feature to simplify the multitude of existing timeout settings. This consolidation merges various complex timeout settings into a single  overarching operation timeout option  enhancing usability and predictability. Existing timeout options will continue to be honored if the operation timeout is not set. The options `serverSelectionTimeoutMS` and `connectTimeoutMS` are still honored even if the operation timeout is set.  Key Changes:  - Implement CSOT across various layers of the driver  applying the new operation timeout configuration to govern execution time in areas such as Authentication  Connection Monitoring and Pooling  Server Discovery and Monitoring  CRUD  GridFS  CSFLE  Transactions  Handshake  and Server Selection.  - Annotate CSOT API as Alpha  indicating that this public API is in the early stages of development and may undergo incompatible changes.  - Centralize the setting of maxTimeMS just before sending commands to the server. This optimization enhances performance and ensures precise enforcement of timeouts.  - Introduce TimeoutContext to centralize timeout-related logic  enhancing the clarity and ease of maintenance for timeout management within the driver.  - Enable a new retry mode when CSOT is active  allowing operations to retry until the timeout is reached. This improves resiliency compared to the previous behavior of retrying only once.  - Enhance MongoClient  MongoDatabase  MongoCollection  and ClientSession to support setting and inheriting the operation timeout.  - Introduce MongoCluster  a new conceptual entity representing server deployments  configurable with specific writeConcern  timeout  and read preference.  - Account for minimum Round Trip Time (RTT) for socket reads and writes when CSOT is enabled to ensure timeouts are adjusted for network latency  avoiding connection churn.  - Standardize CSOT error handling by transforming timeout errors into the new `MongoOperationTimeoutException`.  ---------  Co-authored-by: Ross Lawley <ross@mongodb.com> Co-authored-by: Maxim Katcharov <maxim.katcharov@mongodb.com> Co-authored-by: Jeff Yemin <jeff.yemin@mongodb.com> Co-authored-by: Viacheslav Babanin <slav.babanin@mongodb.com> Co-authored-by: Valentin Kovalenko <valentin.kovalenko@mongodb.com>
itwanger,paicoding,283f04d99451a96963da86380742cf45ed50554e,https://github.com/itwanger/paicoding/commit/283f04d99451a96963da86380742cf45ed50554e,Merge pull request #78 from wznanfang/main  perf: 补充实现方法缺失的注解
itwanger,paicoding,46dbaf6cd97e3cb7570c1c1e56dc648a71365990,https://github.com/itwanger/paicoding/commit/46dbaf6cd97e3cb7570c1c1e56dc648a71365990,perf: 补充实现方法缺失的注解
risesoft-y9,Digital-Infrastructure,51e553cb4a70a23dda706d9083fdd6fe686bd33c,https://github.com/risesoft-y9/Digital-Infrastructure/commit/51e553cb4a70a23dda706d9083fdd6fe686bd33c,perf: 优化授权表查询
risesoft-y9,Digital-Infrastructure,af994d552bda8b05b46279a5cfd682cec0827458,https://github.com/risesoft-y9/Digital-Infrastructure/commit/af994d552bda8b05b46279a5cfd682cec0827458,perf: 使用 HashSet 代替 List
opensolon,solon,22d0cdbf295e2781053c98918bad7567c2f47d03,https://github.com/opensolon/solon/commit/22d0cdbf295e2781053c98918bad7567c2f47d03,perf: 添加 AssistantMessage:getResultContent 获取没有推理标签的内容
opensolon,solon,edca91a62f2e93bb78edd6d8f3abc68cda55fa89,https://github.com/opensolon/solon/commit/edca91a62f2e93bb78edd6d8f3abc68cda55fa89,perf: 添加 AssistantMessage:getResultContent 获取没有推理标签的内容
opensolon,solon,f2a3d718d33b09df0c1d201ff436ee05de660229,https://github.com/opensolon/solon/commit/f2a3d718d33b09df0c1d201ff436ee05de660229,perf: 添加 AssistantMessage:getResultContent 获取没有推理标签的内容
eclipse-collections,eclipse-collections,08af2e983cc9ce298289f9bd210fad076c166409,https://github.com/eclipse-collections/eclipse-collections/commit/08af2e983cc9ce298289f9bd210fad076c166409,Fix performance problem in MutableList.subList() and implement similar optimization as ArrayList.subList()
casbin,jcasbin,677c1061743235a0903514a06b665b9b52b89ee4,https://github.com/casbin/jcasbin/commit/677c1061743235a0903514a06b665b9b52b89ee4,feat: optimize convertInSyntax's performance (#450)  Optimise convertInSyntax method by compiling Pattern only once and creating StringBuffer only when needed.
Netflix,archaius,41cd26db871070363c5774dc6491c8dbfcff9be3,https://github.com/Netflix/archaius/commit/41cd26db871070363c5774dc6491c8dbfcff9be3,Merge pull request #736 from Netflix/type-error-handling  Small performance and memory usage tweaks.  Refactor PropertyImpl#get() to avoid an allocation on each call and to avoid locking. The new implementation should be marginally faster and removes a few dozen bytes from each Property instance.
Netflix,archaius,3cfa68100b263eccc8f7c111926ca7064b48d1c2,https://github.com/Netflix/archaius/commit/3cfa68100b263eccc8f7c111926ca7064b48d1c2,Small performance and memory usage tweaks  Refactor PropertyImpl#get() to avoid an allocation on each call. The new implementation should be marginally faster.  Removed a few unnecessary objects from each instance of Property objects returned from the DefaultPropertyFactory. This will remove a few dozen bytes from each Property instance.
Azure,azure-sdk-for-java,670b4458ed662037652f747940e1b9a2ee62e00d,https://github.com/Azure/azure-sdk-for-java/commit/670b4458ed662037652f747940e1b9a2ee62e00d,Adding readMany() support for findAllByIds() to improve performance. (#43759)  * Adding readMany() support for findAllByIds() to improve performance.  * Updating the changelog  * Update sdk/spring/azure-spring-data-cosmos/CHANGELOG.md  Co-authored-by: Kushagra Thapar <kushuthapar@gmail.com>  ---------  Co-authored-by: Kushagra Thapar <kushuthapar@gmail.com>
Azure,azure-sdk-for-java,28a65f7df30a3032bf74e8d010e0389dbacf304e,https://github.com/Azure/azure-sdk-for-java/commit/28a65f7df30a3032bf74e8d010e0389dbacf304e,GetProperties Blob Performance Test (#42300)
http-kit,http-kit,383198243183b3a77f96d58939e69eece97e8a66,https://github.com/http-kit/http-kit/commit/383198243183b3a77f96d58939e69eece97e8a66,[fix] [client] [#568] [#569] Fix performance regression (@bsless)  It seems that a client performance regression was accidentally introduced with #446:  By using string joins at - keepalives.remove(job.addr.toString() + job.host); - return (addr.toString() + host).equals(obj) || key.equals(obj);  We ended up adding non-trivial string allocation for every object checked for equality against a PersistentConn - harming performance especially when many hosts are being used.  This commit attempts to fix the regression by avoiding the string joining.
FCL-Team,FoldCraftLauncher,53be73d39295180fec5c3dd7e6b31d400f1ce490,https://github.com/FCL-Team/FoldCraftLauncher/commit/53be73d39295180fec5c3dd7e6b31d400f1ce490,Feat: Sustained performance mode
tlaplus,tlaplus,96056d942b70656e7f88349e2f1fa38514c48783,https://github.com/tlaplus/tlaplus/commit/96056d942b70656e7f88349e2f1fa38514c48783,Report distinct variable values observed in TLC coverage statistics.  These statistics show the number of unique values each variable takes during model checking. An unusually high number of values for a particular variable may suggest that the model is not properly constrained  potentially leading to state space explosion during exhaustive analysis.  Note I: The data structure used to estimate these counts is probabilistic—specifically  HyperLogLog—which helps minimize memory usage. As a result  the reported counts may have a small margin of error. Additionally  the use of this structure introduces contention among workers  which can negatively affect performance and scalability. However  empirical measurements (see https://github.com/tlaplus/tlaplus/pull/1183#issuecomment-2870039111) have shown that the performance overhead of variable statistics collection on top of action and ordinary coverage is negligible.  Note II: The `TLC!TLCGet("spec")` named register equals the same data and serves as a more appropriate and structured input for extracting and parsing these numeric values during subsequent processing stages:  ```tla ---- MODULE Spec ---- EXTENDS TLC  Json  ... MyStats == PrintT( ToJson(  \* Alternatively  see CSV!CSVWrite operator. { [name |-> v.name  count |-> v.coverage.distinct] : v \in TLCGet("spec").variables } ) ) ==== ---- CONFIG Spec ---- ... _PERIODIC MyStats POSTCONDITION MyStats ==== ```  Variable statistics can be enabled independently of the `-coverage someTime` option by setting the Java system property `-Dtlc2.TLCGlobals.coverage=2` when running TLC. To activate both action and variable statistics  use `-Dtlc2.TLCGlobals.coverage=3`.  [Feature][TLC]  Signed-off-by: Markus Alexander Kuppe <github.com@lemmster.de>
tlaplus,tlaplus,c61bad9a00b5be5f18e121c19ab885e90d0e6c01,https://github.com/tlaplus/tlaplus/commit/c61bad9a00b5be5f18e121c19ab885e90d0e6c01,Do not override an fp's done state when recording an new <<fp  tidx>> GraphNode (where tidx is some tableau index).  Let s -> t be an action that is inserted into the behavior graph (BG). Liveness checking marks s as done/explored by adding it to a seen set S.  It is important to note that S must be distinct from the BFS' fingerprint set (FPSet) because FPSet is inconsistent with respect to liveness checking due to the concurrent addition of new states by other BFS workers (in practice  the elements of both sets are not states but fingerprints).  After adding s to S  liveness checking processes s->t by creating the cross product of {t} and the set of consistent nodes in the tableau graph. The resulting GraphNodes are inserted into BG. If and only if t is not in S  liveness checking will recursively process all of t's (state graph) successor states that are likewise not in S.  The implementation had the bug where inserting a GraphNode <<t  ...>> into the BG would remove t from S as a side effect  violating a fundamental invariant of the algorithm: [][t \in S => [](t \in S)]_S  This bug was unlikely to occur because state graph nodes are typically explored in BFS order.  This fix has minimal performance implications  except for the addition of an extra method parameter to `TableauNodePtrTable#put`. Unless inlining kicks in  recording a node in the behavior graph will require one extra stack frame.  Fixes Github issue #971 https://github.com/tlaplus/tlaplus/issues/971  [Bug][TLC]  Signed-off-by: Markus Alexander Kuppe <github.com@lemmster.de>
hellokaton,30-seconds-of-java8,18055589e4de0744c4404f8b5ca782dbb62bc633,https://github.com/hellokaton/30-seconds-of-java8/commit/18055589e4de0744c4404f8b5ca782dbb62bc633,chunk and performance test
aws,aws-sdk-java-v2,cd0172d77c3cf9ffd7ee777887a6d186bea6bffa,https://github.com/aws/aws-sdk-java-v2/commit/cd0172d77c3cf9ffd7ee777887a6d186bea6bffa,Remove def 16MB read chunk size (#5941)  * Remove def 16MB read chunk size  This commit removes the hardcoded 16MiB read chunk size for uploadFile() in the TransferManager. This was originally a workaround to improve performance when using CRT; however this is no longer necessary because file uploads are done entirely in CRT without the SDK/Java layer doing any reads.  Without this default size  the default read chunk size reverts back to the default from `FileAsyncRequestBody` which is 16KiB.  * Fix test
aws,aws-sdk-java-v2,9a313431cb1930cb4158a13004b9331115d06041,https://github.com/aws/aws-sdk-java-v2/commit/9a313431cb1930cb4158a13004b9331115d06041,Feature: EmfMetricLoggingPublisher (#5792)  * EmfMetricPublisher class added (#5752)  * EmfMetricPublisher class with basic unit test added  * Edge cases handled  * javadoc for class added  * Java doc for methods added  * minor unit test changes  * Change from using jacksonCore directly to using jsonWriter  * EmfMetricConfiguration class added  * MetricEmfConverter class added  * new module checklist done  * checkStyle fixed  * minor build problem fixed  * internal package added  * fixed the issue passing raw objects  * added java clock to unit test  * added unit test for publish method  * minor changes  * config class to builder pattern  * config class to builder pattern  * minor change  * End to end test passed  * added valid case in WARN_LOG allowlist  * unit tests adjusted  * Change logGroupName to required field  * move MetricValueNormalizer class to utils package  * converter implementation changed  * schemaConformTest added  * SchemaConformTest adjusted  * minor change  * emf metric publisher performance test (#5775)  * Benchmark test for emfMetricPublisher added  * input name changed  * add emfBenchmarkTest to BenchmarkRunner  * enabled METRIC_BENCHMARKS  * checkStyle fixed  * Changed the classname to EmfMetricLoggingPublisher (#5783)  * change class name and artifactId  * test-coverage pom changed  * javadoc fixed  * Cloudwatch and Logging benchmark test added / Changed logGroupName config (#5790)  * more benchmark test added/changed logGroupName config  * teardown method overrided  * logGroupName access method changed  * Snapshot version changed  * LogGroupName in lambda unit test added  * suppression added for system  * changelog added  * changeLog modified  * minor fix  * snapshot version fixed  * Snapshot version changed
aws,aws-sdk-java-v2,ca6c59cc03d07311e242b98fef2aa7f7d476da20,https://github.com/aws/aws-sdk-java-v2/commit/ca6c59cc03d07311e242b98fef2aa7f7d476da20,Smithy RPCv2 CBOR merge (#5621)  * Add RPCv2 module  * Add RPCv2 module (#5445)  * Comment out empty rpcv2 dependency  * Sync version to 2.26.31-SNAPSHOT  * Sugmanue/add byte support (#5477)  * Add support to serialize byte values  * Add tests for byte support  * Address PR comments  * Add rpcv2 protocol core (#5496)  * Add support to serialize byte values  * Add RPCv2 protocol core marshalling/unmarshalling  * Address PR comments  * Address PR comments 2  * Address PR comments 3  * Support for operation without input defined (#5512)  * Support for operation without input defined  * Fix a checkstyle issue  * Code clean up  * Code clean up 2  * Rewrite the condition to conjunctive normal form  * Add codgen tests (#5517)  * Add codgen tests  * Address PR comments  * Address PR comments 2  * Add missing class rename  * Add missing AWS_JSON protocol facts  * Account for null protocol case  * Add RPCv2 benchmark tests (#5526)  * Add RPCv2 benchmark tests  * Give the constants name a meaningful name  * Avoid parsing numbers when using RPCv2 protocol (#5539)  * Avoid parsing numbers when using RPCv2 protocol  * Refactor to avoid impacting JSON with RPCv2 logic (#5544)  * Refactor to avoid impacting JSON with RPCv2 logic  * Avoid making the unmarshallers depend on timestamp formats  * Avoid streams while unmarshalling  * Fix build failures  * Fix build failures 2  * Avoid growing copies of collections of known size (#5551)  * Add the new Smithy RPCv2 package  * Sugmanue/rpcv2 improve cbor performance 04 (#5564)  * Improve lookup by marshalling type  * Improve trait lookup using TraitType  * Add support for Smithy RPCv2 to the new service scripts (#5613)  * Add changelog for the release  * Fix typo in changelog  * Update to next SNAPSHOT version
apache,cloudstack,5bf81cf00233efe4478552e7bb13f3e96dc58ed1,https://github.com/apache/cloudstack/commit/5bf81cf00233efe4478552e7bb13f3e96dc58ed1,Merge release branch 4.19 to main  * 4.19: linstor: Improve copyPhysicalDisk performance (#9417)
apache,cloudstack,3d8d4875fe365e4ceaa60cf097fc57b5b71a32d3,https://github.com/apache/cloudstack/commit/3d8d4875fe365e4ceaa60cf097fc57b5b71a32d3,Merge release branch 4.18 to 4.19  * 4.18: linstor: Improve copyPhysicalDisk performance (#9417)
apache,cloudstack,27f23f4f75a4e19e014dcf986c59f2a365d3d716,https://github.com/apache/cloudstack/commit/27f23f4f75a4e19e014dcf986c59f2a365d3d716,linstor: Improve copyPhysicalDisk performance (#9417)  Tell qemu-img that we don't want to use a write cache (we are a block device) and also specify that we have zeroed devices in most cases.
apache,cloudstack,46f672563ebdab08e3b242fd968fc0b21e08f237,https://github.com/apache/cloudstack/commit/46f672563ebdab08e3b242fd968fc0b21e08f237,Improve migration of external VMware VMs into KVM cluster (#8815)  * Create/Export OVA file of the VM on external vCenter host  to temporary conversion location (NFS)  * Fixed ova issue on untar/extract ovf from ova file "tar -xf" cmd on ova fails with "ovf: Not found in archive" while extracting ovf file  * Updated VMware to KVM instance migration using OVA  * Refactoring and cleanup  * test fixes  * Consider zone wide pools in the destination cluster for instance conversion  * Remove local storage pool support as temporary conversion location - OVA export not possible as the pool is not accessible outside host  NFS pools are supported.  * cleanup unused code  * some improvements  and refactoring  * import nic unit tests  * vmware guru unit tests  * Separate clone VM and create template file for VMware migration - Export OVA (of the cloned VM) to the conversion location takes time. - Do any validations with cloned VM before creating the template (and fail early). - Updated unit tests.  * Check conversion support on host before clone vm / create template on vmware (and fail early)  * minor code improvements  * Auto select the host with instance conversion capability  * Skip instance conversion supported response param for non-KVM hosts  * Show supported conversion hosts in the UI  * Skip persistence map update if network doesn't exist  * Added support to export OVA from KVM host  through ovftool (when installed in KVM host)  * Updated importvm api param 'usemsforovaexport' to 'forcemstodownloadvmfiles'  to be generic  * Updated hardcoded UI messages with message labels  * Updated UI to support importvm api param - forcemstodownloadvmfiles  * Improved instance conversion support checks on ubuntu hosts  and for windows guest vms  * Use OVF template (VM disks and spec files) for instance conversion from VMware  instead of OVA file - this would further increase the migration performance (as it reduces the time for OVA preparation / archiving of the VM files into a single file)  * OVF export tool parallel threads code improvements  * Updated 'convert.vmware.instance.to.kvm.timeout' config default value to 3 hrs  * Config values check & code improvements  * Updated import log  with time taken and vm details  * Support for parallel downloads of VMware VM disk files while exporting OVF from MS  and other changes below. - Skip clone for powered off VMs - Fixes to support standalone host (with its default datacenter) - Some code improvements  * rebase fixes  * rebase fixes  * minor improvement  * code improvements - threads configuration  and api parameter changes to import vm files  * typo fix in error msg
apache,cloudstack,2ca0857bd59fbba87ccf3cfec57041ef5bff52a1,https://github.com/apache/cloudstack/commit/2ca0857bd59fbba87ccf3cfec57041ef5bff52a1,api: listVM API improvement followup  change returning of stats detail (#9177)  - Changes behaviour of details param handling via global setting: - listVirtualMachines API: when the details param is not provided  it returns whether stats are returned controlled by a new global setting `list.vm.default.details.stats` - listVirtualMachinesMetrics API: when the details param is not provided  it uses `all` details including `stats` - Users who are affected slow performance of the listVirtualMachines API response time can set `list.vm.default.details.stats` to `false` - Remove ConfigKey vm.stats.increment.metrics.in.memory which was renamed to `vm.stats.increment.metrics` in #5984 and also remove unused/unnecessary global settings via upgrade path - Changes default value of VM stats accumulation setting `vm.stats.increment.metrics` to false until a better solution emerges. Since #5984  this is true and during the execution of listVM APIs the stats are clubbed/calculated which can immensely slow down list VM API calls. Any costly operations such as summing of stats shouldn't be done during the course of a synchronous API  such as the list VM API. - Fix UI that uses listVirtualMachinesMetrics to not call `stats` detail when in list view without metrics selected.  Signed-off-by: Rohit Yadav <rohit.yadav@shapeblue.com>
google,copybara,98cb0ffde604e4ce41dea56a3073d68f55ccab5f,https://github.com/google/copybara/commit/98cb0ffde604e4ce41dea56a3073d68f55ccab5f,Compute git consistency file hashes from the FS  Previously  to generate the consistency file on git destinations  we would run `git show` on every file and compute the hash from the output. This is quite inefficient due to the overhead of spawning the git command. It is much more efficient to just read the file directly from the local checkout instead.  In my use case that involves ~55k files  this change significantly improves copybara performance  reducing run time from ~14 minutes to ~9 minutes.  Change-Id: Ibd69f243a6b9a52ee872b2ce9da230d3059d54de
google,copybara,5e79da92a01cde059680981739ebc0d5b3cf617f,https://github.com/google/copybara/commit/5e79da92a01cde059680981739ebc0d5b3cf617f,Use git checkout in GitRepository#checkout()  GitRepository#checkout() was previously written in an odd way where it would poorly reinvent `git checkout` by running `git show` on every single file  sequentially. This is extremely inefficient with a large number of files. Change the logic to do the obvious thing  which is to use `git checkout` instead.  In my use case that involves ~55k files with merge import enabled  this change dramatically improves copybara performance  reducing run time from ~26 minutes to ~14 minutes.  Change-Id: Idb3e15a53c7bb78c764a95a5ec5d67e7df048111
google,copybara,49cbd69d0e9f858520e78c8946f73835d279149c,https://github.com/google/copybara/commit/49cbd69d0e9f858520e78c8946f73835d279149c,Allow Sequences of fully qualified files as Glob  Globs perform poorly when files are explicitly listed  this is an easier syntax and a performance boost.  BUG=348385876 PiperOrigin-RevId: 644504452 Change-Id: I1946852ce0e773506a90cea57daa2f344c321fdf
opentripplanner,OpenTripPlanner,a447a26aad345daf52679231d28e417d9ae93c0b,https://github.com/opentripplanner/OpenTripPlanner/commit/a447a26aad345daf52679231d28e417d9ae93c0b,Merge pull request #6260 from HSLdevcom/speculative-rental-fanout  Improve performance of speculative rental vehicle use in reverse search
opentripplanner,OpenTripPlanner,c1bcc7ed253cf6bb19b36268e75f05405034e7b8,https://github.com/opentripplanner/OpenTripPlanner/commit/c1bcc7ed253cf6bb19b36268e75f05405034e7b8,refactor: Add CompositeUtil and more unit tests to the ParetoSet  This extract logic to merge a hierarchy of objects containing composites  flatten the structure into one list of elements. There is a small performance optimization  but the important thing is that we will use this later in the design later.
spring-projects,spring-kafka,53149d4e6557420172b410fea8a12b10bdf320ee,https://github.com/spring-projects/spring-kafka/commit/53149d4e6557420172b410fea8a12b10bdf320ee,GH-3764: Replace LinkedList with ArrayList in listener container for records  Fixes: #3764 Issue link: https://github.com/spring-projects/spring-kafka/issues/3764  Acknowledging an index in a batch has quadratic time `N(N+1)/2` ~ `N^2`  Batch consumers operate on a `LinkedList` of records. If the consumer uses `MANUAL_IMMEDIATE` ack mode  and the listener invokes `acknowledgement.acknowledge(index)` where index is relatively big (e.g. when processing batches of `100k`)  performance takes hit because of the linear lookup `records.get(i)` in a loop  Signed-off-by: Janek Lasocki-Biczysko <janek.lb@gmail.com>  [artem.bilan@broadcom.com: improve commit message] **Auto-cherry-pick to `3.3.x`** Signed-off-by: Artem Bilan <artem.bilan@broadcom.com>
confluentinc,schema-registry,4555074376029cb2227bcb331af67dcfb29699f1,https://github.com/confluentinc/schema-registry/commit/4555074376029cb2227bcb331af67dcfb29699f1,DGS-15999 Add performanceMetric to get config metadata API (#3265) (#3311)
confluentinc,schema-registry,ef26d45780aa99f7a713d22574558c42cd4d6f66,https://github.com/confluentinc/schema-registry/commit/ef26d45780aa99f7a713d22574558c42cd4d6f66,DGS-15999 Add performanceMetric to get config metadata API (#3265)
confluentinc,schema-registry,036d99528fc538b10b844ceaae873cde4361b397,https://github.com/confluentinc/schema-registry/commit/036d99528fc538b10b844ceaae873cde4361b397,DGS-15999 Add performanceMetric to list schema refs API (#3264)
confluentinc,schema-registry,e0b2fdfea7edb9b02069c90ffc23aecfe4d65e09,https://github.com/confluentinc/schema-registry/commit/e0b2fdfea7edb9b02069c90ffc23aecfe4d65e09,DGS-15999 Add performanceMetric to global delete config API (#3263)
confluentinc,schema-registry,7b79ae6cc158a35a0677c4cc36c0953ca4b95d29,https://github.com/confluentinc/schema-registry/commit/7b79ae6cc158a35a0677c4cc36c0953ca4b95d29,Merge pull request #3225 from confluentinc/performance-metric-apis  Add performance metric to all SR APIs
confluentinc,schema-registry,593308bcb5d2f5b79c61cb79230e5294f5db2963,https://github.com/confluentinc/schema-registry/commit/593308bcb5d2f5b79c61cb79230e5294f5db2963,add performance metric to all SR APIs (#21)
prometheus,client_java,4ce0d1513b7b211285478e77b11375da2aecd09d,https://github.com/prometheus/client_java/commit/4ce0d1513b7b211285478e77b11375da2aecd09d,additional improve MetricSnapshots.Builder performance (#985)  Signed-off-by: Andrey Burov <burov4j@yandex.ru>
bytedance,scene,1231f8cda0e3778f1c400f87b0c1e94c3069acd8,https://github.com/bytedance/scene/commit/1231f8cda0e3778f1c400f87b0c1e94c3069acd8,perf: use ThreadLocal to improve checkUIThread performance jiangqi 2024/10/9  15:41
bytedance,scene,d2229932ed597aefd034d0d53066fff53752f326,https://github.com/bytedance/scene/commit/d2229932ed597aefd034d0d53066fff53752f326,perf: create NavigationSceneManager#mThrowableHandler instance lazily
bytedance,scene,ba651c84750db5c90151e13b0be2708c459fb351,https://github.com/bytedance/scene/commit/ba651c84750db5c90151e13b0be2708c459fb351,perf: create Android8DefaultSceneAnimatorExecutor instance lazily
bytedance,scene,fe7cceda183e7c41d9c6981ca6402d9db3e60911,https://github.com/bytedance/scene/commit/fe7cceda183e7c41d9c6981ca6402d9db3e60911,refactor: improve ThreadUtility.checkUIThread performance
bytedance,scene,9226f5a9abc81dfcda357a0481a8322dd368e9c2,https://github.com/bytedance/scene/commit/9226f5a9abc81dfcda357a0481a8322dd368e9c2,refactor: NavigationScene add preloadClasses method to improve startup performance
bytedance,scene,7384f35a43773e4d17bcd5953b97fa2f933fe4b3,https://github.com/bytedance/scene/commit/7384f35a43773e4d17bcd5953b97fa2f933fe4b3,refactor: NavigationSceneOptions add setMergeNavigationSceneView method to improve startup performance
bytedance,scene,b84cde7b924f235a18e6e1d2d85e5b47326a92e8,https://github.com/bytedance/scene/commit/b84cde7b924f235a18e6e1d2d85e5b47326a92e8,refactor: NavigationSceneOptions add LazyLoadNavigationSceneUnnecessaryView option  NavigationScene will not add animation container at startup to improve performance
bytedance,scene,da7a4c142fd4f71c8df4c72751fa8b6e46a37456,https://github.com/bytedance/scene/commit/da7a4c142fd4f71c8df4c72751fa8b6e46a37456,refactor: NavigationSceneOptions add setUseActivityContextAndLayoutInflater method to improve startup performance
bytedance,scene,b6d4ea8c672981880cb6c68db97beecd733acbbf,https://github.com/bytedance/scene/commit/b6d4ea8c672981880cb6c68db97beecd733acbbf,refactor: improve save and restore performance  only restore visible scene
androidx,media,915130eb00c1fd6de370b2614e27862ab3531642,https://github.com/androidx/media/commit/915130eb00c1fd6de370b2614e27862ab3531642,Add `usePlatformDiagnostics` in Transformer.  Added a new `usePlatformDiagnostics` in Transformer. This parameter enables/disables forwarding editing events and performance data to the platform.  This is pre-work for metrics support in Transformer. In the following CLs  metrics collection and forwarding will be implemented.  PiperOrigin-RevId: 707930368
androidx,media,b5a1efdbce3220ef6a5733bd67ceb7dbaca3a5f3,https://github.com/androidx/media/commit/b5a1efdbce3220ef6a5733bd67ceb7dbaca3a5f3,`ForwardingTimeline`: Implement & `final`ize some methods  `equals`  `hashCode`  and `getPeriodByUid` are correctly implemented on `Timeline`. Overriding these in a way that maintains correctness is fiddly  so this CL prevents that for the 'simple' case of subclasses of `ForwardingTimeline`. Implementations of `Timeline` that need to override these methods for performance should extend `Timeline` or `AbstractConcatenatedTimeline` instead of `ForwardingTimeline`.  PiperOrigin-RevId: 703035721
androidx,media,fd3d8e1782bc74c23b545c3f24c25924d188f822,https://github.com/androidx/media/commit/fd3d8e1782bc74c23b545c3f24c25924d188f822,Add RATE_UNSET option to encoder performance setting  This is to allow not setting the MediaFormat OPERATING_RATE and PRIORITY altogether. The current behvaiour  if left the value `UNSET`  it'll apply the our optimizations  but apps might want to disable this optimization.  PiperOrigin-RevId: 675923909
androidx,media,f0fb3862245579ff3874d0bdc10a7481077984d8,https://github.com/androidx/media/commit/f0fb3862245579ff3874d0bdc10a7481077984d8,Add workaround for Galaxy Tab S7 FE device PerformancePoint issue  The Galaxy Tab S7 FE has a device issue that causes 60fps secure H264 streams to be marked as unsupported. This CL adds a workaround for this issue by checking the CDD required support for secure H264 in addition to the current check on standard H264. If the provided performance points do not cover the CDD requirement of support 720p H264 at 60fps  then it falls back to using legacy methods for checking frame rate and resolution support.  Issue: androidx/media#1619 PiperOrigin-RevId: 675920968
androidx,media,6e0e2d0ceeb9ee88434b79b6d2c4dfe74e93ec8f,https://github.com/androidx/media/commit/6e0e2d0ceeb9ee88434b79b6d2c4dfe74e93ec8f,Add QueuingGlShaderProgram for effects that run outside GL context  Implement a QueuingGlShaderProgram which queues up OpenGL frames and allows asynchronous execution of effects that operate on video frames without a performance penalty.  PiperOrigin-RevId: 666326611
androidx,media,931b0e25f1092674f94a43cdbcceb64c575199cc,https://github.com/androidx/media/commit/931b0e25f1092674f94a43cdbcceb64c575199cc,Add a DefaultDecoderFactory option to configure operating rate  This has the largest impact during operations with no encoder  such as frame extraction. Add a matching performance test.  PiperOrigin-RevId: 661220044
JetBrains,intellij-plugins,b64d75f3053076b51b8c91db106247874ccf2d4c,https://github.com/JetBrains/intellij-plugins/commit/b64d75f3053076b51b8c91db106247874ccf2d4c,[p4] prevent caching big content in VF attribute  Alternatively  GistStorage can be used for storing big files  but storing multiple big files can lead to performance problems which may not have been justified by "offline mode" functionality.  GitOrigin-RevId: 2acc4e5fd6b937743275c2567b4e59c1cbdcfed0
PurpurMC,Purpur,db412df46619f2cd57bf8cb79bd6b49c444466ea,https://github.com/PurpurMC/Purpur/commit/db412df46619f2cd57bf8cb79bd6b49c444466ea,Add toggle for RNG manipulation  Paper patches RNG maniplulation by using a shared (and locked) random source. This comes with a performance gain  but technical players may prefer the ability to manipulate RNG.
Nekogram,Nekogram,ff812817953f7d5c52c12ecf7a50d02b69ba458c,https://github.com/Nekogram/Nekogram/commit/ff812817953f7d5c52c12ecf7a50d02b69ba458c,Fix photo viewer blur performance
Nekogram,Nekogram,15db3cec08bd6e3feec985ee9589f853de050489,https://github.com/Nekogram/Nekogram/commit/15db3cec08bd6e3feec985ee9589f853de050489,View large photos only on high performance devices
QuantumBadger,RedReader,652b523508287777a7ba2353c3bc95dfc556fa39,https://github.com/QuantumBadger/RedReader/commit/652b523508287777a7ba2353c3bc95dfc556fa39,Improve download performance
liferay,liferay-portal,bca74f3bb9870af80cd8eabf5fa632a68e45f882,https://github.com/liferay/liferay-portal/commit/bca74f3bb9870af80cd8eabf5fa632a68e45f882,LPD-52246 improve performance
liferay,liferay-portal,297c833c2e059e68f7f8e4fe806a3c720a1bdf9d,https://github.com/liferay/liferay-portal/commit/297c833c2e059e68f7f8e4fe806a3c720a1bdf9d,LPD-51094 Modify query to improve performance of hasSharingPermission and hasShareableSharingPermission
liferay,liferay-portal,a61a43625b765997790c6c914a5f0c37a038760d,https://github.com/liferay/liferay-portal/commit/a61a43625b765997790c6c914a5f0c37a038760d,LPD-50884 Improve performance. No need to count occurrences when creating or dropping schema.
liferay,liferay-portal,7f66949cf856dd4db380660476a4bd35a716483a,https://github.com/liferay/liferay-portal/commit/7f66949cf856dd4db380660476a4bd35a716483a,LPD-48867 Use NestedFields to avoid performance issues  Co-authored-by: Alicia García <alicia.garcia@liferay.com>
liferay,liferay-portal,5c13cd5fdf7c65d494256d0fb7bc4b55c09b31d4,https://github.com/liferay/liferay-portal/commit/5c13cd5fdf7c65d494256d0fb7bc4b55c09b31d4,LPD-49536 Use NestedFields to avoid performance issues
liferay,liferay-portal,ba446225eecb5166b32b1c2bb26a0fd0f9753490,https://github.com/liferay/liferay-portal/commit/ba446225eecb5166b32b1c2bb26a0fd0f9753490,LPD-49787 - Update the configuration directly since we already have it  in order to improve performance.
liferay,liferay-portal,5ac073349d6e75c06f682cafb6792f2fbcd62ce6,https://github.com/liferay/liferay-portal/commit/5ac073349d6e75c06f682cafb6792f2fbcd62ce6,LPD-49787 - Improve performance for the deleteRoleAccessToControlMenu method.
liferay,liferay-portal,c8b8e2b6ca9b8087e3a88de0995bbb8db300a4c6,https://github.com/liferay/liferay-portal/commit/c8b8e2b6ca9b8087e3a88de0995bbb8db300a4c6,LPD-48903 performance optimization
liferay,liferay-portal,3531e921749f441008bd13ce13390e62e5aca2e2,https://github.com/liferay/liferay-portal/commit/3531e921749f441008bd13ce13390e62e5aca2e2,LPD-45318 Improve performance
liferay,liferay-portal,0e6df9512eb3e8c24bde71d171e9b8bfb50d12b5,https://github.com/liferay/liferay-portal/commit/0e6df9512eb3e8c24bde71d171e9b8bfb50d12b5,LPD-49747 Do not execute one call per file to check if file exist in repo. Repo can be external and can cause performance issues.
liferay,liferay-portal,cb01e4647f8800c8804763f7cf1f2a652ba9ba26,https://github.com/liferay/liferay-portal/commit/cb01e4647f8800c8804763f7cf1f2a652ba9ba26,LPD-49747 Use parallel to improve performance.
liferay,liferay-portal,0067887f36a69ed173adb3e6d3c04f0487cf8b2d,https://github.com/liferay/liferay-portal/commit/0067887f36a69ed173adb3e6d3c04f0487cf8b2d,LPD-48242 Add "order by" to make the result consistent with the fetch method generated by persistence layer  this part of logic was originally added by LPD-28122(007246d4c619fbc4eebccb325fe106f119ba2b8c) to improve performance of groupPersistence.fetchByLiveGroupId(liveGroupId)
liferay,liferay-portal,64d464c8042bd396253b8c9e27930193c128b109,https://github.com/liferay/liferay-portal/commit/64d464c8042bd396253b8c9e27930193c128b109,LPD-48419 Improve performance a tiny bit
liferay,liferay-portal,7903764d8887843ee3912f320c126d56389e44de,https://github.com/liferay/liferay-portal/commit/7903764d8887843ee3912f320c126d56389e44de,LPD-45684 In general  it is better for performance to prepopulate the size  but this math is too complicated. So in this case  it's better to optimize for the human brain instead of runtime performance.
liferay,liferay-portal,cd671b68b2e482e273bd101cbda142568f053e80,https://github.com/liferay/liferay-portal/commit/cd671b68b2e482e273bd101cbda142568f053e80,LPD-35071 - Improve performance of implementation by not looping through all frontendTokenDefinitions.
liferay,liferay-portal,8bbb0db5706862e8a357f3934e6e4abc70ad71e7,https://github.com/liferay/liferay-portal/commit/8bbb0db5706862e8a357f3934e6e4abc70ad71e7,LPD-42809 Clean up empty company caches  This makes sure we don't leave empty maps hanging around when all import maps for a company have been removed.  However it does so at the expense of cache write performance. Given how unlikely it is destroying companies and given that it is even less likely that so many are created and destroyed as to leave a noticeable trace in RAM usage due to empty maps maybe we prefer the empty maps than the synchronized blocks.  That's why I'm adding this commit as the last of the PR  in case we prefer removing it.
liferay,liferay-portal,39dfa38b7ef09db0cc8a8697ea087b2839fa8851,https://github.com/liferay/liferay-portal/commit/39dfa38b7ef09db0cc8a8697ea087b2839fa8851,LPD-46069 Improve performance by preventing computing the types of not input-type fragments
liferay,liferay-portal,274e5c9ee0f35190712c93c3cca51893114ab1a4,https://github.com/liferay/liferay-portal/commit/274e5c9ee0f35190712c93c3cca51893114ab1a4,LPD-45340 Improve performance by avoiding iterating all restricted methods every time getAsString is invoked
liferay,liferay-portal,3e7b83ee90154386d725215c4ad333072563825e,https://github.com/liferay/liferay-portal/commit/3e7b83ee90154386d725215c4ad333072563825e,LPD-44673 Improve performance by fetching the SEO entry layout only once to obtain the entire map
liferay,liferay-portal,b61612d15537f4be0b68a60c48865a09c51283bc,https://github.com/liferay/liferay-portal/commit/b61612d15537f4be0b68a60c48865a09c51283bc,LPD-40036 Using clay:navigation-bar instead of clay:tabs for better performance
liferay,liferay-portal,561f8020e0d9f5573d1963cf8a1fe18b07274742,https://github.com/liferay/liferay-portal/commit/561f8020e0d9f5573d1963cf8a1fe18b07274742,LPD-38670 if user has VIEW_ACCOUNTS permission  set the account instead of iterating to increase performance
liferay,liferay-portal,096bcec934b9b5612f85eda13e122d3c17ce9de0,https://github.com/liferay/liferay-portal/commit/096bcec934b9b5612f85eda13e122d3c17ce9de0,LPD-29336 Use finder for better performance
liferay,liferay-portal,60e54bfb5673febd4e20c6d0032f543482f8429a,https://github.com/liferay/liferay-portal/commit/60e54bfb5673febd4e20c6d0032f543482f8429a,LPD-39612 Prefer array because List#get performance is impl specific
liferay,liferay-portal,d7c166ce0666c685f002dd039fb41b878c5a431f,https://github.com/liferay/liferay-portal/commit/d7c166ce0666c685f002dd039fb41b878c5a431f,LPD-40120 SF - Match to SYZ's code for better readability and performance
liferay,liferay-portal,e2fe0c166783e1de4759b75b7096e0b7a519603e,https://github.com/liferay/liferay-portal/commit/e2fe0c166783e1de4759b75b7096e0b7a519603e,LPD-40120 Improve performance by only adding matching entries  rather than adding all then purging non-matches.
liferay,liferay-portal,b2a6f7e74120c2854e5c04ff400a855371babf2d,https://github.com/liferay/liferay-portal/commit/b2a6f7e74120c2854e5c04ff400a855371babf2d,LPD-40070 Use a non-recursive approach by default  In the first version we were extracting inline handlers from all nodes in the HTML but that may introduce issues that CSP is trying to prevent.  So now by default we will be only extracting handlers from the top level nodes of the HTML unless recursive=true is passed to the JSP tag or Util class.  This will also enhance performance.
liferay,liferay-portal,0d7fc3f8a8dbd03c64e86fb98f43a3e36a05b985,https://github.com/liferay/liferay-portal/commit/0d7fc3f8a8dbd03c64e86fb98f43a3e36a05b985,LPD-40070 Substitute <liferay-util:on> by <liferay-ui:csp>  Instead of creating a JSP tag that attaches an event handler to the previous DOM node  which is something that can fail due to SPA moving the nodes  refactoring the code  and so on...  we are creating a tag that rewrites all its body extracting inline handlers to their own <script> node using `ContentSecurityPolicyHTMLRewriterUtil`.  To enahnce performance  this transformation only takes place when a CSP nonce is active for the current request.
liferay,liferay-portal,713138203e1b9863993d033378dc708c202f15d5,https://github.com/liferay/liferay-portal/commit/713138203e1b9863993d033378dc708c202f15d5,LPD-31242 Since we are only upgrading method calls that use super  it is technically implied that we are extending BaseModelListener  so only check those files to improve performance
liferay,liferay-portal,5172e7ae5c3d1c13b68f9538c2874af266d37de4,https://github.com/liferay/liferay-portal/commit/5172e7ae5c3d1c13b68f9538c2874af266d37de4,LPD-36212 Improve performance a tiny bit  (cherry picked from commit 17c3577225b01dd18a0dfca1cd842231940e0d7e)
liferay,liferay-portal,ec26ee13fd3571305f62830f609e47840eea81f5,https://github.com/liferay/liferay-portal/commit/ec26ee13fd3571305f62830f609e47840eea81f5,LPD-37990 Improve performance
liferay,liferay-portal,f88f1eb29c524d8098af96ae11d625d0fbf01bfa,https://github.com/liferay/liferay-portal/commit/f88f1eb29c524d8098af96ae11d625d0fbf01bfa,Revert "LPD-36212 Improve performance a tiny bit"  This reverts commit 17c3577225b01dd18a0dfca1cd842231940e0d7e.
liferay,liferay-portal,17c3577225b01dd18a0dfca1cd842231940e0d7e,https://github.com/liferay/liferay-portal/commit/17c3577225b01dd18a0dfca1cd842231940e0d7e,LPD-36212 Improve performance a tiny bit
liferay,liferay-portal,138733c55cc06a1fee71674db7f4b5faef9baca0,https://github.com/liferay/liferay-portal/commit/138733c55cc06a1fee71674db7f4b5faef9baca0,LPD-37702 Use a constant due to performance since it won't change and we will check it multiple times
liferay,liferay-portal,cb4cb4868c64a0c211c89df50cdec5d66d9deeba,https://github.com/liferay/liferay-portal/commit/cb4cb4868c64a0c211c89df50cdec5d66d9deeba,LPD-36475 Fix performance regression caused by 79da3895 . Delegating ResourceBundle.handleGetObject() to inner ResourceBundle.getObject() and ResourceBundle.handleKeySet() to inner ResourceBundle.keySet() is logcally "wrong" since it introduces a new layer of parent fallback lookup  but if the parent is absent the overall function is still fine. And it is very bad for performance  due to the extra lookup and ResourceBundle.keySet()'s copy protection  for large ResourceBundle  this copy protection generates a lot of garbage.
liferay,liferay-portal,60026c4b1abf2fe5be8490b68d666a6f9442dc4c,https://github.com/liferay/liferay-portal/commit/60026c4b1abf2fe5be8490b68d666a6f9442dc4c,Revert "LPD-33730 show content performance icon navigation only when the feature flag is disabled"  This reverts commit 30f49d781c23d773e3cc3ddc075d883a02a8378f.
liferay,liferay-portal,709c5aa369a834cdfae9e2f41f47abd0d0de860f,https://github.com/liferay/liferay-portal/commit/709c5aa369a834cdfae9e2f41f47abd0d0de860f,LPD-33173 Provide MVC Resource for getting content performance tab
liferay,liferay-portal,d635da4e9fb82fe3d6801b19a49aa6ffe871a049,https://github.com/liferay/liferay-portal/commit/d635da4e9fb82fe3d6801b19a49aa6ffe871a049,LPD-33173 create GetContentPerformanceInfoMVCResourceCommand
liferay,liferay-portal,3becfbec9e7055e6d42b5ea5791f8c4f220b1a38,https://github.com/liferay/liferay-portal/commit/3becfbec9e7055e6d42b5ea5791f8c4f220b1a38,LPD-33730 show content performance icon navigation only when the feature flag is disabled
liferay,liferay-portal,0be2a67fa4fb8ef961e6a3e27fa6d830d1a4121a,https://github.com/liferay/liferay-portal/commit/0be2a67fa4fb8ef961e6a3e27fa6d830d1a4121a,LPD-32570 Improve performance a tiny bit
liferay,liferay-portal,61b4a424b479d5a2030ff5bd7715e680a43cbe7c,https://github.com/liferay/liferay-portal/commit/61b4a424b479d5a2030ff5bd7715e680a43cbe7c,LPD-25985 Add singleton into module commerce-shipping-engine-fixed-api  # breaking  ## What modules/apps/commerce/commerce-shipping-engine-fixed-api/src/main/java/com/liferay/commerce/shipping/engine/fixed/util/comparator/CommerceShippingFixedOptionPriorityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-shipping-engine-fixed-api/src/main/java/com/liferay/commerce/shipping/engine/fixed/util/comparator/CommerceShippingFixedOptionRelCountryIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,4ff28e4d29b8d39f9e6932a3d3787fe36ce287c8,https://github.com/liferay/liferay-portal/commit/4ff28e4d29b8d39f9e6932a3d3787fe36ce287c8,LPD-25985 Add singleton into module sharing-api  # breaking  ## What modules/apps/sharing/sharing-api/src/main/java/com/liferay/sharing/util/comparator/SharingEntryModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,edda3e86859f98cceda96fba09a07b53827869d2,https://github.com/liferay/liferay-portal/commit/edda3e86859f98cceda96fba09a07b53827869d2,LPD-25985 Add singleton into module saved-content-api  # breaking  ## What modules/apps/saved-content/saved-content-api/src/main/java/com/liferay/saved/content/util/comparator/SavedContentEntryClassNameIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,0754766015db663bd73c97291b1d027e149f2c66,https://github.com/liferay/liferay-portal/commit/0754766015db663bd73c97291b1d027e149f2c66,LPD-25955 Add singleton into module portal-reports-engine-console-api  # breaking  ## What modules/dxp/apps/portal-reports-engine-console/portal-reports-engine-console-api/src/main/java/com/liferay/portal/reports/engine/console/util/comparator/DefinitionCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/dxp/apps/portal-reports-engine-console/portal-reports-engine-console-api/src/main/java/com/liferay/portal/reports/engine/console/util/comparator/EntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/dxp/apps/portal-reports-engine-console/portal-reports-engine-console-api/src/main/java/com/liferay/portal/reports/engine/console/util/comparator/SourceCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,46d19d1c8f4d0c0f6d529c1027c73163eb4bda03,https://github.com/liferay/liferay-portal/commit/46d19d1c8f4d0c0f6d529c1027c73163eb4bda03,LPD-25955 Add singleton into module site-navigation-api  # breaking  ## What modules/apps/site-navigation/site-navigation-api/src/main/java/com/liferay/site/navigation/util/comparator/SiteNavigationMenuCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/site-navigation/site-navigation-api/src/main/java/com/liferay/site/navigation/util/comparator/SiteNavigationMenuModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/site-navigation/site-navigation-api/src/main/java/com/liferay/site/navigation/util/comparator/SiteNavigationMenuNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/site-navigation/site-navigation-api/src/main/java/com/liferay/site/navigation/util/comparator/SiteNavigationMenuItemOrderComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,baa550b2319768764350e528368e8e071250da4a,https://github.com/liferay/liferay-portal/commit/baa550b2319768764350e528368e8e071250da4a,LPD-25955 Add singleton into module calendar-api  # breaking  ## What modules/apps/calendar/calendar-api/src/main/java/com/liferay/calendar/util/comparator/CalendarBookingStartTimeComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/calendar/calendar-api/src/main/java/com/liferay/calendar/util/comparator/CalendarNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/calendar/calendar-api/src/main/java/com/liferay/calendar/util/comparator/CalendarResourceCodeComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/calendar/calendar-api/src/main/java/com/liferay/calendar/util/comparator/CalendarResourceNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,4ff0db43c7c3aa9cb507b5090d8170a3d9050015,https://github.com/liferay/liferay-portal/commit/4ff0db43c7c3aa9cb507b5090d8170a3d9050015,LPD-25955 Add singleton into module bookmarks-api  # breaking  ## What modules/apps/bookmarks/bookmarks-api/src/main/java/com/liferay/bookmarks/util/comparator/EntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/bookmarks/bookmarks-api/src/main/java/com/liferay/bookmarks/util/comparator/EntryNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/bookmarks/bookmarks-api/src/main/java/com/liferay/bookmarks/util/comparator/EntryPriorityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/bookmarks/bookmarks-api/src/main/java/com/liferay/bookmarks/util/comparator/EntryURLComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/bookmarks/bookmarks-api/src/main/java/com/liferay/bookmarks/util/comparator/FolderIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,b3a9cdd689c23353cd25bd88c6f67e86f497e0c3,https://github.com/liferay/liferay-portal/commit/b3a9cdd689c23353cd25bd88c6f67e86f497e0c3,LPD-25955 Add singleton into module style-book-api  # breaking  ## What modules/apps/style-book/style-book-api/src/main/java/com/liferay/style/book/util/comparator/StyleBookEntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,94d278ab980b10182758cf8a6c51fbd9837f5b85,https://github.com/liferay/liferay-portal/commit/94d278ab980b10182758cf8a6c51fbd9837f5b85,LPD-25955 Add singleton into module trash-api  # breaking  ## What modules/apps/trash/trash-api/src/main/java/com/liferay/trash/util/comparator/EntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/trash/trash-api/src/main/java/com/liferay/trash/util/comparator/EntryTypeComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/trash/trash-api/src/main/java/com/liferay/trash/util/comparator/EntryUserNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,739f94ca05cfabfc49d04a8e60abdd6d99fe3b2b,https://github.com/liferay/liferay-portal/commit/739f94ca05cfabfc49d04a8e60abdd6d99fe3b2b,LPD-25955 Add singleton into module wiki-api  # breaking  ## What modules/apps/wiki/wiki-api/src/main/java/com/liferay/wiki/util/comparator/NodeLastPostDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/wiki/wiki-api/src/main/java/com/liferay/wiki/util/comparator/NodeNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/wiki/wiki-api/src/main/java/com/liferay/wiki/util/comparator/PageCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/wiki/wiki-api/src/main/java/com/liferay/wiki/util/comparator/PageModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/wiki/wiki-api/src/main/java/com/liferay/wiki/util/comparator/PageTitleComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,821ccba5209638715a0e7e0f4618c742661b9c8e,https://github.com/liferay/liferay-portal/commit/821ccba5209638715a0e7e0f4618c742661b9c8e,LPD-26560 Add singleton into module blogs-api  # breaking  ## What modules/apps/blogs/blogs-api/src/main/java/com/liferay/blogs/util/comparator/EntryDisplayDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/blogs/blogs-api/src/main/java/com/liferay/blogs/util/comparator/EntryIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/blogs/blogs-api/src/main/java/com/liferay/blogs/util/comparator/EntryModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/blogs/blogs-api/src/main/java/com/liferay/blogs/util/comparator/EntryTitleComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,0c174ce05114b331380d9066ee8f65ced7be9eeb,https://github.com/liferay/liferay-portal/commit/0c174ce05114b331380d9066ee8f65ced7be9eeb,LPD-26560 Add singleton into module subscription-api  # breaking  ## What modules/apps/subscription/subscription-api/src/main/java/com/liferay/subscription/util/comparator/SubscriptionClassNameIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,bb39fd392663d1885c6263914507b26768b039b5,https://github.com/liferay/liferay-portal/commit/bb39fd392663d1885c6263914507b26768b039b5,LPD-26560 Add singleton into module object-api  # breaking  ## What modules/apps/object/object-api/src/main/java/com/liferay/object/util/comparator/ObjectFieldCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,1505de432a97bcd6233ff9e92d35873629348225,https://github.com/liferay/liferay-portal/commit/1505de432a97bcd6233ff9e92d35873629348225,LPD-26560 Add singleton into module push-notifications-api  # breaking  ## What modules/apps/push-notifications/push-notifications-api/src/main/java/com/liferay/push/notifications/util/comparator/PushNotificationsDevicePlatformComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,6a9576fb021af773b2bbd6d2084aa6c92abdfdd9,https://github.com/liferay/liferay-portal/commit/6a9576fb021af773b2bbd6d2084aa6c92abdfdd9,LPD-26560 Add singleton into module portal-security-service-access-policy-api  # breaking  ## What modules/apps/portal-security/portal-security-service-access-policy-api/src/main/java/com/liferay/portal/security/service/access/policy/util/comparator/SAPEntryNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,fc79693660f721f68d6ea31c415d4c942f778449,https://github.com/liferay/liferay-portal/commit/fc79693660f721f68d6ea31c415d4c942f778449,LPD-26560 Add singleton into module friendly-url-api  # breaking  ## What modules/apps/friendly-url/friendly-url-api/src/main/java/com/liferay/friendly/url/util/comparator/FriendlyURLEntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/friendly-url/friendly-url-api/src/main/java/com/liferay/friendly/url/util/comparator/FriendlyURLEntryLocalizationComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,14c321c27f4c50c031e3036f9432c0078523da29,https://github.com/liferay/liferay-portal/commit/14c321c27f4c50c031e3036f9432c0078523da29,LPD-26560 Add singleton into module document-library-content-api  # breaking  ## What modules/apps/document-library/document-library-content-api/src/main/java/com/liferay/document/library/content/util/comparator/DLContentVersionComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,2eb50b5ab1e36d8c4291d1f549c399a023e68347,https://github.com/liferay/liferay-portal/commit/2eb50b5ab1e36d8c4291d1f549c399a023e68347,LPD-33654 Add capability to use it without checking permissions for get a better performance when it is not needed to check it
liferay,liferay-portal,b7896873028dcc8001285e42e6d1b9d9407fca21,https://github.com/liferay/liferay-portal/commit/b7896873028dcc8001285e42e6d1b9d9407fca21,LPD-27027 Add singleton into module commerce-inventory-api  # breaking  ## What modules/apps/commerce/commerce-inventory-api/src/main/java/com/liferay/commerce/inventory/util/comparator/CommerceInventoryWarehouseCityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-inventory-api/src/main/java/com/liferay/commerce/inventory/util/comparator/CommerceInventoryWarehouseItemWarehouseNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-inventory-api/src/main/java/com/liferay/commerce/inventory/util/comparator/CommerceInventoryReplenishmentItemAvailabilityDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-inventory-api/src/main/java/com/liferay/commerce/inventory/util/comparator/CommerceInventoryWarehouseItemQuantityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-inventory-api/src/main/java/com/liferay/commerce/inventory/util/comparator/CommerceInventoryWarehouseNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,ebb5f1ce3a2816cadde557db2ae0700a63e35038,https://github.com/liferay/liferay-portal/commit/ebb5f1ce3a2816cadde557db2ae0700a63e35038,LPD-27027 Add singleton into module commerce-product-type-virtual-order-api  # breaking  ## What modules/apps/commerce/commerce-product-type-virtual-order-api/src/main/java/com/liferay/commerce/product/type/virtual/order/util/comparator/CommerceVirtualOrderItemCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,2442863bcdff03e8d91ad7b4b33b92e23ccdee3f,https://github.com/liferay/liferay-portal/commit/2442863bcdff03e8d91ad7b4b33b92e23ccdee3f,LPD-27027 Add singleton into module commerce-product-type-grouped-api  # breaking  ## What modules/apps/commerce/commerce-product-type-grouped-api/src/main/java/com/liferay/commerce/product/type/grouped/util/comparator/CPDefinitionGroupedEntryPriorityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-product-type-grouped-api/src/main/java/com/liferay/commerce/product/type/grouped/util/comparator/CPDefinitionGroupedEntryQuantityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,6616844165799681f918ac7b4ad262ad8df8750a,https://github.com/liferay/liferay-portal/commit/6616844165799681f918ac7b4ad262ad8df8750a,LPD-27027 Add singleton to module commerce-currency-api  # breaking  ## What modules/apps/commerce/commerce-currency-api/src/main/java/com/liferay/commerce/currency/util/comparator/CommerceCurrencyPriorityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,cd4390a07225a534d728fa0ce9666585a09562fc,https://github.com/liferay/liferay-portal/commit/cd4390a07225a534d728fa0ce9666585a09562fc,LPD-27027 Add singleton into module commerce-wish-list-api  # breaking  ## What modules/apps/commerce/commerce-wish-list-api/src/main/java/com/liferay/commerce/wish/list/util/comparator/CommerceWishListCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-wish-list-api/src/main/java/com/liferay/commerce/wish/list/util/comparator/CommerceWishListItemCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-wish-list-api/src/main/java/com/liferay/commerce/wish/list/util/comparator/CommerceWishListNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,cdf5671992038a1fa1ddcff417b0a6507b8e0818,https://github.com/liferay/liferay-portal/commit/cdf5671992038a1fa1ddcff417b0a6507b8e0818,LPD-27027 Add singleton to module commerce-discount-api  # breaking  ## What modules/apps/commerce/commerce-discount-api/src/main/java/com/liferay/commerce/discount/util/comparator/CommerceDiscountRelCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-discount-api/src/main/java/com/liferay/commerce/discount/util/comparator/CommerceDiscountCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-discount-api/src/main/java/com/liferay/commerce/discount/util/comparator/CommerceDiscountRuleCreateDateComparator.java  the class is now a singleton  ## Why  this change was made since the class was unused  ----  # breaking  ## What modules/apps/commerce/commerce-discount-api/src/main/java/com/liferay/commerce/discount/util/comparator/CommerceDiscountCommerceAccountGroupRelCreateDateComparator.java  the class is now a singleton  ## Why  this change was made since the class was unused  ----
liferay,liferay-portal,44ee476c43d68a0e3b4d78ece3054a22f17a813a,https://github.com/liferay/liferay-portal/commit/44ee476c43d68a0e3b4d78ece3054a22f17a813a,LPD-27027 Add singleton to module commerce-tax-engine-fixed-api  # breaking  ## What modules/apps/commerce/commerce-tax-engine-fixed-api/src/main/java/com/liferay/commerce/tax/engine/fixed/util/comparator/CommerceTaxFixedRateAddressRelCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-tax-engine-fixed-api/src/main/java/com/liferay/commerce/tax/engine/fixed/util/comparator/CPTaxCategoryNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-tax-engine-fixed-api/src/main/java/com/liferay/commerce/tax/engine/fixed/util/comparator/CommerceTaxFixedRateCreateDateComparator.java  the class is now a singleton  ## Why  this change was made since the class was unused  ----
liferay,liferay-portal,9f83b40c25adcd9550e5f5aed1d357a3ce9941dd,https://github.com/liferay/liferay-portal/commit/9f83b40c25adcd9550e5f5aed1d357a3ce9941dd,LPD-26917 Add singleton to module dynamic-data-mapping-api  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/DataProviderInstanceModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/DataProviderInstanceNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/DDMFormInstanceNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/DDMFormInstanceRecordIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/StructureCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/StructureIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/StructureLayoutCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/StructureLayoutNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/StructureLinkStructureModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/TemplateIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,e1b76ca20fbfeaee8096f4185b11dccb9ad9d846,https://github.com/liferay/liferay-portal/commit/e1b76ca20fbfeaee8096f4185b11dccb9ad9d846,LPD-26917 Add singleton to module dynamic-data-lists-api  # breaking  ## What modules/apps/dynamic-data-lists/dynamic-data-lists-api/src/main/java/com/liferay/dynamic/data/lists/util/comparator/DDLRecordIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-lists/dynamic-data-lists-api/src/main/java/com/liferay/dynamic/data/lists/util/comparator/DDLRecordSetCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-lists/dynamic-data-lists-api/src/main/java/com/liferay/dynamic/data/lists/util/comparator/DDLRecordCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-lists/dynamic-data-lists-api/src/main/java/com/liferay/dynamic/data/lists/util/comparator/DDLRecordSetNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,ccf4dc703cc3527c2b26d21b29c686f09054578d,https://github.com/liferay/liferay-portal/commit/ccf4dc703cc3527c2b26d21b29c686f09054578d,LPD-26917 Add singleton to module data-engine-api  # breaking  ## What modules/apps/data-engine/data-engine-api/src/main/java/com/liferay/data/engine/util/comparator/DEDataListViewCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/data-engine/data-engine-api/src/main/java/com/liferay/data/engine/util/comparator/DEDataListViewNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,1b766ab9f1cd2a3738af0100a3918794738d6431,https://github.com/liferay/liferay-portal/commit/1b766ab9f1cd2a3738af0100a3918794738d6431,LPD-26917 Add singleton to portal-workflow-metrics-api module  # breaking  ## What modules/dxp/apps/portal-workflow/portal-workflow-metrics-api/src/main/java/com/liferay/portal/workflow/metrics/util/comparator/WorkflowMetricsSLADefinitionVersionIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,3bf860c27150cf5ef3630c456383e14fe3c1b48e,https://github.com/liferay/liferay-portal/commit/3bf860c27150cf5ef3630c456383e14fe3c1b48e,LPD-26917 Add singleton to portal-workflow-kaleo-api module  # breaking  ## What modules/apps/portal-workflow/portal-workflow-kaleo-api/src/main/java/com/liferay/portal/workflow/kaleo/util/comparator/KaleoDefinitionVersionModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/portal-workflow/portal-workflow-kaleo-api/src/main/java/com/liferay/portal/workflow/kaleo/util/comparator/KaleoDefinitionVersionActiveComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/portal-workflow/portal-workflow-kaleo-api/src/main/java/com/liferay/portal/workflow/kaleo/util/comparator/KaleoDefinitionVersionIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/portal-workflow/portal-workflow-kaleo-api/src/main/java/com/liferay/portal/workflow/kaleo/util/comparator/KaleoDefinitionVersionNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/portal-workflow/portal-workflow-kaleo-api/src/main/java/com/liferay/portal/workflow/kaleo/util/comparator/KaleoDefinitionVersionVersionComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----
liferay,liferay-portal,3da7367995a82314176b8705fd12d062cbcf62d9,https://github.com/liferay/liferay-portal/commit/3da7367995a82314176b8705fd12d062cbcf62d9,LPD-26703 Apply singleton into module layout-page-template-api  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionLayoutPageTemplateEntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionLayoutPageTemplateEntryModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionLayoutPageTemplateEntryNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateEntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateEntryModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateEntryNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,a209e133ce1979d483515097bd00f7c9b0cc5005,https://github.com/liferay/liferay-portal/commit/a209e133ce1979d483515097bd00f7c9b0cc5005,LPD-26703 Add singleton into module layout-api  # breaking  ## What modules/apps/layout/layout-api/src/main/java/com/liferay/layout/util/comparator/LayoutClassedModelUsageModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-api/src/main/java/com/liferay/layout/util/comparator/LayoutCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-api/src/main/java/com/liferay/layout/util/comparator/LayoutModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-api/src/main/java/com/liferay/layout/util/comparator/LayoutRelevanceComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,2458e70fe2d5b9e5d18d8a9c94234c55afe3a13d,https://github.com/liferay/liferay-portal/commit/2458e70fe2d5b9e5d18d8a9c94234c55afe3a13d,LPD-26703 Add singleton into module message-boards-api  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MessageCreateDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MessageModifiedDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MessageSubjectComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MessageURLSubjectComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/ThreadCreateDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/CategoryModifiedDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/CategoryTitleComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MBObjectsComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/ThreadModifiedDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/ThreadTitleComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MBObjectsModifiedDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MBObjectsTitleComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/ThreadLastPostDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----
liferay,liferay-portal,098dbee4f21f1961cdb9a00d680820a3a55f5fb1,https://github.com/liferay/liferay-portal/commit/098dbee4f21f1961cdb9a00d680820a3a55f5fb1,LPD-26438 Add singleton to module journal-api  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleDisplayDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleIDComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleReviewDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleTitleComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleVersionComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleResourcePKComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FeedIDComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FeedNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderArticleArticleIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderArticleCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderArticleDisplayDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderArticleModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderArticleTitleComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,c9e093cee980690947fdc1fbbd47afbeb3537662,https://github.com/liferay/liferay-portal/commit/c9e093cee980690947fdc1fbbd47afbeb3537662,LPD-30941 Improve performance
liferay,liferay-portal,c7b603d6234c6416bef9e507bcf8d3f1e7771287,https://github.com/liferay/liferay-portal/commit/c7b603d6234c6416bef9e507bcf8d3f1e7771287,LPD-31137 Improve performance by avoiding the need to parse non java files
liferay,liferay-portal,cdb615ac7e9d636ccdb4979aae7d0028d8bb7e2f,https://github.com/liferay/liferay-portal/commit/cdb615ac7e9d636ccdb4979aae7d0028d8bb7e2f,LPD-31141 Add the groupId improves current performance with millions of files
liferay,liferay-portal,b74c4e909c1a4af89d605053d78d17085dda35c0,https://github.com/liferay/liferay-portal/commit/b74c4e909c1a4af89d605053d78d17085dda35c0,LPD-25552 Make use all the fetch for company type will be directed to fetchCompanyPortalPreferences. This is going to make sure all the fetches for company type will return consistent result when there are multiple results  meanwhile this can improve performance see LPS-196350(2cd9801d2a243ecbc5c1025b614c9300ce53627d)
liferay,liferay-portal,315aa6bd1c09bdbafb6204eccf36c1229f65d488,https://github.com/liferay/liferay-portal/commit/315aa6bd1c09bdbafb6204eccf36c1229f65d488,LPD-2612 Users who have permission to edit Documents  Blogs and Web Contents can access Content Performance
liferay,liferay-portal,96b1d9e43e16f453f54da0114bf6731d822b4c3d,https://github.com/liferay/liferay-portal/commit/96b1d9e43e16f453f54da0114bf6731d822b4c3d,LPD-29508 Replace getLayoutsCount with fetchFirstLayout to improve performance while querying database
liferay,liferay-portal,e5602bda1abbb17eae4930977e0a7eabb4b25280,https://github.com/liferay/liferay-portal/commit/e5602bda1abbb17eae4930977e0a7eabb4b25280,Revert "LPD-2612 Users who have permission to edit Documents  Blogs and Web Contents can access Content Performance"  This reverts commit b34197bee1f56858d2809b5601fb5293af8da6ac.
liferay,liferay-portal,b34197bee1f56858d2809b5601fb5293af8da6ac,https://github.com/liferay/liferay-portal/commit/b34197bee1f56858d2809b5601fb5293af8da6ac,LPD-2612 Users who have permission to edit Documents  Blogs and Web Contents can access Content Performance
hapifhir,hapi-fhir,c97567c1672d29cfcf5516f8785e0dfc5a82d317,https://github.com/hapifhir/hapi-fhir/commit/c97567c1672d29cfcf5516f8785e0dfc5a82d317,Transaction processing improvements (#6874)  * Add junk ine  * Add extension for placeholder ID  * Cleanup  * Header  * Cleanup  * Spotless  * Better logging  * Fix transaction conditional URL prefetch on non-token params  * Drop a check  * Adjust query counts  * Improve performance  * Single token optimized  * Test fixes  * About to try to ensure we prefetch resource bodies for conditional create  * Test fixes  * Some test fixes  * Test fixes  * Test fixes  * Test fixes  * Work on tests  * Test fix  * Spotless  * Add docs  * Add test  * Add grouping  * Spotless  * Add size option to CMD  * Adjust consumer  * Cleanup  * Work on bulk import  * Spotless  * More work  * Add logging  * More diagnostics  * add logs  * Cleanup  * Fix premature finishing of bulk import command  * Add changelogs  * Test cleanup  * Test fixes  * Fix tests  * Test fix  * Spotless  * Cleanup  * Resolve conflicts  * Clean up  * Add to transaction response parser  * TransactionUtil parsing improvements  * Work on tests  * Test fix  * Test fixes  * Spotless  * Bug fix  * Build tweak  * Test cleanup  * Add some test logging  * Bump to trigger CI  * Try to address intermittent  * Address review comments  * Version bump  * License header
hapifhir,hapi-fhir,38b67cdc00527c632635bc0801b975c55d815d7e,https://github.com/hapifhir/hapi-fhir/commit/38b67cdc00527c632635bc0801b975c55d815d7e,Performance tweak to Xml serialization (#6888)  * Performance tweak to Xml serialization  * Add changelog  * Address review comment  * Spotless  * Changelog fix
hapifhir,hapi-fhir,210bb82618f3db24e0b441684ea2e290d6c877d0,https://github.com/hapifhir/hapi-fhir/commit/210bb82618f3db24e0b441684ea2e290d6c877d0,Clean up Bulk Import (#6840)  * Fix transaction conditional URL prefetch on non-token params  * Drop a check  * Adjust query counts  * Improve performance  * Single token optimized  * Test fixes  * About to try to ensure we prefetch resource bodies for conditional create  * Test fixes  * Some test fixes  * Test fixes  * Test fixes  * Test fixes  * Work on tests  * Test fix  * Spotless  * Add docs  * Add test  * Add grouping  * Spotless  * Add size option to CMD  * Adjust consumer  * Cleanup  * Work on bulk import  * Spotless  * More work  * Add logging  * More diagnostics  * add logs  * Cleanup  * Fix premature finishing of bulk import command  * Add changelogs  * Test cleanup  * Test fixes  * Fix tests  * Test fix  * Spotless  * Account for review comments  * Address review comments  * Address review comments  * Add license headers  * Cleanup  * Restore CLI logging  * Test fix  * Add default method
hapifhir,hapi-fhir,eccedb53091cf3df82342fbe241407e4eb3201f6,https://github.com/hapifhir/hapi-fhir/commit/eccedb53091cf3df82342fbe241407e4eb3201f6,Fix transaction conditional URL prefetch on non-token params (#6818)  * Fix transaction conditional URL prefetch on non-token params  * Drop a check  * Adjust query counts  * Improve performance  * Single token optimized  * Test fixes  * About to try to ensure we prefetch resource bodies for conditional create  * Test fixes  * Some test fixes  * Test fixes  * Test fixes  * Test fixes  * Work on tests  * Test fix  * Spotless  * Add docs  * Address review comments
hapifhir,hapi-fhir,43ce89cda614a682200e5c5c48b39b7dbf717385,https://github.com/hapifhir/hapi-fhir/commit/43ce89cda614a682200e5c5c48b39b7dbf717385,Introduce hapi-fhir-client-apache-http5 module for Apache HttpClient 5 support (#6520)  * feat: Introduce hapi-fhir-client-apache-http5 module for Apache HttpClient 5 support  - Added a new module `hapi-fhir-client-apache-http5` to provide HAPI FHIR Client functionality using Apache HttpClient 5. - Supports gradual migration from HttpClient 4 to HttpClient 5. - Aligns with Spring Boot 3.0's adoption of HttpClient 5  enabling consistent HTTP client configuration for users of both libraries.  Key Changes: - Integrated Apache HttpClient 5 for modern  high-performance HTTP requests. - Ensured compatibility with existing `hapi-fhir-client` and `hapi-fhir-client-okhttp` modules. - Added basic tests to validate functionality and coexistence of HttpClient 4 and 5.  Impact: - Non-breaking change; the new module can be adopted independently. - Facilitates eventual migration of HAPI FHIR to HttpClient 5 across the codebase."  * Add new error codes to the apache-httpclient5 client module.
hapifhir,hapi-fhir,eadd8c6f0d3eafb46da58f7ce9ed8411b707434f,https://github.com/hapifhir/hapi-fhir/commit/eadd8c6f0d3eafb46da58f7ce9ed8411b707434f,improved performance of date searching (#6353)
hapifhir,hapi-fhir,9a73079c33133e50dc4fd4f77cd5207ab95bc2c1,https://github.com/hapifhir/hapi-fhir/commit/9a73079c33133e50dc4fd4f77cd5207ab95bc2c1,Improving performance  using caching when testing for primitives (#6252) (#6253)  * Improving performance  using caching when testing for primitives (#6252)  Caching primitive type names for faster lookup if a type is primitive.  * Credit for #6253  ---------  Co-authored-by: James Agnew <jamesagnew@gmail.com>
hapifhir,hapi-fhir,d2923da62bc7b414bcdcb36a882245994fafa6bb,https://github.com/hapifhir/hapi-fhir/commit/d2923da62bc7b414bcdcb36a882245994fafa6bb,Search Param Matcher Performance Improvement (#5999)  * filter function added  * spotless
Netflix,EVCache,74cceb812a1570f8c8648d90a1c5a7425c1cf200,https://github.com/Netflix/EVCache/commit/74cceb812a1570f8c8648d90a1c5a7425c1cf200,Merge pull request #162 from jasonk000/jkoch/get-decompress-async  perf: allow concurrent decompress away from network loop
Netflix,EVCache,60e2a773198e75e2605eeda61a0a9944386ec316,https://github.com/Netflix/EVCache/commit/60e2a773198e75e2605eeda61a0a9944386ec316,perf: allow concurrent decompress away from network loop  We want to ensure the transcode passes everything that is expected to be run asynchronously over to the decode loop. In general  memcached calls us back with gotData  then receivedStatus  then complete. We use gotData to launch the work onto the transcode threadpool and return control to memcached  which would immediately then call receivedStatus.  Previously  receivedStatus and complete were set up to interact and set a value on the underlying future but only by synchronously blocking for the transcode future. Given this callback is happening nearly immediately after the gotData callback  we were firing the transcode and nearly always performing a blocking get  which triggers a synchronous decompression on the network IO loop. This is of course very detrimental to evcache driver performance  since the driver cannot even accept new requests to issue to them to memcached backends  and must wait until decompression completes.  In this fix  we rearrange things a little to ensure that if the async decode is requested  that we push the completion status updates to happen only after the async decode completes. This is a little ugly because of the current arrangement of the memcached decoder. A future change might be to overhaul this integration and pull it out of the memcached transcode framework and use something a bit more friendly.  Also  add a property to control sync decode threading behavior  default to using the existing behaviors for now.
Netflix,EVCache,0cb4ea98c96011f3c646430807180150bfdf9045,https://github.com/Netflix/EVCache/commit/0cb4ea98c96011f3c646430807180150bfdf9045,Merge pull request #161 from jasonk000/jkoch/track-bulk-get-futures  perf: track bulk get operation completions explicitly
Netflix,EVCache,4cb82c272b3e557c5e4bb4bcc98cb6e66420cb4e,https://github.com/Netflix/EVCache/commit/4cb82c272b3e557c5e4bb4bcc98cb6e66420cb4e,Merge pull request #157 from jasonk000/jkoch/fast-key-validator  perf: Avoid charset lookup in key validation
Netflix,EVCache,962b76b867bde2e088ee758655d1a345f381ab29,https://github.com/Netflix/EVCache/commit/962b76b867bde2e088ee758655d1a345f381ab29,Merge pull request #158 from jasonk000/jkoch/add-operations-single-wakeup  perf: batch selector wakeup call when adding bulk requests
Netflix,EVCache,be626edd3167ed0d84d099d0e0053b7cf6b0ac5a,https://github.com/Netflix/EVCache/commit/be626edd3167ed0d84d099d0e0053b7cf6b0ac5a,Merge pull request #159 from jasonk000/jkoch/bulk-lookup-single-getprimary  perf: perform the getPrimary lookup only once during getBulk
Netflix,EVCache,5dd9ba42353b666ff85c5c02056b5a03b5822a57,https://github.com/Netflix/EVCache/commit/5dd9ba42353b666ff85c5c02056b5a03b5822a57,Merge pull request #156 from jasonk000/jkoch/less-locking-1  perf: remove unnecessary blocking in EVCacheOperationFuture
Netflix,EVCache,e4a2a9bdb37c59bc662c357bd6a323d2718e092c,https://github.com/Netflix/EVCache/commit/e4a2a9bdb37c59bc662c357bd6a323d2718e092c,perf: track bulk get operation completions explicitly  This commit makes it so that the operation callbacks are used to collect all the locked state that we might need to access  which makes the code after latch-release run without contending memcached to complete.  Before this commit  callbacks on the bulk get operation accumulate the state and when all operations have completed  we release the latch and then the calller calculates the final state. Unfortunately looking at the state  the caller must take operation locks  which are potentially still hold by memcache driver since it has only momentarily before released the latch.  The Memcache callbacks are made whilst holding the lock on the Operation objects  which means they are a great time to get locked state such as cancellation state. This patch makes the change to collect state during callbacks. The callback future interaction is a little clunky  but this avoids a major rewrite of the implementations.
Netflix,EVCache,6a63f2912d959452dab9c8114abd275f84c6843a,https://github.com/Netflix/EVCache/commit/6a63f2912d959452dab9c8114abd275f84c6843a,perf: Optimize the MD5 hashring algorithm to avoid Charset lookup
Netflix,EVCache,d29bac959ab946eaa998f1c161e0f290db67247d,https://github.com/Netflix/EVCache/commit/d29bac959ab946eaa998f1c161e0f290db67247d,perf: Add a BFS-arranged (aka Eytzinger) layout for node lookup
Netflix,EVCache,de9041dac0856ab89332f012b04737f017de743e,https://github.com/Netflix/EVCache/commit/de9041dac0856ab89332f012b04737f017de743e,perf: perform the getPrimary lookup only once during the bulk processing path  We can avoid doing the (potentially) expensive getPrimary call on the bulk get path with a little refactor. This retains the same functionality but pushes the validation logic and error emit into a callback  which allows metrics to work as-is.
Netflix,EVCache,fe3f5359f7d4012a4b9c1de8a08fdc9fa25e223b,https://github.com/Netflix/EVCache/commit/fe3f5359f7d4012a4b9c1de8a08fdc9fa25e223b,perf: batch selector wakeup call when adding bulk requests
Netflix,EVCache,168aa2b26b7d31f6990d45e22d99e364195722f0,https://github.com/Netflix/EVCache/commit/168aa2b26b7d31f6990d45e22d99e364195722f0,perf: Avoid charset lookup in key validation  The upstream implementation needs to do a "UTF-8" string lookup to Charset for every key validation. We can avoid this completely in our hot path by using a reference to StandardCharsets.
Netflix,EVCache,ca70c1d1458c8fed527830b7b13fbbfcd91c940a,https://github.com/Netflix/EVCache/commit/ca70c1d1458c8fed527830b7b13fbbfcd91c940a,perf: remove unnecessary blocking in EVCacheOperationFuture  Currently the EVCacheOperationFuture does an isCancelled check before emitting a metric  however this metric is now dead code. But  the isCancelled check is still a synchronized method which results in occasional blocking against memcache event loop.
Netflix,EVCache,9ddd6a9141689272f0ddb5ec181721c37008f99a,https://github.com/Netflix/EVCache/commit/9ddd6a9141689272f0ddb5ec181721c37008f99a,Merge pull request #151 from Netflix/jkoch/skip-cancel  perf: skip future cancellation when not needed
Netflix,EVCache,b200a5134ab546ce291edb9b96cc0df567e2c329,https://github.com/Netflix/EVCache/commit/b200a5134ab546ce291edb9b96cc0df567e2c329,perf: skip future cancellation when not needed  it implicitly throws an expensive exception
mybatis-flex,mybatis-flex,475c90c13b6edd8a777c41cefb1bcb176b06bd19,https://github.com/mybatis-flex/mybatis-flex/commit/475c90c13b6edd8a777c41cefb1bcb176b06bd19,refactor: optimize LambdaUtil performance
flyingsaucerproject,flyingsaucer,20e3f8757e5806474011d2fdf91d0476fd3f3045,https://github.com/flyingsaucerproject/flyingsaucer/commit/20e3f8757e5806474011d2fdf91d0476fd3f3045,#340 fix warning "single character strings being used as an argument in indexOf()calls"  A quick-fix is suggested to replace such string literals with equivalent character literals  gaining some performance enhancement.
flyingsaucerproject,flyingsaucer,2c4f0bd82dca0f2c1fb4a99a4f1bcba9a88f3dff,https://github.com/flyingsaucerproject/flyingsaucer/commit/2c4f0bd82dca0f2c1fb4a99a4f1bcba9a88f3dff,#340 cache attribute values of xml elements  My profiler shows that `nsh.getClass((Element) e)` consumes quite a remarkable percentage of total CPU time. So I hope caching these value could improve performance (though  my profiler doesn't show it :) ).
magefree,mage,99ca1e60296dcdbf44623c66dcad952939322db4,https://github.com/magefree/mage/commit/99ca1e60296dcdbf44623c66dcad952939322db4,AI: improved performance and server stability in games with "choose name" effects (related to #11285)
magefree,mage,f17cbbe72b7fff0f9ec3bb0a90698d7cda3d5e28,https://github.com/magefree/mage/commit/f17cbbe72b7fff0f9ec3bb0a90698d7cda3d5e28,AI: improved performance and fixed crashes on use cases with too much target options like "deals 5 damage divided as you choose" (related to #11285): * added DebugUtil.AI_ENABLE_DEBUG_MODE for better IDE's debugging AI code; * it's a target amount optimizations; * it's use a grouping of possible targets due same static and dynamic stats (name  abilities  rules  damage  etc); * instead of going through all possible combinations  AI uses only meaningful targets from particular groups;
magefree,mage,7d229e511cfcd76fb5f2938bc5e72ac28abd8c74,https://github.com/magefree/mage/commit/7d229e511cfcd76fb5f2938bc5e72ac28abd8c74,tests: added AI performance tests to reproduce bad use cases with too much possible targets
magefree,mage,b62ac065c1ed862cfcacf79568c04517c5458539,https://github.com/magefree/mage/commit/b62ac065c1ed862cfcacf79568c04517c5458539,AI: improved performance in tournament games (now computer will play AI vs AI games at the same time);
magefree,mage,83823acec78501e907dc2f5e0a38d9d18914a832,https://github.com/magefree/mage/commit/83823acec78501e907dc2f5e0a38d9d18914a832,GUI  performance: fixed memory/resources leaks on some components rendering
magefree,mage,1f3fad65944f76c81f31cd7dc21381313f35a623,https://github.com/magefree/mage/commit/1f3fad65944f76c81f31cd7dc21381313f35a623,GUI  preferences: reworked size settings: - added size settings for player's panel size (closes #12455  closes #12451  closes #5605); - size settings can be edit by slider or by text edit; - size settings for fonts has preview button with real text sample; - improved some tabs and hints for better UX; - improved GUI rendering performance;
locationtech,jts,ac7a165592ee7c31037e666e50b765f861163aff,https://github.com/locationtech/jts/commit/ac7a165592ee7c31037e666e50b765f861163aff,Improve `LineStringSnapper` performance by using squared distance (#1111)
apache,tinkerpop,55f7e180c9284d29c2b136667aea7e0298f16ff1,https://github.com/apache/tinkerpop/commit/55f7e180c9284d29c2b136667aea7e0298f16ff1,Re-enable GraphSON message serializer for debugging. (#2836)  The GraphSONv4 MessageSerializer doesn't support chunking so the HttpObjectAggregator is added back and the HttpGremlinResponseStreamDecoder is replaced with the original  non-streaming version. This is mainly for debugging and testing purposes so lowered performance is expected.
apache,poi,719e7154a19c00e1c2464e5f93a1567d8ca5ed72,https://github.com/apache/poi/commit/719e7154a19c00e1c2464e5f93a1567d8ca5ed72,Optimize generating numbers for bullets in Word  Using char[] instead of String improves performance of this operation considerably  especially in JDK 11+ where StringBuilder was switched to work on bytes instead of chars.  This is likely only relevant for very large documents  it was visible in a synthetic test-file from fuzzing.  git-svn-id: https://svn.apache.org/repos/asf/poi/trunk@1919239 13f79535-47bb-0310-9956-ffa450edef68
winder,Universal-G-Code-Sender,e47ebaa46e079c9d78026d92f27c0c671c61ba3c,https://github.com/winder/Universal-G-Code-Sender/commit/e47ebaa46e079c9d78026d92f27c0c671c61ba3c,Visualizer performance (#2615)  * Moved the isEnabled to super class and cache it to improve performance * Made grid and plane use a vertex buffer object renderer with shaders to improve performance * Added actions for toggling visualizer features
winder,Universal-G-Code-Sender,8a6343209a2ca8b791f95d68b1a9bd3537b38d52,https://github.com/winder/Universal-G-Code-Sender/commit/8a6343209a2ca8b791f95d68b1a9bd3537b38d52,Added a new visualizer panel for rendering using NEWT for better performance (#2602)
loks666,get_jobs,f8e083c81d9e26de1e47eb8a0f21082f76d08316,https://github.com/loks666/get_jobs/commit/f8e083c81d9e26de1e47eb8a0f21082f76d08316,perf: 加入岗位详情的关键词匹配（关键词匹配实现较粗糙，需要进一步优化）
loks666,get_jobs,c8f2feddd9f8b46a99b20879d3af483ed5291dd7,https://github.com/loks666/get_jobs/commit/c8f2feddd9f8b46a99b20879d3af483ed5291dd7,perf: 更为激进的投递间隔 perf: 更改投递时间
loks666,get_jobs,f63f03dfe60465617e541f028047b58d9decc5da,https://github.com/loks666/get_jobs/commit/f63f03dfe60465617e541f028047b58d9decc5da,perf: 修改默认配置 perf: 缩短投递间隔
loks666,get_jobs,f88cd66ca847fb43790a15c72ae121b866d60ed7,https://github.com/loks666/get_jobs/commit/f88cd66ca847fb43790a15c72ae121b866d60ed7,feat: 支持多条打招呼语句 perf: 修改部分方法调用，让代码更加易读
flutter,flutter-intellij,da70cfa6a6bfeedef5ed24845f1664b3e82e0121,https://github.com/flutter/flutter-intellij/commit/da70cfa6a6bfeedef5ed24845f1664b3e82e0121,[PE] fix read context EDT juggling in `FlutterInitializer` (#8216)  The move to project activities has some nuance. In the long run we'll be able to do some real performance tuning but in the short-run some of our assumptions about being in a read context need to be rethunk. Here's one.  The rub is `autoCreateRunConfig` needs to be in a read context but the remaining initialization needs to be on the EDT.  Fixes #8215  See also: https://github.com/flutter/flutter-intellij/issues/8100   ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,95592638ba5f96d8338f92fc8308eab2bf0e1d9e,https://github.com/flutter/flutter-intellij/commit/95592638ba5f96d8338f92fc8308eab2bf0e1d9e,[CQ] migrate `FlutterStudioStartupActivity` to a project activity (#8211)  The model for this change was set with https://github.com/flutter/flutter-intellij/pull/8200 and ties up our migration off of the deprecated `StartupActivity` in favor of project activities  leveraging IntelliJ's embrace of Kotlin coroutines for improved startup performance.  Fixes: https://github.com/flutter/flutter-intellij/issues/8100    ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,9102f9c3d724701732e2affcbe0f37b812a659e6,https://github.com/flutter/flutter-intellij/commit/9102f9c3d724701732e2affcbe0f37b812a659e6,[CQ] `VMServiceManager` remove legacy heap/frame monitoring (#8174)  Performance monitoring is now in devtools so we can stop heap/frame monitoring (which has been unused since the legacy inspector view went away).  _There are more threads to pull on here (for example the `DisplayRefreshRateManager` may be able to go away too but we need to be sure it doesn't have side-effects that are depended on elsewhere) but this is a good place to start._  ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,db2f786a7e3a977c45c8eca3593873ed423765fd,https://github.com/flutter/flutter-intellij/commit/db2f786a7e3a977c45c8eca3593873ed423765fd,Remove the Flutter Performance window and associated functionality (#7856)  https://github.com/flutter/flutter-intellij/issues/7817
flutter,flutter-intellij,8ea8af00ea8baa074368f55f667918866ad47b27,https://github.com/flutter/flutter-intellij/commit/8ea8af00ea8baa074368f55f667918866ad47b27,Use correct SDK version check for gating performance page (#7639)  Fixes error from https://github.com/flutter/flutter-intellij/pull/7637#pullrequestreview-2286867489  CC @kenzieschmoll
flutter,flutter-intellij,d9f3db3f468c944aed287e055eb269538eb11e31,https://github.com/flutter/flutter-intellij/commit/d9f3db3f468c944aed287e055eb269538eb11e31,Remove performance view for newer Flutter SDK versions (#7637)  Fixes https://github.com/flutter/flutter-intellij/issues/7624
flutter,flutter-intellij,193b5bc2c01cc30ca6b90b3314335d97a34b5e21,https://github.com/flutter/flutter-intellij/commit/193b5bc2c01cc30ca6b90b3314335d97a34b5e21,Make performance tab warning text red (#7581)  Follow up to https://github.com/flutter/flutter-intellij/pull/7577  <img width="581" alt="Screenshot 2024-07-29 at 10 45 47 AM" src="https://github.com/user-attachments/assets/c3316e53-c41a-4866-9592-56731d6bc1f8">
flutter,flutter-intellij,201e5087ec652f8db37c0abf3e07112b3d197313,https://github.com/flutter/flutter-intellij/commit/201e5087ec652f8db37c0abf3e07112b3d197313,Add warning message for removing performance panel (#7577)  <img width="511" alt="Screenshot 2024-07-26 at 1 17 17 PM" src="https://github.com/user-attachments/assets/73d6745b-0ada-4def-b5b0-d62c67d52875">
metasfresh,metasfresh,a86f7399408fb066e258f5b9488558d796e95129,https://github.com/metasfresh/metasfresh/commit/a86f7399408fb066e258f5b9488558d796e95129,Export both desadv-lines within packs and lines that have no packs with OK performance (#20100)  Add a dedicated view to achieve it  because we can't use "case" or "coalesce" to compute a view's key-column
metasfresh,metasfresh,a9131677b0e082b9d678a036534eeafbb6c0288b,https://github.com/metasfresh/metasfresh/commit/a9131677b0e082b9d678a036534eeafbb6c0288b,Fix severe performance problem with C_BPartner_UpdateStats (#19947)  * Fix severe performance problem with C_BPartner_UpdateStats refs: https://github.com/metasfresh/metasfresh/issues/19946  * use parameterized statements refs: https://github.com/metasfresh/metasfresh/issues/19946  * use parameterized statements refs: https://github.com/metasfresh/metasfresh/issues/19946  * use parameterized statements refs: https://github.com/metasfresh/metasfresh/issues/19946  * use parameterized statements refs: https://github.com/metasfresh/metasfresh/issues/19946
metasfresh,metasfresh,cb6510d4ccb5a0b2d8c16bf4e76b3fc783ac5071,https://github.com/metasfresh/metasfresh/commit/cb6510d4ccb5a0b2d8c16bf4e76b3fc783ac5071,Improve performance by using attribute-DB-function instead of view (#19898)  This prevents the DB from doing a sequential scan on M_AttributeInstance
metasfresh,metasfresh,883b1b3cfd38820f5a3a175fbfded857de6777b9,https://github.com/metasfresh/metasfresh/commit/883b1b3cfd38820f5a3a175fbfded857de6777b9,Add sysconfig to ignore MaterialEvents instead of processing them  to increase performance (#19047)
metasfresh,metasfresh,e00076f489dd6b4e309ca6a0c3bfe901f70253e1,https://github.com/metasfresh/metasfresh/commit/e00076f489dd6b4e309ca6a0c3bfe901f70253e1,Merge pull request #18702 from metasfresh/mergify/copy/yoyo_uat/pr-16127  Improve ESR-Performance and fix QuerySelectionToDeleteHelper (copy #16127)
metasfresh,metasfresh,591e985771277a932faba69a77c3f13e9b63381b,https://github.com/metasfresh/metasfresh/commit/591e985771277a932faba69a77c3f13e9b63381b,Improve ESR-Performance and fix QuerySelectionToDeleteHelper (#16127)  * attempt to avoid deadlock  * QuerySelectionToDeleteHelper.scheduleDeleteSelection schedules only UUIDs when needed  * findExistentPaymentId() - if there are no similar payments then don't search for ESR_ImportLine  * * ESR performance improvements (#12946)  * * Access database less  https://github.com/metasfresh/metasfresh/issues/12945  * * get rid of ilike  https://github.com/metasfresh/metasfresh/issues/12945  * * esr referenceNo matcher  https://github.com/metasfresh/metasfresh/issues/12945  * * Save not needed because it's already saved in de.metas.payment.esr.api.impl.ESRImportBL.evaluateLine  https://github.com/metasfresh/metasfresh/issues/12945  * * index and column length improvement  https://github.com/metasfresh/metasfresh/issues/12945  * * Make Amount numeric  https://github.com/metasfresh/metasfresh/issues/12945  * * Avoid NPE  https://github.com/metasfresh/metasfresh/issues/12945  * * That validation was leading to the possible recreation of the lines from an import file  which makes no sense. Get rid of it. https://github.com/metasfresh/metasfresh/issues/12945  * * format https://github.com/metasfresh/metasfresh/issues/12945  * * rolled back since it's out of scope https://github.com/metasfresh/metasfresh/issues/12945  * * avoid npe https://github.com/metasfresh/metasfresh/issues/12945  * * this index (mind the parameters order) improves performance more https://github.com/metasfresh/metasfresh/issues/12945  (cherry picked from commit eb7049697d9e3a86cd2ff6cb7fbc8f654819b464)  solved Conflicts: backend/de.metas.payment.esr/src/main/java/de/metas/payment/esr/api/impl/ESRImportBL.java backend/de.metas.payment.esr/src/main/java/de/metas/payment/esr/api/impl/ESRImportDAO.java  * recreate the index in a better configuration  * minor improvements after static code review  ---------  Co-authored-by: Ruxandra Craciunescu <ruxandra.craciunescu@metasfresh.com> (cherry picked from commit 8a18b475c045f390935e6caa0b56c03fb1662bc9)
metasfresh,metasfresh,8d08a4e5d26e3a09bbda4ad27951fba47d6b449b,https://github.com/metasfresh/metasfresh/commit/8d08a4e5d26e3a09bbda4ad27951fba47d6b449b,Material Cockpit performance improvements (#18257)  * Material Cockpit performance improvements  ---------  Co-authored-by: Teodor Sarca <teo.sarca@metasfresh.com> Co-authored-by: mergify[bot] <37929162+mergify[bot]@users.noreply.github.com>
metasfresh,metasfresh,c5bbeb6cdb4488b0c754ada0cf2f9c3097df6f81,https://github.com/metasfresh/metasfresh/commit/c5bbeb6cdb4488b0c754ada0cf2f9c3097df6f81,Merge pull request #18226 from metasfresh/inner_silence_uat_performance_imp  CP - Improve ESR-Performance and fix QuerySelectionToDeleteHelper
metasfresh,metasfresh,24e169464d90193d4ac409c58b6f5c972758f73d,https://github.com/metasfresh/metasfresh/commit/24e169464d90193d4ac409c58b6f5c972758f73d,Improve ESR-Performance and fix QuerySelectionToDeleteHelper (#16127)  * attempt to avoid deadlock  * QuerySelectionToDeleteHelper.scheduleDeleteSelection schedules only UUIDs when needed  * findExistentPaymentId() - if there are no similar payments then don't search for ESR_ImportLine  * * ESR performance improvements (#12946)  * * Access database less  https://github.com/metasfresh/metasfresh/issues/12945  * * get rid of ilike  https://github.com/metasfresh/metasfresh/issues/12945  * * esr referenceNo matcher  https://github.com/metasfresh/metasfresh/issues/12945  * * Save not needed because it's already saved in de.metas.payment.esr.api.impl.ESRImportBL.evaluateLine  https://github.com/metasfresh/metasfresh/issues/12945  * * index and column length improvement  https://github.com/metasfresh/metasfresh/issues/12945  * * Make Amount numeric  https://github.com/metasfresh/metasfresh/issues/12945  * * Avoid NPE  https://github.com/metasfresh/metasfresh/issues/12945  * * That validation was leading to the possible recreation of the lines from an import file  which makes no sense. Get rid of it. https://github.com/metasfresh/metasfresh/issues/12945  * * format https://github.com/metasfresh/metasfresh/issues/12945  * * rolled back since it's out of scope https://github.com/metasfresh/metasfresh/issues/12945  * * avoid npe https://github.com/metasfresh/metasfresh/issues/12945  * * this index (mind the parameters order) improves performance more https://github.com/metasfresh/metasfresh/issues/12945  (cherry picked from commit eb7049697d9e3a86cd2ff6cb7fbc8f654819b464)  solved Conflicts: backend/de.metas.payment.esr/src/main/java/de/metas/payment/esr/api/impl/ESRImportBL.java backend/de.metas.payment.esr/src/main/java/de/metas/payment/esr/api/impl/ESRImportDAO.java  * recreate the index in a better configuration  * minor improvements after static code review  ---------  Co-authored-by: Ruxandra Craciunescu <ruxandra.craciunescu@metasfresh.com>  (cherry picked from commit 8a18b475c045f390935e6caa0b56c03fb1662bc9)
smithy-lang,smithy,d2460476a16b0e8d94bde7907eb9ae0cc844c347,https://github.com/smithy-lang/smithy/commit/d2460476a16b0e8d94bde7907eb9ae0cc844c347,Improve isVirtualHostableBucket function  This commit removes the use of three regular expressions and string splitting when checking if a value is a virtual hostable bucket. This should help with both startup and runtime performance. It also exposes a static method for performing this check that I'll use in smithy-java.
smithy-lang,smithy,d2139b7a0e339209afccb6cb333b4beecf10f9af,https://github.com/smithy-lang/smithy/commit/d2139b7a0e339209afccb6cb333b4beecf10f9af,Improve BottomUpIndex performance (#2367)  When doing some ad-hoc profiling on the language server using aws service models I noticed that BottomUpIndex was taking up a lot of CPU time. This was because it used selectors and PathFinder to determine the paths from resources/operations up to their enclosing service. Selectors are pretty slow in comparison to regular java code  and when the model is large with many services/operations  this index can take a while to compute. BottomUpIndex is used for validating `@cfnResource` (through CfnResourcePropertyValidator -> CfnResourceIndex)  so anyone using this trait is paying this performance cost (in particular  SDK code generators).  To get a rough idea of how much faster this commit makes BottomUpIndex  I ran model validation using the cli on all aws service models with and without this change a few times each: ``` smithy validate --quiet # 14 - 16 seconds  /path/to/smithy/smithy-cli/build/image/smithy-cli-darwin-aarch64/bin/smithy validate --quiet # 8 - 9 seconds ```
eclipse-jdtls,eclipse.jdt.ls,839831feefca6dbfdf75b6bea85145d76865cdf7,https://github.com/eclipse-jdtls/eclipse.jdt.ls/commit/839831feefca6dbfdf75b6bea85145d76865cdf7,Improvements to code action performance (#3322)  - React to cancellation requests while computing code actions - Avoid expensive calls for "Inline" & "Change Signature" refactorings - Ensure the active source file is updated when between multiple opened files  Signed-off-by: Snjezana Peco <snjezana.peco@redhat.com>
zalando,logbook,1392997d13d228fa3b207baca042c272bc2bfee5,https://github.com/zalando/logbook/commit/1392997d13d228fa3b207baca042c272bc2bfee5,Allow using precise floats in logs (#2005)  * Allow using precise floats in logs  * extract usePreciseFloats check into a separate method  * add a comment about performance penalty  * add strategy to handle different ways of handling floats  ...because no one likes boolean flags anymore ¯\_(ツ)_/¯  * leve only one level of wrappers (remove creators)  * update README
zalando,logbook,605db1ba988cf3836f9706eb4332c6c6973d4a4a,https://github.com/zalando/logbook/commit/605db1ba988cf3836f9706eb4332c6c6973d4a4a,Merge pull request #1839 from aukevanleeuwen/1838-fix-queryfilter-performance  Improve performance of query filters (especially on large bodies)
zalando,logbook,c2a2a2996a37e4a9a1a77992458e2a61cf3148f8,https://github.com/zalando/logbook/commit/c2a2a2996a37e4a9a1a77992458e2a61cf3148f8,Improve performance of query filters (especially on large bodies)  Fixes: #1838
MarquezProject,marquez,7d0b290b2c7bbb706776449e85774522450b950a,https://github.com/MarquezProject/marquez/commit/7d0b290b2c7bbb706776449e85774522450b950a,Optimize Column Lineage Query Performance (#2821)  * Optimize Column Lineage Query Performance  Signed-off-by: Vinh Nguyen <phuvinh97ag@gmail.com>  * Optimize Column Lineage Query Performance - Format query - replace select * with uuid  namespace_name  name  Signed-off-by: Vinh Nguyen <phuvinh97ag@gmail.com>  ---------  Signed-off-by: Vinh Nguyen <phuvinh97ag@gmail.com> Co-authored-by: Peter Hicks <phixMe@users.noreply.github.com>
openjdk,loom,d9b6e4b13200684b69a161e288b9883ff0d96bec,https://github.com/openjdk/loom/commit/d9b6e4b13200684b69a161e288b9883ff0d96bec,8352642: Set zipinfo-time=false when constructing zipfs FileSystem in com.sun.tools.javac.file.JavacFileManager$ArchiveContainer for better performance  Reviewed-by: liach  jpai  jlahoda  lancea
openjdk,loom,84458ec18ce33295636f7b26b8e3ff25ecb349f2,https://github.com/openjdk/loom/commit/84458ec18ce33295636f7b26b8e3ff25ecb349f2,8353013: java.net.URI.create(String) may have low performance to scan the host/domain name from URI string when the hostname starts with number  Reviewed-by: michaelm  xpeng
openjdk,loom,5481021ee64fd457279ea7083be0f977c7ce3e3c,https://github.com/openjdk/loom/commit/5481021ee64fd457279ea7083be0f977c7ce3e3c,8321591: (fs) Improve String -> Path conversion performance (win)  Reviewed-by: alanb
openjdk,loom,8b0602dbed2f7ced190ec81753defab8a4bc316d,https://github.com/openjdk/loom/commit/8b0602dbed2f7ced190ec81753defab8a4bc316d,8319447: Improve performance of delayed task handling  Reviewed-by: vklang  alanb
openjdk,loom,250ff86dc86f73dbf7c944d9b5a792c4bdfeef0d,https://github.com/openjdk/loom/commit/250ff86dc86f73dbf7c944d9b5a792c4bdfeef0d,8349000: Performance improvement for Currency.isPastCutoverDate(String)  Reviewed-by: naoto  aturbanov
openjdk,loom,9a60f4457bb56d0f5039a97e6b943e62a8a2c3ee,https://github.com/openjdk/loom/commit/9a60f4457bb56d0f5039a97e6b943e62a8a2c3ee,8345668: ZoneOffset.ofTotalSeconds performance regression  Reviewed-by: rriggs  aturbanov
openjdk,loom,06c44dd568d91e1bd68f60fd3e57abcbe97e5dca,https://github.com/openjdk/loom/commit/06c44dd568d91e1bd68f60fd3e57abcbe97e5dca,8345465: Fix performance regression on x64 after JDK-8345120  Reviewed-by: mcimadamore
openjdk,loom,5958463cadb04560ec85d9af972255bfe6dcc2f2,https://github.com/openjdk/loom/commit/5958463cadb04560ec85d9af972255bfe6dcc2f2,8343377: Performance regression in reflective invocation of native methods  Reviewed-by: mchung
openjdk,loom,d49f21043b84ebcc8b9176de3a84621ca7bca8fb,https://github.com/openjdk/loom/commit/d49f21043b84ebcc8b9176de3a84621ca7bca8fb,8342040: Further improve entry lookup performance for multi-release JARs  Co-authored-by: Claes Redestad <redestad@openjdk.org> Reviewed-by: redestad
openjdk,loom,81ff91ef27a6a856ae2c453a9a9b8333b91da3ab,https://github.com/openjdk/loom/commit/81ff91ef27a6a856ae2c453a9a9b8333b91da3ab,8339531: Improve performance of MemorySegment::mismatch  Reviewed-by: mcimadamore
openjdk,loom,6be927260a84b1d7542167e526ff41f7dc26cab0,https://github.com/openjdk/loom/commit/6be927260a84b1d7542167e526ff41f7dc26cab0,8338591: Improve performance of MemorySegment::copy  Reviewed-by: mcimadamore
openjdk,loom,7a418fc07464fe359a0b45b6d797c65c573770cb,https://github.com/openjdk/loom/commit/7a418fc07464fe359a0b45b6d797c65c573770cb,8338967: Improve performance for MemorySegment::fill  Reviewed-by: mcimadamore  psandoz
openjdk,loom,ab8071d28027ecbf5e8984c30b35fa1c2d934de7,https://github.com/openjdk/loom/commit/ab8071d28027ecbf5e8984c30b35fa1c2d934de7,8338146: Improve Exchanger performance with VirtualThreads  Reviewed-by: alanb
openjdk,loom,75bea280b9adb6dac9fefafbb3f4b212f100fbb5,https://github.com/openjdk/loom/commit/75bea280b9adb6dac9fefafbb3f4b212f100fbb5,8333867: SHA3 performance can be improved  Reviewed-by: kvn  valeriep
openjdk,loom,a941397327972f130e683167a1b429f17603df46,https://github.com/openjdk/loom/commit/a941397327972f130e683167a1b429f17603df46,8329031: CPUID feature detection for Advanced Performance Extensions (Intel® APX)  Reviewed-by: sviswanathan  kvn
openjdk,loom,d826127970bd2ae8bf4cacc3c55634dc5af307c4,https://github.com/openjdk/loom/commit/d826127970bd2ae8bf4cacc3c55634dc5af307c4,8333462: Performance regression of new DecimalFormat() when compare to jdk11  Reviewed-by: liach  naoto  jlu
zfoo-project,zfoo,452723508ba993eeb6aae6b3c8f0f244a3527540,https://github.com/zfoo-project/zfoo/commit/452723508ba993eeb6aae6b3c8f0f244a3527540,perf[executor]: use & bits operation to improve performance
selenide,selenide,29487c8db51d7c7000e09a20f8a431aa5142e268,https://github.com/selenide/selenide/commit/29487c8db51d7c7000e09a20f8a431aa5142e268,Cached size to avoid repeated calls in loop condition — improves performance and code clarity.
