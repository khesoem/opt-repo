owner,repo,commit_hash,commit_url,message
spring-projects,spring-boot,569519285046967a85f20cefe4200fcfc35a21c8,https://github.com/spring-projects/spring-boot/commit/569519285046967a85f20cefe4200fcfc35a21c8,Ensure descendants are always recalculated on cache refresh  Fix `SpringIterableConfigurationPropertySource` to ensure that cached descendants are always updated on a refresh.  The cache implementation assumes that it is safe to reuse previous `mappings` and `reverseMappings` data since it doesn't matter if superfluous values are included. For `descendants` however  we always want to recalculate values so that we don't get false positives.  Unfortunately  prior to this commit  we only updated the descendants if a reverseMapping was added. The meant that on a cache refresh  existing descendants were removed.  Fixes gh-45639
spring-projects,spring-boot,93113a415f1516b75a21822c4912e7946f8868ae,https://github.com/spring-projects/spring-boot/commit/93113a415f1516b75a21822c4912e7946f8868ae,Improve performance by not checking all indexed elements  Update `IndexedElementsBinder` so that bind operations are faster at the expense of not checking that all elements have been bound. The updated code now uses a window of 10 elements and assumes that if no elements are missing from that window then exhaustive checking is not required.  Closes gh-44867
spring-projects,spring-boot,9c25b69c06badb57ba0bf31ee387bbb23a2a2ca2,https://github.com/spring-projects/spring-boot/commit/9c25b69c06badb57ba0bf31ee387bbb23a2a2ca2,Improve performance of MapBinder by calculating items only once  Closes gh-44868
spring-projects,spring-boot,859d074764989a09911eeb3cba474ffcf1b26fac,https://github.com/spring-projects/spring-boot/commit/859d074764989a09911eeb3cba474ffcf1b26fac,Replace streams on hot paths with for loops  Replace a few usages of stream with simple for loops. Although this doesn't seem to make much difference to performance  it does help when profiling applications since it reduces the stack depth.
spring-projects,spring-boot,189d84d49de19792af7f108417f6510ea776e933,https://github.com/spring-projects/spring-boot/commit/189d84d49de19792af7f108417f6510ea776e933,Add ConfigurationPropertyCaching override support  Add `ConfigurationPropertyCaching.override()` method which can be used to temporarily enable caching for the duration of an operation.  The `Binder` now uses this method to ensure that caching is enabled whilst a set of related binding operations are performed.  Closes gh-44860
spring-projects,spring-boot,ae6908e4d87b0dd9077f53294afb65a87d6fd187,https://github.com/spring-projects/spring-boot/commit/ae6908e4d87b0dd9077f53294afb65a87d6fd187,Cache property mappings in ConfigurationPropertyName  Relocate `SystemEnvironmentPropertyMapper` methods into `ConfigurationPropertyName` so that they can be cached to improve performance  Closes gh-44858
spring-projects,spring-boot,81dee5413723796308372d073de6e76d90ee0a40,https://github.com/spring-projects/spring-boot/commit/81dee5413723796308372d073de6e76d90ee0a40,Improve ConfigurationPropertyName equals/hashCode performance  Update `ConfigurationPropertyName` to improve performance of the `equals(...)` and `hashCode()` methods by making the following changes:  - Move element hashCode logic to Element and cache the results - Exit the equals method early if hashcodes don't match - Exit the equals method early if toString() values match  Closes gh-44857
spring-projects,spring-boot,8ce7da9bb07c52658b743ff76cfa4f49afc789a1,https://github.com/spring-projects/spring-boot/commit/8ce7da9bb07c52658b743ff76cfa4f49afc789a1,Avoid duplicate customization of management web server factory  Previously  customization was performed in two places:  1. By customizers defined in the reactive and servlet web servlet factory auto-configuration - ServletWebServerFactoryAutoConfiguration - ReactiveWebServerFactoryAutoConfiguration 2. By a ManagementWebServerFactoryCustomizer that delegates to customizers of certain types found in the application context hierarchy.  This led to some double customization as the customizers registered by the auto-configuration classes were also found and called by the ManagementWebServerFactoryCustomizer.  Additionally  the ManagementWebServerFactoryCustomizer would find customizers from the parent context registered by EmbeddedWebServerFactoryCustomizerAutoConfiguration.  This commit reworks the customization of the management web server factory to remove the double customization. ManagementWebServerFactoryCustomizer no longer delegates to customizers that it finds in the context hierarchy. This prevents the customizers defined in the reactive and servlet web server factory auto-configuration classes from being called twice. Additionally  EmbeddedWebServerFactoryCustomizerAutoConfiguration is now registered in the child context so that its customizers continue to be called when preparing the management context web server factory.  Closes gh-44151
spring-projects,spring-boot,8e15317e89c57bec0dda12cae79f954b4a41478b,https://github.com/spring-projects/spring-boot/commit/8e15317e89c57bec0dda12cae79f954b4a41478b,Merge pull request #43931 from nosan  * pr/43931: Refine `SystemStatusListener` superfluous output fix Fix SystemStatusListener to prevent superfluous output  Closes gh-43931
spring-projects,spring-boot,c8845295615504c4af95f44ed3ee7d2644f14cd4,https://github.com/spring-projects/spring-boot/commit/c8845295615504c4af95f44ed3ee7d2644f14cd4,Refine `SystemStatusListener` superfluous output fix  Change `SystemStatusListener` to a `OnConsoleStatusListener` to ensure that it cannot be added twice from different threads.  Also add a local `retrospectivePrint()` that is used for non-debug output that will print ERROR and WARN status  but not INFO.  See gh-43931
spring-projects,spring-boot,6ba8e9b089a4be705c96c74fd92d58deb4f43a88,https://github.com/spring-projects/spring-boot/commit/6ba8e9b089a4be705c96c74fd92d58deb4f43a88,Fix SystemStatusListener to prevent superfluous output  Fix `SystemStatusListener` so that superfluous output is not printed when starting an application. This change ensures that the `SystemStatusListener` is not added twice  and that retrospective logging only occurs when `debug` is true.  See gh-43931  Signed-off-by: Dmytro Nosan <dimanosan@gmail.com>
spring-projects,spring-boot,709b9bb149721652fd82a6c56fa049190d3bdd19,https://github.com/spring-projects/spring-boot/commit/709b9bb149721652fd82a6c56fa049190d3bdd19,Allow Jackson to escape new line chars when BOMR adds issues  Remove the escaping logic when building the issue body so that Jackson can perform the actual escaping. Prior to this commit  the message body was double escaped.  Closes gh-43479
spring-projects,spring-boot,9890872a9aa5a7e2dfed223cf41f12f0a6b8914b,https://github.com/spring-projects/spring-boot/commit/9890872a9aa5a7e2dfed223cf41f12f0a6b8914b,Improve performance of ConcurrentReferenceCachingMetadataReaderFactory  Update `ConcurrentReferenceCachingMetadataReaderFactory` with cache by class name.  Closes gh-42949
spring-projects,spring-boot,3f9f0490a63b19456a614a435c171995549e597d,https://github.com/spring-projects/spring-boot/commit/3f9f0490a63b19456a614a435c171995549e597d,Use DataSource.unwrap to get routing data source  This commit uses DataSource.isWrapperFor and DataSource.unwrap to detect if a DataSource is an AbstractRoutingDataSource. Previously  it relied on instanceof which does not account for cases where the datasource has been proxied.  See gh-42313
spring-projects,spring-boot,c693b2bd8cfa7c10b2791aea58f26d55be548c33,https://github.com/spring-projects/spring-boot/commit/c693b2bd8cfa7c10b2791aea58f26d55be548c33,Add support for webjars-locator-lite  This is a follow-up to spring-projects/spring-framework#27619 This commit adds support for "org.webjars:webjars-locator-lite" for enabling the statis resources chain.  As of this commit  support for "org.webjars:webjars-locator-core" is deprecated for obvious performance reasons.  Closes gh-40146
elastic,elasticsearch,3af0568137b53a1ebc430f36a675b3c2024fe428,https://github.com/elastic/elasticsearch/commit/3af0568137b53a1ebc430f36a675b3c2024fe428,Speed up read dimension fields in TS (#128283)  When reading dimension fields in the TS command  we can skip reading values while the `tsid` remains unchanged to improve performance. I benchmarked this change with the following query:  ``` POST /_query { "query": "TS metrics-hostmetricsreceiver.otel-default | WHERE @timestamp >= \"2025-05-08T18:00:08.001Z\" | STATS cpu = avg(rate(`metrics.process.cpu.time`)) BY host.name  BUCKET(@timestamp  5 minute)" } ```  The total query time was reduced from 51ms to 39ms  with processing time in the time-series source operator reduced from 26ms to 17ms.
elastic,elasticsearch,f492bb9a0d4f04404ddd9b4cc37c515bc72f5b29,https://github.com/elastic/elasticsearch/commit/f492bb9a0d4f04404ddd9b4cc37c515bc72f5b29,[Profiling] Add support for variable sampling frequency (#128086)  * [Profiling] Add support for variable sampling frequency  * Update x-pack/plugin/profiling/src/main/java/org/elasticsearch/xpack/profiling/action/TransportGetStackTracesAction.java  Co-authored-by: Christos Kalkanis <christos.kalkanis@elastic.co>  * Add comments and remove superfluous debug log  ---------  Co-authored-by: Christos Kalkanis <christos.kalkanis@elastic.co>
elastic,elasticsearch,3551494b9acc37bcb7506aade144e6e677da2fb5,https://github.com/elastic/elasticsearch/commit/3551494b9acc37bcb7506aade144e6e677da2fb5,ESQL: `text ==` and `text !=` pushdown  (#127355)  Reenables `text ==` pushdown and adds support for `text !=` pushdown.  It does so by making `TranslationAware#translatable` return something we can turn into a tri-valued function. It has these values: * `YES` * `NO` * `RECHECK`  `YES` means the `Expression` is entirely pushable into Lucene. They will be pushed into Lucene and removed from the plan.  `NO` means the `Expression` can't be pushed to Lucene at all and will stay in the plan.  `RECHECK` mean the `Expression` can push a query that makes *candidate* matches but must be rechecked. Documents that don't match the query won't match the expression  but documents that match the query might not match the expression. These are pushed to Lucene *and* left in the plan.  This is required because `txt != "b"` can build a *candidate* query against the `txt.keyword` subfield but it can't be sure of the match without loading the `_source` - which we do in the compute engine.  I haven't plugged rally into this  but here's some basic performance tests: ``` Before: not text eq {"took":460 "documents_found":1000000} text eq {"took":432 "documents_found":1000000}  After: text eq {"took":5 "documents_found":1} not text eq {"took":351 "documents_found":800000} ```  This comes from: ``` rm -f /tmp/bulk* for a in {1..1000}; do echo '{"index":{}}' >> /tmp/bulk echo '{"text":"text '$(printf $(($a % 5)))'"}' >> /tmp/bulk done ls -l /tmp/bulk*  passwd="redacted" curl -sk -uelastic:$passwd -HContent-Type:application/json -XDELETE https://localhost:9200/test curl -sk -uelastic:$passwd -HContent-Type:application/json -XPUT https://localhost:9200/test -d'{ "settings": { "index.codec": "best_compression"  "index.refresh_interval": -1 }  "mappings": { "properties": { "many": { "enabled": false } } } }' for a in {1..1000}; do printf %04d: $a curl -sk -uelastic:$passwd -HContent-Type:application/json -XPOST https://localhost:9200/test/_bulk?pretty --data-binary @/tmp/bulk | grep errors done curl -sk -uelastic:$passwd -HContent-Type:application/json -XPOST https://localhost:9200/test/_forcemerge?max_num_segments=1 curl -sk -uelastic:$passwd -HContent-Type:application/json -XPOST https://localhost:9200/test/_refresh echo curl -sk -uelastic:$passwd https://localhost:9200/_cat/indices?v  text_eq() { echo -n "    text eq " curl -sk -uelastic:$passwd -HContent-Type:application/json -XPOST 'https://localhost:9200/_query?pretty' -d'{ "query": "FROM test | WHERE text == \"text 1\" | STATS COUNT(*)"  "pragma": { "data_partitioning": "shard" } }' | jq -c '{took  documents_found}' }  not_text_eq() { echo -n "not text eq " curl -sk -uelastic:$passwd -HContent-Type:application/json -XPOST 'https://localhost:9200/_query?pretty' -d'{ "query": "FROM test | WHERE NOT text == \"text 1\" | STATS COUNT(*)"  "pragma": { "data_partitioning": "shard" } }' | jq -c '{took  documents_found}' }   for a in {1..100}; do text_eq not_text_eq done ```
elastic,elasticsearch,03d77816cfcb3c5b6b71bad246a6cf32e25b3ad4,https://github.com/elastic/elasticsearch/commit/03d77816cfcb3c5b6b71bad246a6cf32e25b3ad4,[Failure store] Introduce dedicated failure store lifecycle configuration (#127314)  The failure store is a set of data stream indices that are used to store certain type of ingestion failures. Until this moment they were sharing the configuration of the backing indices. We understand that the two data sets have different lifecycle needs.  We believe that typically the failures will need to be retained much less than the data. Considering this we believe the lifecycle needs of the failures also more limited and they fit better the simplicity of the data stream lifecycle feature.  This allows the user to only set the desired retention and we will perform the rollover and other maintenance tasks without the user having to think about them. Furthermore  having only one lifecycle management feature allows us to ensure that these data is managed by default.  This PR introduces the following:  Configuration  We extend the failure store configuration to allow lifecycle configuration too  this configuration reflects the user's configuration only as shown below:  PUT _data_stream/*/options { "failure_store": { "lifecycle": { "data_retention": "5d" } } }  GET _data_stream/*/options  { "data_streams": [ { "name": "my-ds"  "options": { "failure_store": { "lifecycle": { "data_retention": "5d" } } } } ] } To retrieve the effective configuration you need to use the GET data streams API  see #126668  Functionality  The data stream lifecycle (DLM) will manage the failure indices regardless if the failure store is enabled or not. This will ensure that if the failure store gets disabled we will not have stagnant data. The data stream options APIs reflect only the user's configuration. The GET data stream API should be used to check the current state of the effective failure store configuration. Telemetry We extend the data stream failure store telemetry to also include the lifecycle telemetry.  { "data_streams": { "available": true  "enabled": true  "data_streams": 10  "indices_count": 50  "failure_store": { "explicitly_enabled_count": 1  "effectively_enabled_count": 15  "failure_indices_count": 30 "lifecycle": { "explicitly_enabled_count": 5  "effectively_enabled_count": 20  "data_retention": { "configured_data_streams": 5  "minimum_millis": X  "maximum_millis": Y  "average_millis": Z  }  "effective_retention": { "retained_data_streams": 20  "minimum_millis": X  "maximum_millis": Y  "average_millis": Z }  "global_retention": { "max": { "defined": false }  "default": { "defined": true   <------ this is the default value applicable for the failure store "millis": X } } } } } Implementation details  We ensure that partially reset failure store will create valid failure store configuration. We ensure that when a node communicates with a note with a previous version it will ensure it will not send an invalid failure store configuration enabled: null.
elastic,elasticsearch,10336c950ca2defe8ba651f1cdba346b1c0eabc5,https://github.com/elastic/elasticsearch/commit/10336c950ca2defe8ba651f1cdba346b1c0eabc5,ESQL: Speed loading stored fields (#127348)  This speeds up loading from stored fields by opting more blocks into the "sequential" strategy. This really kicks in when loading stored fields like `text`. And when you need less than 100% of documents  but more than  say  10%. This is most useful when you need 99.9% of field documents. That sort of thing. Here's the perf numbers: ``` %100.0 {"took": 403 -> 401 "documents_found":1000000} %099.9 {"took":3990 -> 436 "documents_found": 999000} %099.0 {"took":4069 -> 440 "documents_found": 990000} %090.0 {"took":3468 -> 421 "documents_found": 900000} %030.0 {"took":1213 -> 152 "documents_found": 300000} %020.0 {"took": 766 -> 104 "documents_found": 200000} %010.0 {"took": 397 ->  55 "documents_found": 100000} %009.0 {"took": 352 -> 375 "documents_found":  90000} %008.0 {"took": 304 -> 317 "documents_found":  80000} %007.0 {"took": 273 -> 287 "documents_found":  70000} %005.0 {"took": 199 -> 204 "documents_found":  50000} %001.0 {"took":  46 ->  46 "documents_found":  10000} ```  Let's explain this with an example. First  jump to `main` and load a million documents: ``` rm -f /tmp/bulk for a in {1..1000}; do echo '{"index":{}}' >> /tmp/bulk echo '{"text":"text '$(printf %04d $a)'"}' >> /tmp/bulk done  curl -s -uelastic:password -HContent-Type:application/json -XDELETE localhost:9200/test for a in {1..1000}; do echo -n $a: curl -s -uelastic:password -HContent-Type:application/json -XPOST localhost:9200/test/_bulk?pretty --data-binary @/tmp/bulk | grep errors done curl -s -uelastic:password -HContent-Type:application/json -XPOST localhost:9200/test/_forcemerge?max_num_segments=1 curl -s -uelastic:password -HContent-Type:application/json -XPOST localhost:9200/test/_refresh echo ```  Now query them all. Run this a few times until it's stable: ``` echo -n "%100.0 " curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ "query": "FROM test | STATS SUM(LENGTH(text))"  "pragma": { "data_partitioning": "shard" } }' | jq -c '{took  documents_found}' ```  Now fetch 99.9% of documents: ``` echo -n "%099.9 " curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ "query": "FROM test | WHERE NOT text.keyword IN (\"text 0998\") | STATS SUM(LENGTH(text))"  "pragma": { "data_partitioning": "shard" } }' | jq -c '{took  documents_found}' ```  This should spit out something like: ``` %100.0 { "took":403 "documents_found":1000000} %099.9 {"took":4098  "documents_found":999000} ```  We're loading *fewer* documents but it's slower! What in the world?! If you dig into the profile you'll see that it's value loading: ``` $ curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ "query": "FROM test | STATS SUM(LENGTH(text))"  "pragma": { "data_partitioning": "shard" }  "profile": true }' | jq '.profile.drivers[].operators[] | select(.operator | contains("ValuesSourceReaderOperator"))' { "operator": "ValuesSourceReaderOperator[fields = [text]]"  "status": { "readers_built": { "stored_fields[requires_source:true  fields:0  sequential: true]": 222  "text:column_at_a_time:null": 222  "text:row_stride:BlockSourceReader.Bytes": 1 }  "values_loaded": 1000000  "process_nanos": 370687157  "pages_processed": 222  "rows_received": 1000000  "rows_emitted": 1000000 } } $ curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ "query": "FROM test | WHERE NOT text.keyword IN (\"text 0998\") | STATS SUM(LENGTH(text))"  "pragma": { "data_partitioning": "shard" }  "profile": true }' | jq '.profile.drivers[].operators[] | select(.operator | contains("ValuesSourceReaderOperator"))' { "operator": "ValuesSourceReaderOperator[fields = [text]]"  "status": { "readers_built": { "stored_fields[requires_source:true  fields:0  sequential: false]": 222  "text:column_at_a_time:null": 222  "text:row_stride:BlockSourceReader.Bytes": 1 }  "values_loaded": 999000  "process_nanos": 3965803793  "pages_processed": 222  "rows_received": 999000  "rows_emitted": 999000 } } ```  It jumps from 370ms to almost four seconds! Loading fewer values! The second big difference is in the `stored_fields` marker. In the second on it's `sequential: false` and in the first `sequential: true`.  `sequential: true` uses Lucene's "merge" stored fields reader instead of the default one. It's much more optimized at decoding sequences of documents.  Previously we only enabled this reader when loading compact sequences of documents - when the entire block looks like ``` 1  2  3  4  5  ... 1230  1231 ```  If there are any gaps we wouldn't enable it. That was a very conservative thing we did long ago without doing any experiments. We knew it was faster without any gaps  but not otherwise. It turns out it's a lot faster in a lot more cases. I've measured it as faster for 99% gaps  at least on simple documents. I'm a bit worried that this is too aggressive  so I've set made it configurable and made the default being to use the "merge" loader with 10% gaps. So we'd use the merge loader with a block like: ``` 1  11  21  31  ...  1231  1241 ```
elastic,elasticsearch,d65f34d173bf419df55cf355639be32c929992a5,https://github.com/elastic/elasticsearch/commit/d65f34d173bf419df55cf355639be32c929992a5,Push down field extraction to time-series source (#127445)  This change pushes down field extractions to the time-series source operator  providing these advantages:  - Avoids building `DocVector` and its forward/backward maps.  - Leverages the `DocValues` cache (i.e.  blocks that are already decompressed/decoded) when loading values  which can be lost when reading blocks with the `ValuesSourceReaderOperator`.  - Eliminates the need to rebuild blocks with backward mappings after reading values.  The following query against the TSDB track previously took 19 seconds but was reduced to 13 seconds with this change:  ``` TS tsdb | STATS sum(rate(kubernetes.container.memory.pagefaults)) by bucket(@timestamp  5minute) ```  Note that with this change:  ``` TS tsdb | STATS sum(rate(kubernetes.container.memory.pagefaults)) by bucket(@timestamp  5minute) ``` now performs as well as:  ``` FROM tsdb | STATS sum(last_over_time(kubernetes.container.memory.pagefaults)) by bucket(@timestamp  5minute) ```  when using the shard level data partitioning. This means the performance of the TS command is comparable to the `FROM` command  except that it does not yet support segment-level or doc-level concurrency. I will try to add support for segment-level concurrency  as document-level partitioning is not useful when iterating over documents in order.
elastic,elasticsearch,d39f4725773c3cc4f15bc15e20abf456d20e194f,https://github.com/elastic/elasticsearch/commit/d39f4725773c3cc4f15bc15e20abf456d20e194f,Make can_match code a little easier to reuse (#126588)  Step 1 to refactoring this with reuse in a per-datanode fashion for batched execution. Some obvious cleanup essentially making this a utility  removing one weird indirection and reducing the use of the actual instance of `CanMatchPreFilterSearchPhase` (this also results in a real performance gain from moving work for sorting shards etc. off of the transport_workers and closer to where its result is used).  This should by relatively trivial to review and allows for a simple follow up that extracts the ability to run an individual round in isolation as well as running the coordinator rewrite phase separately.
elastic,elasticsearch,0c90de5ed56e5d4ac3e1ee4b3e085655d7af6251,https://github.com/elastic/elasticsearch/commit/0c90de5ed56e5d4ac3e1ee4b3e085655d7af6251,Fork time-series source to allow field extractions (#127375)  This change prepares for pushing down field extractions to the time-series source for performance reasons. It is a non-issue  as the actual change will occur in a follow-up.
elastic,elasticsearch,b9917086e1c1bf4ab1948476164277bdea1f8440,https://github.com/elastic/elasticsearch/commit/b9917086e1c1bf4ab1948476164277bdea1f8440,Create dedicated factory methods for data lifecycle (#126487)  The class `DataStreamLifecycle` is currently capturing the lifecycle configuration that currently manages all data stream indices  but soon enough it will be split into two variants  the data and the failures lifecycle.  Some pre-work has been done already but as we are progressing in our POC  we see that it will be really useful if the `DataStreamLifecycle` is "aware" of the target index component. This will allow us to correctly apply global retention or to throw an error if a downsampling configuration is provided to a failure lifecycle.  In this PR  we perform a small refactoring to reduce the noise in https://github.com/elastic/elasticsearch/pull/125658. Here we introduce the following:  - A factory method that creates a data lifecycle  for now it's trivial but it will be more useful soon. - We rename the "empty" builder to explicitly mention the index component it refers to.
elastic,elasticsearch,21813604b4e2f1328877b5dc3bb24f87597e2836,https://github.com/elastic/elasticsearch/commit/21813604b4e2f1328877b5dc3bb24f87597e2836,Skip listing MPUs if TTL set to -1 (#127166)  Recent versions of MinIO will sometimes leak multi-part uploads under concurrent load  leaving them in the `ListMultipartUploads` output even though they cannot be aborted. Today this causes repository analysis to fail since compare-and-exchange operations will not even start if there are any pre-existing uploads. This commit makes it possible to skip this pre-flight check (and accept the performance consequences) by adjusting the relevant settings.  Workaround for minio/minio#21189 Closes #122670
elastic,elasticsearch,4f506d47a5c2b3148ec7bca851593708c6adb730,https://github.com/elastic/elasticsearch/commit/4f506d47a5c2b3148ec7bca851593708c6adb730,Optimize time-series source operator (#127095)  This query against the TSDB track took 50 seconds and was reduced to 19 seconds with this changes.  ``` TS tsdb | STATS sum(rate(kubernetes.container.memory.pagefaults)) by bucket(@timestamp  5minute) ```  This change introduces several optimizations to improve the performance of the time-series source operator:  - Split the leaf queue into two: one for `_tsid` and another for `@timestamp`. This avoids repeatedly comparing large `_tsid` values while iterating over a single `_tsid`.  - Track the number of emitted documents per segment and use this data to build forward and backward document maps  reducing the need for expensive sorts.  - Use ordinal blocks to avoid duplicating the same `_tsid` multiple times
elastic,elasticsearch,b972364539cbaba02b2b03c89cf95c70d000e545,https://github.com/elastic/elasticsearch/commit/b972364539cbaba02b2b03c89cf95c70d000e545,Optimize ES819 doc values address offset calculation (#126732)  When writing the doc values addresses  we currently perform an iteration over all the sorted numeric doc values to calculate the addresses. When merging sorted segments  this iteration is expensive as it requires performing a merge sort.  This patch removes this iteration by instead calculating the addresses while we are writing the values  writing the addresses to a  temporary file. Afterwards  they are copied from the temporary file into the merged segment.  Relates to #126111
elastic,elasticsearch,128144dd6dc2f35e3d4c80131ef71b7b65a8d80b,https://github.com/elastic/elasticsearch/commit/128144dd6dc2f35e3d4c80131ef71b7b65a8d80b,ESQL: Add `documents_found` and `values_loaded` (#125631)  This adds `documents_found` and `values_loaded` to the to the ESQL response: ```json { "took" : 194  "is_partial" : false  "documents_found" : 100000  "values_loaded" : 200000  "columns" : [ { "name" : "a"  "type" : "long" }  { "name" : "b"  "type" : "long" } ]  "values" : [[10  1]] } ```  These are cheap enough to collect that we can do it for every query and return it with every response. It's small  but it still gives you a reasonable sense of how much work Elasticsearch had to go through to perform the query.  I've also added these two fields to the driver profile and task status: ```json "drivers" : [ { "description" : "data"  "cluster_name" : "runTask"  "node_name" : "runTask-0"  "start_millis" : 1742923173077  "stop_millis" : 1742923173087  "took_nanos" : 9557014  "cpu_nanos" : 9091340  "documents_found" : 5    <---- THESE "values_loaded" : 15     <---- THESE "iterations" : 6  ... ```  These are at a high level and should be easy to reason about. We'd like to extract this into a "show me how difficult this running query is" API one day. But today  just plumbing it into the debugging output is good.  Any `Operator` can claim to "find documents" or "load values" by overriding a method on its `Operator.Status` implementation: ```java /** * The number of documents found by this operator. Most operators * don't find documents and will return {@code 0} here. */ default long documentsFound() { return 0; }  /** * The number of values loaded by this operator. Most operators * don't load values and will return {@code 0} here. */ default long valuesLoaded() { return 0; } ```  In this PR all of the `LuceneOperator`s declare that each `position` they emit is a "document found" and the `ValuesSourceValuesSourceReaderOperator` says each value it makes is a "value loaded". That's pretty pretty much true. The `LuceneCountOperator` and `LuceneMinMaxOperator` sort of pretend that the count/min/max that they emit is a "document" - but that's good enough to give you a sense of what's going on. It's *like* document.
elastic,elasticsearch,173904924a1024110498d1ba2ded1bd02c82a0ae,https://github.com/elastic/elasticsearch/commit/173904924a1024110498d1ba2ded1bd02c82a0ae,Add avg_over_time (#126572)  This change adds the `avg_over_time` aggregation for time series indices. Similar to other time series aggregations  we need to translate `avg_over_time` into regular aggregations. There are two options for this translation:  1. Translate `avg_over_time` to `EVAL div(sum_over_time  count_over_time)`  then translate `sum_over_time` and `count_over_time` to `sum` and `count`. 2. Translate `avg_over_time` directly to `avg`  and then to `div(sum  count)`.  This PR chooses the latter approach. Below is an example:  ``` TS k8s | STATS sum(avg_over_time(memory_usage)) BY host  bucket(@timestamp  1minute) ```  translates to:  ``` TS k8s | STATS avg_memory_usage = avg(memory_usage)  host_values=VALUES(host) BY _tsid  time_bucket=bucket(@timestamp  1minute) | STATS sum(avg_memory_usage) BY host_values  time_bucket ```  and then:  ``` TS k8s | STATS sum_memory_usage = sum(memory_usage)  count_memory_usage = count(memory_usage)  host_values=VALUES(host) BY _tsid  time_bucket=bucket(@timestamp  1minute) | EVAL avg_memory_usage = sum_memory_usage / count_memory_usage | STATS sum(avg_memory_usage) BY host_values  time_bucket ```  Since we need to substitute `AVG` with `SUM` and `COUNT` after translation  we need to call `SubstituteSurrogates` twice in `LogicalPlanOptimizer`. If there is a performance impact  we can move this rule to `TranslateTimeSeriesAggregate`.
elastic,elasticsearch,0c95d1a48e46bbccba2b377e8e4eb37b4d3b66dd,https://github.com/elastic/elasticsearch/commit/0c95d1a48e46bbccba2b377e8e4eb37b4d3b66dd,Filter out empty top docs results before merging (#126385)  `Lucene.EMPTY_TOP_DOCS` to identify empty to docs results. These were previously null results  but did not need to be send over transport as incremental reduction was performed only on the data node.  Now it can happen that the coord node received a merge result with empty top docs  which has nothing interesting for merging  but that can lead to an exception because the type of the empty array does not match the type of other shards results  for instance if the query was sorted by field. To resolve this  we filter out empty top docs results before merging.  Closes #126118
elastic,elasticsearch,c1a71ff45c06394951716c3e17fa9dbb82dfad5a,https://github.com/elastic/elasticsearch/commit/c1a71ff45c06394951716c3e17fa9dbb82dfad5a,BlobContainer: add copyBlob method (#125737)  * BlobContainer: add copyBlob method  If a container implements copyBlob  then the copy is performed by the store  without client-side IO. If the store does not provide a copy operation then the default implementation throws UnsupportedOperationException.  This change provides implementations for the FS and S3 blob containers. More will follow.  Co-authored-by: elasticsearchmachine <infra-root+elasticsearchmachine@elastic.co> Co-authored-by: David Turner <david.turner@elastic.co>
elastic,elasticsearch,065c5830cbedf0257a7b93bf2e31c598901d0aec,https://github.com/elastic/elasticsearch/commit/065c5830cbedf0257a7b93bf2e31c598901d0aec,First step optimizing tsdb doc values codec merging. (#125403)  The doc values codec iterates a few times over the doc value instance that needs to be written to disk. In case when merging and index sorting is enabled  this is much more expensive  as each time the doc values instance is iterated a merge sorting is performed (in order to get the doc ids of new segment in order of index sorting).  There are several reasons why the doc value instance is iterated multiple times: * To compute stats (num values  number of docs with value) required for writing values to disk. * To write bitset that indicate which documents have a value. (indexed disi  jump table) * To write the actual values to disk. * To write the addresses to disk (in case docs have multiple values)  This applies for numeric doc values  but also for the ordinals of sorted (set) doc values.  This PR addresses solving the first reason why doc value instance needs to be iterated. This is done only when in case of merging and when the segments to be merged with are also of type es87 doc values  codec version is the same and there are no deletes. Note this optimized merged is behind a feature flag for now.
elastic,elasticsearch,6c641c06770d120b9bca6416c47cb510d55aa89e,https://github.com/elastic/elasticsearch/commit/6c641c06770d120b9bca6416c47cb510d55aa89e,Extrapolate rate aggregation (#126331)  This change performs extrapolation for rate aggregation similarly to how PromQL does.
elastic,elasticsearch,7e1e45eaa46dc5034bbcdd2d9e6d3ef0999ba3b7,https://github.com/elastic/elasticsearch/commit/7e1e45eaa46dc5034bbcdd2d9e6d3ef0999ba3b7,ESQL: Speed up TO_IP (#126338)  Speed up the TO_IP method by converting directly from utf-8 encoded strings to the ip encoding. Previously we did: ``` utf-8 -> String -> INetAddress -> ip encoding ```  In a step towards solving #125460 this creates three IP parsing functions  one the rejects leading zeros  one that interprets leading zeros as decimal numbers  and one the interprets leading zeros as octal numbers. IPs have historically been parsed in all three of those ways.  This plugs the "rejects leading zeros" parser into `TO_IP` because that's the behavior it had before.  Here is the performance: ``` Benchmark               Score    Error  Units leadingZerosAreDecimal  14.007 ± 0.093  ns/op leadingZerosAreOctal    15.020 ± 0.373  ns/op leadingZerosRejected    14.176 ± 3.861  ns/op original                32.950 ± 1.062  ns/op ```  So this is roughly 45% faster than what we had.
elastic,elasticsearch,4c174a891fcecfc71474f5df856e7cc2eb821b95,https://github.com/elastic/elasticsearch/commit/4c174a891fcecfc71474f5df856e7cc2eb821b95,Use Lucene101 postings format by default (#126080)  Update the PerFieldFormatSupplier so that new standard indices use the Lucene101PostingsFormat instead of the current default ES812PostingsFormat.  Currently  use of the new codec is gated behind a feature flag.
elastic,elasticsearch,52c2d62103637a86a2a811d1fe1026ba1daf8366,https://github.com/elastic/elasticsearch/commit/52c2d62103637a86a2a811d1fe1026ba1daf8366,Add evaluation context to time-series aggregators (#126089)  Rate aggregators need to access the start time and end time of each group to perform extrapolation in the final evaluation. This change replaces the DriverContext parameter in evaluateFinal with EvaluationContext  allowing the TimeSeriesAggregationOperator to pass an EvaluationContext with these time intervals. I took another approach where the extension is applied only for the time series aggregator  but it could be fragile with the pre-filter of aggregators.
elastic,elasticsearch,6ce8d51c42d49ac2bb89a8926502356b1a04564a,https://github.com/elastic/elasticsearch/commit/6ce8d51c42d49ac2bb89a8926502356b1a04564a,Add time bucket to time-series aggregate (#126010)  We need the bucket interval within the rate function to perform extrapolation. With this change  along with #26089  we will be able to pass the grouping range interval to rate aggregations.
elastic,elasticsearch,b01438a95f1a6e1518a2700ec9f147292fafcb60,https://github.com/elastic/elasticsearch/commit/b01438a95f1a6e1518a2700ec9f147292fafcb60,Re-enable parallel collection for field sorted top hits (#125916)  With #123610 we disabled parallel collection for field and script sorted top hits  aligning its behaviour with that of top level search. This was mainly to work around a bug in script sorting that did not support inter-segment concurrency.  The bug with script sort has been fixed with #123757 and concurrency re-enabled for it.  While sort by field is not optimized for search concurrency  top hits benefits from it and disabling concurrency for sort by field in top hits has caused performance regressions in our nightly benchmarks.  This commit re-enables concurrency for top hits with sort by field is used. This introduces back a discrepancy between top level search and top hits  in that concurrency is applied for top hits despite sort by field normally disables it. The key difference is the context where sorting is applied  and the fact that concurrency is disabled only for performance reasons on top level searches and not for functional reasons.
elastic,elasticsearch,fd2cc975418f16926bff08115c79d89c89c17114,https://github.com/elastic/elasticsearch/commit/fd2cc975418f16926bff08115c79d89c89c17114,Introduce batched query execution and data-node side reduce (#121885)   This change moves the query phase a single roundtrip per node just like can_match or field_caps work already. A a result of executing multiple shard queries from a single request we can also partially reduce each node's query results on the data node side before responding to the coordinating node.  As a result this change significantly reduces the impact of network latencies on the end-to-end query performance  reduces the amount of work done (memory and cpu) on the coordinating node and the network traffic by factors of up to the number of shards per data node!  Benchmarking shows up to orders of magnitude improvements in heap and network traffic dimensions in querying across a larger number of shards.
elastic,elasticsearch,632b9e79bd5c5ae441b6bc30046859026e4fb3f7,https://github.com/elastic/elasticsearch/commit/632b9e79bd5c5ae441b6bc30046859026e4fb3f7,Load FieldInfos from store if not yet initialised through a refresh on IndexShard (#125650)  Load field caps from store if they haven't been initialised through a refresh yet. Keep the plain reads to not mess with performance characteristics too much on the good path but protect against confusing races when loading field infos now (that probably should have been ordered stores in the first place but this was safe due to other locks/volatiles on the refresh path).  Closes #125483
elastic,elasticsearch,a6f685cc2ac1e40039e84a7747985ec51585bbf5,https://github.com/elastic/elasticsearch/commit/a6f685cc2ac1e40039e84a7747985ec51585bbf5,Adding common rerank options to Perform Inference API (#125239)  * wip  * Adding rerank common options  * Linting  * Linting  * [CI] Auto commit changes from spotless  * Update docs/changelog/125239.yaml  * PR feedback  ---------  Co-authored-by: elasticsearchmachine <infra-root+elasticsearchmachine@elastic.co>
elastic,elasticsearch,c5e76847ad9b6900b1cc4dffa44e485bb8bd1e5b,https://github.com/elastic/elasticsearch/commit/c5e76847ad9b6900b1cc4dffa44e485bb8bd1e5b,ESQL: Keep ordinals in conversion functions (#125357)  Make the conversion functions that process `BytesRef`s into `BytesRefs` keep the `OrdinalBytesRefVector`s when processing. Let's use `TO_LOWER` as an example. First  the performance numbers: ``` (operation)  Mode   Score   Error ->  Score    Error Units to_lower  30.662 ± 6.163 -> 30.048 ±  0.479 ns/op to_lower_ords  30.773 ± 0.370 ->  0.025 ±  0.001 ns/op to_upper  33.552 ± 0.529 -> 35.775 ±  1.799 ns/op to_upper_ords  35.791 ± 0.658 ->  0.027 ±  0.001 ns/op ``` The test has a 8192 positions containing alternating `foo` and `bar`. Running `TO_LOWER` via ordinals is super duper faster. No longer `O(positions)` and now `O(unique_values)`.  Let's paint some pictures! `OrdinalBytesRefVector` is a lookup table. Like this: ``` +-------+----------+ | bytes | ordinals | | ----- | -------- | |  FOO  | 0        | |  BAR  | 1        | |  BAZ  | 2        | +-------+ 1        | | 1        | | 0        | +----------+ ```  That lookup table is one block. When you read it you look up the `ordinal` and match it to the `bytes`. Previously `TO_LOWER` would process each value one at a time and make: ``` bytes ----- foo bar baz bar bar foo ```  So it'd run `TO_LOWER` once per `ordinal` and it'd make an ordinal non-lookup table. With this change `TO_LOWER` will now make: ``` +-------+----------+ | bytes | ordinals | | ----- | -------- | |  foo  | 0        | |  bar  | 1        | |  baz  | 2        | +-------+ 1        | | 1        | | 0        | +----------+ ``` We don't even have to copy the `ordinals` - we can reuse those from the input and just bump the reference count. That's why this goes from `O(positions)` to `O(unique_values)`.
elastic,elasticsearch,0930a75642a11b43b6f3429e222924aff4641038,https://github.com/elastic/elasticsearch/commit/0930a75642a11b43b6f3429e222924aff4641038,Prevent default inference model to update the cluster state when deleting (#125369)  The Elastic inference service removes the default models at startup if the node cannot access EIS. Since #125242 we don't store default models in the cluster state but we still try to delete them. This change ensures that we don't try to update the cluster state when a default model is deleted since the delete is not performed on the master node and default models are never stored in the cluster state.
elastic,elasticsearch,ae0b2963d247437f501d95cacfcacff2cfb0695b,https://github.com/elastic/elasticsearch/commit/ae0b2963d247437f501d95cacfcacff2cfb0695b,[Entitlements] Add an option to perform bytecode verification during instrumentation (#124404)    Using ASM CheckClassAdapter was key to diagnose the issue we had with incorrect signatures for some check methods. In this PR I polished up the code I used to pinpoint the issue  and made it available via a system property so it can be turned on if we need it (and it's always on for Entitlements IT tests too).  It is also turned on in case we get VerifyErrors during retransformClasses early in the Entitlement agent bootstrap: retransformClasses runs in the native part of the JVM  so the VerifyError it produces is not so readable (e.g. it lacks a full stack trace and a description); in case this happens  we re-apply the transformation with verification turned on to get a meaningful error before dying.
elastic,elasticsearch,a4d729794462bf11df477ce3af7753b72cc418b5,https://github.com/elastic/elasticsearch/commit/a4d729794462bf11df477ce3af7753b72cc418b5,Permanently switch from SecurityManager to Entitlements (#124865) (#125117)  The JDK team has completely disabled the Java SecurityManager from Java 24. Elasticsearch has always used the Java SecurityManager as an additional protection mechanism; in order to retain this second line of defense  the Elasticsearch Core/Infra team has been working on the Entitlements project.  Similar to SecurityManager  Entitlements only allow calling specific methods in the JDK when the caller has a matching policy attached. In other words  if some code (in the main Elasticsearch codebase  in a plugin/module  or in a script) attempts to perform a "privileged" operation and it is not entitled to do so  a NotEntitledException will be thrown.  This PR includes the minimal set of changes to always use Entitlements  regardless of system properties or Java version.  Relates to ES-10921
elastic,elasticsearch,270ec538c9442d244ffc7583d14bb02a80223407,https://github.com/elastic/elasticsearch/commit/270ec538c9442d244ffc7583d14bb02a80223407,Add ModelRegistryMetadata to Cluster State (#121106)  This commit integrates `MinimalServiceSettings` (introduced in #120560) into the cluster state for all registered models in the `ModelRegistry`. These settings allow consumers to access configuration details without requiring asynchronous calls to retrieve full model configurations.  To ensure consistency  the cluster state metadata must remain synchronized with the models in the inference index. If a mismatch is detected during startup  the master node performs an upgrade to load all model settings from the index.
elastic,elasticsearch,0b6a3cd138573a59028736eb23902d711e031751,https://github.com/elastic/elasticsearch/commit/0b6a3cd138573a59028736eb23902d711e031751,Expose `input_type` option at root level for `text_embedding` task type in Perform Inference API (#122638)  * wip  * wip  * [CI] Auto commit changes from spotless  * Adding internal input types  * [CI] Auto commit changes from spotless  * Throwing validation exception for services that don't support input type  * linting  * hugging face  * voyage ai  * google ai studio  * bedrock updates  * Fixing tests  * Fixing tests  * Fixing tests  * bedrock updates  * elasticsearch  * azure openai  * [CI] Auto commit changes from spotless  * Refactoring all the things  * [CI] Auto commit changes from spotless  * Everything compiles  * spotless  * external actions tests  * external request tests  * service tests  * Fixing integration tests  * Cleanup  * Update docs/changelog/122638.yaml  * Cleanup  * Update x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/services/azureopenai/AzureOpenAiService.java  Co-authored-by: David Kyle <david.kyle@elastic.co>  * Update x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/services/ServiceUtils.java  Co-authored-by: David Kyle <david.kyle@elastic.co>  * PR feedback  ---------  Co-authored-by: elasticsearchmachine <infra-root+elasticsearchmachine@elastic.co> Co-authored-by: David Kyle <david.kyle@elastic.co>
elastic,elasticsearch,36874e866368e5b6d2e9e2d7838646ca239443ea,https://github.com/elastic/elasticsearch/commit/36874e866368e5b6d2e9e2d7838646ca239443ea,Prevent work starvation bug if using scaling EsThreadPoolExecutor with core pool size = 0  (#124732)  When `ExecutorScalingQueue` rejects work to make the worker pool scale up while already being at max pool size (and a new worker consequently cannot be added)  available workers might timeout just about at the same time as the task is then force queued by `ForceQueuePolicy`. This has caused starvation of work as observed for `masterService#updateTask` in #124667 where max pool size 1 is used. This configuration is most likely to expose the bug.  This PR changes `EsExecutors.newScaling` to not use `ExecutorScalingQueue` if max pool size is 1 (and core pool size is 0). A regular `LinkedTransferQueue` works perfectly fine in this case.  If max pool size > 1  a probing approach is used to ensure the worker pool is adequately scaled to at least 1 worker after force queueing work in `ForceQueuePolicy`.  Fixes #124667 Relates to #18613
elastic,elasticsearch,b76048ddd299129737e808f58c6ac7234f2e2641,https://github.com/elastic/elasticsearch/commit/b76048ddd299129737e808f58c6ac7234f2e2641,add load_native_libraries entitlement to java.desktop (#124852)    The ingest-attachment module uses Tika to parse some content; Tika in turn uses some libraries from java.desktop to perform its tasks.  In turn  the JDK loads one (or more) native libraries for its implementation as part of class initialization. This means we need to grant load_native_libraries to java.desktop so that because AWT can load libraries for itself.
elastic,elasticsearch,49254b0bdcb1211e560d441834feb5842007e07a,https://github.com/elastic/elasticsearch/commit/49254b0bdcb1211e560d441834feb5842007e07a,Remove page alignment in exchange sink (#124610)  I see that planning the ExchangeSinkExec takes a few milliseconds when benchmarking simple queries with 10K fields. It spends time checking if we need to realign the incoming pages. However  the exchange has the exact same attributes as its child  so the incoming layout should match its attributes perfectly. This change removes the realignment.
elastic,elasticsearch,fc4d8d65e5e0a17e38baee2cb50a74d77f351e8b,https://github.com/elastic/elasticsearch/commit/fc4d8d65e5e0a17e38baee2cb50a74d77f351e8b,ESQL: Enable visualizing a query profile (#124361)  To understand query performance  we often peruse the output of `_query`-requests run with `"profile": true`.  This is difficult when the query runs in a large cluster with many nodes and shards  or in case of CCQ.  This adds an option to visualize a query using Chromium's/Chrome's builtin `about:tracing` - or  for even better visuals and querying the different drivers via SQL  perfetto (c.f. https://ui.perfetto.dev/).  To use  save the JSON output of a query run with `"profile": true` to a file  like `output.json` and then invoke the following Gradle task:  ``` ./gradlew x-pack:plugin:esql:tools:parseProfile --args='~/output.json ~/parsed_profile.json' ```  Either open `about:tracing` in Chromium/Chrome ![image](https://github.com/user-attachments/assets/75e17ddf-f032-4aa1-bf3e-61b985b4e0b6) Or head over to https://ui.perfetto.dev (build locally in case of potentially sensitive data in the profille): ![image](https://github.com/user-attachments/assets/b3372b7d-fbec-45aa-a68c-b24e62a8c704)  Every slice is a driver  the colors indicating the ratio of cpu time over total time. - In Perfetto  essentials like duration  cpu duration  timestamp and a few others can be queried via SQL - this allows e.g. querying for all drivers that spent more than 50% of their time waiting and other fun things. ![image](https://github.com/user-attachments/assets/4a0ab2ce-3585-4953-b2eb-71991777b3fa)  - Details about a driver  esp. which operators it ran  are available when clicking the driver's slice. ![image](https://github.com/user-attachments/assets/e1c0b30d-0a31-468c-9ff4-27ca452716fc)
elastic,elasticsearch,ce3a778fa140c9931dc79686b1a5d616f267e32d,https://github.com/elastic/elasticsearch/commit/ce3a778fa140c9931dc79686b1a5d616f267e32d,Improve downsample performance by buffering docids and do bulk processing. (#124477)
elastic,elasticsearch,2761af000b9863042f54587e85b8162f38c7f7a9,https://github.com/elastic/elasticsearch/commit/2761af000b9863042f54587e85b8162f38c7f7a9,ESQL: Lazy collection copying during node transform (#124424)  * ESQL: Lazy collection copying during node transform  A set of optimization for tree traversal: 1. perform lazy copying during children transform 2. use long hashing to avoid object creation 3. perform type check first before collection checking  Relates #124395
elastic,elasticsearch,def4c890bcb438197bb2b13aa72db42d283ae1a0,https://github.com/elastic/elasticsearch/commit/def4c890bcb438197bb2b13aa72db42d283ae1a0,Fix concurrency issue in ScriptSortBuilder (#123757)  Inter-segment concurrency is disabled whenever sort by field  included script sorting  is used in a search request.  The reason why sort by field does not use concurrency is that there are some performance implications  given that the hit queue in Lucene is build per slice and the different search threads don't share information about the documents they have already visited etc.  The reason why script sort has concurrency disabled is that the script sorting implementation is not thread safe. This commit addresses such concurrency issue and re-enables search concurrency for search requests that use script sorting. In addition  missing tests are added to cover for sort scripts that rely on _score being available and top_hits aggregation with a scripted sort clause.
elastic,elasticsearch,79a1626160602fbdc11fd691e2119125415834cd,https://github.com/elastic/elasticsearch/commit/79a1626160602fbdc11fd691e2119125415834cd,Speed up block serialization (#124394)  Currently  we use NamedWriteable for serializing blocks. While convenient  it incurs a noticeable performance penalty when pages contain thousands of blocks. Since block types are small and already centered in ElementType  we can safely switch from NamedWriteable to typed code. For example  the NamedWriteable alone of a small page with 10K fields would be 180KB  whereas the new method reduces it to 10KB. Below are the serialization improvements with FROM idx | LIMIT 10000 where the target index has 10K fields:  - write_exchange_response executed 173 times took: 73.2ms -> 26.7ms - read_exchange_response executed 173 times took: 49.4ms -> 25.8ms
elastic,elasticsearch,a88d6458dca6b756de6a8e0c5dbf847c77e3ba33,https://github.com/elastic/elasticsearch/commit/a88d6458dca6b756de6a8e0c5dbf847c77e3ba33,[ML] Improve EIS authorization to perform requests on a periodic basis instead of only once (#123639)  * Refactoring  * Add internal cluster setting to aid testing  * [CI] Auto commit changes from spotless  * Allowing the auth interval to be configurable via a setting  * Removing unused code  * Adding revocation functionality back  ---------  Co-authored-by: elasticsearchmachine <infra-root+elasticsearchmachine@elastic.co>
elastic,elasticsearch,333e252aee006fe6a7578495088c659132fbbb3a,https://github.com/elastic/elasticsearch/commit/333e252aee006fe6a7578495088c659132fbbb3a,Avoid over collecting in Limit or Lucene Operator (#123296)  Currently  we rely on signal propagation for early termination. For example  FROM index | LIMIT 10 can be executed by multiple Drivers: several Drivers to read document IDs and extract fields  and the final Driver to select at most 10 rows. In this scenario  each Lucene Driver can independently collect up to 10 rows until the final Driver has enough rows and signals them to stop collecting. In most cases  this model works fine  but when extracting fields from indices in the warm/cold tier  it can impact performance. This change introduces a Limiter used between LimitOperator and LuceneSourceOperator to avoid over-collecting. We will also need a follow-up to ensure that we do not over-collect between multiple stages of query execution.
elastic,elasticsearch,5b0591e04a03d776031d1b4391e56e0332b7f362,https://github.com/elastic/elasticsearch/commit/5b0591e04a03d776031d1b4391e56e0332b7f362,Consider entitlement lib as system module (#123315)  * Consider entitlement lib as system module  Entitlements sometimes needs to perform sensitive operations  particularly within the FileAccessTree. This commit expands the trivially allowed check to include entitlements as one of the system modules alongside the jdk. One consequence is that the self test must be moved outside entitlements.  * [CI] Auto commit changes from spotless  * remove old method call  ---------  Co-authored-by: elasticsearchmachine <infra-root+elasticsearchmachine@elastic.co> Co-authored-by: Elastic Machine <elasticmachine@users.noreply.github.com>
elastic,elasticsearch,4d2b8dc4f2e908821dfb34d4ffc14244fce83c41,https://github.com/elastic/elasticsearch/commit/4d2b8dc4f2e908821dfb34d4ffc14244fce83c41,Fix early termination in LuceneSourceOperator (#123197)  The LuceneSourceOperator is supposed to terminate when it reaches the limit; unfortunately  we don't have a test to cover this. Due to this bug  we continue scanning all segments  even though we discard the results as the limit was reached. This can cause performance issues for simple queries like FROM .. | LIMIT 10  when Lucene indices are on the warm or cold tier. I will submit a follow-up PR to ensure we only collect up to the limit across multiple drivers.
elastic,elasticsearch,cae7f0a80973310cf321aabcdb409276499e3950,https://github.com/elastic/elasticsearch/commit/cae7f0a80973310cf321aabcdb409276499e3950,Use inheritance instead of composition to simplify search phase transitions (#119272)  We only need the extensibility for testing and it's a lot easier to reason about the code if we have explicit methods instead of overly complicated composition with lots of redundant references being retained all over the place.  -> lets simplify to inheritance and get shorter code that performs more predictably (especially when it comes to memory) as a first step. This also opens up the possibility of further simplifications and removing more retained state/memory as we go through the search phases.
elastic,elasticsearch,8d1f5d322336087139b224c7820d11b2adc90fd6,https://github.com/elastic/elasticsearch/commit/8d1f5d322336087139b224c7820d11b2adc90fd6,Hold store reference in InternalEngine#performActionWithDirectoryReader(...) (#123010)  This method gets called from `InternalEngine#resolveDocVersion(...)`  which gets during indexing (via `InternalEngine.index(...)`).  When `InternalEngine.index(...)` gets invoked  the InternalEngine only ensures that it holds a ref to the engine via Engine#acquireEnsureOpenRef()  but this doesn't ensure whether it holds a reference to the store.  Closes #122974  * Update docs/changelog/123010.yaml
elastic,elasticsearch,8e34393227dbe83a2879c58469e02f911db12bc1,https://github.com/elastic/elasticsearch/commit/8e34393227dbe83a2879c58469e02f911db12bc1,Fail primary term and generation listeners on a closed shard (#122713)  If a shard has been closed  we should quickly bail out and fail all waiting primary term and generation listeners. Otherwise  the engine implementation may try to successfully to complete the provided listeners and perform operations on an already closed shard and cause some unexpected errors.
elastic,elasticsearch,780cac5a6daf8b6d57a191733589d27cafd65634,https://github.com/elastic/elasticsearch/commit/780cac5a6daf8b6d57a191733589d27cafd65634,Enable a sparse doc values index for `@timestamp` in LogsDB (#122161)  This PR extends the work done in #121751 by enabling a sparse doc values index for the @timestamp field in LogsDB.  Similar to the previous PR  the setting index.mapping.use_doc_values_skipper will override the index mapping parameter when all of the following conditions are met:  * The index mode is LogsDB. * The field name is @timestamp. * Index sorting is configured on @timestamp (regardless of whether it is a primary sort field or not). * Doc values are enabled.  This ensures that only one index structure is defined on the @timestamp field: * If the conditions above are met  the inverted index is replaced with a sparse doc values index. * This prevents both the inverted index and sparse doc values index from being enabled together  reducing unnecessary storage overhead.  This change aligns with our goal of optimizing LogsDB for storage efficiency while possibly maintaining reasonable query latency performance. It will enable us to run benchmarks and evaluate the impact of sparse indexing on the @timestamp field as well.
elastic,elasticsearch,9adb91d4fe5fe17c2e92706f4dcd8be65b54f9f8,https://github.com/elastic/elasticsearch/commit/9adb91d4fe5fe17c2e92706f4dcd8be65b54f9f8,Knn vector rescoring to sort score docs (#122653)  RescoreKnnVectorQuery rewrites to KnnScoreDocQuery  which takes a sorted array of doc ids and corresponding array including scores fo such docs. A binary search is performed on top of the docs array  and such global ids are converted back to segment level ids (subtracting the context docbase) when scoring docs.  RescoreKnnVectoryQuery did not sort the array of docs which caused binary search to return non deterministic results  which in turn made us look up wrong docs  something using out of bound ids. One symptom of this was observed in a DFSProfilerIT test failure which triggered a Lucene assertion around doc id being outside of the range of the bitset of live docs.  The fix is to simply sort the score docs array before extracting docs ids and scores and providing them to KnnScoreDocQuery upon rewrite.  Relates to #116663  Closes #119711
elastic,elasticsearch,77f8558d0ba4c6f5b7966b481eb4651b02d03647,https://github.com/elastic/elasticsearch/commit/77f8558d0ba4c6f5b7966b481eb4651b02d03647,ESQL: Add description to status and profile (#121783)  This adds a `task_description` field to `profile` output and task `status`. This looks like: ``` ... "profile" : { "drivers" : [ { "task_description" : "final"  "start_millis" : 1738768795349  "stop_millis" : 1738768795405  ... "task_description" : "node_reduce"  "start_millis" : 1738768795392  "stop_millis" : 1738768795406  ... "task_description" : "data"  "start_millis" : 1738768795391  "stop_millis" : 1738768795404  ... ```  Previously you had to look at the signature of the operators in the driver to figure out what the driver is *doing*. You had to know enough about how ESQL works to guess. Now you can look at this description to see what the server *thinks* it is doing. No more manual classification.  This will be useful when debugging failures and performance regressions because it is much easier to use `jq` to group on it: ``` | jq '.profile[] | group_by(.task_description)[]' ```
elastic,elasticsearch,6a526755de4b560e6c4d9a211fb783723b1f2807,https://github.com/elastic/elasticsearch/commit/6a526755de4b560e6c4d9a211fb783723b1f2807,Use synthetic recovery source by default if synthetic source is enabled (#119110)  We experimented with using synthetic source for recovery and observed quite positive impact on indexing throughput by means of our nightly Rally benchmarks. As a result  here we enable it by default when synthetic source is used. To be more precise  if `index.mapping.source.mode` setting is `synthetic` we enable recovery source by means of synthetic source.  Moreover  enabling synthetic source recovery is done behind a feature flag. That would allow us to enable it in snapshot builds which in turn will allow us to see performance results in Rally nightly benchmarks.
elastic,elasticsearch,e1c6c3f9b2516574267000e33563d90c75e9d673,https://github.com/elastic/elasticsearch/commit/e1c6c3f9b2516574267000e33563d90c75e9d673,Configurable limit on concurrent shard closing (#121267)  Today we limit the number of shards concurrently closed by the `IndicesClusterStateService`  but this limit is currently a function of the CPU count of the node. On nodes with plentiful CPU but poor IO performance we may want to restrict this limit further. This commit exposes the throttling limit as a setting.
elastic,elasticsearch,0393e56fa72890d4b46aa5d2b5923a08faf27a69,https://github.com/elastic/elasticsearch/commit/0393e56fa72890d4b46aa5d2b5923a08faf27a69,ESQL: introduce a pre-mapping logical plan processing step (#121260)  This adds a pre-mapping logical plan processing step  occurring after the logical optimisation  but before mapping it to a physical plan. This step can perform async actions  if needed  and involves using a new `TransportActionServices` record with all available services.  Furthermore  the query rewriting step part of the `FullTextFunction`s planning (occurring on the coordinator only) is refactored a bit to update the queries in-place. The verification done by `Match` and `Term` involving checking on the argument type is also now pulled back from post-optimisation to post-analysis. Their respective tests are moved accordingly as well.
elastic,elasticsearch,2f3053d117e55d2afa6fb16e2513e645651ceaa2,https://github.com/elastic/elasticsearch/commit/2f3053d117e55d2afa6fb16e2513e645651ceaa2,Fix NPE in deprecation API (#121263)  **Reproduction path**  1. Create an composable index template that does not have a `template` section. 2. Set some settings in `deprecation.skip_deprecated_settings` 3. Run the deprecation API `GET _migration/deprecations?error_trace` 4. **Result:** we receive an error `500` with the following error and stack trace:  ``` .... "reason": "Cannot invoke \"org.elasticsearch.cluster.metadata.Template.settings()\" because \"template\" is null"  "stack_trace": "java.lang.NullPointerException: Cannot invoke \"org.elasticsearch.cluster.metadata.Template.settings()\" because \"template\" is null org.elasticsearch.xpack.deprecation.DeprecationInfoAction.lambda$removeSkippedSettings$9(DeprecationInfoAction.java:408) .... ```  **Fix** There was a typo when we were performing the null-check  we used `templateName` instead of `template`. In this PR we fix this and we extend the current test to capture this case as well.  The bug is not released so it's marked as a non-issue.
elastic,elasticsearch,d5bccfeca49cdc041bcfbfbb3cd037c05f701a6c,https://github.com/elastic/elasticsearch/commit/d5bccfeca49cdc041bcfbfbb3cd037c05f701a6c,Drier and faster SumAggregator and AvgAggregator (#120436)  Dried up (and moved to the much faster inline logic) for the summation here for both implementations. Obviously this could have been done even drier but it didn't seem like that was possible without a performance hit (we really don't want to sub-class the leaf-collector I think). Benchmarks suggest this variant is ~10% faster than the previous iteration of `SumAggregator` (probably from making the grow method smaller) and a bigger than that improvement for the `AvgAggregator`.
elastic,elasticsearch,fb5d364a633d6b11ee57bf85daa96003b33f3fe7,https://github.com/elastic/elasticsearch/commit/fb5d364a633d6b11ee57bf85daa96003b33f3fe7,Optimize indexing points with index and doc values set to true (#120271)  Introducing at LonPoint and XYPoint that can add doc values sto improve indexing perfromance.
elastic,elasticsearch,a8671278ff5a0591036ab4a7b936058df263b92f,https://github.com/elastic/elasticsearch/commit/a8671278ff5a0591036ab4a7b936058df263b92f,ESQL: extend TranslationAware to all pushable expressions (#120192)  This expands the `TranslationAware` interface for expressions that support translations to Lucene query and moves the implementations of these translations from a centralised place (ExpressionTranslators) to the respective expression classes.  `TranslationAware` has now a subinterface  `SingleValueTranslationAware`  for expressions that need to implement the single-value logic (`null` out on MVs). So the `SingleValueQuery` wrapping no longer needs to be performed explicitly by the implementer.  `TranslationAware` is now part of the `org.elasticsearch.xpack.esql.capabilities` package  together with the other interfaces that extensions needs to implement to be used by the core services (verifier and optimizer). To allow this  some logical nodes have been moved from core in the ESQL proper (where also `LucenePushdownPredicates` resides  used by `TranslationAware`).
elastic,elasticsearch,40c34cd896e9a5e60ca71b3e90bfa68c3822fc2f,https://github.com/elastic/elasticsearch/commit/40c34cd896e9a5e60ca71b3e90bfa68c3822fc2f,Optimize ST_EXTENT_AGG for geo_shape and cartesian_shape (#119889)  Support for `ST_EXTENT_AGG` was added in https://github.com/elastic/elasticsearch/pull/118829  and then partially optimized in https://github.com/elastic/elasticsearch/pull/118829. This optimization worked only for cartesian_shape fields  and worked by extracting the Extent from the doc-values and re-encoding it as a WKB `BBOX` geometry. This does not work for geo_shape  where we need to retain all 6 integers stored in the doc-values  in order to perform the datelline choice only at reduce time during the final phase of the aggregation.  Since both geo_shape and cartesian_shape perform the aggregations using integers  and the original Extent values in the doc-values are integers  this PR expands the previous optimization by: * Saving all Extent values into a multi-valued field in an IntBlock for both cartesian_shape and geo_shape * Simplifying the logic around merging intermediate states for all cases (geo/cartesian and grouped and non-grouped aggs) * Widening test cases for testing more combinations of aggregations and types  and fixing a few bugs found * Enhancing cartesian extent to convert from 6 ints to 4 ints at block loading time (for efficiency) * Fixing bugs in both cartesian and geo extents for generating intermediate state with missing groups (flaky tests in serverless) * Moved the int order to always match Rectangle for 4-int and Extent for 6-int cases (improved internal consistency)  Since the PR already changed the meaning of the invalid/infinite values of the intermediate state integers  it was already not compatible with the previous cluster versions. We disabled mixed-cluster testing to prevent errors as a result of that. This leaves us the opportunity to make further changes that are mixed-cluster incompatible  hence the decision to perform this consistency update now.
elastic,elasticsearch,7b8f545e19cb370e72281b33bdfeb0cc74fe2f2e,https://github.com/elastic/elasticsearch/commit/7b8f545e19cb370e72281b33bdfeb0cc74fe2f2e,Fix realtime get of nested fields with synthetic source (#119575)  Today  for get-from-translog operations  we only need to reindex the root document into an in-memory Lucene  as the _source is stored in the root document and is sufficient. However  synthesizing the source for nested fields requires both the root document and its child documents. This causes realtime-get operations (as well as update and update-by-query operations) to miss nested fields.  Another issue is that the translog operation is reindexed lazily during get-from-translog operations. As a result  two realtime-get operations can return slightly different outputs: one reading from the translog and the other from Lucene.  This change resolves both issues. However  addressing the second issue can degrade the performance of realtime-get and update operations. If slight inconsistencies are acceptable  the translog operation should be reindexed lazily instead.  Closes #119553
elastic,elasticsearch,7750cf5b94e05bf7ee1c5c5ce796d62b04407d88,https://github.com/elastic/elasticsearch/commit/7750cf5b94e05bf7ee1c5c5ce796d62b04407d88,Remove default auto_expand_replicas of lookup indices (#120073)  This change disables auto_expand_replicas on lookup indices to enhance the lookup join user experience. Users can  however  enable this setting at any time to optimize performance.
elastic,elasticsearch,c990377c955ffeca7d08235411a0c468bba1ac36,https://github.com/elastic/elasticsearch/commit/c990377c955ffeca7d08235411a0c468bba1ac36,ESQL: Limit memory usage of `fold` (#118602)  `fold` can be surprisingly heavy! The maximally efficient/paranoid thing would be to fold each expression one time  in the constant folding rule  and then store the result as a `Literal`. But this PR doesn't do that because it's a big change. Instead  it creates the infrastructure for tracking memory usage for folding as plugs it into as many places as possible. That's not perfect  but it's better.  This infrastructure limit the allocations of fold similar to the `CircuitBreaker` infrastructure we use for values  but it's different in a critical way: you don't manually free any of the values. This is important because the plan itself isn't `Releasable`  which is required when using a real CircuitBreaker. We could have tried to make the plan releasable  but that'd be a huge change.  Right now there's a single limit of 5% of heap per query. We create the limit at the start of query planning and use it throughout planning.  There are about 40 places that don't yet use it. We should get them plugged in as quick as we can manage. After that  we should look to the maximally efficient/paranoid thing that I mentioned about waiting for constant folding. That's an even bigger change  one I'm not equipped to make on my own.
elastic,elasticsearch,ad264f7bf392a563688b94744e630dfbc7c4aef6,https://github.com/elastic/elasticsearch/commit/ad264f7bf392a563688b94744e630dfbc7c4aef6,ESQL: Add interfaces to distribute the post-analysis verification (#119798)  This adds a PostAnalysisVerificationAware interface that allows an expression  plan or even command to perform post-analysis verifications "locally"  vs. having them centralized in the core verifier.
elastic,elasticsearch,01504dbde26142d02a6d6d54ee2fc3e9bc8fe9c4,https://github.com/elastic/elasticsearch/commit/01504dbde26142d02a6d6d54ee2fc3e9bc8fe9c4,ESQL: Do not fold in Range.foldable (#119766)  The upper and lower bounds can be incompatible  in which case the range is foldable to FALSE independently of the foldability of the field.  But we shouldn't try and fold the bounds to find out. Calling `foldable` really shouldn't already perform folding.  Instead  only perform the check for the invalid bounds if the bounds are already literals. This means that folding will still take place  but it requires the range's child expressions to be folded first.
elastic,elasticsearch,6ca7e755bd23537abbe4a2859782f9baa17bd0d6,https://github.com/elastic/elasticsearch/commit/6ca7e755bd23537abbe4a2859782f9baa17bd0d6,Add possibility to acquire permits on primary shards with different checks (#119794)  Since #42241 we check that the shard must be in a primary mode for acquiring a primary permit on it. We would like customize this check and an option to perform different checks before running the `onPermitAcquired` listener. For example  we would to skip the primary mode check when we acquire primary permits during recovering of a hollow indexing shard.  See ES-10487
elastic,elasticsearch,7804a25b8ed4019b3a3c073745d0dd4c1dd465e9,https://github.com/elastic/elasticsearch/commit/7804a25b8ed4019b3a3c073745d0dd4c1dd465e9,Refactor QueryBuilderResolver Rewrite Logic (#119740)  * Refactor QueryBuilderResolver Rewrite Logic  This commit improves the rewrite logic by switching to reference comparison for termination checks. While the existing implementation functions correctly  the rewrite contract is designed to compare references rather than performing a full object comparison  which is unnecessary.  Additionally  this change guarantees that only a single rewrite pass is executed per query builder.  * avoid getting the value  ---------  Co-authored-by: Elastic Machine <elasticmachine@users.noreply.github.com>
elastic,elasticsearch,c0553d472152f273961d144b3bf085b94a0cde9f,https://github.com/elastic/elasticsearch/commit/c0553d472152f273961d144b3bf085b94a0cde9f,Remove ChunkedToXContentBuilder  (#119310)  Reverts the introduction of the ChunkedToXContentBuilder to fix the various performance regressions it introduced and the theoretical impossibility of fixing its performance to rival that of the iterator based solution. With the exception of a few minor adjustments that came out of changes already made on top of the builder migration this simply returns to the previous implementations (and some of the stuff in that code could be done better with the utilities available now). I also verified that this solves the performance issues that we've been running into with the builder.  closes #118647  This reverts commit 918a9cc35ada3a348f0bd4ed24e7ab6f836d468e This reverts commit 8c378754 This reverts commit 11c2eb29 This reverts commit c3115156
elastic,elasticsearch,a5c57ba966cfd088b8d79fd51fe3fb35163b22a2,https://github.com/elastic/elasticsearch/commit/a5c57ba966cfd088b8d79fd51fe3fb35163b22a2,Adjust random_score default field to _seq_no field (#118671)  In an effort to improve performance and continue to provide unique seeded scores for documents in the same index  we are switching from _id to _seq_no.  Requiring a field that is "unique" for a field and to help with random scores is burdensome for the user. So  we should default to a unique field (per index) when the user provides a seed.  Using `_seq_no` should be better as:  - We don't have to grab stored fields values - Bytes used are generally smaller  Additionally this removes the deprecation warning.  Marking as "breaking" as it does change the scores & behavior  but the API provide is the same.
elastic,elasticsearch,0a6ce27825d15a7e8294a99ee1aca52eb05f8ae0,https://github.com/elastic/elasticsearch/commit/0a6ce27825d15a7e8294a99ee1aca52eb05f8ae0,Add ReindexDatastreamIndexAction (#116996)  Add an action to reindex a single index from a source index to a destination index. Unlike the reindex action  this action copies settings and mappings from the source index to the dest index before performing the reindex. This action is part of work to reindex data streams and will be called on each of the backing indices within a data stream.
elastic,elasticsearch,8bbc6b314149163be3fa26d7f9066cc79f68a866,https://github.com/elastic/elasticsearch/commit/8bbc6b314149163be3fa26d7f9066cc79f68a866,Suppress the for-loop warnings since it is a conscious performance choice. (#118530)
elastic,elasticsearch,730f42a293ca06560deae2dcfdc2c3943662ce2c,https://github.com/elastic/elasticsearch/commit/730f42a293ca06560deae2dcfdc2c3943662ce2c,Handle all exceptions in data nodes can match (#117469)  During the can match phase  prior to the query phase  we may have exceptions that are returned back to the coordinating node  handled gracefully as if the shard returned canMatch=true.  During the query phase  we perform an additional rewrite and can match phase to eventually shortcut the query phase for the shard. That needs to handle exceptions as well. Currently  an exception there causes shard failures  while we should rather go ahead and execute the query on the shard.  Instead of adding another try catch on consumers code  this commit adds exception handling to the method itself so that it can no longer throw exceptions and similar mistakes can no longer be made in the future.  At the same time  this commit makes the can match method more easily testable without requiring a full-blown SearchService instance.  Closes #104994
elastic,elasticsearch,34b7e60f7589f0fd01b04a5cf28a088a254ad295,https://github.com/elastic/elasticsearch/commit/34b7e60f7589f0fd01b04a5cf28a088a254ad295,Re-add ResolvedExpression wrapper (#118174)  This PR reapplies #114592 along with an update to remove the performance regression introduced with the original change.
elastic,elasticsearch,eb0a21efd8946df04a3f6f0457bb0a1b73b50bf0,https://github.com/elastic/elasticsearch/commit/eb0a21efd8946df04a3f6f0457bb0a1b73b50bf0,Speedup OsStats initialization (#118141)  Similar to other OS/FS type stats we can optimize here. Found this as a slowdown when profiling tests in a loop during test fixing. This helps node startup and maybe more importantly test performance. No need to initialize the stats eagerly when we can just get them as we load them the first time.
elastic,elasticsearch,c54d4b687f3658fadcb158dbe43befa1edcb0e38,https://github.com/elastic/elasticsearch/commit/c54d4b687f3658fadcb158dbe43befa1edcb0e38,Don't skip shards in coord rewrite if timestamp is an alias (#117271)  The coordinator rewrite has logic to skip indices if the provided date range filter is not within the min and max range of all of its shards. This mechanism is enabled for event.ingested and @timestamp fields  against searchable snapshots.  We have basic checks that such fields need to be of date field type  yet if they are defined as alias of a date field  their range will be empty  which indicates that the shards are empty  and the coord rewrite logic resolves the alias and ends up skipping shards that may have matching docs.  This commit adds an explicit check that declares the range UNKNOWN instead of EMPTY in these circumstances. The same check is also performed in the coord rewrite logic  so that shards are no longer skipped by mistake.
elastic,elasticsearch,79ce6e38728a7710f01f18d9769cd6941c2312f6,https://github.com/elastic/elasticsearch/commit/79ce6e38728a7710f01f18d9769cd6941c2312f6,Improve performance of H3.h3ToGeoBoundary (#117812)  There are two clear code paths depending if a h3 bin belongs to even resolutions (class II) or uneven resolutions (class III). especializing the code paths for each type leads to an improvement in performance.
elastic,elasticsearch,e90eb7ab0df06239a69a1945ca6ef5effc065433,https://github.com/elastic/elasticsearch/commit/e90eb7ab0df06239a69a1945ca6ef5effc065433,Improve halfbyte transposition performance  marginally improving bbq performance (#117350)  The transposition of the bits in half-byte queries for BBQ is pretty convoluted and slow. This commit greatly simplifies & improves performance for this small part of bbq queries and indexing.  Here are the results of a small JMH benchmark for this particular function.  ``` TransposeBinBenchmark.transposeBinNew     1024  thrpt    5  857.779 ± 44.031  ops/ms TransposeBinBenchmark.transposeBinOrig    1024  thrpt    5   94.950 ±  2.898  ops/ms ```  While this is a huge improvement for this small function  the impact at query and index time is only marginal. But  the code simplification itself is enough to warrant this change in my opinion.
elastic,elasticsearch,0e986c50e786985ae92aae725bb9f9e46428d442,https://github.com/elastic/elasticsearch/commit/0e986c50e786985ae92aae725bb9f9e46428d442,Make Async-Search cleanup handle multiple projects (MP-1742)  The `AsyncTaskMaintenanceService` deletes async search results after a period of time (default: 1h) This change makes this service work correctly when there are multiple projects (and potentially multiple async-search indices).  A single instance of the service runs on each data node (this is unchanged beahviour). The service checks each project for a copy of the `.async-search` index  where the primary shard#0 is on the local node (the only change here is to check each project) and then performs a delete-by-query on the `.async-search` index in that project (the change here is to make it project aware)
elastic,elasticsearch,7369c0818df0166ee18d50f5a1d9be0ba0bc005b,https://github.com/elastic/elasticsearch/commit/7369c0818df0166ee18d50f5a1d9be0ba0bc005b,Add new multi_dense_vector field for brute-force search (#116275)  This adds a new `multi_dense_vector` field that focuses on the maxSim usecase provided by Col[BERT|Pali].  Indexing vectors in HNSW as it stands makes no sense. Performance wise or for cost. However  we should totally support rescoring and brute-force search over vectors with maxSim.  This is step one of many. Behind a feature flag  this adds support for indexing any number of vectors of the same dimension.  Supports bit/byte/float.  Scripting support will be a follow up.  Marking as non-issue as its behind a flag and unusable currently.
elastic,elasticsearch,534de3a361e3283810264255a4c00b7f054bff1d,https://github.com/elastic/elasticsearch/commit/534de3a361e3283810264255a4c00b7f054bff1d,Make more of bulk action project-aware (MP-1752)  `TransportBulkAction.populateMissingTargets` is responsible for resolving the list of index-like targets need to have actions performed before the bulk ingestion can be processed. This includes - indices which need to created - data-streams which need to rollover - failure-stores that need to rollover  This change makes this method resolve the correct project rather than operating on the default project.
elastic,elasticsearch,c00abac6d1f14fe1cb793562d04c715f3b89b822,https://github.com/elastic/elasticsearch/commit/c00abac6d1f14fe1cb793562d04c715f3b89b822,Simplify AbstractSearchAsyncAction.doPerformPhaseOnShard (#116104)  1. No need to catch here any longer  we fixed the connection related exceptions separately now  throwing from an method that consumes a listener was smelly to begin with. 2. Not need to try-with-resources that `releasable`  just release it as soon as possible to get the next per-shard request going while we process a result. No need to waste time on an idle data node here.
elastic,elasticsearch,5cc2a47eaf4e4e74fe22e31e78f6645da170e73b,https://github.com/elastic/elasticsearch/commit/5cc2a47eaf4e4e74fe22e31e78f6645da170e73b,ESQL: Basic enrich-like lookup loading (#115667)  This adds super basic way to perform a lookup-style LEFT JOIN thing. It's *like* ENRICH  except it can use an index_mode=lookup index rather than an ENRICH policy. It's like a LEFT JOIN but it can't change the output cardinality. That's a genuinely useful thing!  This intentionally forks some portion of the ENRICH infrastructure and shares others. I *believe* these are the right parts to fork and the right parts to share. Namely: * We *share* the internal implementaions * We fork the request * We fork the configuration of what to join  This should allow us to iterate the on the requests without damaging anything in ENRICH but any speed ups that we build for these lookup joins *can* be shared with ENRICH if we decide that they work.  Relies on #115143
elastic,elasticsearch,e304c1d5c1dfd20c9b7ea4da3bf0560c0c82c1e9,https://github.com/elastic/elasticsearch/commit/e304c1d5c1dfd20c9b7ea4da3bf0560c0c82c1e9,ESQL: Speed up grouping by bytes (#114021)  This speeds up grouping by bytes valued fields (keyword  text  ip  and wildcard) when the input is an ordinal block: ``` bytes_refs 22.213 ± 0.322 -> 19.848 ± 0.205 ns/op (*maybe* real  maybe noise. still good) ordinal didn't exist   ->  2.988 ± 0.011 ns/op ``` I see this as 20ns -> 3ns  an 85% speed up. We never hard the ordinals branch before so I'm expecting the same performance there - about 20ns per op.  This also speeds up grouping by a pair of byte valued fields: ``` two_bytes_refs 83.112 ± 42.348  -> 46.521 ± 0.386 ns/op two_ordinals 83.531 ± 23.473  ->  8.617 ± 0.105 ns/op ``` The speed up is much better when the fields are ordinals because hashing bytes is comparatively slow.  I believe the ordinals case is quite common. I've run into it in quite a few profiles.
elastic,elasticsearch,7942f3eab080b053f97abbf6abef3619d515f32f,https://github.com/elastic/elasticsearch/commit/7942f3eab080b053f97abbf6abef3619d515f32f,Fix dim validation for bit element_type (#114533)  A silly bug has reared its ugly head. Apparently  our dimension validations are predicated on JSON parsing order  that is not good.  So  this commit adjusts the dim validations so that it is an actual validation  instead of something that occurs during parsing.  Additionally  I found that our custom formats were not overriding `getMaxDimensions` correctly. Typically  and in production  this isn't that big of a deal  but I have found it useful to do this for other testing purposes (so that we don't have to rely on the perfield codec for more direct and advanced testing).
elastic,elasticsearch,14f0b4840464816a149e0bc0bab0961040c40780,https://github.com/elastic/elasticsearch/commit/14f0b4840464816a149e0bc0bab0961040c40780,Improve performance of LongObjectPagedHashMap#removeAndAdd and ObjectObjectPagedHashMap#removeAndAdd (#114280)
elastic,elasticsearch,e129822f11befad0558773d8626e5286715dbff8,https://github.com/elastic/elasticsearch/commit/e129822f11befad0558773d8626e5286715dbff8,Improve performance of Int3Hash#removeAndAdd (#114383)
elastic,elasticsearch,58cc37922c159714c5c384a44cdf444686021e51,https://github.com/elastic/elasticsearch/commit/58cc37922c159714c5c384a44cdf444686021e51,Improve performance of LongLongHash#removeAndAdd (#114230)  remove some unnecessary manipulation of the keys in the method removeAndAdd.
elastic,elasticsearch,c4731aaf08949174e00a29fffc3ada326b0df14d,https://github.com/elastic/elasticsearch/commit/c4731aaf08949174e00a29fffc3ada326b0df14d,Improve performance of LongHash#removeAndAdd (#114199)  Remove unnecessary remove and later add of the key.
elastic,elasticsearch,052dbb4dacca29c3abf96e1b1579c569a2ec7095,https://github.com/elastic/elasticsearch/commit/052dbb4dacca29c3abf96e1b1579c569a2ec7095,Optimize error handling after lazy rollovers (#111572)  This commit improves the performance of the error-handling process after a lazy rollover or an index creation failed.
elastic,elasticsearch,5c91edda9f9fd2e0dd044e8b6f47a5b0465d4e95,https://github.com/elastic/elasticsearch/commit/5c91edda9f9fd2e0dd044e8b6f47a5b0465d4e95,ESQL: Speed up CASE for some parameters (#112295)  This speeds up the `CASE` function when it has two or three arguments and both of the arguments are constants or fields. This works because `CASE` is lazy so it can avoid warnings in cases like ``` CASE(foo != 0  2 / foo  1) ```  And  in the case where the function is *very* slow  it can avoid the computations.  But if the lhs  and rhs of the `CASE` are constant then there isn't any work to avoid.  The performance improvment is pretty substantial: ``` (operation)  Before   Error   After    Error  Units case_1_lazy  97.422 ± 1.048  101.571 ± 0.737  ns/op case_1_eager  79.312 ± 1.190    4.601 ± 0.049  ns/op ```  The top line is a `CASE` that has to be lazy - it shouldn't change. The 4 nanos change here is noise. The eager version improves by about 94%.
elastic,elasticsearch,d9e0cbeb59638cb7476942b4cb4b9ea857a6703e,https://github.com/elastic/elasticsearch/commit/d9e0cbeb59638cb7476942b4cb4b9ea857a6703e,Small performance improvement in h3 library (#113385)  Changing some FDIV's into FMUL's leads to performance improvements
elastic,elasticsearch,90e343cfef0c3bbe120c5fe629652b49190d6fa5,https://github.com/elastic/elasticsearch/commit/90e343cfef0c3bbe120c5fe629652b49190d6fa5,Use ChannelFutureListener in Netty code to reduce capturing lambdas (#112967)  Mainly motivated by simplifying the reference chains for Netty buffers and have easier to analyze heap dumps in some spots but also a small performance win in and of itself.
elastic,elasticsearch,73c40b9567a4a7b852ccec8daefc5722bdcb17c6,https://github.com/elastic/elasticsearch/commit/73c40b9567a4a7b852ccec8daefc5722bdcb17c6,Remove legacy validation of search source in data nodes (#113081)  We moved the validation of incoming search requests to data nodes with #105150. The legacy validation performed on the data nodes was left around for bw comp reasons  as there could still be coordinating nodes in the cluster not performing that validation. This is no longer the case in main. This commit removes the validation in favour of validation already performed while coordinating the search request.  Relates to #105150
elastic,elasticsearch,4a0ccbf4b4ac741b0d3a6aada4c8a407d92d563d,https://github.com/elastic/elasticsearch/commit/4a0ccbf4b4ac741b0d3a6aada4c8a407d92d563d,Fix verbose get data stream API not requiring extra privileges (#112973)  * Fix verbose get data stream API not requiring extra privileges  When a user uses the `GET /_data_stream?verbose` API to retrieve the verbose version of the response (which includes the `maximum_timestamp`  as added in #112303)  the response object should be performed with the same privilege-checking as the get-data-stream API  meaning that no extra priveleges should be required return the field.  This commit makes the Transport action use an entitled client so that extra privileges are not required  and adds a test to ensure that it works.  * Update docs/changelog/112973.yaml
elastic,elasticsearch,b8a24b16f8089f5128d9fabcdb5291217efa4207,https://github.com/elastic/elasticsearch/commit/b8a24b16f8089f5128d9fabcdb5291217efa4207,Spatial search functions support multi-valued fields in compute engine (#112063)  * Using enum to control mv-predicate combinations with ANY or ALL  * Update docs/changelog/112063.yaml  * Fix changelog  * Refactored to use generic MvCombiner for more flexibility  This opens the door to combiners which work with more types than just boolean.  * Spotless  and disabled failing cartesian-point tests  Reported the failing cases at https://github.com/elastic/elasticsearch/issues/112102  * Fix changelog with better summary  * Fix changelog with better summary and highlight text  * More spotless checks  * Remove low-value comment edit  * Refined MvCombiner to maintain state to deal with ST_CONTAINS  We have a special case in ST_CONTAINS in that lucenes triangle-tree implementation causes a situation where we need to reject contains results when there are other geometries that do not contain  but do intersect. This does not make sense from a pure geospatial perspective  but is a necessary consequence of the triangle-tree.  * Code review fixes  and fix for long doc-values MV  * Cleanup and fix fold() serialization  The fold was returning the intermediate ContainsResult  which cannot be serialized  instead of the correct final boolean result.  * Fix to multi-contains-multi case using BitArray  Since a multi-value field should be seen as an alternative to a geometry collection  it is insufficient to consider `ANY` for multi-value contains.  There are two approaches to this: * Pre-build a geometry collection before converting to docValuesReader * Maintain more state so we can assert that all components are contained within at least one of the field values  In an effort to minimize the changes to the generated code  the second approach was taken  and in fact was achievable without any changes at all to generated code.  However  this approach uses BigArrays  and does not get the correct one passed in. We need to change generated code a small bit to pass that in. We'll do that in a followup commit  but only if the alternative approach of creating a combined multi-value docValueReader is deemed more complex.  * Fix to multi-contains-multi case using GeometryCollection  This is an alternative approach to the previous one which used a BitArray to maintain state. Now we rely entirely on the internals of the DocValuesReader  and instead pre-create the GEOMETRYCOLLECTION of all the values in the multi-value field  so the triangle tree already considers the necessary combinations.  This approach moves the responsibility of iterating over the multi-value from the generated code into the non-generated code. In total the number of lines of code goes down  as fewer code paths are possible.  * Add addition fixed issue to changelog  * Added csv-spec tests for testing multi-valued geometries  * More tests for multi-value literals and one fix in fold()  * Fix bug with doc values extraction for non-indexed fields for centroid  Initially this work was about adding more tests  but discovered the bug at #112505. This commit fixes hat issue and expands the tests in a few areas: * PhysicalPlanOptimizerTests expanded to verify that physical planning now considers if the field has doc-values * SpatialPushDownPointsTestCase simple point-in-polygon tests expanded to consider ST_CENTROID as well  so that this behaviour is tested better there  * Note that this PR also fixes the doc-values field extract bug  This could have been fixed in a separate PR  but fixing it here was needed because the tests we wrote were failing without it.  * Multi-point test cases  * Added capability to prevent test failing on older clusters  Also removed a test that was sensitive to multi-node cluster results ordering  * Support BlockBuilder multivalue combining for ST_WITHIN  This is similar too  but simpler than the ST_CONTAINS solution. In addition we added support for two fields to handle multi-values by using ST_CONTAINS surrogate with parameters swapped.  * Require capability for BWC tests  * Added multivalue fields tests for points  * Support multivalues for CONTAINS/WITHIN between two fields  This included taking into account that CONTAINS and WITHIN are not symmetrical in the case that the indexed geometry contains multiple intersecting polygons.  We need to document this behaviour.  * Small optimization to not create collections over single geometries  * Simplification of iterating over multi-value BytesRef  * Update docs/changelog/112063.yaml  * Update docs/changelog/112063.yaml  * Added back removed bug-fix link  * Merge conflict  * Support point doc-values for ST_WITHIN  * Simplify ST_CONTAINS to not consider intersecting polygons  This turns out to already be handled by combined doc-values  * Last CONTAINS evaluators moved to BlockBuilder approach  * Revert usage of MyCombiner in spatial predicates  Since ST_CONTAINS and ST_WITHIN could not use the ANY/ALL logic and needed to first collect all values into a single geometry before applying the predicate  we decided to move ST_INTERSECTS and ST_DISJOINT to this same approach so all spatial predicates have the same level of complexity and are easier to maintain.  * Revert ability to perform ANY/ALL predicate evaluations  This was only being used by the spatial predicates  and since they have reverted to doing this logic internally  we remove this capability from the code-base. If we wish to implement ANY/ALL logic in any other predicates  this could be brought back by reverting this commit.  * Simplify code paths for evaluators  Now that all evaluators use the Block.Builder approach we can move all the common code down to the SpatialRelations class.  This means that all static evaluator methods now contain only a single line of code  and all of them are identical between all four spatial functions  making comparison and maintenance much easier.  * Cleanup code for easier review  * Fixed bug with empty multivalue params and doc-values  This was failing a test in ENRICH  * After renaming the evaluator parameters we need to update the unit tests
elastic,elasticsearch,d7cc4074175f9a91b1a455b1d31bdfb050215aa2,https://github.com/elastic/elasticsearch/commit/d7cc4074175f9a91b1a455b1d31bdfb050215aa2,ESQL: Compute support for filtering ungrouped aggs (#112717)  Adds support to the compute engine for filtering which positions are processed by ungrouping aggs. This should allow syntax like:  ``` | STATS success = COUNT(*) WHERE 200 <= response_code AND response_code < 300  redirect = COUNT(*) WHERE 300 <= response_code AND response_code < 400  client_err = COUNT(*) WHERE 400 <= response_code AND response_code < 500  server_err = COUNT(*) WHERE 500 <= response_code AND response_code < 600  total_count = COUNT(*) ```  We could translate the WHERE expression into an `ExpressionEvaluator` and run it  then plug it into the filtering support added in this PR.  The actual filtering is done by creating a `FilteredAggregatorFunction` which wraps a regular `AggregatorFunction` first executing the filter against the incoming `Page` and then passing the resulting mask to the `AggregatorFunction`. We've then added a `mask` to `AggregatorFunction#process` which each aggregation function must use for filtering.  We keep the unfiltered behavior by sending a constant block with `true` in it. Each agg detects this and takes an "unfiltered" path  preserving the original performance.  Importantly  when you don't turn this on it doesn't effect performance:  ``` (blockType)  (grouping)   (op)  Score    Error -> Score    Error  Units vector_longs        none  count  0.007 ±  0.001 -> 0.007 ±  0.001  ns/op vector_longs        none    min  0.123 ±  0.004 -> 0.128 ±  0.005  ns/op vector_longs       longs  count  4.311 ±  0.192 -> 4.218 ±  0.053  ns/op vector_longs       longs    min  5.476 ±  0.077 -> 5.451 ±  0.074  ns/op ```
elastic,elasticsearch,01fb50142e9c8ad0a2de13a2f0311c9c50604b57,https://github.com/elastic/elasticsearch/commit/01fb50142e9c8ad0a2de13a2f0311c9c50604b57,Speedup HealthNodeTaskExecutor (#112558)  The introduction of this class introduced a significant regression in cluster state update performance and increased test execution times visibly. The `clusterHasFeature` check is very expensive  lets do it laster and do the effectively free checks first.
elastic,elasticsearch,2a9e47458bb394cd9d08c5a3d8b08156f23b4f66,https://github.com/elastic/elasticsearch/commit/2a9e47458bb394cd9d08c5a3d8b08156f23b4f66,Rework fix for stale data in synthetic source to improve performance (#112480)
elastic,elasticsearch,3a8edf51977ceed59ea2763dd8341df74685e0c8,https://github.com/elastic/elasticsearch/commit/3a8edf51977ceed59ea2763dd8341df74685e0c8,Fix O(N) list building in TransportSearchAction.asyncSearchExecutor (#112474)  This at least avoids the O(N) list building which is needlessly heavy for large index counts. Not sure the logic makes perfect sense in all cases  but it should remain practically unchanged for now (except when there's more than 2 indices and they're all system ones).
elastic,elasticsearch,f0d7b006043075736cd81310393aab2de0eb4e25,https://github.com/elastic/elasticsearch/commit/f0d7b006043075736cd81310393aab2de0eb4e25,Adapt auto-exand replicas for multi project (MP-1626)  This changes `AutoExpandReplicas` and `AllocationService` to handle multiple projects. Replica auto-expansion is performed across all projects.
elastic,elasticsearch,306491aa9dc80176ddde58902caa7b2d87f0e178,https://github.com/elastic/elasticsearch/commit/306491aa9dc80176ddde58902caa7b2d87f0e178,Fix a few toString implementations+usages that affect test performance (#112380)  No need to precompute the toString for `ActionListener` and `Releasable`  that's quite expensive at times. Also string concat is way faster than formating these days  so use that in the transport channels. Lastly  short-circuit some obvious spots in network address serialization and remove code that duplicates the JDK (remove the IPV4 specific forbidden API because it makes no sense  but still needed to disable the check to make the build green because of the exclude on the parent class).
elastic,elasticsearch,c05f7e9c81169f710be92ec7c913118aa0982902,https://github.com/elastic/elasticsearch/commit/c05f7e9c81169f710be92ec7c913118aa0982902,ESQL: Add way for `Block` to `keepMask` (#112160)  This adds a `Block#keepMask(BooleanVector)` method that will make a new block  keeping all of the values where the vector is `true` and `null`ing all of the velues where the vector is false.  This will be useful for implementing partial aggregation application like `| STATS MAX(a WHERE b > 1)  MIN(j WHERE b > 2) BY bar`. Or however the syntax ends up being. We already skip `null` group keys and we can evaluate the `b > 2` bits to a mask pretty easily. It should also be useful in optimizing `CASE(a > 2  foo)` - but only when the RHS of the CASE is `null` and the LHS is a constant or constant-like.  This is something that's very optimize-able. I haven't really optimized it in this PR  but it should be possible to speed this up a ton and remove a lot of copying. Here's where the benchmarks start: ``` (dataTypeAndBlockKind)  Mode  Cnt  Score   Error  Units int/array  avgt    7  3.705 ± 0.153  ns/op int/vector  avgt    7  3.234 ± 0.078  ns/op ```  That's about the same speed as reading the block. In a few of these cases I expect we can get them to constant performance rather than per-record performance.
elastic,elasticsearch,a02dc7165c75f12701f8d47a2bdefe5283735267,https://github.com/elastic/elasticsearch/commit/a02dc7165c75f12701f8d47a2bdefe5283735267,Improve performance of grok pattern cycle detection (#111947)
elastic,elasticsearch,17339198d8ef563661f6189853130ea968cb76fe,https://github.com/elastic/elasticsearch/commit/17339198d8ef563661f6189853130ea968cb76fe,Allow legacy_* index.codec options to be configured. (#111867)  For escape hatch reasons when zstd unexpectedly worse performance.
elastic,elasticsearch,70dfb5216bab5f17cdbed1e0153e0df13d0ca6de,https://github.com/elastic/elasticsearch/commit/70dfb5216bab5f17cdbed1e0153e0df13d0ca6de,Speedup InternalEngine setup (#111801)  We can speed up the setup of the InternalEngine quite a bit  mostly to help test performance by not re-reading the system properties over and over and saving deserializing the latest commit info redundantly in the constructor.
elastic,elasticsearch,d8b5fa356868a490261821d46e1a9c99ebb19859,https://github.com/elastic/elasticsearch/commit/d8b5fa356868a490261821d46e1a9c99ebb19859,Change default project id to "default" (MP-1599)  This commit changes the behaviour of `Metadata` so: - The "default" or "single" project is the project with the id `default` (it no longer tracks the cluster UUID) - A default project is only created when absolutely needed  this includes: - When attempting to perform project-scoped operations on the cluster level `Metadata` - When constructing a `Metadata` object from a `Builder` that does not have any projects in it.
elastic,elasticsearch,9fbdfcf650fe817a39b3e9a9ada1a42970fdeb0c,https://github.com/elastic/elasticsearch/commit/9fbdfcf650fe817a39b3e9a9ada1a42970fdeb0c,Fix unnecessary mustache template evaluation (#110986)  Addresses the performance issue in the date ingest processor where Mustache template evaluation is unnecessarily applied inside a loop. The timezone and locale templates are now evaluated once before the loop  improving efficiency.  closes #110191 --------- Co-authored-by: Joe Gallo <joegallo@gmail.com>
elastic,elasticsearch,3e6b0612803e6901df5ebc72bf9dc0e15c4d0e08,https://github.com/elastic/elasticsearch/commit/3e6b0612803e6901df5ebc72bf9dc0e15c4d0e08,Fix logsdb mapping rest tests on serverless (#110900)  Currently fails due to validation that is only performed in serverless:  ``` java.lang.AssertionError: Failure at [logsdb/20_mapping:94]: Expected: "Failed to parse mapping: Indices with with index mode [logs] only support synthetic source" but: was "Failed to parse mapping: Parameter [mode=disabled] is not allowed in source" ```
elastic,elasticsearch,85bc57b4ab2b5e0cacb912af6b7dacd36595654b,https://github.com/elastic/elasticsearch/commit/85bc57b4ab2b5e0cacb912af6b7dacd36595654b,Add size_in_bytes to enrich cache stats (#110578)  As preparation for #106081  this PR adds the `size_in_bytes` field to the enrich cache. This field is calculated by summing the ByteReference sizes of all the search hits in the cache. It's not a perfect representation of the size of the enrich cache on the heap  but some experimentation showed that it's quite close.
elastic,elasticsearch,5f8fd74073577e4a47e6513bdc7bf398b5f7d9aa,https://github.com/elastic/elasticsearch/commit/5f8fd74073577e4a47e6513bdc7bf398b5f7d9aa,[Transform] log search payload for preview (#110653)  A quick change to help debug search requests performed by the validate and preview API.  Co-authored-by: Elastic Machine <elasticmachine@users.noreply.github.com>
elastic,elasticsearch,5b8a62960764bdcc3cb2b9204ad8cffb12bd95a3,https://github.com/elastic/elasticsearch/commit/5b8a62960764bdcc3cb2b9204ad8cffb12bd95a3,Deduplicate FieldInfo attributes and field names (#110561)  We can use a similar strategy to what worked with mappers+settings and reuse the string deduplicator to deal with a large chunk (more than 70% from heap dumps we've seen in production)  of the `FieldInfo` duplication overhead without any Lucene changes. There's generally only a very limited number of attribute maps out there and the "dedup up to 100" logic in here deals with all scenarios I have observed in the wild thus far. As a side effect of deduplicating the field name and always working with an interned string now  I would expect the performance of field caps filtering for empty fields to improve measurably.
elastic,elasticsearch,dfcb822aeebdd63936aa2dd38a943917b1b00762,https://github.com/elastic/elasticsearch/commit/dfcb822aeebdd63936aa2dd38a943917b1b00762,Fix bug in union-types with type-casting in grouping key of STATS (#110476)  * Allow auto-generated type-cast fields in CsvTests  This allows  for example  a csv-spec test result header like `client_ip::ip:ip`  which is generated with a command like `STATS count=count(*) BY client_ip::ip`  It is also a small cleanup of the header parsing code  since it was using Strings.split() in an odd way.  * Fix bug in union-types with type-casting in grouping key of STATS  * Update docs/changelog/110476.yaml  * Added casting_operator required capability  Using the new `::` syntax requires disabling support for older versions in multi-cluster tests.  * Added more tests for inline stats over long/datetime  * Trying to fix the STATS...STATS bug  This makes two changes:  * Keeps the Alias in the aggs.aggregates from the grouping key  so that ReplaceStatsNestedExpressionWithEval still works * Adds explicit support for union-types conversion at grouping key loading in the ordinalGroupingOperatorFactory  Neither fix the particular edge case  but do seem correct  * Added EsqlCapability for this change  So that mixed cluster tests don't fail these new queries.  * Fix InsertFieldExtract for union types  Union types require a FieldExtractExec to be performed first thing at the bottom of local physical plans.  In queries like ``` from testidx* | eval x = to_string(client_ip) | stats c = count(*) by x | keep c ``` The `stats` has the grouping `x` but the aggregates get pruned to just `c`. In cases like this  we did not insert a FieldExtractExec  which this fixes.  * Revert query that previously failed  With Alex's fix  this query now passes.  * Revert integration of union-types to ordinals aggregator  This is because we have not found a test case that actually demonstrates this is necessary.  * More tests that would fail without the latest fix  * Correct code style  * Fix failing case when aggregating on union-type with invalid grouping key  * Capabilities restrictions on the new YML tests  * Update docs/changelog/110476.yaml  ---------  Co-authored-by: Alexander Spies <alexander.spies@elastic.co>
elastic,elasticsearch,1bb58ccff0c88cc1066f8477ab7442b8e91385a0,https://github.com/elastic/elasticsearch/commit/1bb58ccff0c88cc1066f8477ab7442b8e91385a0,Fix logsdb mapping rest tests on serverless (#110900)  Currently fails due to validation that is only performed in serverless:  ``` java.lang.AssertionError: Failure at [logsdb/20_mapping:94]: Expected: "Failed to parse mapping: Indices with with index mode [logs] only support synthetic source" but: was "Failed to parse mapping: Parameter [mode=disabled] is not allowed in source" ```
elastic,elasticsearch,86727a8741509b2029a0192c7dcfa55d835ed917,https://github.com/elastic/elasticsearch/commit/86727a8741509b2029a0192c7dcfa55d835ed917,Add size_in_bytes to enrich cache stats (#110578)  As preparation for #106081  this PR adds the `size_in_bytes` field to the enrich cache. This field is calculated by summing the ByteReference sizes of all the search hits in the cache. It's not a perfect representation of the size of the enrich cache on the heap  but some experimentation showed that it's quite close.
elastic,elasticsearch,4ec94be1df93ec40e67484d77533f2eb62383bf4,https://github.com/elastic/elasticsearch/commit/4ec94be1df93ec40e67484d77533f2eb62383bf4,[Transform] log search payload for preview (#110653)  A quick change to help debug search requests performed by the validate and preview API.  Co-authored-by: Elastic Machine <elasticmachine@users.noreply.github.com>
elastic,elasticsearch,67da6ba645afc5a6c9bf5478ce3e92a7d8cb7d08,https://github.com/elastic/elasticsearch/commit/67da6ba645afc5a6c9bf5478ce3e92a7d8cb7d08,Deduplicate FieldInfo attributes and field names (#110561)  We can use a similar strategy to what worked with mappers+settings and reuse the string deduplicator to deal with a large chunk (more than 70% from heap dumps we've seen in production)  of the `FieldInfo` duplication overhead without any Lucene changes. There's generally only a very limited number of attribute maps out there and the "dedup up to 100" logic in here deals with all scenarios I have observed in the wild thus far. As a side effect of deduplicating the field name and always working with an interned string now  I would expect the performance of field caps filtering for empty fields to improve measurably.
elastic,elasticsearch,35c44f7ade82d08ac50b654ff899b7c2ed20f44d,https://github.com/elastic/elasticsearch/commit/35c44f7ade82d08ac50b654ff899b7c2ed20f44d,Fix bug in union-types with type-casting in grouping key of STATS (#110476)  * Allow auto-generated type-cast fields in CsvTests  This allows  for example  a csv-spec test result header like `client_ip::ip:ip`  which is generated with a command like `STATS count=count(*) BY client_ip::ip`  It is also a small cleanup of the header parsing code  since it was using Strings.split() in an odd way.  * Fix bug in union-types with type-casting in grouping key of STATS  * Update docs/changelog/110476.yaml  * Added casting_operator required capability  Using the new `::` syntax requires disabling support for older versions in multi-cluster tests.  * Added more tests for inline stats over long/datetime  * Trying to fix the STATS...STATS bug  This makes two changes:  * Keeps the Alias in the aggs.aggregates from the grouping key  so that ReplaceStatsNestedExpressionWithEval still works * Adds explicit support for union-types conversion at grouping key loading in the ordinalGroupingOperatorFactory  Neither fix the particular edge case  but do seem correct  * Added EsqlCapability for this change  So that mixed cluster tests don't fail these new queries.  * Fix InsertFieldExtract for union types  Union types require a FieldExtractExec to be performed first thing at the bottom of local physical plans.  In queries like ``` from testidx* | eval x = to_string(client_ip) | stats c = count(*) by x | keep c ``` The `stats` has the grouping `x` but the aggregates get pruned to just `c`. In cases like this  we did not insert a FieldExtractExec  which this fixes.  * Revert query that previously failed  With Alex's fix  this query now passes.  * Revert integration of union-types to ordinals aggregator  This is because we have not found a test case that actually demonstrates this is necessary.  * More tests that would fail without the latest fix  * Correct code style  * Fix failing case when aggregating on union-type with invalid grouping key  * Capabilities restrictions on the new YML tests  * Update docs/changelog/110476.yaml  ---------  Co-authored-by: Alexander Spies <alexander.spies@elastic.co>
elastic,elasticsearch,5409aa7dcf1de1db3938dc30c942935c4b959149,https://github.com/elastic/elasticsearch/commit/5409aa7dcf1de1db3938dc30c942935c4b959149,Support mixed aggregates in METRICS (#110206)  This pull request supports mixed aggregates in the METRICS command. Non-rate aggregates will be rewritten as a pair of `to_partial` and `from_partial` aggregates:  - The `to_partial` aggregates will be executed in the first pass and always produce an intermediate output regardless of the aggregate mode.  - The `from_partial` aggregates will be executed in the second pass and always receive the intermediate output produced by `to_partial`.  Example:  **METRICS k8s max(rate(request))  max(memory_used)** becomes:  ``` METRICS k8s | STATS rate(request)  $p1=to_partial(max(memory_used)) BY _tsid | STATS max(`rate(request)`)  `max(memory_used)` = from_partial($p1  max($_)) ```  **METRICS k8s max(rate(request))  avg(memory_used) BY host** becomes:  ``` METRICS k8s | STATS rate(request)  $p1=to_partial(sum(memory_used))  $p2=to_partial(count(memory_used))  values(host) BY _tsid | STATS max(`rate(request)`)  $sum=from_partial($p1  sum($_))  $count=from_partial($p2  count($_)) BY host=`values(host)` | EVAL `avg(memory_used)` = $sum / $count | KEEP `max(rate(request))`  `avg(memory_used)`  host ```  **METRICS k8s min(memory_used)  sum(rate(request)) BY pod  bucket(@timestamp  5m)** becomes:  ``` METRICS k8s | EVAL `bucket(@timestamp  5m)` = datetrunc(@timestamp  '5m') | STATS rate(request)  $p1=to_partial(min(memory_used))  VALUES(pod) BY _tsid  `bucket(@timestamp  5m)` | STATS sum(`rate(request)`)  `min(memory_used)` = from_partial($p1  min($)) BY pod=`VALUES(pod)`  `bucket(@timestamp  5m)` | KEEP `min(memory_used)`  `sum(rate(request))`  pod  `bucket(@timestamp  5m)` ``` ---- I also took a different approach for this. The alternative is to extend the runtime to support scatter/gather via exchange. We could have two pipelines: one aggregate grouped by _tsid (and time bucket)  and another grouped by the user-specified keys. These pipelines expand to fill necessary blocks so that they have the same output. However  this requires replicating most of the aggregate rules for dual aggregates.  Hence  I opted for the approach in this PR  which doesn't change anything with non-metrics  making it safer. However  the dual aggregates should have better performance and use less memory than the approach in this PR.  Relates #109979
elastic,elasticsearch,97651dfb9f89cadffa377caba4fd87ebcda075b2,https://github.com/elastic/elasticsearch/commit/97651dfb9f89cadffa377caba4fd87ebcda075b2,Support rate aggregation in ES|QL (#109979)  Rate aggregation is special because it must be computed per time series  regardless of the grouping keys. The keys must be `_tsid` or a pair of `_tsid` and `time_bucket`. To support user-defined grouping keys  we first execute the rate aggregation using the time-series keys  then perform another aggregation with the resulting rate using the user-specific keys.  This PR translates the aggregates in the METRICS commands to standard aggregates. This approach helps avoid introducing new plans and operators for metrics aggregations only.  Examples:  **METRICS k8s max(rate(request))** becomes: ``` METRICS k8s | STATS rate(request) BY _tsid | STATS max(`rate(request)`) ```  **METRICS k8s max(rate(request)) BY host** becomes: ``` METRICS k8s | STATS rate(request)  VALUES(host) BY _tsid | STATS max(`rate(request)`) BY host=`VALUES(host)` ```  **METRICS k8s avg(rate(request)) BY host** becomes: ``` METRICS k8s | STATS rate(request)  VALUES(host) BY _tsid | STATS sum=sum(`rate(request)`)  count(`rate(request)`) BY host=`VALUES(host)` | EVAL `avg(rate(request))` = `sum(rate(request))` / `count(rate(request))` | KEEP `avg(rate(request))`  host ```  **METRICS k8s avg(rate(request)) BY host  time_bucket=bucket(\@timestamp  1minute)**  becomes:  ``` METRICS k8s | EVAL  `bucket(@timestamp  1minute)`=datetrunc(@timestamp  1minute) | STATS rate(request)  VALUES(host) BY _tsid `bucket(@timestamp  1minute)` | STATS sum=sum(`rate(request)`)  count(`rate(request)`) BY host=`VALUES(host)`  `bucket(@timestamp  1minute)` | EVAL `avg(rate(request))` = `sum(rate(request))` / `count(rate(request))` | KEEP `avg(rate(request))`  host  `bucket(@timestamp  1minute)` ```
elastic,elasticsearch,c310256af6a0411b8d063eab8cdb87da158ad2ea,https://github.com/elastic/elasticsearch/commit/c310256af6a0411b8d063eab8cdb87da158ad2ea,Register rate aggregation function in snapshot (#109983)  Rate aggregation is special. Regardless of the grouping keys  it must be computed per time series. This requires the keys to be _tisd or a pair of _tisd and time_bucket. To support user-defined grouping keys  we first execute the rate aggregation using the time-series keys. Then  we perform another aggregation with the resulting rate using the user-specific keys. Therefore  the rate aggregation will not be available in regular FROM commands. This change registers the rate aggregation function in the snapshot function registry  enabling us to support it in METRICS commands in subsequent pull requests.  Spin-off from #109979
elastic,elasticsearch,42e1a1be0114a4bbb5908e0843c218e1c95934ac,https://github.com/elastic/elasticsearch/commit/42e1a1be0114a4bbb5908e0843c218e1c95934ac,Remove time_interval from time series source operator (#109982)  This change removes the time_bucket from the time-series source operator. This should simplify the planner. Running a separate date trunc eval specified by bucket should yield the same output and performance.  Spin-off from #109979
elastic,elasticsearch,d1e3c0afc419a73bdb7b6305e0663f81e80cec2e,https://github.com/elastic/elasticsearch/commit/d1e3c0afc419a73bdb7b6305e0663f81e80cec2e,ESQL: Union Types Support (#107545)  * Union Types Support  The second prototype replaced MultiTypeField.Unresolved with MultiTypeField  but this clashed with existing behaviour around mapping unused MultiTypeFields to `unsupported` and `null`  so this new attempt simply adds new fields  resulting in more than one field with the same name. We still need to store this new field in EsRelation  so that physical planner can insert it into FieldExtractExec  so this is quite similar to the second protototype.  The following query works in this third prototype:  ``` multiIndexIpString FROM sample_data* METADATA _index | EVAL client_ip = TO_IP(client_ip) | KEEP _index  @timestamp  client_ip  event_duration  message | SORT _index ASC  @timestamp DESC ```  As with the previous prototyep  we no longer need an aggregation to force the conversion function onto the data node  as the 'real' conversion is now done at field extraction time using the converter function previously saved in the EsRelation and replanned into the EsQueryExec.  Support row-stride-reader for LoadFromMany  Add missing ESQL version after rebase on main  Fixed missing block release  Simplify UnresolvedUnionTypes  Support other commands  notably WHERE  Update docs/changelog/107545.yaml  Fix changelog  Removed unused code  Slight code reduction in analyser of union types  Removed unused interface method  Fix bug in copying blocks (array overrun)  Convert MultiTypeEsField.UnresolvedField back to InvalidMappedField  This is to ensure older behaviour still works.  Simplify InvalidMappedField support  Rather than complex code to recreate InvalidMappedField from MultiTypeEsField.UnresolvedField  we rely on the fact that this is the parent class anyway  so we can resolve this during plan serialization/deserialization anyway. Much simpler  Simplify InvalidMappedField support further  Combining InvalidMappedField and MultiTypeEsField.UnresolvedField into one class simplifies plan serialization even further.  InvalidMappedField is used slightly differently in QL  We need to separate the aggregatable used in the original really-invalid mapped field from the aggregatable used if the field can indeed be used as a union-type in ES|QL.  Updated version limitation after 8.14 branch  Try debug CI failures in multi-node clusters  Support type conversion in rowstride reader on single leaf  Disable union_types from CsvTests  Keep track of per-shard converters for LoadFromMany  Simplify block loader convert function  Code cleanup  Added unit test for ValuesSourceReaderOperator including field type conversions at block loading  Added test for @timestamp and fixed related bug  It turns out that most  but not all  DataType values have the same esType as typeName  and @timestamp is one that does not  using `date` for esType and `datetime` for typename. Our EsqlIndexResolver was recording multi-type fields with `esType`  while later the actual type conversion was using an evaluator that relied on DataTypes.typeFromName(typeName). So we fixed the EsqlIndexResolver to rather use typeName.  Added more tests  with three indices combined and two type conversions  Disable lucene-pushdown on union-type fields  Since the union-type rewriter replaced conversion functions with new FieldAttributes  these were passing the check for being possible to push-down  which was incorrect. Now we prevent that.  Set union-type aggregatable flag to false always  This simplifies the push-down check.  Fixed tests after rebase on main  Add unit tests for union-types (same field  different type)  Remove generic warnings  Test code cleanup and clarifying comments  Remove -IT_tests_only in favor of CsvTests assumeFalse  Improved comment  Code review updates  Code review updates  Remove changes to ql/EsRelation  And it turned out the latest version of union type no longer needed these changes anyway  and was using the new EsRelation in the ESQL module without these changes.  Port InvalidMappedField to ESQL  Note  this extends the QL version of InvalidMappedField  so is not a complete port. This is necessary because of the intertwining of QL IndexResolver and EsqlIndexResolver. Once those classes are disentangled  we can completely break InvalidMappedField from QL and make it a forbidden type.  Fix capabilities line after rebase on main  Revert QL FieldAttribute and extend with ESQL FieldAttribute  So as to remove any edits to QL code  we extend FieldAttribute in the ESQL code with the changes required  since is simply to include the `field` in the hascode and equals methods.  Revert "Revert QL FieldAttribute and extend with ESQL FieldAttribute"  This reverts commit 168c6c75436e26b83e083cd3de8e18062e116bc9.  Switch UNION_TYPES from EsqlFeatures to EsqlCapabilities  Make hashcode and equals aligned  And removed unused method from earlier union-types work where we kept the NodeId during re-writing (which we no longer do).  Replace required_feature with required_capability after rebase  Switch union_types capability back to feature  because capabilities do not work in mixed clusters  Revert "Switch union_types capability back to feature  because capabilities do not work in mixed clusters"  This reverts commit 56d58bedf756dbad703c07bf4cdb991d4341c1ae.  Added test for multiple columns from same fields  Both IP and Date are tested  Fix bug with incorrectly resolving invalid types  And added more tests  Fixed bug with multiple fields of same name  This fix simply removes the original field already at the EsRelation level  which covers all test cases but has the side effect of having the final field no-longer be unsupported/null when the alias does not overwrite the field with the same name. This is not exactly the correct semantic intent. The original field name should be unsupported/null unless the user explicitly overwrote the name with `field=TO_TYPE(field)`  which effectively deletes the old field anyway.  Fixed bug with multiple conversions of the same field  This also fixes the issue with the previous fix that incorrectly reported the converted type for the original field.  More tests with multiple fields and KEEP/DROP combinations  Replace skip with capabilities in YML tests  Fixed missing ql->esql import change afer merging main  Merged two InvalidMappedField classes  After the QL code was ported to esql.core  we can now make the edits directly in InvalidMappedField instead of having one extend the other.  Move FieldAttribute edits from QL to ESQL  ESQL: Prepare analyzer for LOOKUP (#109045)  This extracts two fairly uncontroversial changes that were in the main LOOKUP PR into a smaller change that's easier to review.  ESQL: Move serialization for EsField (#109222)  This moves the serialization logic for `EsField` into the `EsField` subclasses to better align with the way rest of Elasticsearch works. It also switches them from ESQL's home grown `writeNamed` thing to `NamedWriteable`. These are wire compatible with one another.  ESQL: Move serialization of `Attribute` (#109267)  This moves the serialization of `Attribute` classes used in ESQL into the classes themselves to better line up with the rest of Elasticsearch.  ES|QL: add MV_APPEND function (#107001)  Adding `MV_APPEND(value1  value2)` function  that appends two values creating a single multi-value. If one or both the inputs are multi-values  the result is the concatenation of all the values  eg.  ``` MV_APPEND([a  b]  [c  d]) -> [a  b  c  d] ```  ~I think for this specific case it makes sense to consider `null` values as empty arrays  so that~ ~MV_APPEND(value  null) -> value~ ~It is pretty uncommon for ESQL (all the other functions  apart from `COALESCE`  short-circuit to `null` when one of the values is null)  so let's discuss this behavior.~  [EDIT] considering the feedback from Andrei  I changed this logic and made it consistent with the other functions: now if one of the parameters is null  the function returns null  [ES|QL] Convert string to datetime when the other size of an arithmetic operator is date_period or time_duration (#108455)  * convert string to datetime when the other side of binary operator is temporal amount  ESQL: Move `NamedExpression` serialization (#109380)  This moves the serialization for the remaining `NamedExpression` subclass into the class itself  and switches all direct serialization of `NamedExpression`s to `readNamedWriteable` and friends. All other `NamedExpression` subclasses extend from `Attribute` who's serialization was moved ealier. They are already registered under the "category class" for `Attribute`. This also registers them as `NamedExpression`s.  ESQL: Implement LOOKUP  an "inline" enrich (#107987)  This adds support for `LOOKUP`  a command that implements a sort of inline `ENRICH`  using data that is passed in the request:  ``` $ curl -uelastic:password -HContent-Type:application/json -XPOST \ 'localhost:9200/_query?error_trace&pretty&format=txt' \ -d'{ "query": "ROW a=1::LONG | LOOKUP t ON a"  "tables": { "t": { "a:long":     [    1      4      2]  "v1:integer": [   10     11     12]  "v2:keyword": ["cat"  "dog"  "wow"] } }  "version": "2024.04.01" }' v1       |      v2       |       a ---------------+---------------+--------------- 10             |cat            |1 ```  This required these PRs: * #107624 * #107634 * #107701 * #107762 *  Closes #107306  parent 32ac5ba755dd5c24364a210f1097ae093fdcbd75 author Craig Taverner <craig@amanzi.com> 1717779549 +0200 committer Craig Taverner <craig@amanzi.com> 1718115775 +0200  Fixed compile error after merging in main  Fixed strange merge issues from main  Remove version from ES|QL test queries after merging main  Fixed union-types on nested fields  Switch to Luigi's solution  and expand nested tests  Cleanup after rebase  * Added more tests from code review  Note that one test  `multiIndexIpStringStatsInline` is muted due to failing with the error:  UnresolvedException: Invalid call to dataType on an unresolved object ?client_ip  * Make CsvTests consistent with integration tests for capabilities  The integration tests do not fail the tests if the capability does not even exist on cluster nodes  instead the tests are ignored. The same behaviour should happen with CsvTests for consistency.  * Return assumeThat to assertThat  but change order  This way we don't have to add more features to the test framework in this PR  but we would probably want a mute feature (like a `skip` line).  * Move serialization of MultiTypeEsField to NamedWritable approach  Since the sub-fields are AbstractConvertFunction expressions  and Expression is not yet fully supported as a category class for NamedWritable  we need a few slight tweaks to this  notably registering this explicitly in the EsqlPlugin  as well as calling PlanStreamInput.readExpression() instead of StreamInput.readNamedWritable(Expression.class). These can be removed later once Expression is fully supported as a category class.  * Remove attempt to mute two failed tests  We used required_capability to mute the tests  but this caused issues with CsvTests which also uses this as a spelling mistake checker for typing the capability name wrong  so we tried to use muted-tests.yml  but that only mutes tests in specific run configurations (ie. we need to mute each and every IT class separately).  So now we just remove the tests entirely. We left a comment in the muted-tests.yml file for future reference about how to mute csv-spec tests.  * Fix rather massive issue with performance of testConcurrentSerialization  Recreating the config on every test was very expensive.  * Code review by Nik  ---------  Co-authored-by: Elastic Machine <elasticmachine@users.noreply.github.com>
elastic,elasticsearch,23fc1513d48ec1c139d44c41f20e69d0e8bc6971,https://github.com/elastic/elasticsearch/commit/23fc1513d48ec1c139d44c41f20e69d0e8bc6971,[ML] StartTrainedModelDeployment Request query params override body params (#109487)  * Perform checks on request values regardless of the existance of a body  * Update docs/changelog/109487.yaml  * Update yaml summary  * Fix yaml summary  * Check for body and query parameters  verify they are same if set  * update yaml  * add same param check for default cache size  * Fix (remove) defaults  * handle null request wait for state  * Refactor handling of default values  * Fix tests; setDefaults in toXContent to avoid serialization errors  * Fix test by correctly checking for null allocations or threads  * update yaml  * Change rest action creation to assume that body parameters which are equal to the defaults were not specified in the body. This means that errors will not be thrown if the body and query parameters don't match as long as the body parameter is the same as the default.  * fix bad merge  * Fixes from review: remove extraneous enums  genersize param validation  * added javadoc and renamed validateParameters function  ---------  Co-authored-by: Elastic Machine <elasticmachine@users.noreply.github.com>
elastic,elasticsearch,0ae5aa35b7e5d1b4835d81a1d95dd1475b9c6a1f,https://github.com/elastic/elasticsearch/commit/0ae5aa35b7e5d1b4835d81a1d95dd1475b9c6a1f,Optimize BytesReference related code field access patterns (#109782)  Cache object fields (even when final  see https://openjdk.org/jeps/8132243) to generate smaller byte code as well as more optimized compiled code for this performance critical code.
elastic,elasticsearch,b99b5d5f253bc0f91783aa52f79d23443a8205ce,https://github.com/elastic/elasticsearch/commit/b99b5d5f253bc0f91783aa52f79d23443a8205ce,Remove unused seek-tracking plugin (#109600)  This was used for some performance investigations but is not currently needed  and would need updating in order to complete #100878. Instead  this commit removes it.
TheAlgorithms,Java,df0c997e4bce827246ee9d93ec3b1fe3c55a4332,https://github.com/TheAlgorithms/Java/commit/df0c997e4bce827246ee9d93ec3b1fe3c55a4332,General performance improvement (#6078)
TheAlgorithms,Java,921821214fb4a51dda558918e5d821ced2bff6a0,https://github.com/TheAlgorithms/Java/commit/921821214fb4a51dda558918e5d821ced2bff6a0,Add a new method to check Perfect Square (#5917)
Stirling-Tools,Stirling-PDF,35304a1491bb6a615282c8ebc0328d9920228db3,https://github.com/Stirling-Tools/Stirling-PDF/commit/35304a1491bb6a615282c8ebc0328d9920228db3,Enhance email error handling and expand test coverage (#3561)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed** - **EmailController**: Added a `catch (MailSendException)` block to handle invalid-address errors  log the exception  and return a 500 response with the raw error message. - **EmailServiceTest**: Added unit tests for attachment-related error cases (missing filename  null filename  missing file  null file) and invalid “to” address (null or empty)  expecting `MessagingException` or `MailSendException`. - **MailConfigTest**: New test class verifying `MailConfig.java` correctly initializes `JavaMailSenderImpl` with host  port  username  password  default encoding  and SMTP properties. - **EmailControllerTest**: Refactored into a parameterized test (`shouldHandleEmailRequests`) covering four scenarios: success  generic messaging error  missing `to` parameter  and invalid address formatting.  - **Why the change was made** - To ensure invalid email addresses and missing attachments are handled gracefully at the controller layer  providing clearer feedback to API clients. - To improve overall test coverage and guard against regressions in email functionality. - To enforce correct mail configuration via automated tests.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,b65624cf57a5a3a1a1edf822623566a7f54669b1,https://github.com/Stirling-Tools/Stirling-PDF/commit/b65624cf57a5a3a1a1edf822623566a7f54669b1,Enforce `Locale.US` for Consistent Decimal Formatting in Byte-Size Output (#3562)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed** - Added `import java.util.Locale;` - Updated the `String.format` call in `humanReadableByteCount` to use `Locale.US`  - **Why the change was made** By default  `String.format` uses the JVM’s default locale  which in some environments (e.g.  Germany) formats decimals with a comma. Tests expected a dot (`.`) as the decimal separator (e.g.  `"1.0 KB"`)  so we force `Locale.US` to ensure consistent output across all locales.   ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,70349fb7e32551b0afa9ba8c5ae3d85bfcc30e48,https://github.com/Stirling-Tools/Stirling-PDF/commit/70349fb7e32551b0afa9ba8c5ae3d85bfcc30e48,remove legacy homepage (#3518)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,523240554f29068c2b70a1738951e735ecb031a4,https://github.com/Stirling-Tools/Stirling-PDF/commit/523240554f29068c2b70a1738951e735ecb031a4,Fix empty-parameter issue in `updateUserSettings` by using `@RequestBody` map (#3536)  # Description of Changes  Please provide a summary of the changes  including:   - **What was changed:** - Refactored the `updateUserSettings` method in `UserController` to accept a `@RequestBody Map<String  String>` named `updates` instead of pulling parameters from `HttpServletRequest`. - Removed the now-unused `HashMap` import and the manual parameter-extraction loop.  - **Why the change was made:** - **Bug Fix:** The previous implementation relied on `request.getParameterMap()`  which was consistently empty  so no settings were ever applied. - Simplifies controller logic by leveraging Spring’s request-body binding. - Improves readability and maintainability  removing boilerplate and error-prone code.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,21832729d2d33c8307c886f387ea0f3cfbcdfc2a,https://github.com/Stirling-Tools/Stirling-PDF/commit/21832729d2d33c8307c886f387ea0f3cfbcdfc2a,JUnits JUnits JUnits  so many JUnits (#3537)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com> Co-authored-by: pixeebot[bot] <104101892+pixeebot[bot]@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,f94b8c3b22c3d26df43800ac773579b97c1ff675,https://github.com/Stirling-Tools/Stirling-PDF/commit/f94b8c3b22c3d26df43800ac773579b97c1ff675,Floating keys for pro users (#3535)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,52f09f1840ba0531238109acf79b8b354b695f6d,https://github.com/Stirling-Tools/Stirling-PDF/commit/52f09f1840ba0531238109acf79b8b354b695f6d,Improve Type Safety and OpenAPI Schema for PDF API Controllers and Models (#3470)  # Description of Changes  - **What was changed** - Updated controller methods to use strongly‐typed primitives (`int`  `long`  `boolean`) instead of `String` for numeric and boolean parameters  eliminating calls to `Integer.parseInt`/`Long.parseLong` and improving null‐safety (`Boolean.TRUE.equals(...)`). - Enhanced all API request model classes with richer Swagger/OpenAPI annotations: added `requiredMode`  `defaultValue`  `allowableValues`  `format`  `pattern`  and tightened schema descriptions for all fields. - Refactored HTML form templates for “Remove Blank Pages” to include `min`  `max`  and `step` attributes on numeric inputs  matching the updated validation rules.  - **Why the change was made** - **Type safety & robustness**: Shifting from `String` to native types prevents runtime parsing errors  simplifies controller logic  and makes default values explicit. - **Better API documentation & validation**: Enriching the Swagger annotations ensures generated docs accurately reflect required fields  default values  and permitted ranges  which improves client code generation and developer experience. - **Consistency across codebase**: Aligning all request models and controllers enforces a uniform coding style and reduces bugs.  #3406  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,c660ad80ce1a2f1fc82fdfaee141c7b4d8124ae0,https://github.com/Stirling-Tools/Stirling-PDF/commit/c660ad80ce1a2f1fc82fdfaee141c7b4d8124ae0,Update legal URLs and improve OpenAPI metadata configuration (#3522)  # Description of Changes  Please provide a summary of the changes  including:  - Updated default Terms & Conditions URL from `/terms-and-conditions` to `/terms` in: - `InitialSetup.java` - `settings.yml.template` - `allEndpointsRemovedSettings.yml` - Improved OpenAPI metadata in `OpenApiConfig.java`: - Added contact information (`name`  `url`  `email`) - Added license section with MIT license - Included terms of service link - Changed string comparison in `MetricsConfig.java` to use `"constant".equals(...)` format - Cleaned up and unified YAML formatting and comments - Merged and restructured `enterpriseEdition` settings under `premium.proFeatures`  ### Why the change was made  - Ensure legal links are consistent and up-to-date - Improve clarity and completeness of the OpenAPI specification for external consumers - Follow best practices for code readability and configuration structure - Prevent misconfiguration from outdated or redundant YAML sections  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,9ffc0037b76764d802ffff3c935437d9cb1c5036,https://github.com/Stirling-Tools/Stirling-PDF/commit/9ffc0037b76764d802ffff3c935437d9cb1c5036,Refactor permission variable names (#3457)  ## Refactor: Improve clarity of permission variable names  Renamed confusing `can[Action]` boolean variables to `prevent[Action]` (e.g.  `canPrint` -> `preventPrinting`) in `PasswordController.java`  `AddPasswordRequest.java`  and `add-password.html`.   The previous `can[Action]` convention was misleading  as `true` meant the action was *disallowed*. The new `prevent[Action]` naming directly reflects the intent (`true` = prevented)  improving code clarity.  **Changes:**  *   Updated variable names in controller logic *   Updated `@Schema` descriptions in `AddPasswordRequest.java` * Updated corresponding HTML element attributes (`id`  `name`  `for`) in `add-password.html`  **Important Notes:**  * The underlying logic still inverts the boolean when setting permissions (e.g.  `AccessPermission.setCanPrint(!preventPrinting)`). * User-facing UI text remains unchanged per request of @Frooodle in #3420.  **Why not invert the API logic** *   Inverting API (to can[action] logic) would either invalidate the UI * Inverting API AND changing UI would warrant bigger translation effort to change it in all languages * This version is consistent (meaning what the UI says is actually done) and preserve the UI language (meaning no translations needed) however it is inconsistent with PDFBox methods naming scheme  **PDFBox**  * **PDFBox Interaction:** This refactor addresses the naming *within* Stirling-PDF's API and Front-end layers only. The controller logic intentionally inverts the `prevent[Action]` boolean (`ap.setCanPrint(!preventPrinting)`) to correctly interact with the underlying PDFBox methods. No further renaming related to these permissions is necessary as the PDFBox methods themselves retain the `can[Action]` names.   Underlying logic is not changed so it should work but just in case I tested locally on an Adobe PDF that contained form in Chrome.    ## New variable names in API  ![new API variable names](https://github.com/user-attachments/assets/f3d56aaf-0455-4f65-af14-c1a07a02d11a)  **Related Issues:**  Closes #3427 Closes #3420  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,e5e793545653d42a9a824507025780269dd430e4,https://github.com/Stirling-Tools/Stirling-PDF/commit/e5e793545653d42a9a824507025780269dd430e4,pixel changes  redact color fix  version bump  aggressive compression (#3502)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,5b0eaec4365ac2a9ba1cd300d96c2980c9cb51c6,https://github.com/Stirling-Tools/Stirling-PDF/commit/5b0eaec4365ac2a9ba1cd300d96c2980c9cb51c6,Add Email Sending Service with Attachment Support (#3455)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed** - Introduced a new `EmailService` for asynchronous email delivery with attachment support. - Added `MailConfig` to configure a `JavaMailSender` bean using SMTP settings from `ApplicationProperties`. - Created `EmailController` endpoint (`/api/v1/general/send-email`) to accept multipart/form-data requests for sending emails. - Defined an `Email` API model to encapsulate recipient  subject  body  and file input. - Extended `ApplicationProperties` to include a nested `Mail` class for SMTP host  port  username/password  and sender address. - Updated `settings.yml.template` to include SMTP configuration placeholders. - Enhanced `.github/labeler-config.yml` to cover all new security- and API-related source files for automated labeling.  - **Why the change was made** - To enable Stirling-PDF to notify users via email—particularly useful for sending generated PDFs or alerts—directly from the application. - To centralize mail server configuration in application properties and streamline onboarding for new environments.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,2ac606608aa545f83c78b8baf95626cdf18c2536,https://github.com/Stirling-Tools/Stirling-PDF/commit/2ac606608aa545f83c78b8baf95626cdf18c2536,Fix cert-sign API NullPointerException when pageNumber is omitted for invisible signatures (#3463)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed** - Updated `SignPDFWithCertRequest` to use `Boolean` for `showSignature` and `showLogo`  and made `pageNumber` nullable. - In `CertSignController`: - Added an `@InitBinder` to convert empty multipart fields to `null`. - Extended `@PostMapping` to consume both `multipart/form-data` and `application/x-www-form-urlencoded`. - Wrapped `pageNumber` calculation in a null-check (`pageNumber = request.getPageNumber() != null ? request.getPageNumber() - 1 : null`). - Changed signature-visualization and logo checks to `Boolean.TRUE.equals(...)` to avoid unboxing NPE. - Cleaned up imports and schema annotations in the request model.  - **Why the change was made** - Prevent a 500 Internal Server Error caused by calling `.intValue()` on a null `pageNumber` when `showSignature=false` (invisible signatures). - Ensure that omitting `pageNumber` doesn’t break clients using the “try it out” swagger UI or `curl`-based requests.  - **Any challenges encountered** - Configuring Spring’s data binder to treat empty file inputs as `null` required a custom `PropertyEditorSupport`. - Balancing backward compatibility with stricter type handling (switching from primitive `boolean` to boxed `Boolean`).  Closes #3459  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,e2a5874a887a7ca895b59ce70d2b5d26913257d4,https://github.com/Stirling-Tools/Stirling-PDF/commit/e2a5874a887a7ca895b59ce70d2b5d26913257d4,fix read wrong properties (#3472)  # Description of Changes  Please provide a summary of the changes  including:  Test file:   [12345678.pdf](https://github.com/user-attachments/files/20028981/12345678.pdf)  Behavior without readOnly  ```json { "creator": null  "modificationDate": "java.util.GregorianCalendar[time=1746381303000 areFieldsSet=true areAllFieldsSet=true lenient=false zone=java.util.SimpleTimeZone[id=GMT offset=0 dstSavings=3600000 useDaylight=false startYear=0 startMode=0 startMonth=0 startDay=0 startDayOfWeek=0 startTime=0 startTimeMode=0 endMode=0 endMonth=0 endDay=0 endDayOfWeek=0 endTime=0 endTimeMode=0] firstDayOfWeek=1 minimalDaysInFirstWeek=1 ERA=1 YEAR=2025 MONTH=4 WEEK_OF_YEAR=19 WEEK_OF_MONTH=2 DAY_OF_MONTH=4 DAY_OF_YEAR=124 DAY_OF_WEEK=1 DAY_OF_WEEK_IN_MONTH=1 AM_PM=1 HOUR=5 HOUR_OF_DAY=17 MINUTE=55 SECOND=3 MILLISECOND=0 ZONE_OFFSET=0 DST_OFFSET=0]"  "keywords": null  "author": ""  "subject": null  "producer": "Stirling-PDF v0.46.0"  "title": "Microsoft Word - Dokument1"  "creationDate": "java.util.GregorianCalendar[time=1746381238000 areFieldsSet=true areAllFieldsSet=true lenient=false zone=java.util.SimpleTimeZone[id=GMT+02:00 offset=7200000 dstSavings=3600000 useDaylight=false startYear=0 startMode=0 startMonth=0 startDay=0 startDayOfWeek=0 startTime=0 startTimeMode=0 endMode=0 endMonth=0 endDay=0 endDayOfWeek=0 endTime=0 endTimeMode=0] firstDayOfWeek=1 minimalDaysInFirstWeek=1 ERA=1 YEAR=2025 MONTH=4 WEEK_OF_YEAR=19 WEEK_OF_MONTH=2 DAY_OF_MONTH=4 DAY_OF_YEAR=124 DAY_OF_WEEK=1 DAY_OF_WEEK_IN_MONTH=1 AM_PM=1 HOUR=7 HOUR_OF_DAY=19 MINUTE=53 SECOND=58 MILLISECOND=0 ZONE_OFFSET=7200000 DST_OFFSET=0]" } ```  with readOnly=true  ```json { "creator": null  "modificationDate": "java.util.GregorianCalendar[time=1746381238000 areFieldsSet=true areAllFieldsSet=true lenient=false zone=java.util.SimpleTimeZone[id=GMT+02:00 offset=7200000 dstSavings=3600000 useDaylight=false startYear=0 startMode=0 startMonth=0 startDay=0 startDayOfWeek=0 startTime=0 startTimeMode=0 endMode=0 endMonth=0 endDay=0 endDayOfWeek=0 endTime=0 endTimeMode=0] firstDayOfWeek=1 minimalDaysInFirstWeek=1 ERA=1 YEAR=2025 MONTH=4 WEEK_OF_YEAR=19 WEEK_OF_MONTH=2 DAY_OF_MONTH=4 DAY_OF_YEAR=124 DAY_OF_WEEK=1 DAY_OF_WEEK_IN_MONTH=1 AM_PM=1 HOUR=7 HOUR_OF_DAY=19 MINUTE=53 SECOND=58 MILLISECOND=0 ZONE_OFFSET=7200000 DST_OFFSET=0]"  "keywords": null  "author": ""  "subject": null  "producer": "Microsoft: Print To PDF"  "title": "Microsoft Word - Dokument1"  "creationDate": "java.util.GregorianCalendar[time=1746381238000 areFieldsSet=true areAllFieldsSet=true lenient=false zone=java.util.SimpleTimeZone[id=GMT+02:00 offset=7200000 dstSavings=3600000 useDaylight=false startYear=0 startMode=0 startMonth=0 startDay=0 startDayOfWeek=0 startTime=0 startTimeMode=0 endMode=0 endMonth=0 endDay=0 endDayOfWeek=0 endTime=0 endTimeMode=0] firstDayOfWeek=1 minimalDaysInFirstWeek=1 ERA=1 YEAR=2025 MONTH=4 WEEK_OF_YEAR=19 WEEK_OF_MONTH=2 DAY_OF_MONTH=4 DAY_OF_YEAR=124 DAY_OF_WEEK=1 DAY_OF_WEEK_IN_MONTH=1 AM_PM=1 HOUR=7 HOUR_OF_DAY=19 MINUTE=53 SECOND=58 MILLISECOND=0 ZONE_OFFSET=7200000 DST_OFFSET=0]" } ```  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,b8aa9f0cdf94ec06f9d3a84d694795bed955e612,https://github.com/Stirling-Tools/Stirling-PDF/commit/b8aa9f0cdf94ec06f9d3a84d694795bed955e612,Fix NullPointerException by Enabling Constructor Injection for Color Replacement Components (#3469)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed** Added the `final` modifier to the `ReplaceAndInvertColorService` field in `ReplaceAndInvertColorController` and to the `ReplaceAndInvertColorFactory` field in `ReplaceAndInvertColorService`. This ensures that Lombok’s `@RequiredArgsConstructor` generates constructors for these dependencies  enabling proper constructor-based injection instead of leaving them null.  - **Why the change was made** Without the `final` keyword  Lombok does not include non-final fields in the generated constructor  causing Spring to leave them uninitialized and resulting in a `NullPointerException` during runtime when invoking `replaceAndInvert` on the factory/service.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,aef64cd7ccf99c5df3f92e11dd2d5868658ad33f,https://github.com/Stirling-Tools/Stirling-PDF/commit/aef64cd7ccf99c5df3f92e11dd2d5868658ad33f,Validate H2 Database Type and URL Consistency for Custom Databases (#3458)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed** Introduced a local `isCustomDatabase` flag (based on `datasource.isEnableCustomDatabase()`) to ensure that the H2-specific URL/type consistency checks (and corresponding warnings/exceptions) only run when a custom database configuration is enabled. Refactored the return statement to use this flag (`return !isCustomDatabase || isH2;`) instead of calling `isEnableCustomDatabase()` directly.  - **Why the change was made** Previously  even when custom database support was disabled  the method would still validate H2 configuration and potentially throw an `IllegalStateException`. By guarding those checks  we avoid spurious warnings or exceptions in default (non-custom) setups and make the method’s behavior more predictable.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,1377aa4f8df8b5cbf648a78f26e5aa07ce7126dc,https://github.com/Stirling-Tools/Stirling-PDF/commit/1377aa4f8df8b5cbf648a78f26e5aa07ce7126dc,Internationalize logout message (#3450)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed** - Controller now uses the `login.logoutMessage` i18n key instead of hard-coded text. - Added `login.logoutMessage` entry to `messages_de_DE.properties` and `messages_en_GB.properties`. - Updated `login.html` to resolve the logout message via `th:text="#{…}"`.  - **Why the change was made** - To support localization for logout feedback. - To eliminate hard-coded strings from the view layer and rely on message bundles.  before:   ![image](https://github.com/user-attachments/assets/5e9975f6-717f-4035-8e3c-76df8c0275bb)   after:   ![image](https://github.com/user-attachments/assets/934f45ad-d490-4a34-9399-5c9031f2db2d)   ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [x] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,4b8670308236145b2eda7bf0dbc1a6ee2019d491,https://github.com/Stirling-Tools/Stirling-PDF/commit/4b8670308236145b2eda7bf0dbc1a6ee2019d491,Validate H2 datasource configuration in DatabaseService (#3449)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed** Updated the `isH2Database()` method in `DatabaseService.java` to perform additional consistency checks between the configured datasource type and the JDBC URL: - Compute `isTypeH2` based on `datasource.getType().equalsIgnoreCase("H2")`. - Compute `isDBUrlH2` by checking if `datasource.getCustomDatabaseUrl()` contains “h2” (case-insensitive). - Log a warning and throw `IllegalStateException` when the type is H2 but URL doesn’t contain “h2”  or vice versa. - Return the original boolean logic (`!enableCustomDatabase || isH2`) only when both type and URL agree.  - **Why the change was made** To prevent runtime misconfigurations where the declared database driver (H2) does not match the actual JDBC URL (or vice versa)  providing early  clear feedback to users and avoiding obscure errors later in startup.   Closes #3428  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,715445a8dd73fb782f5f9f82cef39a5f35ccb44c,https://github.com/Stirling-Tools/Stirling-PDF/commit/715445a8dd73fb782f5f9f82cef39a5f35ccb44c,Remove read only from forms (#3423)  # Description of Changes  Create new tool to remove read-only properties of form fields.  - Added new html file to provide a page for the tool (misc/unlock-pdf-forms.html)  as well as new endpoint (/unlock-pdf-forms) under config/EndpointConfiguration.java - Added the tool to the list of "view & edit" tools under the home page in home-legacy.html and navElements.html - Mapped the frontend in controller/web/OtherWebController.java - Created a new controller (controller/api/misc/UnlockPDFFormsController.java) to handle AcroForm /Ff flags  /Lock tags and XFA Forms  removing the read-only properties of all form fields of a PDF document. - Added language entries to all the language files  to correctly display the tool's title  header description  etc.  Closes #2965  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes  - [x] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)   ![image](https://github.com/user-attachments/assets/2890d3c0-0535-487c-aa0a-83ad9597d898)  ![image](https://github.com/user-attachments/assets/631e729c-d68d-4da9-b925-64b5362aeea4)  ![image](https://github.com/user-attachments/assets/376a98d5-ca1d-45e9-910f-b5c7639eae8c)    ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,5f8b208db407fa1abcde0a0d7b0fa5b08d9390d1,https://github.com/Stirling-Tools/Stirling-PDF/commit/5f8b208db407fa1abcde0a0d7b0fa5b08d9390d1,Refactor codebase to replace explicit constructors with Lombok annotations and remove boilerplat (#3415)  # Description of Changes  - **What was changed:** - Removed explicit constructor definitions annotated with `@Autowired` across services  controllers  filters  and schedulers. - Added Lombok’s `@RequiredArgsConstructor` to automatically generate required-args constructors and eliminate boilerplate. - Introduced other Lombok annotations (`@Data`  `@Getter`  `@Setter`  `@EqualsAndHashCode`  `@NoArgsConstructor`) on model and API classes to replace manual getters/setters and constructors. - Standardized string comparisons to use the constant-first form (e.g.  `"value".equals(variable)`). - Cleaned up unused imports and organized OpenAPI configuration by extracting default title/description constants.  - **Why the change was made:** - To reduce repetitive boilerplate code and improve maintainability. - To leverage Lombok for cleaner  more consistent dependency injection and data modeling. - To ensure a uniform coding style across the entire codebase.  #3406  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,413911210fe51dfea73d02d0ab893dc41b78857e,https://github.com/Stirling-Tools/Stirling-PDF/commit/413911210fe51dfea73d02d0ab893dc41b78857e,Changes from Version 2.2.0 to 2.8.6 `org.springdoc:springdoc-openapi-starter-webmvc-ui` (#3400)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #3399  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,d66997a8d6c7ada0ceeeb783eb2bcf8491f84fc6,https://github.com/Stirling-Tools/Stirling-PDF/commit/d66997a8d6c7ada0ceeeb783eb2bcf8491f84fc6,Support domain `User` instances in `getCurrentUsername` method (#3383)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed** The `getCurrentUsername()` method in `UserService` now recognizes and handles principals of type `stirling.software.SPDF.model.User`. Previously  only `UserDetails` and `OAuth2User` were supported; any `User` domain object was falling through to the default case and not returning the expected username.  - **Why the change was made** In order to allow our custom domain `User` entities to be used directly as the authenticated principal (for example  when loading a user via JWT or session)  we need to extract the username from that object. This makes authentication flows more consistent and prevents unexpected `null` or fallback values when the principal is our own `User` type.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,e5cd8ce901ce0ca0b3e68bf3d8260ef7c7b9862d,https://github.com/Stirling-Tools/Stirling-PDF/commit/e5cd8ce901ce0ca0b3e68bf3d8260ef7c7b9862d,Add SHOW_SURVEY Environment variable to Docker (#3378)  In the previous implementation  the survey was displayed on the main screen when the homepage was opened for the 5th  10th  15th  22nd  30th  50th  75th  100th  150th  or 200th time  as long as the "Do not show again" option hadn't been selected.  With this new feature  if the SHOW_SURVEY environment variable is set to true or not set at all in the Docker configuration  the survey will continue to be shown as before.  <img width="1679" alt="Screenshot 2025-04-18 at 08 17 37" src="https://github.com/user-attachments/assets/696b9dc2-9502-4d66-9991-d2b81b52cd02" />  However  if the SHOW_SURVEY parameter is explicitly set to false  the survey will no longer be displayed.  <img width="1707" alt="Screenshot 2025-04-18 at 08 18 39" src="https://github.com/user-attachments/assets/b57c568a-b5e7-4927-bccf-f9a398bea702" />   Closes #1573  ---  ## Checklist  ### General  - [X] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [X] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [X] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [X] I have performed a self-review of my own code - [X] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [X] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [X] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,7bdefb69c23ea11b024c8ef5d747f22eb5920fc5,https://github.com/Stirling-Tools/Stirling-PDF/commit/7bdefb69c23ea11b024c8ef5d747f22eb5920fc5,Make file extension checks case-insensitive in pipeline (#3368)  # Description of Changes  File extensions in the pipeline were being checked in a case-sensitive manner. Since supported extensions were defined in lowercase only  files with uppercase extensions were being rejected directly  and logs like the following were being printed:  <img width="1542" alt="Screenshot 2025-04-17 at 00 14 16" src="https://github.com/user-attachments/assets/a584b8d8-0a56-4a76-b409-9d6cd38f1a80" />  With this change  the uploaded file’s extension is now converted to lowercase using toLowerCase  making the extension check case-insensitive. After this change  the logs flow as expected  as shown below:  <img width="1317" alt="Screenshot 2025-04-17 at 00 49 52" src="https://github.com/user-attachments/assets/2abdcfc7-4c74-4b06-bbea-ef12e0f737b4" />  Closes #3243  ---  ## Checklist  ### General  - [X] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [X] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [X] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [X] I have performed a self-review of my own code - [X] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [X] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,def0552f244fdeef75b69b05c5342b4c46c091d7,https://github.com/Stirling-Tools/Stirling-PDF/commit/def0552f244fdeef75b69b05c5342b4c46c091d7,fix pipelines via changing to service (#3358)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,ac231e0c9274bfe21924928e4facbcb83b3833b7,https://github.com/Stirling-Tools/Stirling-PDF/commit/ac231e0c9274bfe21924928e4facbcb83b3833b7,3335 feature request add app version to posthog (#3348)  # Description of Changes  Please provide a summary of the changes  including:  Added app_version to all posthog captures  Closes #(3335)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,f0ed60a933f7a667a7a8dc0ce5fba8d657c53000,https://github.com/Stirling-Tools/Stirling-PDF/commit/f0ed60a933f7a667a7a8dc0ce5fba8d657c53000,Allow non cert files to be enterprise (#3346)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,1c655f0ba01b7efef00a9dfd3547bfd6c1bf42e1,https://github.com/Stirling-Tools/Stirling-PDF/commit/1c655f0ba01b7efef00a9dfd3547bfd6c1bf42e1,Upload File Size Limit (#3334)  # Description of Changes  The change this PR aims to introduce is a setting for enabling an upload file size limit. The author of the issue mentioned in this PR wanted this feature as they themselves enforced a limit of 50MB file sizes on their NGINX configuration. This was implemented by adding an entry to the [settings.yml.template](https://github.com/PedroPF1234/Stirling-PDF/blob/e52fc0e478e279169329b7e30782d57b2dbd8cbe/src/main/resources/settings.yml.template) file. This entry has two sub-configurations in which you declare if the application should enable upload file size limiting and then you declare the limit itself.  For this to be available in code  a new field in the [System](https://github.com/PedroPF1234/Stirling-PDF/blob/e52fc0e478e279169329b7e30782d57b2dbd8cbe/src/main/java/stirling/software/SPDF/model/ApplicationProperties.java#L280) class was added  one named `uploadLimit`.  After that  inside the [AppConfig](url) class  a new thymeleaf bean was created  one called `uploadLimit`. This bean takes the values available in the `System` class and creates a `long` value representing the limit value. This value is interpreted as non-existent if it is `0`  otherwise it is the value in `bytes` of the upload limit.  In order to make this value available in the [common.html](https://github.com/PedroPF1234/Stirling-PDF/blob/e52fc0e478e279169329b7e30782d57b2dbd8cbe/src/main/resources/templates/fragments/common.html) file  where the submitFile form is imported from  a new controller [GlobalUploadLimitWebController](https://github.com/PedroPF1234/Stirling-PDF/blob/e52fc0e478e279169329b7e30782d57b2dbd8cbe/src/main/java/stirling/software/SPDF/controller/web/GlobalUploadLimitWebController.java) was created. This controller has the tag `ControllerAdvice` so that every controller has the `ModelAttributes` defined within it. I am not sure if this was a good approach but upon first investigations  I couldn't find another method to make these attributes available in every Controller  or template. If there is already a place like this in the code with this specific purpose  please let me know so I can fix it.  After making these attributes available  I updated the code in `common.html`to now display the upload limit if it is defined. This was done with localization in mind.  Lastly  the [downloader.js](https://github.com/PedroPF1234/Stirling-PDF/blob/e52fc0e478e279169329b7e30782d57b2dbd8cbe/src/main/resources/static/js/downloader.js) and [fileInput.js](https://github.com/PedroPF1234/Stirling-PDF/blob/e52fc0e478e279169329b7e30782d57b2dbd8cbe/src/main/resources/static/js/fileInput.js) files to include logic to enforce the upload limit if it is defined.  The UI updates  when the upload limit is defined  are as so: <img width="708" alt="image" src="https://github.com/user-attachments/assets/4852fa10-2ec3-45cb-83e6-41a102f256d4" />  When the limit is disabled  the page looks exactly as it did before any implementation: <img width="707" alt="image" src="https://github.com/user-attachments/assets/21e5e810-ffdc-4a99-a16d-491aea103709" />\\  Thank you.  Closes #2903  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [x] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [x] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [x] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,7f8e3d676d9a9de649bda83a52b9af17dd12174b,https://github.com/Stirling-Tools/Stirling-PDF/commit/7f8e3d676d9a9de649bda83a52b9af17dd12174b,Pipeline shows disabled endpoints fix (#2881) (#3282)  # Description of Changes  Previously  the dropdown menu in the pipeline configuration displayed all endpoints  including disabled ones  and allowed API calls to them.  Changes:  - Updated EndpointInterceptor to correctly parse request URIs and match them to corresponding endpoint names in settings.yml  ensuring disabled endpoints are blocked.  - Added a new API endpoint in SettingsController to expose the endpointStatus map  allowing the frontend to check which endpoints are disabled.  - Updated pipeline.js to use this new API and hide disabled endpoints from the dropdown menu.  Tests:  - Created a new Docker Compose setup using a custom settings.yml where all endpoints are disabled.  - Implemented a test script to run this setup  send API requests to disabled endpoints  and verify they are correctly blocked.  [Bug Fix Video](https://youtu.be/L1z3jZh8z8E)  Closes #2881  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,4e63a684b5bb10a075e75c20b082cc236ff9ab93,https://github.com/Stirling-Tools/Stirling-PDF/commit/4e63a684b5bb10a075e75c20b082cc236ff9ab93,Exclude Internal API User from Total User Count (#3299)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed**: Modified the `getTotalUsersCount()` method in `UserService` to subtract one user from the count if the internal API user is present in the database. This ensures that the internal service account does not skew user metrics.  - **Why the change was made**: To prevent the internal API user (used for backend operations) from being included in total user statistics  which should reflect only real user accounts.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,4d6f95160429595c63507a2b61b689ad0fa03a5d,https://github.com/Stirling-Tools/Stirling-PDF/commit/4d6f95160429595c63507a2b61b689ad0fa03a5d,Deprecate `EnterpriseEdition` (#3291)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed**: Added the `@Deprecated(since = "0.45.0")` annotation to the `EnterpriseEdition` inner class within `ApplicationProperties`.  - **Why the change was made**: This class is marked for removal after the migration process. Deprecating it now provides a clear signal to developers and automated tools that this class should no longer be used  easing future maintenance and cleanup.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,0b0c2c7c9e435f9da73b4a54dbf174b3e8b75545,https://github.com/Stirling-Tools/Stirling-PDF/commit/0b0c2c7c9e435f9da73b4a54dbf174b3e8b75545,Removing redundant logoutUrl from oauth (#3281)  Removed redundant logoutUrl from oauth code  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [x] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [x] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [x] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [x] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,bcf7fab825bc672ab6745e2fe77b2403a5cb1ad0,https://github.com/Stirling-Tools/Stirling-PDF/commit/bcf7fab825bc672ab6745e2fe77b2403a5cb1ad0,Add default authority assignment and enhanced user creation method (#3266)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed**: - Automatically assign the `USER` role to newly created users in the `saveUser(String username  String password)` method. - Introduced a new `saveUser(String username  String password  boolean firstLogin  boolean enabled)` method to allow setting `firstLogin` and `enabled` flags at creation time. - Added `"anonymoususer"` to the list of restricted usernames in `isUsernameValid`.  - **Why the change was made**: - Ensures users have proper default roles assigned to avoid permission issues post-creation. - Provides more flexibility for user creation in scenarios like pre-provisioning or scripting users with specific states. - Prevents the creation of potentially reserved or insecure usernames like `anonymoususer`.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,74df26db0c39a3f1019ac5bc80948c84ea885231,https://github.com/Stirling-Tools/Stirling-PDF/commit/74df26db0c39a3f1019ac5bc80948c84ea885231,🔧 Replace toList() with collect(Collectors.toList()) (#3259)  # Description of Changes  Please provide a summary of the changes  including:  - Replaced the usage of `toList()` with `collect(Collectors.toList())` in `DatabaseService.java`.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,3420a8633b173451692ac538d8bd1418552a49e1,https://github.com/Stirling-Tools/Stirling-PDF/commit/3420a8633b173451692ac538d8bd1418552a49e1,Cleanups and making distinction between pro and enterprise  (#3250)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: a <a>
Stirling-Tools,Stirling-PDF,989c468db2b0789a34a965f3d45994f030d594e9,https://github.com/Stirling-Tools/Stirling-PDF/commit/989c468db2b0789a34a965f3d45994f030d594e9,Change PDF load Get Info on PDF to readonly (#3254)  # Description of Changes  Please provide a summary of the changes  including:  - Fixed the behavior  that the metadata of a pdf was changed when using the "Get info on PDF" function. Notably the Producer and ModificationDate are updated currently  wich is not in line with the description "Get Info".  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  Co-authored-by: maxi322 <maxi322@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,5ce941dda0b50e070fcec0325f91535e9c3c3943,https://github.com/Stirling-Tools/Stirling-PDF/commit/5ce941dda0b50e070fcec0325f91535e9c3c3943,Sanataize PDF improvements (#3251)  # Description of Changes  Please provide a summary of the changes  including:  - Make distinction between metadata removal and XMP metadata removal - Change file loaders to only edit metadata for certain ops  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,9951695eb18e2a1d7e9de3d3b60be691edd42e34,https://github.com/Stirling-Tools/Stirling-PDF/commit/9951695eb18e2a1d7e9de3d3b60be691edd42e34,Python fix for new release (#3247)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: a <a> Co-authored-by: pixeebot[bot] <104101892+pixeebot[bot]@users.noreply.github.com> Co-authored-by: Connor Yoh <con.yoh13@gmail.com>
Stirling-Tools,Stirling-PDF,e15128633718cb5f9262986b3770ca592af60cda,https://github.com/Stirling-Tools/Stirling-PDF/commit/e15128633718cb5f9262986b3770ca592af60cda,Security fixes  enterprise stuff and more (#3241)  # Description of Changes  Please provide a summary of the changes  including:  - Enable user to add custom JAVA ops with env JAVA_CUSTOM_OPTS - Added support for prometheus (enabled via JAVA_CUSTOM_OPTS + enterprise license) - Changed settings from enterprise naming to 'Premium' - KeygenLicense Check to support offline licenses - Disable URL-to-PDF due to huge security bug - Remove loud Split PDF logs - addUsers renamed to adminSettings - Added Usage analytics page - Add user button to only be enabled based on total users free - Improve Merge memory usage   Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: a <a> Co-authored-by: pixeebot[bot] <104101892+pixeebot[bot]@users.noreply.github.com> Co-authored-by: Connor Yoh <con.yoh13@gmail.com>
Stirling-Tools,Stirling-PDF,e04cfcdde7813c51d2e7a5102ae0b506d97ec59e,https://github.com/Stirling-Tools/Stirling-PDF/commit/e04cfcdde7813c51d2e7a5102ae0b506d97ec59e,Fix: Session of admin is destroyed instead of the deleted user (#3218)  # Description of Changes  Please provide a summary of the changes  including:  - Replaced `authentication.getPrincipal()` with `username` in the `sessionRegistry.getAllSessions(...)` call inside the `deleteUser` method of `UserController`. - The original implementation incorrectly used the currently authenticated principal to fetch sessions  which could lead to only invalidating the sessions of the user performing the deletion — not the target user being deleted. - By using the `username` parameter directly  this ensures **all sessions of the user being deleted are properly expired and removed**.  Closes #(issue_number)  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,d8cca66560664cfd24134bbe0ae245f1f3587ff0,https://github.com/Stirling-Tools/Stirling-PDF/commit/d8cca66560664cfd24134bbe0ae245f1f3587ff0,Add default languages to OCR  fix compression for QPDF and embedded images (#3202)  # Description of Changes  This pull request includes several changes to the codebase  focusing on enhancing OCR support  improving endpoint management  and adding new functionality for PDF compression. The most important changes are detailed below.  ### Enhancements to OCR support:  * `Dockerfile` and `Dockerfile.fat`: Added support for multiple new OCR languages including Chinese (Simplified)  German  French  and Portuguese. (Our top 5 languages including English) [[1]](diffhunk://#diff-dd2c0eb6ea5cfc6c4bd4eac30934e2d5746747af48fef6da689e85b752f39557R69-R72) [[2]](diffhunk://#diff-571631582b988e88c52c86960cc083b0b8fa63cf88f056f26e9e684195221c27L78-R81)  ### Improvements to endpoint management:  * [`src/main/java/stirling/software/SPDF/config/EndpointConfiguration.java`](diffhunk://#diff-750f31f6ecbd64b025567108a33775cad339e835a04360affff82a09410b697dR51-R66): Added a new method `isGroupEnabled` to check if a group of endpoints is enabled. * [`src/main/java/stirling/software/SPDF/config/EndpointConfiguration.java`](diffhunk://#diff-750f31f6ecbd64b025567108a33775cad339e835a04360affff82a09410b697dL179-L193): Updated endpoint groups and removed redundant qpdf endpoints. [[1]](diffhunk://#diff-750f31f6ecbd64b025567108a33775cad339e835a04360affff82a09410b697dL179-L193) [[2]](diffhunk://#diff-750f31f6ecbd64b025567108a33775cad339e835a04360affff82a09410b697dL243-L244) * [`src/main/java/stirling/software/SPDF/config/EndpointInspector.java`](diffhunk://#diff-845de13e140bb1264014539714860f044405274ad2a9481f38befdd1c1333818R1-R291): Introduced a new `EndpointInspector` class to discover and validate GET endpoints dynamically.  ### New functionality for PDF compression:  * [`src/main/java/stirling/software/SPDF/controller/api/misc/CompressController.java`](diffhunk://#diff-c307589e9f958f2593c9567c5ad9d63cd03788aa4803b3017b1c13b0d0485805R10): Enhanced the `CompressController` to handle nested images within form XObjects  improving the accuracy of image compression in PDFs. Remove Compresses Dependency on QPDF [[1]](diffhunk://#diff-c307589e9f958f2593c9567c5ad9d63cd03788aa4803b3017b1c13b0d0485805R10) [[2]](diffhunk://#diff-c307589e9f958f2593c9567c5ad9d63cd03788aa4803b3017b1c13b0d0485805R28-R44) [[3]](diffhunk://#diff-c307589e9f958f2593c9567c5ad9d63cd03788aa4803b3017b1c13b0d0485805L49-R61) [[4]](diffhunk://#diff-c307589e9f958f2593c9567c5ad9d63cd03788aa4803b3017b1c13b0d0485805R77-R99) [[5]](diff hunk://#diff-c307589e9f958f2593c9567c5ad9d63cd03788aa4803b3017b1c13b0d0485805L92-R191) Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: a <a>
Stirling-Tools,Stirling-PDF,8d9c304ad7e951982af989ed4d8d15509db13362,https://github.com/Stirling-Tools/Stirling-PDF/commit/8d9c304ad7e951982af989ed4d8d15509db13362,Normalize File Path for Font Resource Loading (#3179)  # Description of Changes  Please provide a summary of the changes  including:  - Added logic to normalize file paths when retrieving font resources. - Ensured that file paths starting with `file:` are properly sanitized and formatted to prevent inconsistencies. - Replaced `\*` and `/*` in the path to ensure proper pattern matching. - Used `Paths.get(rawPath).normalize()` to avoid potential path traversal issues. - Updated `locationPattern` to ensure it uses a consistent format across different operating systems.  This change improves reliability in loading fonts from local file paths and prevents potential errors related to improperly formatted paths.  Closes #3178  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,964f948c64e1d396e240b7aca59cf2366f10026c,https://github.com/Stirling-Tools/Stirling-PDF/commit/964f948c64e1d396e240b7aca59cf2366f10026c,Fix: Context Path Handling and Static Asset Loading Improvements (#3198)  # Description of Changes  ### Summary This PR improves how Stirling-PDF handles the `context-path` configuration in both backend and frontend components. It ensures proper URL generation when a custom `context-path` is set in the application properties. Additionally  it fixes static asset loading issues related to relative paths.  ### Changes Implemented: - **Backend Enhancements:** - Introduced `contextPathStatic` as a static variable in `SPDFApplication.java` to store the configured `server.servlet.context-path`. - Modified log outputs and UI initialization URLs to include `contextPathStatic`. - Registered `contextPath` as a Spring Bean in `AppConfig.java` to make it accessible in templates.  - **Frontend Fixes:** - Updated JavaScript files (`downloader.js`  `home.js`) to dynamically retrieve and use `contextPath`. - Adjusted Thymeleaf template files (`navbar.html`  `home.html`  `merge-pdfs.html`) to reference `contextPath` correctly. - Fixed incorrect static file paths (`pdf.worker.mjs`  `pdf.mjs`) by replacing absolute paths (`/`) with relative ones (`./`).  ### Why These Changes? - Fixes issues where deployments under subpaths (e.g.  `example.com/stirling-pdf/`) resulted in incorrect asset and navigation links. - Ensures compatibility with different deployment configurations where `context-path` is not `/`.  ### Challenges Encountered - Ensuring all JavaScript and template references were correctly updated to use `contextPath` dynamically. - Maintaining backward compatibility for deployments that use `/` as the context path.  Closes #3193 #3181  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,c7a8b9f0114de07dad678e08d7e79530d232c8a9,https://github.com/Stirling-Tools/Stirling-PDF/commit/c7a8b9f0114de07dad678e08d7e79530d232c8a9,Further compression fixes (#3177)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: a <a>
Stirling-Tools,Stirling-PDF,05686021637bb7d1f25c4cd32109f528bb14b03b,https://github.com/Stirling-Tools/Stirling-PDF/commit/05686021637bb7d1f25c4cd32109f528bb14b03b,Add: Validation for rotation angle and create unit tests for RotationController (#3162)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,4408ecfa5bfc4902a730995844fcf97192cf6878,https://github.com/Stirling-Tools/Stirling-PDF/commit/4408ecfa5bfc4902a730995844fcf97192cf6878,Fix: string comparison and formatting inconsistencies in `CompressController` (#3168)  # Description of Changes  Please provide a summary of the changes  including:  - Replaced `format.equals("jpeg")` with `"jpeg".equals(format)` to prevent potential `NullPointerException` - Standardized percentage reduction logging by formatting values before passing them into the log statement - Fixed inconsistent formatting in log messages by replacing `{:.1f}%` with pre-formatted string values  `63.32 MB → 61.77 MB (reduced by {:.1f}%)` -> `63.32 MB → 61.77 MB (reduced by 2.5%)`  These changes improve code robustness and ensure consistent logging output.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,e4dbe7f9b04a6ad3b96cb3828c0e2deccf9dbfc9,https://github.com/Stirling-Tools/Stirling-PDF/commit/e4dbe7f9b04a6ad3b96cb3828c0e2deccf9dbfc9,Rename `CustomPDDocumentFactory` to `CustomPDFDocumentFactory` across multiple controllers (#3163)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,45b3a14da497d3760ba7772ea7198f282936ed7c,https://github.com/Stirling-Tools/Stirling-PDF/commit/45b3a14da497d3760ba7772ea7198f282936ed7c,Add unit test for FileInfo.getFormattedFileSize (#3132)  # Description of Changes  Add Unit Test for FileInfo.getFormattedFileSize  Closes #3089  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [x] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,a61749d5003e2a2f3bc483154b9f59bbd0c99f44,https://github.com/Stirling-Tools/Stirling-PDF/commit/a61749d5003e2a2f3bc483154b9f59bbd0c99f44,removal of all getByte loads (#3153)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: a <a>
Stirling-Tools,Stirling-PDF,1ec8112efc4580403e323ca5b4f9e4a8401c1775,https://github.com/Stirling-Tools/Stirling-PDF/commit/1ec8112efc4580403e323ca5b4f9e4a8401c1775,New Claim Attributes `mail` & `uid` (#3154)  # Description of Changes  Added new claims to `UsernameAttributes`: - `mail` - `uid`  Closes #3115  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [x] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [x] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [x] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,ed2ef016905daf66f5169bfbf455ecc8be93fece,https://github.com/Stirling-Tools/Stirling-PDF/commit/ed2ef016905daf66f5169bfbf455ecc8be93fece,Memory enhancements and PDF decompress API (#3129)  # Description of Changes  - PDF split by size to check size of PDF as it splits  avoids issue were a PDFs size is different viewed vs saved due to compression caused by repeated data etc. - Additionally memory enhancements for PDF load to dynamically load in memory vs scratch - PDF Decompress API for PDF testing   ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,15d5387fdc01d50fafa4a0443816dff4d0968206,https://github.com/Stirling-Tools/Stirling-PDF/commit/15d5387fdc01d50fafa4a0443816dff4d0968206,fix desktop client stuck at 90% (#3111)  So I have added a timer to force show the desktop client after 7seconds of intiliazation (if not already visible) because it gets stuck at 90% sometimes  #2487 #2595  ---  ## Checklist  ### General  - [X] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [X] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [X] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [X] I have performed a self-review of my own code - [X] My changes generate no new warnings  ### Documentation -- No functionality change. ### UI Changes (if applicable)  - [X] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)   - [X] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.     https://github.com/user-attachments/assets/e889701e-bb21-4a06-b221-98a0faad6f2e
Stirling-Tools,Stirling-PDF,c047a97c1343daee67448470139d5a2a769471e5,https://github.com/Stirling-Tools/Stirling-PDF/commit/c047a97c1343daee67448470139d5a2a769471e5,Weasyprint forms #3077 (#3084)  This pull request introduces a small but important change to the PDF conversion functionality. The change ensures that PDF forms are supported by adding the `--pdf-forms` option to the command lists in two methods.  Changes to support PDF forms:  * [`src/main/java/stirling/software/SPDF/controller/api/converters/ConvertWebsiteToPDF.java`](diffhunk://#diff-0e78e0f49bdd0d38127cd04656de55c2eca0b56197e098c6bfceb65e8cc3cff5R75): Added the `--pdf-forms` option to the command list in the `urlToPdf` method. * [`src/main/java/stirling/software/SPDF/utils/FileToPdf.java`](diffhunk://#diff-337516e2839031154412aa3e7c9a73402f3a630813a1946eae78f8a84e9bbe7fR56): Added the `--pdf-forms` option to the command list in the `convertHtmlToPdf` method.  Closes #3077  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,3cfcfb2d4ea52dc35904defa66b3b185329ca3c2,https://github.com/Stirling-Tools/Stirling-PDF/commit/3cfcfb2d4ea52dc35904defa66b3b185329ca3c2,Compression fixes (#3081)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,f3a413fe82d5df1c9f2c9ad4fb68b7ea4c2ce30e,https://github.com/Stirling-Tools/Stirling-PDF/commit/f3a413fe82d5df1c9f2c9ad4fb68b7ea4c2ce30e,Remove book site and translation entries (#3078)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,9152e64b9f091dc2fdfe4ad01b4e10bf74cbb853,https://github.com/Stirling-Tools/Stirling-PDF/commit/9152e64b9f091dc2fdfe4ad01b4e10bf74cbb853,Remove `convertBookTypeToPdf` and Improve File Sanitization in `FileToPdf` (#3072)  # Description of Changes  Please provide a summary of the changes  including:  - **Removed `convertBookTypeToPdf` method**: - This method used `ebook-convert` from Calibre  which required external dependencies. - Its removal eliminates unnecessary process execution and simplifies the codebase.  - **Enhanced `sanitizeZipFilename` function**: - Added handling for drive letters (e.g.  `C:\`). - Ensured all slashes are normalized to forward slashes. - Improved recursive path traversal removal to prevent directory escape vulnerabilities.  - **Refactored `ProcessExecutor` output handling**: - Replaced redundant `.size() > 0` checks with `.isEmpty()`.  - **Expanded unit tests in `FileToPdfTest`**: - Added tests for `sanitizeZipFilename` to cover edge cases. - Improved test descriptions and added assertion messages. - Added debug print statements for easier test debugging.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,7a7338c6def2b84fb8eddd42afaaff29f3e38b19,https://github.com/Stirling-Tools/Stirling-PDF/commit/7a7338c6def2b84fb8eddd42afaaff29f3e38b19,OAuth 2 `redirectUri` hotfix (#3066)  # Description of Changes  - Reverted path in `OAuth2Configuration` for `redirectUri` back to 'oidc' to fix the Redirect Uri error users were facing when using SSO with Authentik - Changed log level for some logs  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [x] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [x] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [x] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [x] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,ac10c9fa4354886f3670585bd3dbc2ce9754fffd,https://github.com/Stirling-Tools/Stirling-PDF/commit/ac10c9fa4354886f3670585bd3dbc2ce9754fffd,Improved Configuration and YAML Management (#2966)  # Description of Changes  **What was changed:** - **Configuration Updates:** Replaced all calls to `GeneralUtils.saveKeyToConfig` with the new `GeneralUtils.saveKeyToSettings` method across multiple classes (e.g.  `LicenseKeyChecker`  `InitialSetup`  `SettingsController`  etc.). This update ensures consistent management of configuration settings.  - **File Path and Exception Handling:** Updated file path handling in `SPDFApplication` by creating `Path` objects from string paths and logging these paths for clarity. Also refined exception handling by catching more specific exceptions (e.g.  using `IOException` instead of a generic `Exception`).  - **Analytics Flag and Rate Limiting:** Changed the analytics flag in the application properties from a `String` to a `Boolean`  and updated related logic in `AppConfig` and `PostHogService`. The rate-limiting property retrieval in `AppConfig` was also refined for clarity.  - **YAML Configuration Management:** Replaced the previous manual  line-based YAML merging logic in `ConfigInitializer` with a new `YamlHelper` class. This helper leverages the SnakeYAML engine to load  update  and save YAML configurations more robustly while preserving comments and formatting.  **Why the change was made:** - **Improved Maintainability:** Consolidating configuration update logic into a single utility method (`saveKeyToSettings`) reduces code duplication and simplifies future maintenance.  - **Enhanced Robustness:** The new `YamlHelper` class ensures that configuration files are merged accurately and safely  minimizing risks of data loss or format corruption.  - **Better Type Safety and Exception Handling:** Switching the analytics flag to a Boolean and refining exception handling improves code robustness and debugging efficiency.  - **Clarity and Consistency:** Standardizing file path handling and logging practices enhances code readability across the project.  **Challenges encountered:** - **YAML Merging Complexity:** Integrating the new `YamlHelper` required careful handling to preserve existing settings  comments  and formatting during merges.  - **Type Conversion and Backward Compatibility:** Updating the analytics flag from a string to a Boolean required extensive testing to ensure backward compatibility and proper functionality.  - **Exception Granularity:** Refactoring exception handling from a generic to a more specific approach involved a detailed review to cover all edge cases.  Closes #<issue_number>  ---  ## Checklist  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,2ab951e080bf58f024bb5281658ac2abe56ed6a6,https://github.com/Stirling-Tools/Stirling-PDF/commit/2ab951e080bf58f024bb5281658ac2abe56ed6a6,Improve Type-Safe Casting with Pattern Matching (#2990)  # Description of Changes  Please provide a summary of the changes  including:  This PR refactors multiple instances of type casting throughout the codebase by replacing them with Java's pattern matching for `instanceof`. This approach eliminates redundant type casting  improves code readability  and reduces the chances of `ClassCastException`. The changes primarily affect authentication handling  PDF processing  and certificate validation.  ### Key Changes: - Replaced traditional `instanceof` checks followed by explicit casting with pattern matching. - Improved readability and maintainability of type-related operations. - Applied changes across security modules  PDF utilities  and image processing functions.  This refactor does not introduce new functionality but enhances the robustness and clarity of the existing code.  pending until #2818 is published  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,a1f7bb3e4aa1ae586630421f85ff142dca5c613a,https://github.com/Stirling-Tools/Stirling-PDF/commit/a1f7bb3e4aa1ae586630421f85ff142dca5c613a,Refactor Path Handling (#3041)  # Description of Changes  Please provide a summary of the changes  including:  What was changed:  - Refactored path constructions in multiple classes (e.g.  SPDFApplication.java  InstallationPathConfig.java  RuntimePathConfig.java) to use Java NIO’s Paths.get() and Path.of() instead of manual string concatenation.   Why the change was made:  - To improve code readability  maintainability  and robustness by leveraging modern Java NIO utilities. - To ensure better portability across different operating systems by avoiding hardcoded file separators.   Challenges encountered:  - Maintaining backward compatibility while transitioning from manual string concatenation to using Paths for file path construction. - Ensuring that the refactored path resolution works consistently across all supported environments (Windows  macOS  Linux  and Docker).  @Frooodle can you check the docker path `/.dockerenv`?  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [x] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,e328833f02b1d383afeec787ed2782e4a5d736dc,https://github.com/Stirling-Tools/Stirling-PDF/commit/e328833f02b1d383afeec787ed2782e4a5d736dc,Restrict Backup Import to Initialization Process and Refactor API Key Handling (#3061)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed:** - Updated the backup import logic in `InitialSecuritySetup` so that the database backup is only imported during initialization when there are no users present. If no backup exists  the admin user is initialized instead. - Refactored the API key addition in `UserService` by extracting the logic into a private helper method `saveUser(Optional<User> user)` and added a call to export the database after updating the user's API key.  - **Why the change was made:** - To prevent accidental or unintended backup imports outside the initialization process  ensuring the system only imports backups when necessary. - To improve code clarity and maintainability in the user API key management process  while ensuring that the database state is preserved via an export after key updates.  Closes https://github.com/Stirling-Tools/Stirling-PDF/discussions/3057  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,4c701b2e698af9c3e42f8aefcf180a954d6cc599,https://github.com/Stirling-Tools/Stirling-PDF/commit/4c701b2e698af9c3e42f8aefcf180a954d6cc599,SSO Refactoring (#2818)  # Description of Changes  * Refactoring of SSO code around OAuth & SAML 2 * Enabling auto-login with SAML 2 via the new `SSOAutoLogin` property * Correcting typos & general cleanup  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [x] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [x] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [x] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,69da443096b0bbaa279dee2fbebadc447453169f,https://github.com/Stirling-Tools/Stirling-PDF/commit/69da443096b0bbaa279dee2fbebadc447453169f,dynamic port for UI from 8080 up (8081 etc) (#3042)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,ee6fbdd61f16887ab84cbcfdf16d66a76c322d91,https://github.com/Stirling-Tools/Stirling-PDF/commit/ee6fbdd61f16887ab84cbcfdf16d66a76c322d91,Enhance AppUpdateService: Add `@Configuration` Annotation for Improved Spring Boot Integration (#3036)  # Description of Changes  Please provide a summary of the changes  including:  The AppUpdateService class now includes the `@Configuration` annotation in addition to the existing `@Service` annotation.  This update ensures that the class is properly registered as a configuration class within the Spring application context  thereby improving bean management and integration.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,f5ca02df1d0146f15e8f6a1cd7902606edf0e3ab,https://github.com/Stirling-Tools/Stirling-PDF/commit/f5ca02df1d0146f15e8f6a1cd7902606edf0e3ab,Dynamic paths for tools and removal of unused book endpoints (#3018)  # Description of Changes  This pull request includes several changes primarily focused on improving configuration management  removing deprecated methods  and updating paths for external dependencies. The most important changes are summarized below:  ### Configuration Management Improvements: * Added a new `RuntimePathConfig` class to manage dynamic paths for operations and pipeline configurations (`src/main/java/stirling/software/SPDF/config/RuntimePathConfig.java`). * Removed the `bookAndHtmlFormatsInstalled` bean and its associated logic from `AppConfig` and `EndpointConfiguration` (`src/main/java/stirling/software/SPDF/config/AppConfig.java`  `src/main/java/stirling/software/SPDF/config/EndpointConfiguration.java`). [[1]](diffhunk://#diff-4d774ec79aa55750c0a4739bee971b68877078b73654e863fd40ee924347e143L130-L138) [[2]](diffhunk://#diff-750f31f6ecbd64b025567108a33775cad339e835a04360affff82a09410b697dL12-L35) [[3]](diffhunk://#diff-750f31f6ecbd64b025567108a33775cad339e835a04360affff82a09410b697dL275-L280)  ### External Dependency Path Updates: * Updated paths for `weasyprint` and `unoconvert` in `ExternalAppDepConfig` to use values from `RuntimePathConfig` (`src/main/java/stirling/software/SPDF/config/ExternalAppDepConfig.java`). [[1]](diffhunk://#diff-c47af298c07c2622aa98b038b78822c56bdb002de71081e102d344794e7832a6R12-L33) [[2]](diffhunk://#diff-c47af298c07c2622aa98b038b78822c56bdb002de71081e102d344794e7832a6L104-R115)   ### Minor Adjustments: * Corrected a typo from "Unoconv" to "Unoconvert" in `EndpointConfiguration` (`src/main/java/stirling/software/SPDF/config/EndpointConfiguration.java`).  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,1e29cf43fb65b00ed81229518fe9ed5a1dc88d43,https://github.com/Stirling-Tools/Stirling-PDF/commit/1e29cf43fb65b00ed81229518fe9ed5a1dc88d43,Fix: Analytics Initialization Behavior (#3031)  # Description of Changes  Please provide a summary of the changes  including:  What was changed:  - Modified the default value of enableAnalytics in settings.yml.template from `true` to `undefined`.  Why the change was made:  - The analytics setting was updated to prevent the value from defaulting to true during initialization  which suppressed the display of the prompt dialog. Changing it to `undefined` ensures that the user is explicitly prompted to enable or disable analytics  thereby improving user control.  Closes #(issue_number)  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,6ac804e994301eea0274796b291720b192df5c3c,https://github.com/Stirling-Tools/Stirling-PDF/commit/6ac804e994301eea0274796b291720b192df5c3c,Feature/convert to grayscale (#3003)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed -Grayscale Image Compression: Modified the compressImagesInPDF method to optionally convert images to grayscale during compression. This is achieved by checking if the grayScale flag is true (or if the image is already in grayscale) and then processing the image accordingly.  UI Update: Updated the compress-pdf.html file by adding a checkbox for grayscale compression. Additionally  a new translation key compress.grayscale.label with the text "Apply Grayscale for Compression" has been added across all supported languages. - Why the change was made -Enhanced Compression Options: This feature provides users with an option to compress images in PDFs more effectively by reducing the color complexity  which can lead to smaller file sizes.  Improved Flexibility: It allows users to decide whether they want to maintain the original color images or opt for a grayscale version. - Any challenges encountered - The translation for compress.grayscale.label was generated using an automated translator  so it might not be completely accurate and could require further review.  Closes #2603  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,167c85e73fc0ac8053724a0b78d817f3131b19de,https://github.com/Stirling-Tools/Stirling-PDF/commit/167c85e73fc0ac8053724a0b78d817f3131b19de,Update UserService.java to generate API key if empty (#3016)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,d34c44ed7bbf60d85f96744f99cd38a38911daa3,https://github.com/Stirling-Tools/Stirling-PDF/commit/d34c44ed7bbf60d85f96744f99cd38a38911daa3,[Test PR] Desktop fix and unoconv to unoserver  (#2971)  # Description of Changes  This pull request includes several updates to the Docker configuration and Java application UI scaling. The changes enhance environment variable management  dependency installation  and UI responsiveness to different screen sizes.  ### Docker Configuration Updates: * Added new environment variables `STIRLING_PDF_DESKTOP_UI`  `PYTHONPATH`  `UNO_PATH`  and `URE_BOOTSTRAP` to `Dockerfile` and `Dockerfile.fat` to improve the configuration and integration of the LibreOffice environment. [[1]](diffhunk://#diff-dd2c0eb6ea5cfc6c4bd4eac30934e2d5746747af48fef6da689e85b752f39557L38-R46) [[2]](diffhunk://#diff-571631582b988e88c52c86960cc083b0b8fa63cf88f056f26e9e684195221c27L40-R49) * Updated the `CMD` instruction in `Dockerfile` and `Dockerfile.fat` to run both the Java application and `unoserver` simultaneously. [[1]](diffhunk://#diff-dd2c0eb6ea5cfc6c4bd4eac30934e2d5746747af48fef6da689e85b752f39557L87-R96) [[2]](diffhunk://#diff-571631582b988e88c52c86960cc083b0b8fa63cf88f056f26e9e684195221c27L87-R100) * Modified the `RUN` instruction to include additional Python dependencies and setup a virtual environment. [[1]](diffhunk://#diff-dd2c0eb6ea5cfc6c4bd4eac30934e2d5746747af48fef6da689e85b752f39557L68-R81) [[2]](diffhunk://#diff-571631582b988e88c52c86960cc083b0b8fa63cf88f056f26e9e684195221c27R72-R86)  ### Workflow Enhancements: * Added `STIRLING_PDF_DESKTOP_UI` environment variable to the GitHub Actions workflows (`PR-Demo-Comment.yml` and `push-docker.yml`) to ensure consistent environment settings. [[1]](diffhunk://#diff-145fe5c0ed8c24e4673c9ad39800dd171a2d0a2e8050497cff980fc7e3a3df0dR106) [[2]](diffhunk://#diff-76056236de05155107f6a660f1e3956059e37338011b8f0e72188afcb9b17b6fR41)  ### Java Application UI Scaling: * Introduced `UIScaling` utility to dynamically adjust the size of UI components based on screen resolution in `DesktopBrowser` and `LoadingWindow` classes. [[1]](diffhunk://#diff-dff83b0fe53cba8ee80dc8cee96b9c2bfec612ec1f2c636ebdf22dedb36671e8L218-R219) [[2]](diffhunk://#diff-dff83b0fe53cba8ee80dc8cee96b9c2bfec612ec1f2c636ebdf22dedb36671e8L267-R270) [[3]](diffhunk://#diff-3e287daf297213b698b3c94d6e6ed4aae139d570ba6b115da459d72b5c36c42fL44-R64) [[4]](diffhunk://#diff-3e287daf297213b698b3c94d6e6ed4aae139d570ba6b115da459d72b5c36c42fL86-R102) * Improved the loading of icons by using the `UIScaling` utility for better visual quality.  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: pixeebot[bot] <104101892+pixeebot[bot]@users.noreply.github.com> Co-authored-by: a <a>
Stirling-Tools,Stirling-PDF,82b1ab4263b40c67951a3fc555885c7f9fa7a45b,https://github.com/Stirling-Tools/Stirling-PDF/commit/82b1ab4263b40c67951a3fc555885c7f9fa7a45b,Fix/full invert crash 2942 (#2957)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Modified the `convertToBufferedImageTpFile` to use `File.createTempFile()` instead of writing to `"image.png"` in the current directory. - This change ensures the file is saved in the default temporary directory  preventing permission issues.  - Why the change was made - Previously  the method attempted to save the file in the current working directory  which caused permission errors (`java.io.FileNotFoundException: image.png (Permission denied)`).  - Any challenges encountered  Closes #2942  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,41b41996c582b7dca84c4743211889054f479110,https://github.com/Stirling-Tools/Stirling-PDF/commit/41b41996c582b7dca84c4743211889054f479110,fix(pdf): resolve compression error and file overwrite issues (#2937)  # Description of Changes  - **What was changed:** - Modified the file handling logic to avoid overwriting the source PDF while it is being read  which previously led to corrupted output files. -Modified the logic where optimizeLevel is 9 and we are chacking for optimizeLevel < 9. - **Why the change was made:** - The original compression process would stuck when dealing with larger files  failing to meet the specified target size limits. - Overwriting the input file during processing was causing warnings and potential file corruption  which could lead to instability and incorrect outputs.  - **Any challenges encountered:**  Closes #2930  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally
Stirling-Tools,Stirling-PDF,6a3064f7f2baa9db871dd7f8a8e082ea1b3c1163,https://github.com/Stirling-Tools/Stirling-PDF/commit/6a3064f7f2baa9db871dd7f8a8e082ea1b3c1163,Fix issue #2511: Fix broken ZIP issue by adding zipOut.finish() (#2890)  ---  # Description of Changes  ### What was changed - Added `zipOut.finish()` to ensure the ZIP file is properly finalized after writing all entries. - This ensures the central directory metadata is written  fixing the issue where the ZIP file was incomplete or broken.  ### Why the change was made - The issue (#2511) reported that splitting a PDF resulted in a broken ZIP file. The root cause was the missing central directory due to improper stream finalization. - Adding `zipOut.finish()` explicitly ensures the ZIP file is correctly structured and can be extracted without errors.  ### Challenges encountered  Closes #2511  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details. - Tested with various PDFs to ensure the ZIP file is created correctly. - Verified ZIP integrity using `unzip -t` and manual extraction.  ---
Stirling-Tools,Stirling-PDF,0233086487b15149c2784e7451754fbe6d055416,https://github.com/Stirling-Tools/Stirling-PDF/commit/0233086487b15149c2784e7451754fbe6d055416,pipeline bug  doc bugs  auto split new URL and doc (#2906)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed Pipeline bug where files would be processed even when incorrect format some API docs had spaces causing format issues Auto split doc now links to [stirlingpdf.com](http://stirlingpdf.com/) not github + updated old logo removed old docs not used  - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: a <a>
Stirling-Tools,Stirling-PDF,bf65c456d1829dd4109aff51bf9c59e4845bd381,https://github.com/Stirling-Tools/Stirling-PDF/commit/bf65c456d1829dd4109aff51bf9c59e4845bd381,PDFA fixes (#2896)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,507d21772d212299514f254471008433a64ed3ff,https://github.com/Stirling-Tools/Stirling-PDF/commit/507d21772d212299514f254471008433a64ed3ff,Fix issue #2842: Handle qpdf exit code 3 as success with warnings (#2883)  # Description of Changes  Please provide a summary of the changes  including:  - **What was changed**: - Modified the `ProcessExecutor` class to accept exit code `3` from **qpdf** as a success with warnings. - Added a check to ensure that only **qpdf**’s exit code `3` is treated as a warning. - Added a warning log for **qpdf** exit code `3` to provide better visibility into the repair process.  - **Why the change was made**: - The repair process was failing when **qpdf** returned exit code `3`  even though the operation succeeded with warnings. This caused unnecessary errors for users. - The changes ensure that PDFs with minor structural issues (e.g.  mismatched object counts) are still repaired successfully  while logging warnings for transparency. - Added a check to ensure that only **qpdf**’s exit code `3` is treated as a warning  preventing potential issues with other tools that might use exit code `3` for actual errors.  Closes #2842  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Testing (if applicable)  - [x] I have tested my changes locally. - Verified that exit code `3` is only treated as a warning for **qpdf** and not for other tools.  ---  ### Additional Notes - The changes align with **qpdf**'s behavior  where exit code `3` indicates a successful operation with warnings. - Added a check to ensure that only **qpdf**’s exit code `3` is treated as a warning  preventing potential issues with other tools.  Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,2d3611fd00d6e5638294fca67b0e6d9b97aaef70,https://github.com/Stirling-Tools/Stirling-PDF/commit/2d3611fd00d6e5638294fca67b0e6d9b97aaef70,Pipeline fix for some features missing documentation (#2882)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,69d4b52b06a0f7f738c1edd6bcb5ea976397084b,https://github.com/Stirling-Tools/Stirling-PDF/commit/69d4b52b06a0f7f738c1edd6bcb5ea976397084b,Update sonarqube.yml and removal of gradle keys (#2866)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,b37457b41d935b806d8c610b797f1310df1449ea,https://github.com/Stirling-Tools/Stirling-PDF/commit/b37457b41d935b806d8c610b797f1310df1449ea,Add: Configurable UI Language Support with Dynamic Filtering (#2846)  # Description of Changes  ### Summary - Added support for configuring UI languages via `settings.yml` (`languages` field). - Modified `LanguageService` to respect the configured languages  while ensuring British English (`en_GB`) is always enabled. - Updated Thymeleaf templates to dynamically display only the allowed languages. - Improved logging and refactored some list-to-set conversions for better efficiency.  ### Why the Change? - Allows administrators to limit available UI languages instead of displaying all detected languages. - Provides better customization options and simplifies language management.  ### Challenges Encountered - Ensuring backwards compatibility: If `languages` is empty  all languages remain enabled. - Handling `Set<String>` instead of `List<String>` in `LanguageService` for optimized lookups.  ---  ## Checklist  ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [x] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [x] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [x] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.
Stirling-Tools,Stirling-PDF,6ae2fddd486dcd4f51ea9c54d81d33d9a739f714,https://github.com/Stirling-Tools/Stirling-PDF/commit/6ae2fddd486dcd4f51ea9c54d81d33d9a739f714,added option for disabling HTML Sanitize (#2831)  # Description of Changes  Please provide a summary of the changes  including:  - added disableSanitize: false # set to 'true' to disable Sanitize HTML  set to false to enable Sanitize HTML; (can lead to injections in HTML) - Some users uses this on local boxes  and uses Google Fonts  and base64 image src.   ### General  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [x] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [x] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [x] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: blaz.carli <blaz.carli@arctur.si> Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,382f5603a8219289e7351b703e972f0011e296b9,https://github.com/Stirling-Tools/Stirling-PDF/commit/382f5603a8219289e7351b703e972f0011e296b9,Config rework (#2823)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: a <a>
Stirling-Tools,Stirling-PDF,60cc613c633ba2c43d216aff03d00361c04047a1,https://github.com/Stirling-Tools/Stirling-PDF/commit/60cc613c633ba2c43d216aff03d00361c04047a1,Homepage update (#2663)  # Description  Please provide a summary of the changes  including relevant motivation and context.  Closes #(issue_number)  ## Checklist  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [ ] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ---------  Co-authored-by: Reece Browne <reece@stirling.pdf>
Stirling-Tools,Stirling-PDF,f59e02480205c66a50ef41929cef9b29a0fa50e4,https://github.com/Stirling-Tools/Stirling-PDF/commit/f59e02480205c66a50ef41929cef9b29a0fa50e4,[Fix] Handle missing end page in PDF split range (#2816)  ## Summary of Changes  **What was changed:** - Updated the `handlePart` method to handle cases where the end page is not specified (e.g.  '1-'). - The method now defaults to the last page of the PDF  improving the feature's usability.  **Why the change was made:** - Users often forget the total page count when splitting PDFs. The new feature ensures that when only the starting page is specified  the range defaults to the last page  preventing errors like `ArrayIndexOutOfBoundsException`.  **Any challenges encountered:** - No significant challenges encountered while implementing this feature.  **Closes #1576**  ---  ## Checklist  ### General - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) - [x] I have performed a self-review of my own code - [x] My changes generate no new warnings   ### Testing - [x] I have tested my changes locally.
Stirling-Tools,Stirling-PDF,94ea723326aaffe44619b122be086f49fbd2c08f,https://github.com/Stirling-Tools/Stirling-PDF/commit/94ea723326aaffe44619b122be086f49fbd2c08f,Posthog to log to log file (#2813)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: a <a>
Stirling-Tools,Stirling-PDF,dab6613f1ba6a5d00c7fdeddaf8f84294485e31b,https://github.com/Stirling-Tools/Stirling-PDF/commit/dab6613f1ba6a5d00c7fdeddaf8f84294485e31b,Test cleanup  JVM GC and api (#2787)  # Description of Changes  Please provide a summary of the changes  including:  - What was changed - Why the change was made - Any challenges encountered  Closes #(issue_number)  ---  ## Checklist  ### General  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have read the [Stirling-PDF Developer Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md) (if applicable) - [ ] I have read the [How to add new languages to Stirling-PDF](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md) (if applicable) - [ ] I have performed a self-review of my own code - [ ] My changes generate no new warnings  ### Documentation  - [ ] I have updated relevant docs on [Stirling-PDF's doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) (if functionality has heavily changed) - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ### UI Changes (if applicable)  - [ ] Screenshots or videos demonstrating the UI changes are attached (e.g.  as comments or direct attachments in the PR)  ### Testing (if applicable)  - [ ] I have tested my changes locally. Refer to the [Testing Guide](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/DeveloperGuide.md#6-testing) for more details.  ---------  Co-authored-by: a <a>
Stirling-Tools,Stirling-PDF,a46a570c8a4c1bd342f52f3e9ae91c43887a6cd3,https://github.com/Stirling-Tools/Stirling-PDF/commit/a46a570c8a4c1bd342f52f3e9ae91c43887a6cd3,Pdf to markdown (#2730)  # Description  Please provide a summary of the changes  including relevant motivation and context.  Closes #(issue_number)  ## Checklist  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [ ] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ---------  Co-authored-by: a <a>
Stirling-Tools,Stirling-PDF,2229f38602defecbeed1c0bdcbd26d9c7a624c03,https://github.com/Stirling-Tools/Stirling-PDF/commit/2229f38602defecbeed1c0bdcbd26d9c7a624c03,#2418 updating jpackage config (#2713)  # Description  Updating the `jpackage` configuration in our build.gradle and CI to enable installation of the app on multiple OSs  Closes #2418  ## Checklist  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have performed a self-review of my own code - [x] I have attached images of the change if it is UI based - [x] I have commented my code  particularly in hard-to-understand areas - [x] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [x] My changes generate no new warnings - [x] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
Stirling-Tools,Stirling-PDF,3799ab529f5b92e68eb25e06d7f0751c3ce60de2,https://github.com/Stirling-Tools/Stirling-PDF/commit/3799ab529f5b92e68eb25e06d7f0751c3ce60de2,Update OptimizePdfRequest.java (#2720)  # Description  Please provide a summary of the changes  including relevant motivation and context.  Closes #2417  ## Checklist  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [ ] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
Stirling-Tools,Stirling-PDF,3e216872ce20305182d520186cf3e09a1d49649e,https://github.com/Stirling-Tools/Stirling-PDF/commit/3e216872ce20305182d520186cf3e09a1d49649e,test all pages load correctly  (#2699)  # Description  Please provide a summary of the changes  including relevant motivation and context.  Closes #(issue_number)  ## Checklist  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [ ] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
Stirling-Tools,Stirling-PDF,e0f553c15a12f805123a820988012f0de22b5132,https://github.com/Stirling-Tools/Stirling-PDF/commit/e0f553c15a12f805123a820988012f0de22b5132,Add tests via TestDriverAI (#1957) (#2005)  * initial Commit  * update prerun  * tweak the prompt  * update the test  * finetune prompt  * change the prompt  * minor change to retry test  * add debug  ---------  # Description  Please provide a summary of the changes  including relevant motivation and context.  Closes #(issue_number)  ## Checklist  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have performed a self-review of my own code - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ---------  Signed-off-by: GitHub Action <action@github.com> Signed-off-by: dependabot[bot] <support@github.com> Signed-off-by: Ludovic Ortega <ludovic.ortega@adminafk.fr> Co-authored-by: Tarun Kumar S <srfashions.tarun@gmail.com> Co-authored-by: Ian Jennings <ian@meetjennings.com> Co-authored-by: Corbinian Grimm <23664150+pixma140@users.noreply.github.com> Co-authored-by: albanobattistella <34811668+albanobattistella@users.noreply.github.com> Co-authored-by: Eric <71648843+sbplat@users.noreply.github.com> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: GitHub Action <action@github.com> Co-authored-by: swanemar <107953493+swanemar@users.noreply.github.com> Co-authored-by: pixeebot[bot] <104101892+pixeebot[bot]@users.noreply.github.com> Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Co-authored-by: Manuel Mora Gordillo <manuito@gmail.com> Co-authored-by: Manu <manuel@fusiontelecom.co> Co-authored-by: Ludy <Ludy87@users.noreply.github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: Florian Fish <florian@poissonmail.fr> Co-authored-by: reecebrowne <74901996+reecebrowne@users.noreply.github.com> Co-authored-by: Dimitrios Kaitantzidis <james_k23@hotmail.gr> Co-authored-by: Rania Amina <reaamina@gmail.com> Co-authored-by: Copilot Autofix powered by AI <62310815+github-advanced-security[bot]@users.noreply.github.com> Co-authored-by: Ludovic Ortega <ludovic.ortega@adminafk.fr> Co-authored-by: Philip H. <47042125+pheiduck@users.noreply.github.com> Co-authored-by: Saud Fatayerji <Sf298@users.noreply.github.com> Co-authored-by: MaratheHarshad <97970262+MaratheHarshad@users.noreply.github.com> Co-authored-by: Harshad Marathe <harshad@DESKTOP-1MNKUHA> Co-authored-by: ninjat <hotanya.r@gmail.com> Co-authored-by: Peter Dave Hello <hsu@peterdavehello.org> Co-authored-by: Rafael Encinas <rafael.encinas@encora.com> Co-authored-by: Renan <82916964+thisisrenan@users.noreply.github.com> Co-authored-by: leo-jmateo <128976497+leo-jmateo@users.noreply.github.com> Co-authored-by: S. Neuhaus <neuhaus@users.noreply.github.com> Co-authored-by: Dimitris Kaitantzidis <44621809+DimK10@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,8619b1cf590dfbdbffdeacf51381a6341f4f0bd3,https://github.com/Stirling-Tools/Stirling-PDF/commit/8619b1cf590dfbdbffdeacf51381a6341f4f0bd3,Restriction of username and email (#2676)  # Description  - https://github.com/Stirling-Tools/Stirling-PDF/security/code-scanning/8 - https://github.com/Stirling-Tools/Stirling-PDF/security/code-scanning/9 - https://github.com/Stirling-Tools/Stirling-PDF/security/code-scanning/21 - https://github.com/Stirling-Tools/Stirling-PDF/security/code-scanning/22  ## Checklist  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [x] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
Stirling-Tools,Stirling-PDF,1bd7e420e53f752ed2192c58619db3c1388748ad,https://github.com/Stirling-Tools/Stirling-PDF/commit/1bd7e420e53f752ed2192c58619db3c1388748ad,install paths dynmaic (#2668)  # Description  Please provide a summary of the changes  including relevant motivation and context.  Closes #(issue_number)  ## Checklist  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [ ] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
Stirling-Tools,Stirling-PDF,cce4693aea0b04069a5aa0f26c69470cd9a0665e,https://github.com/Stirling-Tools/Stirling-PDF/commit/cce4693aea0b04069a5aa0f26c69470cd9a0665e,Fix: `NoSuchFileException` if `configs\db\backup` is not present on first start (#2665)  # Description  ```bash 20:38:21.452 [restartedMain] ERROR s.s.S.c.s.database.DatabaseService - Error reading backup directory: configs\db\backup java.nio.file.NoSuchFileException: configs\db\backup at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85) at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103) at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108) at java.base/sun.nio.fs.WindowsDirectoryStream.<init>(WindowsDirectoryStream.java:86) at java.base/sun.nio.fs.WindowsFileSystemProvider.newDirectoryStream(WindowsFileSystemProvider.java:541) at java.base/java.nio.file.Files.newDirectoryStream(Files.java:613) at stirling.software.SPDF.config.security.database.DatabaseService.getBackupList(DatabaseService.java:80) at stirling.software.SPDF.config.security.database.DatabaseService.exportDatabase(DatabaseService.java:156) at stirling.software.SPDF.config.security.UserService.saveUser(UserService.java:214) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:359) at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:723) at stirling.software.SPDF.config.security.UserService$$SpringCGLIB$$3.saveUser(<generated>) at stirling.software.SPDF.config.security.InitialSecuritySetup.createDefaultAdminUser(InitialSecuritySetup.java:76) at stirling.software.SPDF.config.security.InitialSecuritySetup.initializeAdminUser(InitialSecuritySetup.java:67) at stirling.software.SPDF.config.security.InitialSecuritySetup.init(InitialSecuritySetup.java:42) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMethod.invoke(InitDestroyAnnotationBeanPostProcessor.java:457) at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeInitMethods(InitDestroyAnnotationBeanPostProcessor.java:401) at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:219) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:423) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:601) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523) at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:336) at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:289) at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:334) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) at org.springframework.beans.factory.support.DefaultListableBeanFactory.instantiateSingleton(DefaultListableBeanFactory.java:1122) at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingleton(DefaultListableBeanFactory.java:1093) at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:1030) at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:987) at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:627) at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146) at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:752) at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:439) at org.springframework.boot.SpringApplication.run(SpringApplication.java:318) at stirling.software.SPDF.SPDFApplication.main(SPDFApplication.java:117) at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50) ```  ## Checklist  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [x] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
Stirling-Tools,Stirling-PDF,b98f8627ac3c4c677c3ff224d3d8299032f993e1,https://github.com/Stirling-Tools/Stirling-PDF/commit/b98f8627ac3c4c677c3ff224d3d8299032f993e1,Csrf fix and ssoAutoLogin for enterprise users (#2653)  This pull request includes several changes to the `SecurityConfiguration` and other related classes to enhance security and configuration management. The most important changes involve adding new beans  modifying logging levels  and updating dependency injections.  Enhancements to security configuration:  * [`src/main/java/stirling/software/SPDF/config/security/SecurityConfiguration.java`](diffhunk://#diff-49df1b16b72e9fcaa7d0c58f46c94ffda0033f5f5e3ddab90a88e2f9022b66f4L3-L36): Added new dependencies and beans for `GrantedAuthoritiesMapper`  `RelyingPartyRegistrationRepository`  and `OpenSaml4AuthenticationRequestResolver`. Removed unused imports and simplified the class by removing the `@Lazy` annotation from `UserService`. [[1]](diffhunk://#diff-49df1b16b72e9fcaa7d0c58f46c94ffda0033f5f5e3ddab90a88e2f9022b66f4L3-L36) [[2]](diffhunk://#diff-49df1b16b72e9fcaa7d0c58f46c94ffda0033f5f5e3ddab90a88e2f9022b66f4L46-L63) [[3]](diffhunk://#diff-49df1b16b72e9fcaa7d0c58f46c94ffda0033f5f5e3ddab90a88e2f9022b66f4L75-R52) [[4]](diffhunk://#diff-49df1b16b72e9fcaa7d0c58f46c94ffda0033f5f5e3ddab90a88e2f9022b66f4R66-L98) [[5]](diffhunk://#diff-49df1b16b72e9fcaa7d0c58f46c94ffda0033f5f5e3ddab90a88e2f9022b66f4L109-R85) [[6]](diffhunk://#diff-49df1b16b72e9fcaa7d0c58f46c94ffda0033f5f5e3ddab90a88e2f9022b66f4R96-R98)  Logging improvements:  * [`src/main/java/stirling/software/SPDF/EE/KeygenLicenseVerifier.java`](diffhunk://#diff-742f789731a32cb5aa20f7067ef18049002eec2a4909ef6f240d2a26bdcb53c4L97-R97): Changed the logging level from `info` to `debug` for the license validation response body to reduce log verbosity in production.  Configuration updates:  * [`src/main/java/stirling/software/SPDF/EE/EEAppConfig.java`](diffhunk://#diff-d842c2a4cf43f37ab5edcd644b19a51d614cb0e39963789e1c7e9fb28ddc1de8R30-R34): Added a new bean `ssoAutoLogin` to manage single sign-on auto-login configuration in the enterprise edition.  These changes collectively enhance the security configuration and logging management of the application.  Please provide a summary of the changes  including relevant motivation and context.  Closes #(issue_number)  ## Checklist  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [ ] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
Stirling-Tools,Stirling-PDF,f379c27bd7e153c88ee8ae9ee21d0e59cd4e52bc,https://github.com/Stirling-Tools/Stirling-PDF/commit/f379c27bd7e153c88ee8ae9ee21d0e59cd4e52bc,Add Conditional Handling for H2SQL Databases and Improve Database Compatibility (#2632)  # Description  1. **Conditional Support for DatabaseController**: - The `DatabaseController` is now annotated with `@Conditional(H2SQLCondition.class)` to ensure it is only available for H2SQL database setups. - This prevents unnecessary exposure of endpoints when the application is configured for H2SQL.  2. **Database Web Template Adjustments**: - The UI elements related to database management are conditionally hidden when the database type is not supported (e.g.  `databaseVersion == 'Unknown'`). - Improves user experience by avoiding unsupported operations for non-H2SQL or unknown databases.  3. **Model Attribute Updates**: - Added a check in `DatabaseWebController` to set an informational message (`notSupported`) when the database version is unknown.  4. **H2 Database Compatibility**: - Additional adjustments to ensure the application gracefully handles H2-specific functionality without affecting other database configurations.  5. **Build File Updates**: - Updated the `build.gradle` file to exclude `H2SQLCondition` and related controllers when specific configurations (e.g.  security or database type) are disabled.  ### Benefits: - Enhances application flexibility by adapting to the configured database type. - Improves user feedback with clear messaging and UI adjustments for unsupported operations. - Prevents accidental exposure of database endpoints in H2SQL setups.  ## Checklist  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [x] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [x] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
Stirling-Tools,Stirling-PDF,41dce0680435789c9f83620ee542bf9a8829015e,https://github.com/Stirling-Tools/Stirling-PDF/commit/41dce0680435789c9f83620ee542bf9a8829015e,#2270: External DB Support (#2457)  # Description  External DB support for Stirling PDF. You can now choose between the default H2 or PostgreSQL by setting the new `enableCustomDatabase` property to `true` or `false`.  To enable your own custom (PostgreSQL) database: - Set `enableCustomDatabase` to `true` - Add your database url to `customDatabaseUrl` - Set your `username` and `password`  Closes #2270  ## Checklist  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have performed a self-review of my own code - [x] I have attached images of the change if it is UI based - [x] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [x] My changes generate no new warnings - [x] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
Stirling-Tools,Stirling-PDF,ed633616e7e9b570adb136e926079ea72c3c86d3,https://github.com/Stirling-Tools/Stirling-PDF/commit/ed633616e7e9b570adb136e926079ea72c3c86d3,File paths dynamic  (#2605)  # Description  Please provide a summary of the changes  including relevant motivation and context.  Closes #(issue_number)  ## Checklist  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [ ] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ---------  Co-authored-by: pixeebot[bot] <104101892+pixeebot[bot]@users.noreply.github.com> Co-authored-by: a <a>
Stirling-Tools,Stirling-PDF,22af79a279e026d5937ac7b744b1dc209f198c81,https://github.com/Stirling-Tools/Stirling-PDF/commit/22af79a279e026d5937ac7b744b1dc209f198c81,Feature: Support manual redaction (#2433)  # Description  ## Manual Redaction:  - ### Text Selection-based redaction: - ![image](https://github.com/user-attachments/assets/e59c5e6c-ef52-4f54-a98e-fc26e3226c8e) - Users can now redact currently selected text by selecting the text then clicking `ctrl + s` shortcut or by pressing on **apply/save/disk icon** in the toolbar. - Users can delete/cancel the redacted area by clicking on the box containing the text  then clicking on `delete/trash` icon or by using the shortcut `delete`. - Users can customize the color of the redacted area/text (after the redaction was applied) by simply clicking on the box containing the text/area then clicking on the `color palette` icon and choosing the color they want. - Users can choose to select the color of redaction before redacting text or applying changes (this only affects newly created redaction areas  to change the color of an existing one; check the previous bullet point).   - ### Draw/Area-based redaction: - ![image](https://github.com/user-attachments/assets/e2968ae3-ebaf-497e-b3bd-0c8c8f4ee157) - Users can now redact an area in the page by selecting the then clicking `ctrl + s` shortcut or by pressing on **apply/save/disk icon** in the toolbar. - Users can delete/cancel the redacted area by clicking on the drawn box  then clicking on `delete/trash` icon or by using the shortcut `delete` (requires temporarily turning off drawing mode). - Users can customize the color of the redacted area (after the redaction was applied) by simply clicking on the box containing the area then clicking on the `color palette` icon and choosing the color they want. - Users can choose to select the color of redaction before drawing the box or applying changes (this only affects newly created redaction areas  to change the color of an existing one; check the previous bullet point).  - ### Page-based redaction: - ![image](https://github.com/user-attachments/assets/aba59432-23e7-4fe6-aa28-872c23a91242) - Users can now redact **ENTIRE** pages by specifying the page number(s)  range(s) or functions. - Users can customize the color of page-based redaction (doesn't affect text-based nor draw-based redactions).  ### Redaction modes: There are three modes of redaction/operation currently supported - Text Selection-based redaction (TEXT) - Draw/Area-based redaction (DRAWING) - None - by simply not choosing any of the above modes (NONE).  ## How to use:  - **Text Selection-based redaction:** click on this icon in the toolbar ![image](https://github.com/user-attachments/assets/52cc31ef-6946-482c-84a2-1ddb79646dd8) to enable `text-selection redaction mode` then select the text you want to redact then press `ctrl + s` or click on the disk/save icon ![image](https://github.com/user-attachments/assets/f2bdf2f2-ee07-4682-bb9a-95e13a1004cf). - **Draw/Area-based redaction:** click on this icon in the toolbar ![image](https://github.com/user-attachments/assets/fe00dca9-761e-47a0-a748-2041830dc73e) to enable `draw/area-based redaction` then `left mouse click (LMB)` on the starting point of the rectangle  then once you are satisfied with the rectangle's placement/dimensions then `left mouse click (LMB)` again to apply the redaction. - **Example:** `Left mouse click (LMB)` then move mouse to the right then bottom then `Left mouse click (LMB)`. - Note: Red box/rectangle borders indicate that you have not yet saved (you need to left click on the page to save) ![image](https://github.com/user-attachments/assets/5ce5f789-9d6f-4984-8555-e8fef2a3e3cc) once saved the borders will become green ![image](https://github.com/user-attachments/assets/85cabb9f-e7ee-4268-90cd-80493b625466) (they also become clickable/hover-able when drawing mode is off). - **Page-based redactions:**: Insert the page number(s)  range(s) and/or functions (separated by ` `) then select your preferred color and click on `Redact` to submit.  ![image](https://github.com/user-attachments/assets/ed8a0a98-32b2-4ae1-a3c7-c54bfe0fea66)  - **Color Customizations:** - You can change the redaction color for new redactions by clicking on this icon in the toolbar ![image](https://github.com/user-attachments/assets/bad573ee-0545-4329-b131-2022f970f134). - You can change the redaction color for existing redactions by hovering over the redaction box then clicking on it (`Left mouse click LMB`) then clicking on color palette (highlighted in red in the picture) ![image](https://github.com/user-attachments/assets/22281a81-2cd9-4771-9a93-a75b6dd93433) then select your preferred color.  - **Deletions:** - You can delete a redacted area by hovering over the redaction box then clicking on it (`Left mouse click LMB`) then clicking on the trash icon (highlighted in red in the picture) ![image](https://github.com/user-attachments/assets/f0347279-8211-4b1c-a91d-c1fcb929cc5d).  ## Card in the home page:  ![image](https://github.com/user-attachments/assets/b3fb16eb-5ff0-4548-9f22-b1b8fe162c8b)  Closes #465  ## Checklist  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have performed a self-review of my own code - [x] I have attached images of the change if it is UI based - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [ ] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ---------  Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,b8303e3860842cb3a84e3958fd28f4e1b3fd60c4,https://github.com/Stirling-Tools/Stirling-PDF/commit/b8303e3860842cb3a84e3958fd28f4e1b3fd60c4,Pdf to image custom page selection (#2576)  # Description  Implemented custom page selection for the pdf-to-image feature  allowing users to specify which PDF pages to convert to images.  1. Variable Renaming: Changed singleOrMultiple to imageResultType because it supports three options: single  multiple  and custom. 2. New Field: Added pageNumbers to accept user-defined page selections. This field appears only when custom is selected in the UI. 3. New Method: Added getPageIndicesToConvert to process and validate the specified page numbers. 4. Method Update: Updated convertFromPdf to handle custom page numbers  ensuring only selected pages are converted. 5. Translation Properties: Added two new English translation properties  custom and customPageNumber  to all language files with placeholder values. These will need to be translated into country-specific languages in the future.  Note: If an invalid page number is provided (zero  negative  or exceeds page count)  a single image containing all PDF pages is generated.  Closes #918  ## Checklist  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have performed a self-review of my own code - [x] I have attached images of the change if it is UI based - [x] I have commented my code  particularly in hard-to-understand areas - [x] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [x] My changes generate no new warnings - [x] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)  ![Screenshot 2025-01-02 at 12 31 29 AM](https://github.com/user-attachments/assets/c4ba3f31-5dd6-4a17-991e-51b86c2eb466) ![Screenshot 2025-01-02 at 12 31 49 AM](https://github.com/user-attachments/assets/3e800a95-2088-4f69-8a01-bd03d7b9e471)  ---------  Co-authored-by: Sai Kumar J <saikumar@Sais-MacBook-Air.local> Co-authored-by: Ludy <Ludy87@users.noreply.github.com> Co-authored-by: saikumar <saikumar.jetti@gmail.com> Co-authored-by: Anthony Stirling <77850077+Frooodle@users.noreply.github.com>
Stirling-Tools,Stirling-PDF,f45de05c99ddc11cb2562b7e448211b1bbf1f2d1,https://github.com/Stirling-Tools/Stirling-PDF/commit/f45de05c99ddc11cb2562b7e448211b1bbf1f2d1,Fixes SSO login rejection (#2566)  # Description  ## Checklist  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [x] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [x] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
Stirling-Tools,Stirling-PDF,d5faddbc8516b7ae9ea3b08ad0368d6725d28368,https://github.com/Stirling-Tools/Stirling-PDF/commit/d5faddbc8516b7ae9ea3b08ad0368d6725d28368,Enhancement: Enhance NFunction evaluation and support advanced NFunctions (#2577)  # Description  Enhance NFunction sanitization and support advanced functions: - Start page counting from 1 rather than 0 as PDFs are one based from the user's perspective  thus functions results would be affected by starting with "0" rather than "1". - Ignore out of bound results rather than stopping iterations to work with functions such as (n - 4) when page count is 10 as we would get positive values when n > 4. - Remove spaces to support expressions such as 2n + 1 rather just 2n+1. - Support advanced functions as follows: - Support expressions such as follows 5(n-1)  n(n-1)  expressions followed by opening rounded without '*' operator. - Support expressions such as follows (n-1)5  (n-1)n  expressions preceded closing rounded without '*' operator. - Support consecutive "n" expressions  examples: nnn  2nn  nn*3  nnnn.  Closes #(issue_number)  ## Checklist  - [x] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [x] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [x] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [x] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
Stirling-Tools,Stirling-PDF,9884c65b10acfccfdd2974f3700fa4343df9f54a,https://github.com/Stirling-Tools/Stirling-PDF/commit/9884c65b10acfccfdd2974f3700fa4343df9f54a,formattingand autowired constructors (#2557)  # Description This pull request includes several changes aimed at improving the code structure and removing redundant code. The most significant changes involve reordering methods  removing unnecessary annotations  and refactoring constructors to use dependency injection. Autowired now comes via constructor (which also doesn't need autowired annotation as its done by default for configuration)    ## Checklist  - [ ] I have read the [Contribution Guidelines](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/CONTRIBUTING.md) - [ ] I have performed a self-review of my own code - [ ] I have attached images of the change if it is UI based - [ ] I have commented my code  particularly in hard-to-understand areas - [ ] If my code has heavily changed functionality I have updated relevant docs on [Stirling-PDFs doc repo](https://github.com/Stirling-Tools/Stirling-Tools.github.io/blob/main/docs/) - [ ] My changes generate no new warnings - [ ] I have read the section [Add New Translation Tags](https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToAddNewLanguage.md#add-new-translation-tags) (for new translation tags only)
spring-projects,spring-framework,eee45c358315574fac80b8e5cbdd625eb37471e3,https://github.com/spring-projects/spring-framework/commit/eee45c358315574fac80b8e5cbdd625eb37471e3,Refine CORS preflight requests handling with no configuration  This commit makes CORS preflight requests handling more flexible by just skipping setting CORS response headers when no configuration is defined instead of rejecting them.  That will have the same effect on user agent side (the preflight request will be considered as not authorized and the actual request not performed) but is more flexible and more efficient.  Closes gh-31839
spring-projects,spring-framework,dbd47ff4f9a7cf241eda414ca7be6af9db55aae6,https://github.com/spring-projects/spring-framework/commit/dbd47ff4f9a7cf241eda414ca7be6af9db55aae6,Implement additional micro performance optimizations  See gh-34717
spring-projects,spring-framework,0f2308e85f327600653b0f3f8aaf4e0e3131b4aa,https://github.com/spring-projects/spring-framework/commit/0f2308e85f327600653b0f3f8aaf4e0e3131b4aa,Implement micro performance optimizations  - ClassUtils.isAssignable(): Avoid Map lookup when the type is not a primitive.  - AnnotationsScanner: Perform low cost array length check before String comparisons.  - BeanFactoryUtils: Use char comparison instead of String comparison. The bean factory prefix is '&'  so we can use a char comparison instead of more heavyweight String.startsWith("&").  - AbstractBeanFactory.getMergedBeanDefinition(): Perform the low cost check first. Map lookup  while cheap  is still more expensive than instanceof.  Closes gh-34717  Signed-off-by: Olivier Bourgain <olivierbourgain02@gmail.com>
spring-projects,spring-framework,e7db15b3255a23bbdb7dcf786d5ba6df0915c8d7,https://github.com/spring-projects/spring-framework/commit/e7db15b3255a23bbdb7dcf786d5ba6df0915c8d7,Perform type check before singleton check for early FactoryBean matching  Closes gh-34710
spring-projects,spring-framework,466ac6b703787663e300614033c9fb922b140175,https://github.com/spring-projects/spring-framework/commit/466ac6b703787663e300614033c9fb922b140175,Improve SimpleKey hashing function  Prior to this commit  `SimpleKey` would be used in Spring Framework's caching support and its `hashCode` value would be used to efficiently store this key in data structures.  While the current hashcode strategy works  the resulting values don't spread well enough when input keys are sequential (which is often the case). This can have negative performance impacts  depending on the data structures used by the cache implementation.  This commit improves the `hashCode` function with a mixer to better spread the hash values. This is using the mixer function from the MurMur3 hash algorithm.  Closes gh-34483
spring-projects,spring-framework,f895d762cd7c393861d34a391b9a685e7545f305,https://github.com/spring-projects/spring-framework/commit/f895d762cd7c393861d34a391b9a685e7545f305,Remove duplicate Content-Type header in error cases  Prior to this commit  the `DispatcherServlet` would try and reset the response buffer in case of errors  if the response is not committed already. This allows for more flexible error handling  even if the response was being handled already when it errored.  Resetting the response buffer clears the body but leaves HTTP response headers intact. This is done on purpose as to not clear headers previously added by Servlet Filters. By leaving in place some headers like "Content-Type"  this does not take into account the fact that the response body was cleared and that error handling will perform another round of content negotiation. While this isn't a problem for some Servlet containers which enforce a single "Content-Type" header value  this can cause multiple/duplicate values for some others.  This commit ensures that the "Content-Type" response header is removed at the same time as we clear the "producible media types" attribute: another pass of content negotiation will be performed for error handling.  Fixes gh-34366
spring-projects,spring-framework,6a0c5ddf6849da1192699ab0acfef8103b4bd327,https://github.com/spring-projects/spring-framework/commit/6a0c5ddf6849da1192699ab0acfef8103b4bd327,Refactoring in AbstractBufferingClientHttpRequest  Extract a protected method for subclasses to use to perform the actual (end-of-chain) request execution.  See gh-33785
spring-projects,spring-framework,fec2ed5540d075fc9364302fc82c9d7de3745ee6,https://github.com/spring-projects/spring-framework/commit/fec2ed5540d075fc9364302fc82c9d7de3745ee6,Implement new GraalVM reachability metadata format  As of GraalVM 23  a new and simplified reachability metadata format is available. Metadata now consists of a single "reachability-metadata.json" file that contains all the information previously spread in multiple files. The new format does not include some introspection flags  as they're now automatically included when a hint is registered against a type. Also  "typeReachable" has been renamed as "typeReached" to highlight the fact that the event considered is the static initialization of the type  not when the static analysis performed during native compilation is reaching the type.  This new format ships with a JSON schema  which this commit is tested against.  See gh-33847
spring-projects,spring-framework,3b65506c13f18535e2fb055fd790bf45efb13de5,https://github.com/spring-projects/spring-framework/commit/3b65506c13f18535e2fb055fd790bf45efb13de5,Use ByteBuffer support in ServletHttpHandlerAdapter  As of Servlet 6.1  the `ServletInputStream` and `ServletOutputStream` offer read and write variants based on `ByteBuffer` instead of byte arrays. This can improve performance and avoid memory copy for I/O calls.  This was already partially supported for some servers like Tomcat through specific adapters. This commit moves this support to the standard `ServletHttpHandlerAdapter` and makes it available for all Servlet 6.1+ containers.  Closes gh-33748
spring-projects,spring-framework,1c69a3c521d3ad87c77ad067389e1217f0acb188,https://github.com/spring-projects/spring-framework/commit/1c69a3c521d3ad87c77ad067389e1217f0acb188,Fix `PathMatchingResourcePatternResolver` manifest classpath discovery  Update `PathMatchingResourcePatternResolver` so that in addition to searching the `java.class.path` system property for classpath enties  it also searches the `MANIFEST.MF` files from within those jars.  Prior to this commit  the `addClassPathManifestEntries()` method expected that the JVM had added `Class-Path` manifest entries to the `java.class.path` system property  however  this did not always happen.  The updated code now performs a deep search by loading `MANIFEST.MF` files from jars discovered from the system property. To deal with potential performance issue  loaded results are also now cached.  The updated code has been tested with Spring Boot 3.3 jars extracted using `java -Djarmode=tools`.  See gh-33705
spring-projects,spring-framework,e32a2f339d611053c2b53f0bb3d5f322e4df24d7,https://github.com/spring-projects/spring-framework/commit/e32a2f339d611053c2b53f0bb3d5f322e4df24d7,Propagate method error in some cases of reactive `findInCaches` errors  In a Cacheable reactive method  if an exception is propagated from both the method and the caching infrastructure  an NPE could previously surface due to the `CacheAspectSupport` attempting to perform an `onErrorResume` with a `null`. This change ensures that in such a case the user-level exception from the method is propagated instead.  Closes gh-33492
spring-projects,spring-framework,557dbba58567cf838dbdc6ef565a8828c8540ac5,https://github.com/spring-projects/spring-framework/commit/557dbba58567cf838dbdc6ef565a8828c8540ac5,Remove superfluous addToClassHierarchy call for Enum types  Closes gh-32906
NationalSecurityAgency,ghidra,e386550016c10db36052c6d394a4fd5cd25befcd,https://github.com/NationalSecurityAgency/ghidra/commit/e386550016c10db36052c6d394a4fd5cd25befcd,Merge remote-tracking branch 'origin/GP-2941_dev747368_definedstringstable_performance--SQUASHED' into Ghidra_11.4 (Closes #5726  Closes #8134  Closes #3498)
NationalSecurityAgency,ghidra,c396867209edf0de7dbeabed14da0764492386ab,https://github.com/NationalSecurityAgency/ghidra/commit/c396867209edf0de7dbeabed14da0764492386ab,GP-4512 Constant propagation and stack analysis performance changes
NationalSecurityAgency,ghidra,6fa543c2e2698773ebc0b537abf9ef47be817577,https://github.com/NationalSecurityAgency/ghidra/commit/6fa543c2e2698773ebc0b537abf9ef47be817577,GP-5477 - Decompiler - Fixed performance when using many global highlighters; updated the highlight service to allow for function-specific highlighting
NationalSecurityAgency,ghidra,17d6f49bd7c73c0ef920a3f05c145008e6299792,https://github.com/NationalSecurityAgency/ghidra/commit/17d6f49bd7c73c0ef920a3f05c145008e6299792,GP-4611: not a perfect solution  but better
NationalSecurityAgency,ghidra,86e77bd9efce139844830f9ec8a884b16c3dc83a,https://github.com/NationalSecurityAgency/ghidra/commit/86e77bd9efce139844830f9ec8a884b16c3dc83a,Merge remote-tracking branch 'origin/GP-4949_ghidra1_StructureEditorPerformance--SQUASHED' into Ghidra_11.2 (Closes #6936  Closes #6504)
NationalSecurityAgency,ghidra,ef724708df27c042509d8324d1a2a2f5c568bd04,https://github.com/NationalSecurityAgency/ghidra/commit/ef724708df27c042509d8324d1a2a2f5c568bd04,GP-4949 Added Structure.setLength method and made structure editor performance improvements and various bug fixes.
NationalSecurityAgency,ghidra,e5df54da2c9edd8048a11bdff96886d5acf43dd9,https://github.com/NationalSecurityAgency/ghidra/commit/e5df54da2c9edd8048a11bdff96886d5acf43dd9,Merge remote-tracking branch 'origin/GP-4712_emteere_PR-6650_sad-dev_ParallelPerf' (Closes #6650  Closes #6649  #2791)
NationalSecurityAgency,ghidra,85a94fc566333a8b875602ee37602e0e23324016,https://github.com/NationalSecurityAgency/ghidra/commit/85a94fc566333a8b875602ee37602e0e23324016,GP-4798 Corrected transaction error when DT Archive upgrade is performed
NationalSecurityAgency,ghidra,789cbd9241b907dafa5d3862adfbcc3b25587923,https://github.com/NationalSecurityAgency/ghidra/commit/789cbd9241b907dafa5d3862adfbcc3b25587923,SleighLanguage: Use more performant ConcurrentHashMap
google,guava,75da92419a7d414bd3de23c89f2ddfd83754767d,https://github.com/google/guava/commit/75da92419a7d414bd3de23c89f2ddfd83754767d,Use `AtomicReferenceFieldUpdater` instead of `VarHandle` when `guava-android` runs under a JVM.  For much more discussion  see the code comments  especially in the backport copy of `AbstractFutureState`. (For a bit more on performance  see https://shipilev.net/blog/2015/faster-atomic-fu/  including its notes that `Unsafe` is not necessarily faster than `AtomicReferenceFieldUpdater`.)  Now that we no longer use `VarHandle` under Android  we can remove some Proguard rules for it: - We no longer need the `-dontwarn` rule for `VarHandleAtomicHelper`  since that type no longer exists in `guava-android`. - We no longer need the `-assumevalues` rule for `mightBeAndroid`  since it served only to strip the `VarHandle` code (to hide it from optimizers that run on the optimized code). And all else being equal  we'd rather _not_ have that rule _just in case_ someone is running `guava-android` through an optimizer and using it under the JVM (in which case we'd like to follow the JVM code path so that we don't try to use `Unsafe`). (OK  maybe it would be nice to keep the rule just so that Android doesn't have to perform a check of the `java.runtime.name` system property once during `AbstractFutureState` initialization. If anyone finds this to be an issue  please let us know.)  I've also updated the tests to better reflect which ones we run only under a JVM  not in an Android emulator. (I should really have done that back in cl/742859752.)  Fixes https://github.com/google/guava/issues/7769  RELNOTES=`util.concurrent`: Removed our `VarHandle` code from `guava-android`. While the code was never used at runtime under Android  it was causing [problems under the Android Gradle Plugin](https://github.com/google/guava/issues/7769) with a `minSdkVersion` below 26. To continue to avoid `sun.misc.Unsafe` under the JVM  `guava-android` will now always use `AtomicReferenceFieldUpdater` when run there. PiperOrigin-RevId: 746800729
google,guava,7336af1831b54319fd42ea83fa594e0f930227c8,https://github.com/google/guava/commit/7336af1831b54319fd42ea83fa594e0f930227c8,Make the `guava-android` copy of `AbstractFuture` try `VarHandle` when run under the JVM.  Under Android  I still don't have us even _try_ `VarHandle`: - I'm not sure that using `VarHandle` is possible with our current internal Android build setup  as discussed in the internal commentary on cl/711733182. - Using `VarHandle` there may at least somewhat more complex: My testing suggests that some versions of Android expose `VarHandle` to our reflective check but don't actually expose `MethodHandles.Lookup.findVarHandle`. Accordingly  sgjesse@ [recommends using a check on `SDK_INT`](https://issuetracker.google.com/issues/134377851#comment4) ([33](https://developer.android.com/reference/java/lang/invoke/MethodHandles.Lookup#findVarHandle(java.lang.Class%3C?%3E %20java.lang.String %20java.lang.Class%3C?%3E)) or higher) instead of reflection under Android. Since `guava-android` can be used under the JVM  too  we'd need to use a combination of the SDK check _and_ reflection. And we'd need to perform the `SDK_INT` check reflectively  as [we do in `TempFileCreator`](https://github.com/google/guava/blob/266ce0022a1c54244df1af0bde81116ed7703577/guava/src/com/google/common/io/TempFileCreator.java#L75) (which  hmm  I should clean up as obsolete one of these days...). (We could at least _reduce_ the amount of reflection we need if we were to depend on the Android SDK at build time  as discussed in b/403282918.) - I have no idea whether `VarHandle` is faster or slower than `Unsafe` there (and I can't easily tell because of the build problems above). - I'm not aware of efforts to remove `Unsafe` under Android.  This CL has two advantages for JVM users of `guava-android`: - It eliminates a warning about usage of `Unsafe` under newer JDKs. Note that `AbstractFuture` _already_ has run correctly if access to `Unsafe` is outright disallowed (as with `-sun-misc-unsafe-memory-access=deny`): It detects this and falls back to an alternative implementation. However  if `Unsafe` is available but produces warnings  `guava-android` would use it  triggering those warnings. This CL makes Guava not even try to use it under newer JVMs because it now tries `VarHandle` first. - `VarHandle` may be marginally faster than `Unsafe` under the JVM  as discussed in cl/711733182. (It also doesn't lead to VM crashes if you manage to pass `null` where you shouldn't  as I did back in b/397641020 :) But that's more something that's nice for us as Guava developers  not something that's nice for Guava _users_.)  This CL is probably the most _prominent_ part of [our migration of `guava-android` off `Unsafe`](https://github.com/google/guava/issues/7742). It is debatable whether it is the most _important_  given that [at least one class  `Striped64`  does not seem to have a fallback at all if `Unsafe` is unavailable](https://github.com/google/guava/issues/6806#issuecomment-2738313040). Still  this CL addresses the warning that most users are seeing  and it gives us some precedent for how to deal with `Striped64`.  Finally  it looks like our existing tests for `VarHandle` had [a mismatch between the context class loader and the class loader that we load `AbstractFutureTest` in](https://github.com/google/guava/blob/266ce0022a1c54244df1af0bde81116ed7703577/guava-tests/test/com/google/common/util/concurrent/AbstractFutureFallbackAtomicHelperTest.java#L105-L107)? That seems fairly bad. This CL fixes it  extracting a method to guard against future such mismatches. Out of an abundance of caution  I made a similar change in `AggregateFutureStateFallbackAtomicHelperTest`  even though there's not really an opportunity for a mismatch there  given that there's only one alternative class loader.  RELNOTES=`util.concurrent`: Changed the `guava-android` copy of `AbstractFuture` to try `VarHandle` before `Unsafe`  eliminating a warning under newer JDKs. PiperOrigin-RevId: 743198569
google,guava,96d0d4b48f1b6981798c42ff2ed0820a2ddb81fa,https://github.com/google/guava/commit/96d0d4b48f1b6981798c42ff2ed0820a2ddb81fa,Rename fields to make them harder to use by accident.  Nearly all access should be performed through the various methods (e.g.  `casListeners(...)`  `listeners()`). In fact  we could be principled and perform _all_ access through those methods (except of course for within the implementations of those methods themselves). I haven't done that here just out of fear that it will somehow affect performance or cause stack overflows.  Accidental usage has been something that I've historically worried about in  e.g.  `AbstractFuture.set(V value)`  whose parameter has had the same name as a field. That _particular_ example matters less at the moment because the field recently became `private` to a new superclass  `AbstractFutureState`  and so it's not accessible in the subclass `AbstractFuture`.  But it's going to matter again: I'm likely to make the fields package-private as part of [work to migrate `guava-android` off `Unsafe`](https://github.com/google/guava/issues/7742). Currently  the fields can be `private` because we call `MethodHandles.lookup()` from within `AbsractFutureState`. (Yes  it would initially seem that [we shouldn't have to](https://github.com/google/guava/blob/c7363f7fb40698bb5f99d198cc45884f38642f86/guava/src/com/google/common/util/concurrent/AbstractFutureState.java#L612-L632)  but we do.) But that requires `AbstractFutureState` to refer to `MethodHandles.Lookup` in one of its method signatures\[*\]  and that makes old versions of Android upset. To avoid that  I will make `value` package-private  at which point I won't need the `MethodHandles.Lookup` reference in `AbstractFutureState` itself.  And when I tried making `value` package-private  _one test_ started to fail. It should have taken me less time to figure out  but I eventually discovered that the problem was that [the test refers to "`value`" inside an `AbstractFuture` subclass](https://github.com/google/guava/blob/c7363f7fb40698bb5f99d198cc45884f38642f86/guava-tests/test/com/google/common/util/concurrent/AbstractFutureTest.java#L78). Previously  this referred to the local variable `value` from the test method; with my change  it was instead referring to the `value` field.  (I wouldn't have to have gone down the road of making the field non-`private` in the first place [if not for Java 8 compatibility](https://github.com/google/guava/issues/6614#issuecomment-2548660841).... Still  as discussed above  this rename could protect against problems _within_ the file  too  and such problems could arise even if the field were to remain `private`.)  \[*\] Or maybe I could declare the method as returning `Object` instead of `MethodHandles.Lookup`  and the caller could cast it back? But we found during [our `SequencedCollection` work](https://github.com/google/guava/issues/6903) that Android (and I think maybe the JVM  but I can't find my record of this) can produce a `VerifyError` in some cases in which _implementation_ code refers to an unknown type  I think specifically when it needs to check whether a `return someThingOfTypeFoo` from that method is compatible with the return type `Bar` of the method. We _might_ be able to work around that by performing an explicit  "redundant" cast to `Object`  but I'm not even sure how to get javac to do that  and it feels very fragile  especially in the presence of optimization/minification tools.  RELNOTES=n/a PiperOrigin-RevId: 741607075
google,guava,6ce7830421a57decf7441b5a68b2544424caef9a,https://github.com/google/guava/commit/6ce7830421a57decf7441b5a68b2544424caef9a,Fix a minor weirdness in the implementation of `ByteSource.contentEquals(ByteSource)` and move the part of its implementation that compares two `InputStream`s to a package-private method in `ByteStreams` while I'm at it.  The minor weirdness was this: when the same number of bytes was read into both buffers _but_ the number of bytes read was less than the length of the buffers  we would then use `Arrays.equals` to compare the _full_ contents of both buffers even though the reads only partially filled them.  This was not a correctness issue because it was always the case that if we got to that point  all previous reads into the buffers must have filled both buffers with the _same_ bytes  so the bytes in the section of the buffers that was not filled in the most recent read were guaranteed to be the same.  This was also not much of a performance issue since it could only occur once per `contentEquals` call  when the end of both `InputStream`s had been reached (the buffers are always completely filled otherwise).  That said  it's more obviously correct (and saves a tiny bit of work) to ensure that we only compare the bytes that were actually read in the most recent read operations.  RELNOTES=n/a PiperOrigin-RevId: 738928305
google,guava,2dd82ad97cbe1cab3b06d7269ee8a5711cbac8ae,https://github.com/google/guava/commit/2dd82ad97cbe1cab3b06d7269ee8a5711cbac8ae,Refactor the main `AbstractFuture` implementation to prepare for using it for more environments.  That includes J2KT and [GWT/J2CL](https://github.com/google/guava/issues/2934).  Changes include: - Suppress nullness at smaller scopes  fixing errors that were hidden by the old  broad suppression. - Remove `synchronized` from an override of `fillInStackTrace`. J2KT doesn't like `synchronized` except on very specific types  and we don't need it. - Introduce `rethrowIfErrorOtherThanStackOverflow` and `interruptCurrentThread`  `Platform` methods that will require different implementations for J2KT and J2CL/GWT. - Introduce helper methods like `casListeners(expect  update)` for `AtomicHelper` operations. These reduce verbosity relative to `ATOMIC_HELPER.casListeners(this  expect  update)`. They also prepare for `AbstractFuture` implementations that don't use `AtomicHelper`. - Introduce `notInstanceOfSetFuture`. This is arguably nicer than `!(localValue instanceof SetFuture)`  but the real purpose is to prepare for when code in a different file needs to check `instance SetFuture`. (I could be convinced that I should just make `SetFuture` package-private instead  even though no one outside the file should use it for anything but an `instanceof` check.) - Mysteriously move things around  increase visibility of members  and introduce and sometimes use accessors. This is to prepare for when some of the code will be moving to a separate file so that the remainder of `AbstractFuture` can be wholly shared across different platforms. - Fix a few typos in comments.  Also  rename `SetFuture`. This isn't directly related  but now is as good a time as any to do it.  Additional bonus: This CL probably makes [the logging at the "end" of `AbstractFuture` static initialization](https://github.com/google/guava/blob/7ec362ec68b630363231d5292cd6b2577c710be6/guava/src/com/google/common/util/concurrent/AbstractFuture.java#L210) have a better chance of actually working in the hypothetical situation that a logger uses `AbstractFuture`: Currently  `AbstractFuture` performs some further initialization _after_ that logging (such as the initialization of `NULL`). Now  it performs all that initialization before the `static` block that might log.  RELNOTES=n/a PiperOrigin-RevId: 729313044
google,guava,7719744040d05004bd7051fc33a782fa78b01634,https://github.com/google/guava/commit/7719744040d05004bd7051fc33a782fa78b01634,Make `Atomic*FieldUpdater` fields `static` for better performance.  Compare cl/713006636.  (Also  better document the similar code in `AbstractFuture`.)  Note that I also evaluated performance with `VarHandle`  and I found it no better. (Maybe we did a similar experiment with `Unsafe` way back when and came to a similar conclusion?)  Note that that's all based on _JVM_ performance (and on benchmarks that are not necessarily great). It's possible that Android it worth a further look someday. But our only option there _today_ might be `Unsafe`  and new usages of `Unsafe` would be moving backward.  RELNOTES=n/a PiperOrigin-RevId: 724013501
google,guava,400af25292096746ed3f6164f0ff88209acbb19f,https://github.com/google/guava/commit/400af25292096746ed3f6164f0ff88209acbb19f,Make `AtomicReferenceFieldUpdater` fields `static` for [better performance](https://shipilev.net/blog/2015/faster-atomic-fu/#:~:text=thrown%20out%20of%20the%20window).  This may eliminate the reason for [an `Unsafe`-based implementation](https://github.com/google/guava/issues/6806) even under Java 8  but we will ideally confirm that with benchmarks before ripping that implementation out. (There's also Android: `AtomicReferenceFieldUpdater` is available there  but we should look into performance  including under older versions  especially with AFAIK no plans to remove `Unsafe` there.)  (Also  make a few other tiny edits to the backport copy so that it matches the mainline copy more closely.)  RELNOTES=n/a PiperOrigin-RevId: 713006636
google,guava,1a300f6b2f7ba03ae9bc3620a80c4d4589c65b69,https://github.com/google/guava/commit/1a300f6b2f7ba03ae9bc3620a80c4d4589c65b69,Make `AbstractFuture` use `VarHandle` when available.  For now  this is only for the JRE flavor  not for Android.  Our not entirely great benchmarks suggest that this may actually improve performance slightly over the current `Unsafe`-based implementation. This matches my sense that [alternatives to `Unsafe` have gotten faster](https://github.com/google/guava/issues/6806#issuecomment-2518256341).  There are plenty of other [places in Guava that we use `Unsafe`](https://github.com/google/guava/issues/6806)  but this is a start.  Also: I'm going to consider this CL to "fix" https://github.com/google/guava/issues/6549  even though: - We already started requiring newer versions of Java to build our _tests_ in cl/705512728. (This CL is the first to require a newer JDK to build _prod_ code  again only to _build_ it  not to _run_ it.) - We already started requiring newer versions of Java to build our _GWT_ module in cl/711487270. - This CL requires only Java 9  not Java 11. - None of the changes so far should require people who _build our Maven project_ to do anything (aside from GWT users)  since our build automatically downloads a new JDK to use for javac since cl/655647768. RELNOTES=n/a PiperOrigin-RevId: 711733182
google,guava,f7bb71157e6f02a5d5a3ba80aeea1b19fd8600ad,https://github.com/google/guava/commit/f7bb71157e6f02a5d5a3ba80aeea1b19fd8600ad,Document that `isInetAddress` and `forString` perform slightly different validation.  ...and blame it on Java :)  We'd gotten this right for the lesser-used `isUriInetAddress` and `forUriString` but not for these more commonly used methods.  See https://github.com/google/guava/issues/2587#issuecomment-2474699497  RELNOTES=n/a PiperOrigin-RevId: 696253161
google,guava,a4a7f6bd00ca1acd2efcb81e493720569ba58424,https://github.com/google/guava/commit/a4a7f6bd00ca1acd2efcb81e493720569ba58424,Recommend the JDK `compareUnsigned` methods over our equivalents.  This is a followup to cl/655152611. As noted in the description of that CL  I didn't migrate the _implementations_ because of GWT+J2CL constraints. Those constraints bind very few users  so I don't think we need to acknowledge them in the Javadoc.  (If we start to think that an implementation migration might actually pay off in performance improvements  we could arrange for it by setting up GWT+J2CL supersource.)  Also  note that the methods are [available under Android even without opting in to library desugaring](https://r8.googlesource.com/r8/+/5c88be7bffa502cddc989f80beffc0dd5402a057/src/main/java/com/android/tools/r8/ir/desugar/BackportedMethodRewriter.java#961).  RELNOTES=n/a PiperOrigin-RevId: 662217380
google,guava,f74135f69c0743f8ada1cdd8756f5499007c3a7c,https://github.com/google/guava/commit/f74135f69c0743f8ada1cdd8756f5499007c3a7c,Eliminate the need for a few `rawtypes` and `unchecked` suppressions.  The changes here are similar to some of those from cl/609475939: If we have an `AtomicReferenceFieldUpdater<AbstractFuture  ...>`  we don't have to keep the raw type or perform an unchecked cast. Instead  we can use it directly as an `AtomicReferenceFieldUpdater<? super AbstractFuture<?>  ...>`.  RELNOTES=n/a PiperOrigin-RevId: 638008218
dbeaver,dbeaver,c476c0293fdb0b22463c29e81c14172641719dd4,https://github.com/dbeaver/dbeaver/commit/c476c0293fdb0b22463c29e81c14172641719dd4,#35466 Launch performance fix
dbeaver,dbeaver,d59815e792f55e79d6c5dae4cde64c05b0b6e49a,https://github.com/dbeaver/dbeaver/commit/d59815e792f55e79d6c5dae4cde64c05b0b6e49a,dbeaver/dbeaver#36752 Hints cache refresh performance (#36792)  * dbeaver/dbeaver#36752 Hints cache refresh performance  * dbeaver/dbeaver#36752 Find/replace performance  ---------  Co-authored-by: Matvey16 <82543000+Matvey16@users.noreply.github.com>
dbeaver,dbeaver,ae82dd5397fb1a9991b58a8067daf4b75f6a05e6,https://github.com/dbeaver/dbeaver/commit/ae82dd5397fb1a9991b58a8067daf4b75f6a05e6,dbeaver/dbeaver#35712 Added option to use a comma as row delimiter when performing advanced copying (#36101)  Co-authored-by: MashaKorax <84867187+MashaKorax@users.noreply.github.com>
dbeaver,dbeaver,b8c4efb12cd1bdbdc8a7f2ab413abb47b103a44e,https://github.com/dbeaver/dbeaver/commit/b8c4efb12cd1bdbdc8a7f2ab413abb47b103a44e,dbeaver/pro#3561 UDBT connection test (#36236)  * dbeaver/pro#3561 UDBT connection test  * dbeaver/pro#3561 Temporary data source disposal fix  * dbeaver/pro#3561 Save connection before performing connection test  * dbeaver/pro#3561 Fix typo  * dbeaver/pro#3561 Refactor connection wizard finalization logic  ---------  Co-authored-by: kseniaguzeeva <112612526+kseniaguzeeva@users.noreply.github.com> Co-authored-by: Serge Rider <serge@jkiss.org>
dbeaver,dbeaver,0d350529df9f4e5f919fcf49aaecd5b1da6bb55d,https://github.com/dbeaver/dbeaver/commit/0d350529df9f4e5f919fcf49aaecd5b1da6bb55d,dbeaver/pro#3410 Init-performance improvement. Read only 1 setting. (#35800)
dbeaver,dbeaver,7bd668a9bf63d24df939013c12e401f0042e13f8,https://github.com/dbeaver/dbeaver/commit/7bd668a9bf63d24df939013c12e401f0042e13f8,dbeaver/pro#3198 Refactor `DBUtils#getAttributeValue` (#35413)  * dbeaver/pro#3198 Refactor `DBUtils#getAttributeValue`  * dbeaver/pro#3198 Remove unused code  * dbeaver/pro#3198 Properly show first deepest collection element  * dbeaver/pro#3198 Slightly clean up the code  * dbeaver/pro#3300 Use indices instead of deque  * dbeaver/pro#3198 Improve logging  * dbeaver/pro#3198 Improve formatting for collections  * dbeaver/pro#3198 Expanders labels  * dbeaver/pro#3198 Value update redesign  * dbeaver/pro#3198 Grid complex value edit fixes. UI and performance improvements  * dbeaver/pro#3198 Changes reset fix  * dbeaver/pro#3198 Grid complex value edit fixes  * dbeaver/pro#3198 Grid complex value edit fixes  * dbeaver/pro#3198 Grid complex value edit fixes  * dbeaver/pro#3198 Expand/collapse icons  * dbeaver/pro#3198 Code cleanup  * dbeaver/pro#3198 Fix connection invalidate during data save  * dbeaver/pro#3237 Boolean values toggle and render fix  * dbeaver/pro#3237 Record mode fixes. Keep elements in nested rows.  * dbeaver/pro#3237 Record mode fixes. Keep elements in nested rows.  * dbeaver/pro#3237 Record mode fixes  * dbeaver/pro#3237 Record mode fixes + edit fixes  * dbeaver/pro#3237 Record mode resize fix  * dbeaver/pro#3237 Record mode editor fix  * dbeaver/pro#3237 Nested records edit fix (FQN generation)  * dbeaver/pro#3237 UI enhancement + default value handler error improvement  * dbeaver/pro#3237 Clickhouse arrays handle fix  ---------  Co-authored-by: Serge Rider <serge@jkiss.org> Co-authored-by: serge-rider <serge@dbeaver.com> Co-authored-by: Diana <31996417+uslss@users.noreply.github.com>
dbeaver,dbeaver,fdc170b372156c8033efc06f4ef61f9d2e5cdcad,https://github.com/dbeaver/dbeaver/commit/fdc170b372156c8033efc06f4ef61f9d2e5cdcad,App shutdown performance improvement (do not init task manager)
dbeaver,dbeaver,be88eb0a52fa98f1d332abe41c058be4c462e0c4,https://github.com/dbeaver/dbeaver/commit/be88eb0a52fa98f1d332abe41c058be4c462e0c4,Project discovery performance improvement
dbeaver,dbeaver,0ec82bd5e82cbb7d3f93cd2d9ca665fc21bc539e,https://github.com/dbeaver/dbeaver/commit/0ec82bd5e82cbb7d3f93cd2d9ca665fc21bc539e,Minor performance fixes
dbeaver,dbeaver,ba5cfa2a1924d92f8f4cfa3ed1d4e89d9e081adb,https://github.com/dbeaver/dbeaver/commit/ba5cfa2a1924d92f8f4cfa3ed1d4e89d9e081adb,dbeaver/pro#3135 Oracle session query performance + connection page fix (#35084)  * dbeaver/pro#3135 Oracle session query performance + connection page fix  * dbeaver/pro#3135 Oracle session query fix
dbeaver,dbeaver,19a4432c2a7dcc1ee86de3ebf46275718740fa6c,https://github.com/dbeaver/dbeaver/commit/19a4432c2a7dcc1ee86de3ebf46275718740fa6c,3085 sql completion performance (#34826)  * #34450 Workaround for word-wrap (lazy activation)  * #34450 Workaround for word-wrap (lazy activation)  * #34450 Workaround for word-wrap (lazy activation)  * #34450 Workaround for word-wrap (lazy activation)  * #3085 SQL completion performance: cache tables  ---------  Co-authored-by: Elizabeth <e1izabeth.k@outlook.com>
dbeaver,dbeaver,e4e88b152ec67e7f3f4b1140f83775f56000a2ac,https://github.com/dbeaver/dbeaver/commit/e4e88b152ec67e7f3f4b1140f83775f56000a2ac,#29986 [CUBRID] Create and edit View using UI (#34335)  * #29986 Create and edit View using UI  * #29986 Improve code optimizations for better performance  * #29986 Change value to constant variable  ---------  Co-authored-by: Dziyana <31996417+uslss@users.noreply.github.com>
dbeaver,dbeaver,fbc607b78a6d5d54801ec6b2126fb3e81e524e73,https://github.com/dbeaver/dbeaver/commit/fbc607b78a6d5d54801ec6b2126fb3e81e524e73,DB2 metadata loading performance (load index columns in a single select)
dbeaver,dbeaver,72c23215f91704efda4737c9660f6c7295002fea,https://github.com/dbeaver/dbeaver/commit/72c23215f91704efda4737c9660f6c7295002fea,#23346 Fix `Restore Default` for Formatting preference page (#34199)  * #23343 Fix `Restore Default` for Formatting preference page  * #23346 Refactor settings loading and performing defaults  ---------  Co-authored-by: MashaKorax <84867187+MashaKorax@users.noreply.github.com> Co-authored-by: Serge Rider <serge@jkiss.org>
termux,termux-app,da3a0ac4e219b7a924af3a94cf4cfc4901f0f059,https://github.com/termux/termux-app/commit/da3a0ac4e219b7a924af3a94cf4cfc4901f0f059,Fixed: Add explicit `serialVersionUID` to `Serializable` classes like `ReportInfo` and `TextIOInfo`  Reading `ReportInfo` with `Bundle.getSerializable()` by `ReportActivity` is triggering exception when default algorithm is used for `serialVersionUID` in Termux:API plugin app when error notification created in `ResultReturner.returnData()` by `TermuxPluginUtils.sendPluginCommandErrorNotification()` is clicked.  ``` java.lang.RuntimeException: Unable to start activity ComponentInfo{com.termux/com.termux.shared.activities.ReportActivity}: android.os.BadParcelableException: Parcelable encountered IOException reading a Serializable object (name = com.termux.shared.models.ReportInfo) at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:4280) at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:4467) at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:222) at android.app.servertransaction.TransactionExecutor.executeNonLifecycleItem(TransactionExecutor.java:133) at android.app.servertransaction.TransactionExecutor.executeTransactionItems(TransactionExecutor.java:103) at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:80) at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2823) at android.os.Handler.dispatchMessage(Handler.java:110) at android.os.Looper.loopOnce(Looper.java:248) at android.os.Looper.loop(Looper.java:338) at android.app.ActivityThread.main(ActivityThread.java:9067) at java.lang.reflect.Method.invoke(Native Method) at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:593) at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:932) Caused by: android.os.BadParcelableException: Parcelable encountered IOException reading a Serializable object (name = com.termux.shared.models.ReportInfo) at android.os.Parcel.readSerializableInternal(Parcel.java:5520) at android.os.Parcel.readValue(Parcel.java:5038) at android.os.Parcel.readValue(Parcel.java:4702) at android.os.Parcel.-$$Nest$mreadValue(Unknown Source:0) at android.os.Parcel$LazyValue.apply(Parcel.java:4811) at android.os.Parcel$LazyValue.apply(Parcel.java:4764) at android.os.BaseBundle.unwrapLazyValueFromMapLocked(BaseBundle.java:446) at android.os.BaseBundle.getValueAt(BaseBundle.java:426) at android.os.BaseBundle.getValue(BaseBundle.java:397) at android.os.BaseBundle.getValue(BaseBundle.java:380) at android.os.BaseBundle.getValue(BaseBundle.java:373) at android.os.BaseBundle.getSerializable(BaseBundle.java:1522) at android.os.Bundle.getSerializable(Bundle.java:1339) at com.termux.shared.activities.ReportActivity.updateUI(ReportActivity.java:140) at com.termux.shared.activities.ReportActivity.onCreate(ReportActivity.java:93) at android.app.Activity.performCreate(Activity.java:9155) at android.app.Activity.performCreate(Activity.java:9133) at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1521) at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:4262) ... 13 more Caused by: java.io.InvalidClassException: com.termux.shared.models.ReportInfo; local class incompatible: stream classdesc serialVersionUID = -5165426368218339031  local class serialVersionUID = 1 at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:652) at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1743) at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1624) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1902) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1442) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:430) at android.os.Parcel.readSerializableInternal(Parcel.java:5507) ... 31 more  ```  If using release APK with obfuscation enabled  then following exception will be triggered.  ``` java.lang.RuntimeException: Unable to start activity ComponentInfo{com.termux/com.termux.shared.activities.ReportActivity}: android.os.BadParcelableException: Parcelable encountered ClassNotFoundException reading a Serializable object (name = I0.a) at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3864) at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:4006) at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:111) at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135) at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95) at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2462) at android.os.Handler.dispatchMessage(Handler.java:106) at android.os.Looper.loopOnce(Looper.java:240) at android.os.Looper.loop(Looper.java:351) at android.app.ActivityThread.main(ActivityThread.java:8377) at java.lang.reflect.Method.invoke(Native Method) at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:584) at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1013) Caused by: android.os.BadParcelableException: Parcelable encountered ClassNotFoundException reading a Serializable object (name = I0.a) at android.os.Parcel.readSerializableInternal(Parcel.java:5113) at android.os.Parcel.readValue(Parcel.java:4655) at android.os.Parcel.readValue(Parcel.java:4363) at android.os.Parcel.-$$Nest$mreadValue(Unknown Source:0) at android.os.Parcel$LazyValue.apply(Parcel.java:4461) at android.os.Parcel$LazyValue.apply(Parcel.java:4420) at android.os.BaseBundle.getValueAt(BaseBundle.java:394) at android.os.BaseBundle.getValue(BaseBundle.java:374) at android.os.BaseBundle.getValue(BaseBundle.java:357) at android.os.BaseBundle.getValue(BaseBundle.java:350) at android.os.BaseBundle.getSerializable(BaseBundle.java:1451) at android.os.Bundle.getSerializable(Bundle.java:1144) at com.termux.shared.activities.ReportActivity.updateUI(ReportActivity.java:136) at com.termux.shared.activities.ReportActivity.onCreate(ReportActivity.java:89) at android.app.Activity.performCreate(Activity.java:8397) at android.app.Activity.performCreate(Activity.java:8370) at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1403) at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3837) ... 12 more Caused by: java.lang.ClassNotFoundException: I0.a at java.lang.Class.classForName(Native Method) at java.lang.Class.forName(Class.java:536) at android.os.Parcel$2.resolveClass(Parcel.java:5090) at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1733) at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1624) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1902) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1442) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:430) at android.os.Parcel.readSerializableInternal(Parcel.java:5096) ... 29 more Caused by: java.lang.ClassNotFoundException: I0.a ... 38 more ```  Related issue https://github.com/termux/termux-api/issues/762
termux,termux-app,f80b46487df539c7e9214550002f461e5c66131c,https://github.com/termux/termux-app/commit/f80b46487df539c7e9214550002f461e5c66131c,Fixed: Improve handling of empty ';' SGR sequences  Currently the Termux terminal emulator prints "HI" in red with:  ```sh printf "\e[31;m HI \e[0m" ```  This is not how other terminals (tested on xterm  gnome-terminal  alacritty and the mac built in terminal) handle it  since they parse ""\e[31;m" as "\e[31;0m"  where the "0" resets the colors.  This change aligns with other terminals  as well as improves performance by avoiding allocating a new int[] array for each byte processed by `parseArg()`  and most importantly simplifies things by removing the `mIsCSIStart` and `mLastCSIArg` state  preparing for supporting ':' separated sub parameters such as used in https://sw.kovidgoyal.net/kitty/underlines/
apache,dubbo,d6b11cf466919a8888f1d3a2481cd9cef0a828bd,https://github.com/apache/dubbo/commit/d6b11cf466919a8888f1d3a2481cd9cef0a828bd,[ISSUE #15377] The value of grpc-timeout may not comply with the specification (#15378)  * perf: Optimize the grpc-timeout generation method  * fix: Fix format errors
apache,dubbo,e2987318b2a22fbd7d9976b7e47a5bc2c31ebac2,https://github.com/apache/dubbo/commit/e2987318b2a22fbd7d9976b7e47a5bc2c31ebac2,perf: Remove duplicate checks (#15327)
apache,dubbo,3771339964ae52c21dac6fb34d7988314dde7e46,https://github.com/apache/dubbo/commit/3771339964ae52c21dac6fb34d7988314dde7e46,Optimize parseCharset method for best performance. (#15203)
apache,dubbo,51f4f7454e70c2acd31355763a5186f9d6f257f9,https://github.com/apache/dubbo/commit/51f4f7454e70c2acd31355763a5186f9d6f257f9,Performance tuning (#14604)  Co-authored-by: Albumen Kevin <jhq0812@gmail.com>
apache,dubbo,f90314ab334cf287bbe5c9eb8bf23a3673dac074,https://github.com/apache/dubbo/commit/f90314ab334cf287bbe5c9eb8bf23a3673dac074,A couple of performance and logging issue fix (#14564)
eugenp,tutorials,853a309b2fad5e6e541daefa663cb632cb05b27c,https://github.com/eugenp/tutorials/commit/853a309b2fad5e6e541daefa663cb632cb05b27c,[JAVA-41877] Move code of core-java-perf to core-java-perf-2 module (#18243)
eugenp,tutorials,07e76518780781e3d24fe01aff2bb14bcb3f3553,https://github.com/eugenp/tutorials/commit/07e76518780781e3d24fe01aff2bb14bcb3f3553,Merge pull request #17799 from balasr21/master  BAEL-5972: added implementation for performing seek in Java
eugenp,tutorials,ce09611387b8a6c59f1ed06a918c044ab5e8ec5c,https://github.com/eugenp/tutorials/commit/ce09611387b8a6c59f1ed06a918c044ab5e8ec5c,BAEL-5972: added implementation for performing seek in Java
halo-dev,halo,eb969122ff19fa6deb31a9120ca070dfa3c09596,https://github.com/halo-dev/halo/commit/eb969122ff19fa6deb31a9120ca070dfa3c09596,perf: add caching for extension getter to enhance performance (#7102)  #### What type of PR is this? /kind improvement /area core /milestone 2.20.x  #### What this PR does / why we need it: 为扩展获取增加缓存以提高网站整体性能  在此之前，每个请求都要经过很多过滤器，而一些过滤器会获取扩展因此导致频繁查询扩展和扩展点定义拖慢了速度  **测试情况**  初始化一个全新环境，安装并启用以下插件和主题 - 已激活主题: [Earth 1.11.0](https://github.com/halo-dev/theme-earth) - 已启动插件: - [SEO 工具集 1.0.1](https://github.com/f2ccloud/plugin-seo-tools) - [OAuth2 认证 1.5.0](https://github.com/halo-sigs/plugin-oauth2) - [Trailing Slash 1.0.0](https://github.com/halo-sigs/plugin-trailing-slash) - [评论组件 2.5.1](https://github.com/halo-dev/plugin-comment-widget) - [KaTeX 2.1.0](https://github.com/halo-sigs/plugin-katex) - [应用市场 1.9.0](https://www.halo.run/store/apps/app-VYJbF)  通过 Apache Benchmark (ab) 进行 1w 次请求并发 100 个，测试访问首页，得到以下测试结果：  核心指标对比  |指标|改进前|改进后|提升情况| |---|---|---|---| |**总耗时 (Time taken)**|27.030 秒|25.718 秒|减少约 **4.9%**| |**每秒请求数 (RPS)**|369.96 req/sec|388.83 req/sec|提升约 **5.1%**| |**单请求平均耗时**|270.298 ms|257.181 ms|减少约 **4.9%**| |**传输速率 (Transfer Rate)**|6346.44 KB/s|6670.12 KB/s|提升约 **5.1%**|  综合分析 - 性能提升主要体现在：请求处理时间（Processing）、等待时间（Waiting）以及每秒请求数（RPS）均有 约5% 左右的提升。 - 传输效率更高：通过更快的处理时间，传输速率提高了 5.1%。 - 长尾请求优化显著：最大响应时间减少了约 14.9%，意味着极端情况下的性能更优。  #### Does this PR introduce a user-facing change?  ```release-note 为扩展获取增加缓存使网站整体性能提升 5% 以上 ```
halo-dev,halo,2b4d1ab8d81ea1ce1767da526fec4144d40d4265,https://github.com/halo-dev/halo/commit/2b4d1ab8d81ea1ce1767da526fec4144d40d4265,perf: add caching for system configuration fetcher to enhance performance (#7100)  #### What type of PR is this? /kind improvement /area core /milestone 2.20.x  #### What this PR does / why we need it: 为系统配置获取增加缓存以提高路由和主题模板渲染的速度  #### Special notes for your reviewer: 1. 系统能正确初始化 2. 测试修改系统配置后 http://localhost:8090/actuator/globalinfo 和主题端 `${site}` 是否都是新的 3. 更改了文章路由规则后能正确调整到新的规则  #### Does this PR introduce a user-facing change?  ```release-note 为系统配置的获取增加缓存以提高路由和主题模板渲染的速度 ```
halo-dev,halo,25c54d792e46030fe57c6044f3338e799b37771b,https://github.com/halo-dev/halo/commit/25c54d792e46030fe57c6044f3338e799b37771b,perf: replace concatMap to flatMapSequential to improve parallelism and efficiency (#6706)  #### What type of PR is this? /kind improvement /area core /milestone 2.20.x  #### What this PR does / why we need it: 将 concatMap 替换为 flatMapSequential 以提高并行度和执行效率  可以看一下这个场景示例来模拟像文章列表 API 的数据组装 假如每个步骤的执行时间是 1s 有 4 个步骤 同时 Flux 发出 4 条数据:  ```java @Test void test() { var startMs = System.currentTimeMillis();  var monoA = Mono.fromSupplier( () -> { sleep(); return "A"; })        .subscribeOn(Schedulers.boundedElastic());  var monoB = Mono.fromSupplier( () -> { sleep(); return "B"; })        .subscribeOn(Schedulers.boundedElastic());  var monoC = Mono.fromSupplier( () -> { sleep(); return "C"; })        .subscribeOn(Schedulers.boundedElastic());  var monoD = Mono.fromSupplier( () -> { sleep(); return "D"; })        .subscribeOn(Schedulers.boundedElastic());  var convert = Mono.when(monoA  monoB  monoC  monoD);  Flux.just("1"  "2"  "3"  "4") // concatMap(convert::thenReturn) .flatMapSequential(convert::thenReturn) .collectList() .block();  System.out.println("Time: " + (System.currentTimeMillis() - startMs)); }  private static void sleep() { try { Thread.sleep(1000); } catch (InterruptedException e) { throw new RuntimeException(e); } } ``` **结果:** 1. 如果每个步骤没有加  subscribeOn 且使用 concatMap 耗时: 16362 ms 2. 每个步骤使用 subscribeOn 且使用 concatMap 耗时: 4174 ms 3. 每个步骤使用 subscribeOn 且使用 flatMapSequential 耗时: 1185 ms  #### Does this PR introduce a user-facing change? ```release-note 提升页面访问速度 ```
halo-dev,halo,157b7ad281ae58a2560780a4405193c2e75fc625,https://github.com/halo-dev/halo/commit/157b7ad281ae58a2560780a4405193c2e75fc625,Fix the problem of LockObtainFailedException while performing a rolling update (#6543)  #### What type of PR is this?  /kind bug /area core /milestone 2.19.0  #### What this PR does / why we need it:  This PR refactors LuceneSearchEngine to let IndexWriter and SearcherManager load lazily to prevent LockObtainFailedException from performing a rolling update.  #### Which issue(s) this PR fixes:  Fixes #6541  #### Special notes for your reviewer:  1. Use MySQL or PostgreSQL as database for Halo 2. Start an instance of Halo 3. Try to initialize Halo and search posts 4. Change the `server.port` and start another instance of Halo 5. Check the status of another instance  #### Does this PR introduce a user-facing change?  ```release-note 修复滚动更新时无法启动新的 Halo 实例的问题 ```
halo-dev,halo,c10862d6fe9b4dd978ba8cf80907a54f35606ec7,https://github.com/halo-dev/halo/commit/c10862d6fe9b4dd978ba8cf80907a54f35606ec7,refactor: index mechanism to enhance overall performance (#6039)  #### What type of PR is this? /kind improvement /area core /milestone 2.17.x  #### What this PR does / why we need it: 重构索引机制的查询和排序以提升整体性能  **how to test it?** 使用 postgre 数据库，初始化 Halo ，然后执行以下脚本创建 30w 文章数据进行测试: <details> <summary>点击展开查看 SQL</summary>  ```sql DO $$ DECLARE i integer; postNameIndex integer; snapshotName varchar; totalRecords integer; BEGIN postNameIndex := 1; totalRecords := 300000;  FOR i IN 1..3 LOOP INSERT INTO "public"."extensions" ("name"  "data"  "version") VALUES ( '/registry/content.halo.run/categories/category-'||i  convert_to( jsonb_build_object( 'spec'  jsonb_build_object( 'displayName'  '分类-'||i  'slug'  'category-'||i  'description'  '测试分类'  'cover'  ''  'template'  ''  'priority'  0  'children'  '[]'::jsonb )  'status'  jsonb_build_object( 'permalink'  '/categories/category-'||i  'postCount'  totalRecords  'visiblePostCount'  totalRecords )  'apiVersion'  'content.halo.run/v1alpha1'  'kind'  'Category'  'metadata'  jsonb_build_object( 'finalizers'  jsonb_build_array('category-protection')  'name'  'category-' || i  'annotations'  jsonb_build_object( 'content.halo.run/permalink-pattern'  'categories' )  'version'  0  'creationTimestamp'  '2024-06-12T03:56:40.315592Z' ) )::text  'UTF8')  0 ); END LOOP;   FOR i IN 1..3 LOOP INSERT INTO "public"."extensions" ("name"  "data"  "version") VALUES ( '/registry/content.halo.run/tags/tag-' || i  convert_to( jsonb_build_object( 'spec'  jsonb_build_object( 'displayName'  'Halo tag ' || i  'slug'  'tag-'||i  'color'  '#ffffff'  'cover'  '' )  'status'  jsonb_build_object( 'permalink'  '/tags/tag-' || i  'visiblePostCount'  totalRecords  'postCount'  totalRecords  'observedVersion'  0 )  'apiVersion'  'content.halo.run/v1alpha1'  'kind'  'Tag'  'metadata'  jsonb_build_object( 'finalizers'  jsonb_build_array('tag-protection')  'name'  'tag-'||i  'annotations'  jsonb_build_object( 'content.halo.run/permalink-pattern'  'tags' )  'version'  0  'creationTimestamp'  '2024-06-12T03:56:40.406407Z' ) )::text  'UTF8')  0); END LOOP;  FOR i IN postNameIndex..totalRecords LOOP -- Generate snapshotName snapshotName := 'snapshot-' || i;  -- Insert post data INSERT INTO "public"."extensions" ("name"  "data"  "version") VALUES ( '/registry/content.halo.run/posts/post-' || postNameIndex  convert_to( jsonb_build_object( 'spec'  jsonb_build_object( 'title'  'title-' || postNameIndex  'slug'  'slug-' || postNameIndex  'releaseSnapshot'  snapshotName  'headSnapshot'  snapshotName  'baseSnapshot'  snapshotName  'owner'  'admin'  'template'  ''  'cover'  ''  'deleted'  false  'publish'  true  'pinned'  false  'allowComment'  true  'visible'  'PUBLIC'  'priority'  0  'excerpt'  jsonb_build_object( 'autoGenerate'  true  'raw'  '' )  'categories'  ARRAY['category-kEvDb'  'category-XcRVk'  'category-adca']  'tags'  ARRAY['tag-RtKos'  'tag-vEsTR'  'tag-UBKCc']  'htmlMetas'  '[]'::jsonb )  'status'  jsonb_build_object( 'phase'  'PUBLISHED'  'conditions'  ARRAY[ jsonb_build_object( 'type'  'PUBLISHED'  'status'  'TRUE'  'lastTransitionTime'  '2024-06-11T10:16:15.617748Z'  'message'  'Post published successfully.'  'reason'  'Published' )  jsonb_build_object( 'type'  'DRAFT'  'status'  'TRUE'  'lastTransitionTime'  '2024-06-11T10:16:15.457668Z'  'message'  'Drafted post successfully.'  'reason'  'DraftedSuccessfully' ) ]  'permalink'  '/archives/slug-' || postNameIndex  'excerpt'  '如果你看到了这一篇文章，那么证明你已经安装成功了，感谢使用 Halo 进行创作，希望能够使用愉快。'  'inProgress'  false  'contributors'  ARRAY['admin']  'lastModifyTime'  '2024-06-11T10:16:15.421467Z'  'observedVersion'  0 )  'apiVersion'  'content.halo.run/v1alpha1'  'kind'  'Post'  'metadata'  jsonb_build_object( 'finalizers'  ARRAY['post-protection']  'name'  'post-' || postNameIndex  'labels'  jsonb_build_object( 'content.halo.run/published'  'true'  'content.halo.run/deleted'  'false'  'content.halo.run/owner'  'admin'  'content.halo.run/visible'  'PUBLIC'  'content.halo.run/archive-year'  '2024'  'content.halo.run/archive-month'  '06'  'content.halo.run/archive-day'  '11' )  'annotations'  jsonb_build_object( 'content.halo.run/permalink-pattern'  '/archives/{slug}'  'content.halo.run/last-released-snapshot'  snapshotName  'checksum/config'  '73e40d4115f5a7d1e74fcc9228861c53d2ef60468e1e606e367b01efef339309' )  'version'  0  'creationTimestamp'  '2024-06-11T05:51:46.059292Z' ) )::text  'UTF8')  1 );  -- Insert content data INSERT INTO "public"."extensions" ("name"  "data"  "version") VALUES ( '/registry/content.halo.run/snapshots/' || snapshotName  convert_to( jsonb_build_object( 'spec'  jsonb_build_object( 'subjectRef'  jsonb_build_object( 'group'  'content.halo.run'  'version'  'v1alpha1'  'kind'  'Post'  'name'  'post-' || postNameIndex )  'rawType'  'HTML'  'rawPatch'  '<p style=\"\">测试内容</p>'  'contentPatch'  '<p style=\"\">测试内容</p>'  'lastModifyTime'  '2024-06-11T06:01:25.748755Z'  'owner'  'admin'  'contributors'  ARRAY['admin'] )  'apiVersion'  'content.halo.run/v1alpha1'  'kind'  'Snapshot'  'metadata'  jsonb_build_object( 'name'  snapshotName  'annotations'  jsonb_build_object( 'content.halo.run/keep-raw'  'true' )  'creationTimestamp'  '2024-06-11T06:01:25.748925Z' ) )::text  'UTF8')  1 );  postNameIndex := postNameIndex + 1; END LOOP; END $$; ```  </details>  使用以下 API 查询文章 ``` curl 'http://localhost:8090/apis/api.console.halo.run/v1alpha1/posts?page=1&size=20&labelSelector=content.halo.run%2Fdeleted%3Dfalse&labelSelector=content.halo.run%2Fpublished%3Dtrue&fieldSelector=spec.categories%3Dcategory-1&fieldSelector=spec.tags%3Dc33ceabb-d8f1-4711-8991-bb8f5c92ad7c&fieldSelector=status.contributors%3Dadmin&fieldSelector=spec.visible%3DPUBLIC' \ --header 'Authorization: Basic YWRtaW46YWRtaW4=' ``` Before:  ![SCR-20240612-o20](https://github.com/halo-dev/halo/assets/38999863/fc27a265-6571-4361-a707-a683ea040837) After:  ![SCR-20240612-q1c](https://github.com/halo-dev/halo/assets/38999863/c0a241b8-5ed4-4973-8dfc-c260ffccd727)  #### Does this PR introduce a user-facing change? ```release-note 重构索引机制的查询和排序使整体性能提升 50% 以上 ```
airbnb,lottie-android,5deb2deeb3507a39fca4ee17d29e45113302b93e,https://github.com/airbnb/lottie-android/commit/5deb2deeb3507a39fca4ee17d29e45113302b93e,Drop shadow overhaul: improve correctness and performance (#2548)  ## High-level summary  This PR introduces a large change to how drop shadows are rendered  introducing an `applyShadowsToLayers` flag which  by analogy to `applyOpacitiesToLayers`  allows layers to be treated as a whole for the purposes of drop shadows  improving the accuracy and bringing lottie-android in line with other renderers (lottie-web and lottie-ios).  Several different codepaths for different hardware/software combinations are introduced to ensure the fastest rendering available  even on legacy devices.  The calculation of shadow direction with respect to transforms is improved so that the output matches lottie-web and lottie-ios.  Image layers now cast shadows correctly thanks to a workaround to device-specific issues when combining `Paint.setShadowLayer()` and bitmap rendering.  Even in non-`applyShadowsToLayers` mode  correctness is improved by allowing the shadow-to-be-applied to propagate in a similar way as alpha. This allows some amount of visual fidelity to be recovered for animations or environments where enabling `applyShadowsToLayers` is not possible.  A number of issues that caused incorrect rendering in some other cases have been fixed.  ## Background  ### Drop shadows in Lottie  Lottie specifies drop shadows as a tuple of (angle  distance  radius  color  alpha)  with each element being animatable.  The consensus behavior for the rendering of a layer with a drop shadow  which seems to be mostly respected in lottie-web and lottie-ios  seems to be:  1. Evaluate the values at the current frame for angle (`theta`)  distance (`d`)  radius (`r`)  color with alpha (`C`). 2. Apply the layer transform and render the layer normally to a surface `So` (original layer). 3. Copy `So` to new surface `Ss` (shadow). 4. Apply a gaussian blur of radius `r' = c * r` to `Ss`  where `c` is some platform-specific constant intended to normalize blur implementations between platforms. (Ours is 0.33  lottie-web's is 0.25; see https://github.com/airbnb/lottie-android/pull/2541). 5. Tint `Ss` with the color and combine the alpha by applying the following for each pixel `P`: `P.rgb = C.rgb * P.a; P.a = C.a * P.a`. 6. Now the shadow is ready on `Ss`  and needs to be drawn into its final position. 7. Convert from polar coordinates `theta` and `d` into `dx` and `dy`  with the 0 position at 12 o'clock: `dx = d * cos(theta - pi/2); dy = d*sin(theta - pi/2)`. 8. Draw `Ss` onto `Si` (intermediate surface) with a translation of `(dx  dy)`. 9. Draw `So` (original layer) onto `Si` with identity transform. 10. Compose `Si` into the framebuffer using the layer's alpha and blend mode.  Some non-obvious consequences of the definition above: - The angle  distance  and radius are relative to the layer post-transform  not pre-transform. That is  rotating the layer (via its transform) still keeps the same screen-space direction of the shadow  and scaling the layer (via its transform) still keeps the same screen-space shadow blur radius. - The drop shadow is not based on any derived outline  so a layer's drop shadow can be seen through its non-fully-opaque pixels. At the same time  reducing the alpha of a pixel in a layer reduces its alpha in the drop shadow. - A layer's shadow and the layer do not blend on top of each other on the final canvas in case the layer has a blend mode or alpha. Instead  the shadow and the layer are alpha-blended with each other  and the result is then composited onto the canvas. - In case the layer has a normal blend mode  this is equivalent to alpha-blending the layer's shadow and then the shadow onto the canvas separately.  ### Drop shadows in lottie-android currently  lottie-android's current implementation of drop shadows differs in important ways: 1. **Shadows are applied per-shape.** This means that a case like a shape with both fill and stroke has incorrect shadows  since both the fill and the stroke render a separate shadow on top of each other. 2. **Precomp layer shadows are ignored.** This means that a precomp cannot cause any of its child shapes to cast a shadow. This is a consequence of the current implementation of (1). 3. **Image layers do not render correct shadows ** due to the minefield that is the support matrix (or in Android's case  a more apt name would be a support tensor) of Android's graphics stack - `setShadowLayer()` simply doesn't work for images consistently. (See the last image in https://github.com/airbnb/lottie-android/pull/2523#issue-2428578510.)  ## Contributions of this PR  This PR introduces the following improvements and additions.  1. **Move the drop shadow model from individual content elements to layers ** and add some missing keypath callbacks. This is a prerequisite for handling drop shadows on a layer level. 2. **An `OffscreenLayer` implementation ** which serves as an abstraction that can replace `canvas.saveLayer()` for off-screen rendering and composition onto the final bitmap  but with the important distinction that it can also handle drop shadows  and possibly use hardware-accelerated `RenderNode`s and `RenderEffects` where available. - To use an `OffscreenLayer`  call its `.start()` method with a parent canvas and a `ComposeOp`  and draw on the *returned canvas.* Once finished  call `OffscreenLayer.finish()` to compose everything from the returned canvas to the parent canvas  applying alpha  blend mode  drop shadows  and color filters. - `OffscreenLayer` makes a dynamic decision on what to use for rendering - a no-op  forward to `.saveLayer()`  a HW-accelerated `RenderNode`  or a software bitmap  depending on the requested `ComposeOp` and hardware/SDK support. - The hope is that `OffscreenLayer` becomes a useful abstraction that can be extended to e.g. support hardware blurs  multiple drop shadows  or to support mattes in a hardware-accelerated fashion where possible. 3. **The `applyShadowsToLayers` flag** which  by analogy to `applyOpacityToLayers`  turns on a more accurate mode that implements the drop shadow algorithm described above. - `OffscreenLayer` is used to apply alpha if `applyOpacityToLayers` is enabled  and to apply shadows if `applyShadowsToLayers` is enabled. The cost is paid only once if both alpha and drop shadows are present on a layer. - Not all `saveLayer()` calls in the code have been rewritten to use `OffscreenLayer` - the blast radius is minimized. `OffscreenLayer` is presently used only to apply alpha and drop shadows  and blend mode and color filters are still applied in `BaseLayer` using `saveLayer()` directly. 4. **More accurate shadow transformations.** Previously  the angle and distance were pre-transform  and only the radius was post-transform (contrary to step (2) of the algorithm). We correct this to match other renderers. 5. **More complete shadow handling even when `applyShadowsToLayers` is `false`:** we plumb the shadow through `.draw()` and `drawLayer()` calls similarly to alpha  and this allows us to render per-shape shadows on children of composition layers too. 6. ***Workaround for drop shadows on image layers.** - The workaround relies on `OffscreenLayer` as well  and image layers now render shadows properly in all cases. 7. **Fixes to a few subtle issues** causing incorrect rendering in other cases. (will be marked using PR comments  I might have forgotten some)  ## Open questions  * **Should `applyShadowsToLayers` be `true` by default?** Some codepaths  such as when rendering purely via software  can be slow if shadow-casting layers are exceedingly large. But  the performance is still acceptable  and in the vast majority of cases everything is quite snappy. * **Have I introduced any regressions?** The snapshot tests should answer this. * **How does this perform on older devices?** `applyShadowsToLayers` plus an old device should trigger the purely-software shadow rendering mode. Simulating this in condition manually yields accurate results  and the performance seems surprisingly good  but it's unclear what will happen on a lower-end phone. There's also always the possibility of some device subtlety being missed. I don't have access to an older Android device.  ## Testcases  These files now match between lottie-web and lottie-android:  [drop_shadow_comparator.json](https://github.com/user-attachments/files/16997070/drop_shadow_comparator.json)  [simple_shadow_casters_ll2.json](https://github.com/user-attachments/files/16997084/simple_shadow_casters_ll2.json)  The files from this earlier PR still all render the same: https://github.com/airbnb/lottie-android/pull/2523  with the exception of the fix for image layer bug  which fixes the rendering of the Map icon as mentioned in the comment of that PR.  This file has been used as a perf stress test with many <255 opacity precomps  some stacked inside each other  that must all be blended separately: [precomp_opacity_killer.json](https://github.com/user-attachments/files/16997261/precomp_opacity_killer.json)
netty,netty,e3af14773c63c99c38a129727384979aacea1e8d,https://github.com/netty/netty/commit/e3af14773c63c99c38a129727384979aacea1e8d,Deprecate MqttPropertyType (#15225)  Motivation:  It seems like the only purpose for the `MqttPropertyType` was `valueOf` usage in Encoder and Decoder. However  this is not actually necessary. Instead  we can directly do switch based on `int propertyId`. We don't need to resolve enum to perform switch.  Modification:  Deprecated `MqttPropertyType`. Removed unnecessary `MqttPropertyType.valueOf` usages. Replaced enum usages with direct int constant usages.  Result:  The code is simpler and does less.
netty,netty,7e96030afa2a781f9cd1482b095081dcc014f2fe,https://github.com/netty/netty/commit/7e96030afa2a781f9cd1482b095081dcc014f2fe,Always prefer direct buffers for pooled allocators if not explicit di… (#15232)  …sabled  Motivation:  We should always prefer direct buffers if the ByteBufAllocator implementations are pooled as deallocating these buffers should be rare.  Modifications:  - Add new method to PlatformDependent that can be used to check if default direct buffer usage was explicit disabled - Use this method in our pooled ByteBufAllocator implementations to check if we should use direct buffers or not by default  Result:  Less memory copies and better performance when using the pooled allocators even if Unsafe can not be used.
netty,netty,90dff73a0f698525cbb9492d94331fe6692609cb,https://github.com/netty/netty/commit/90dff73a0f698525cbb9492d94331fe6692609cb,Simplify MQTT remaining length parsing (#15196)  Motivation:  Simplify the MQTT remaining length variable parsing in the MqttDecoder.  Modification:  Extracted the parsing logic to a separate method and refactored it.  Result:  The code becomes clearer and easier to understand. Also  the performance got slightly improved: ``` Benchmark                                                (size)  Mode  Cnt   Score   Error  Units MqttRemainingLengthBench.remainingLengthWithDoWhileLoop       1  avgt    3  31.790 ± 2.197  ns/op MqttRemainingLengthBench.remainingLengthWithDoWhileLoop       2  avgt    3  32.582 ± 1.884  ns/op MqttRemainingLengthBench.remainingLengthWithDoWhileLoop       3  avgt    3  32.656 ± 1.432  ns/op MqttRemainingLengthBench.remainingLengthWithDoWhileLoop       4  avgt    3  32.619 ± 1.795  ns/op MqttRemainingLengthBench.remainingLengthWithForLoop           1  avgt    3  31.516 ± 2.110  ns/op MqttRemainingLengthBench.remainingLengthWithForLoop           2  avgt    3  32.459 ± 2.740  ns/op MqttRemainingLengthBench.remainingLengthWithForLoop           3  avgt    3  32.407 ± 1.630  ns/op MqttRemainingLengthBench.remainingLengthWithForLoop           4  avgt    3  32.238 ± 3.994  ns/op ```  ---------  Co-authored-by: Norman Maurer <norman_maurer@apple.com>
netty,netty,c88cdcd3ca6cf5b0cbd0a77ca79f8a59d562f052,https://github.com/netty/netty/commit/c88cdcd3ca6cf5b0cbd0a77ca79f8a59d562f052,Remove Result instance allocation in MqttDecoder (#15193)  Motivation:  MqttDecoder allocates a number of `Result` class instances to decode almost any MQTT message. This reduces the performance and creates more garbage  making life harder for the garbage collector later.  ![image](https://github.com/user-attachments/assets/c2df372b-dd79-48ba-8cfd-44b0c72cb440)  Modification:  Removed `Result` class instance allocations in MqttDecoder.  Result:  Reduced class instance allocations lead to improved performance and reduced garbage.
netty,netty,eaf38df0eb87c8562e044b512386afd62ffc38d2,https://github.com/netty/netty/commit/eaf38df0eb87c8562e044b512386afd62ffc38d2,MQTT: Improve `isValidPublishTopicName` method performance (#15198)  Motivation:  We can refactor the method to improve performance.  Modifications:  Refactored `MqttCodecUtil.isValidPublishTopicName()` method to boost the performance.  Result:  According to my benchmarks  it is boosted almost twice: ``` Benchmark                           Mode  Cnt   Score   Error  Units TwoIndexOfVSLoopImprove.charAt      avgt    3  13.973 ± 0.137  ns/op TwoIndexOfVSLoopImprove.twoIndexOf  avgt    3  24.519 ± 7.310  ns/op ```  Co-authored-by: Norman Maurer <norman_maurer@apple.com>
netty,netty,31c033a1d95eb4a74002c5f17ea01c967b07add9,https://github.com/netty/netty/commit/31c033a1d95eb4a74002c5f17ea01c967b07add9,Optimize ByteBuf.setCharSequence for adaptive allocator (#15165)  Motivation: The `setCharSequence` method is used a fair bit when encoding HTTP headers. I noticed the AdaptiveByteBuf was sending the call down a path with a more indirections than strictly necessary.  Modification: Implement `setCharSequence` in the `AdaptiveByteBuf` so we skip a level of indirections. This is especially profitable for direct/off-heap buffers  where we see up to a 50% performance improvement in the benchmark. The benefit is greater the longer the string is.  Interestingly  the benchmark also show that it isn't profitable to do this for the `getCharSequence` sibling method.  Result: Direct buffers from the adaptive allocator now have much faster `setCharSequence`  which will help with HTTP header encoding.
netty,netty,4bbe6f71d0d0bbf514b281d920aed276b5156919,https://github.com/netty/netty/commit/4bbe6f71d0d0bbf514b281d920aed276b5156919,Optimize capacity bumping for adaptive ByteBufs (#15080)  Motivation: It is quite common in downstream code to not specify a buffer size when allocating. People instead rely on the ByteBuf ability to increase its size automatically. The ByteBufs allocated by the adaptive allocator also support this  but has thus far required a round-trip through the allocator every time. In contrast  the pooled allocator is reserving the chunk run-size when allocating buffers  and very cheaply allow smaller buffers to bump up to the run-size limit. This is an important performance optimisation in practice. We can do something similar in the adaptive allocator  because we keep statistics about the buffer sizes that we allocate.  Modification: The adaptive allocator now reserves space equal to the 99-percentile buffer size (though with some reasonable heuristic limits for smaller buffers)  when allocating buffers  and allow its ByteBufs to cheaply bump their capacities up to this limit. We already compute this percentile during histogram rotation  so it is simply a matter of storing this value in the magazine  and then using it when reserving space out of chunks.  The collection of buffer size statistics is now also delayed and moved to when the `ByteBuf` instances are reused. This way  we collect statistics about the _final_ buffer size  rather than the requested sizes or the sizes of capacity bumps. This prevents oscillations that would otherwise be caused by undersized buffers producing a lot of statistics as they resize to their final size  and then no statistics being produced for the final buffer sizes if they are correctly predicted at allocation time.  Result: Calls like `ByteBuf.ensureWritable()` will now on average be much cheaper when using the adaptive allocator.  Based on https://github.com/netty/netty/pull/15062
netty,netty,4c1a4bd08d435c44f20cc07e37a45321f520bb18,https://github.com/netty/netty/commit/4c1a4bd08d435c44f20cc07e37a45321f520bb18,IoUring: Allow users to explicit enable RECVSEND_BUNDLE support (#15135)  Motivation:  In the past we did disable support for RECVSEND_BUNDLE completely as we did see issues on our CI. After more debugging we were able to report the issue to the io_uring kernel maintainers as it turned out to be a kernel bug:   https://lore.kernel.org/io-uring/364679fa-8fc3-4bcb-8296-0877f39d6f2c@gmail.com/T/#ma949ad361d376247a16db73e741cb1043e56e6a4  Once there is a kernel which has this bug-fix it is save to use RECVSEND_BUNDLE again and so make use of the extra performance. Because of this we should enable support but disable the feature explicit if not told otherwise. This will allow the end-user to explicit enable it if it is know that there current kernel is not affected anymore.  Modifications:  - Reenable support for RECVSEND_BUNDLE - Disable usage of RECVSEND_BUNDLE by default but let users explicit enable it again via -Dio.netty.iouring.recvsendBundleEnabled=true - Simplify code to use the same code path for RECVSEND_BUNDLE usage and non-usage  Result:  Allow users to explicit opt in for RECVSEND_BUNDLE
netty,netty,7611905d3315c86da8aa4d55469b50f59a9af949,https://github.com/netty/netty/commit/7611905d3315c86da8aa4d55469b50f59a9af949,Fix init order of PlatformDependent0 fields (#15077)  Motivation:  After #14975  `EXPLICIT_NO_UNSAFE_CAUSE` has a dependency to `JAVA_VERSION`  which is initialized after it. When `explicitNoUnsafeCause0()` tries to set `io.netty.noUnsafe` to true by default on Java 24  it fails  because `javaVersion()` always returns 0 at that point.  Also  `isAndroid0()` is called twice  once in `javaVersion0()` and once to initialize `IS_ANDROID`. This is not a bug or a performance issue  but it does have a visible side-effect when debug logging is enabled.  Modification:  The declaration of `EXPLICIT_NO_UNSAFE_CAUSE` is moved after `JAVA_VERSION`  ensuring `javaVersion()` returns the correct version when `explicitNoUnsafeCause0()` calls it.  `IS_ANDROID` is also moved before `JAVA_VERSION`  such that the `isAndroid0()` method is called only once.  Result:  Fixes #14942 correctly
netty,netty,778ba3e54e0694c4dd6b82773c5ab9f569e9afa0,https://github.com/netty/netty/commit/778ba3e54e0694c4dd6b82773c5ab9f569e9afa0,fix: WebSocketClientCompressionHandler shouldn't claim window bits support when jzlib is not available (#15018)  Motivation:  WebSocketClientCompressionHandler and PerMessageDeflateClientExtensionHandshaker use ZlibCodecFactory.isSupportingWindowSizeAndMemLevel()  which in netty 4.2 always returns true.  Actually  `java.util.zip.Deflater` doesn't support window bits and only works if the server provides a value of 15  which is unlikely and should only be considered a lucky optimization path.  Different values would only work if `jzlib` is available in the classpath. But it's an optional dependency and it's less performant than the JDK implementation as it's pure Java.  Currently  when jzlib is not available in the classpath and the server decides to take over client window bits with a value different from 15 (eg python would use 12)  Netty crashes with a CNFE.  Motivation:  Only advertise window bits support if jzlib is available in the classpath.  Result:  No more CNFE when the WebSocket server reacts to client bits support with a non 15 value and jzlib is not available in the classpath.
netty,netty,2e9dadb9ae4ffb5a720a680a5195be8193710783,https://github.com/netty/netty/commit/2e9dadb9ae4ffb5a720a680a5195be8193710783,IoUring: Retry buffer ring based ready directly once we receive a ENO… (#14866)  …BUFS.  Motivation:  When we use a buffer ring and receive ENOBUFS we should just let the user know and retry with a buffer ring again. This works as expected as we always refill the buffer ring once we used a buffer out of it. At the moment we will fallback to do a read loop without a buffer ring which slows down things without any benefits.  Modifications:  Just retry directly with a buffer ring.  Result:  Performance improvements when the buffer ring is exhausted in between.  Before:  ``` ./src/tcpkali -m xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx   -c 50 -T 30 127.0.0.1:8081 Destination: [127.0.0.1]:8081 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8081 Ramped up to 50 connections. Total data sent:     130258.4 MiB (136585805824 bytes) Total data received: 130229.7 MiB (136555722112 bytes) Bandwidth per channel: 1456.511⇅ Mbps (182063.8 kBps) Aggregate bandwidth: 36408.753↓  36416.774↑ Mbps Packet rate estimate: 3333377.7↓  3125676.8↑ (12↓  45↑ TCP MSS/op) Test duration: 30.005 s. ```  With this change:  ``` ./src/tcpkali -m xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx   -c 50 -T 30 127.0.0.1:8081 Destination: [127.0.0.1]:8081 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8081 Ramped up to 50 connections. Total data sent:     201195.0 MiB (210968248320 bytes) Total data received: 201185.2 MiB (210958005952 bytes) Bandwidth per channel: 2249.524⇅ Mbps (281190.5 kBps) Aggregate bandwidth: 56236.732↓  56239.462↑ Mbps Packet rate estimate: 5148636.2↓  4827071.8↑ (12↓  45↑ TCP MSS/op) Test duration: 30.01 s. ```
netty,netty,cc934fd06ebdf00ef5c1e1ea9d06c51fb6ddb91f,https://github.com/netty/netty/commit/cc934fd06ebdf00ef5c1e1ea9d06c51fb6ddb91f,Add QueryStringDecoder option to leave '+' alone (#14850) (#14857)  Motivation:  The URI standard RFC 3986 does not specify that query components have their spaces encoded as `+`. It is implied that the encoding is `%20` instead. However  the whatwg HTML standard says explicitly that the query must be encoded using `application/x-www-form-urlencoded` rules  which does use `+` for space. This is also what browsers do.  QueryStringDecoder should offer a way to parse either format.  Modification:  - Modify QueryStringDecoder to use a builder to accommodate the increasing number of flags - Add a `htmlQueryDecoding` flag  enabled by default  to control the `+` decoding  The default value of `htmlQueryDecoding` is appropriate for most use cases  I don't think it should be changed even in netty 5.  Also fixed the benchmark harness for Java 21.  Result:  Query strings encoded purely according to the URI spec can be decoded properly.  I measured the performance of the new builder  and it didn't look any different.  Co-authored-by: Jonas Konrad <me@yawk.at> Co-authored-by: Chris Vest <mr.chrisvest@gmail.com>
netty,netty,a4acd66fee7efc8955ede65fef63012085551ea1,https://github.com/netty/netty/commit/a4acd66fee7efc8955ede65fef63012085551ea1,IoUring: Refill buffer ring more eagerly (#14842)  Motivation:  IoUring: Refill buffer ring more eagerly  Motivation:  We should refill the buffer ring as soon as possible to reduce the possibility of seeing ERRNO_NO_BUFFER  Modifications:  Refill buffer as soon as there is an empty slot.  Result.  Much better performance.  Before:  ``` src/./tcpkali  -m xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx   -c 10 -T 30 127.0.0.1:8081 Destination: [127.0.0.1]:8081 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8081 Ramped up to 10 connections. Total data sent:     200090.4 MiB (209809965056 bytes) Total data received: 200066.2 MiB (209784590336 bytes) Bandwidth per channel: 11185.687⇅ Mbps (1398210.9 kBps) Aggregate bandwidth: 55925.055↓  55931.819↑ Mbps Packet rate estimate: 5120114.0↓  4800666.6↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0094 s. ```  After:  ``` src/./tcpkali  -m xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx   -c 10 -T 30 127.0.0.1:8081 Destination: [127.0.0.1]:8081 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8081 Ramped up to 10 connections. Total data sent:     251409.6 MiB (263622033408 bytes) Total data received: 251353.1 MiB (263562850304 bytes) Bandwidth per channel: 14058.190⇅ Mbps (1757273.7 kBps) Aggregate bandwidth: 70283.059↓  70298.841↑ Mbps Packet rate estimate: 6434619.5↓  6033797.9↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0002 s. ```
netty,netty,0d7320ca4c87bec34db9e2020712e2b93cc3bbf5,https://github.com/netty/netty/commit/0d7320ca4c87bec34db9e2020712e2b93cc3bbf5,Reduce pipeline stack depth and improve performance (#14705)  Motivation: Pipeline calls  such as `ctx.fireChannelRead` and `ctx.write`  currently go through multiple methods. This increases the stack depth in event loop threads  which makes it harder to debug (people have more ceremony mixed in with their code)  and hurts performance (JIT inliner budget gets used up faster).  Furthermore  there are other places in this machinery which can be made faster.  Modification:  **1.** In the `AbstractChannelHandlerContext` all inbound methods  e.g. `fireChannelRead`  have been manually inlined. This means that when a handler calls `ctx.fireChannelRead`  this method call in turn now directly call the `channelRead` method of the _next_ handler in the pipeline. Previously  we had two extra method calls here.  As a consequence of this inlining  we now have to re-compute the target context when we trampoline tasks onto the event-loop. This is presumably rare  and worth the cost. This also means that some code now moves from the executor of the target context  to the executor of the calling context. This can create different behaviors from Netty 4.1  if the pipeline has multiple handlers  is modified by the handlers during the call  and the handlers use child-executors.  **2.** A few outbound methods - `read`  `write`  `writeAndFlush` - are likewise inlined. Their inherent complexity and number of overloads means we can't realistically get them down to a single method call  but we can get them down to two. This is still a nice improvement. The `flush` method is usually not implemented by handlers  so there's no point in inlining that further.  **3.** In every such call  after finding the next context  we have to decide if we can call the handler directly  or need to trampoline onto a different event loop (due to the executor off-loading feature). This means we have to inspect the context and either pick its child executor  or load the channel event loop of the target context  and this latter part (which is the most common case) is a sequence of dependent loads. Dependent loads cause cache misses and CPU pipeline stalls  so to deal with this the `AbstractChannelHandlerContext` now has a `contextExecutor` field  which caches the result of computing the concrete executor. This means our executor is only one dependent (on the channel handler context) load away. The speedup from this is quite noticeable because it's such a common operation.    Results: The `DefaultChannelPipelineBenchmark` on my M1 Pro  running JDK 17  tells an encouraging story:  ``` Before: Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt        Score        Error  Units DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   10  6515502.983 ± 253375.107  ops/s DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   10  7628162.835 ±  40168.895  ops/s  After: Benchmark                                         (extraHandlers)  (pipelineArrayLength)   Mode  Cnt        Score        Error  Units DefaultChannelPipelineBenchmark.propagateEvent                 16                   1024  thrpt   50  6899710.275 ± 115305.284  ops/s DefaultChannelPipelineBenchmark.propagateVariety               16                   1024  thrpt   50  7985208.748 ±  31532.793  ops/s ```
netty,netty,f5bc749b192b64c2bc3c080e24ebb0eb94505aee,https://github.com/netty/netty/commit/f5bc749b192b64c2bc3c080e24ebb0eb94505aee,IOUring: Add supported for provided buffers (#14690)  Motivation:  In scenarios with a large number of inactive connections  reduce memory usage and improve performance.  see https://lwn.net/Articles/815491/  Modification:  Implement IoUringBufferRing and IOSQE_BUFFER_SELECT flag for socket recv:  - Enable the use of IOSQE_BUFFER_SELECT in the socket recv when IoUringBufferRing is supported by the current Linux kernel version and IOUringSocketChannelConfig is configured to enable the feature - IoUringBufferRing elements are not populated immediately; only when submitting recv ops  if there is available space in the current BufferRing  the allocator will be used to allocate a buffer. - If the res in the recv CQE is -ERRNO_NO_BUFFER  mark the bufferRing as temporarily unavailable. The current and subsequent recv operations will fall back to the old recv path. - When a ByteBuf allocated from IoUringBufferRing is released by the user  it will be returned to the IoUringBufferRing  and the buffer will be marked as available for reuse.  Result:  Fixes #14614  ---------  Co-authored-by: Norman Maurer <norman_maurer@apple.com>
netty,netty,cf340037c90a04011c0e9bf45b17f9ff17f49437,https://github.com/netty/netty/commit/cf340037c90a04011c0e9bf45b17f9ff17f49437,Revert "IoUring: Only execute write related completion inline" (#14711)  Motivation:  This change drops perf by 10-15%. Let's revert it for now.  Modifications:  Reverts netty/netty#14704  Result:  Fix performance drop.
netty,netty,f17fb0547575fb0adba3bd2f0ccadf025c1d7d4b,https://github.com/netty/netty/commit/f17fb0547575fb0adba3bd2f0ccadf025c1d7d4b,IoUring: Create ring with IORING_SETUP_SINGLE_ISSUER and IORING_SETUP_DEFER_TASKRUN to reduce overhead (#14699)  Motivation:  We should create our ring with IORING_SETUP_SINGLE_ISSUER and IORING_SETUP_DEFER_TASKRUN to ensure we can make progress as fast as possible. This improves performance quite a bit:  See https://manpages.debian.org/unstable/liburing-dev/io_uring_setup.2.en.html#IORING_SETUP_DEFER_TASKRUN  Modifications:  - Create ring with IORING_SETUP_SINGLE_ISSUER and IORING_SETUP_DEFER_TASKRUN (and also IORING_SETUP_R_DISABLED to be able to select the correct thread)  Result:  Much faster handling of a lot of concurrent connections (while still maintain the same performance with small number).  Before: ``` Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1000 connections. Total data sent:     47317.6 MiB (49616085120 bytes) Total data received: 47430.3 MiB (49734300462 bytes) Bandwidth per channel: 26.492⇅ Mbps (3311.5 kBps) Aggregate bandwidth: 13261.733↓  13230.211↑ Mbps Packet rate estimate: 1137136.1↓  1139280.4↑ (11↓  45↑ TCP MSS/op) Test duration: 30.0017 s. ```  After: ``` Destination: [127.0.0.1]:8088 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1000 connections. Total data sent:     80425.0 MiB (84331771459 bytes) Total data received: 80513.6 MiB (84424659415 bytes) Bandwidth per channel: 44.998⇅ Mbps (5624.7 kBps) Aggregate bandwidth: 22511.278↓  22486.510↑ Mbps Packet rate estimate: 2077374.7↓  1936466.0↑ (11↓  45↑ TCP MSS/op) ```
netty,netty,595a07d432281bbd27e57fd1154477efcd841143,https://github.com/netty/netty/commit/595a07d432281bbd27e57fd1154477efcd841143,IoUring: Submit after completion queue was processed (#14696)  Motivation:  We should try to submit one more time after we did run all completions. Otherwise we might introduce latency that will hurt performance.  Modifications:  Add one more submit call.  Result:  Better troughput.  Before the change: ``` Destination: [127.0.0.1]:8088 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1 connections. Total data sent:     134066.2 MiB (140578586624 bytes) Total data received: 134066.1 MiB (140578537472 bytes) Bandwidth per channel: 74755.196⇅ Mbps (9344399.5 kBps) Aggregate bandwidth: 37377.591↓  37377.605↑ Mbps Packet rate estimate: 3424327.9↓  3208145.5↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0883 s. ```  After this change: ``` Destination: [127.0.0.1]:8088 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1 connections. Total data sent:     250820.2 MiB (263004028928 bytes) Total data received: 250812.7 MiB (262996131840 bytes) Bandwidth per channel: 139956.971⇅ Mbps (17494621.4 kBps) Aggregate bandwidth: 69977.435↓  69979.536↑ Mbps Packet rate estimate: 6406626.3↓  6006391.8↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0664 s. ```
netty,netty,4d868a6da3aeb0b222ac24c8bf5f63a416f74ecf,https://github.com/netty/netty/commit/4d868a6da3aeb0b222ac24c8bf5f63a416f74ecf,IoUring: Force submit and instantly running completions when Channel becomes unwritable (#14693)  Motivation: When the Channel becomes unwritable we need to ensure we can write and release the data as soon as possible. Failing to do so increase the memory usage and also hurts the performance in general and so will reduce throughtput  Modifications:  - Add a new class which is used to drain the CompletionQueue before finally run the completions - When we do a flush() and detect that the Channel became unwritable trigger a submit (io_uring_enter) and directly obtain the completions and run them. This will ensure we can release buffers directly when the socket is writable as the write will be executed inline by io_uring_enter.  Result:  Before the change:  ``` Destination: [127.0.0.1]:8088 Interface lo address [127.0.0.1]:0 Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1 connections. Total data sent:     100726.8 MiB (105619652608 bytes) Total data received: 100717.7 MiB (105610199040 bytes) Bandwidth per channel: 56149.221⇅ Mbps (7018652.6 kBps) Aggregate bandwidth: 28073.354↓  28075.867↑ Mbps Packet rate estimate: 2575952.0↓  2409771.0↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0955 s. ```  After this change:  ``` Using interface lo to connect to [127.0.0.1]:8088 Ramped up to 1 connections. Total data sent:     134226.4 MiB (140746555392 bytes) Total data received: 134226.5 MiB (140746670080 bytes) Bandwidth per channel: 74841.324⇅ Mbps (9355165.5 kBps) Aggregate bandwidth: 37420.677↓  37420.647↑ Mbps Packet rate estimate: 3427561.0↓  3211839.9↑ (12↓  45↑ TCP MSS/op) Test duration: 30.0896 s. ```  ---------  Co-authored-by: Chris Vest <christianvest_hansen@apple.com>
netty,netty,d25d6667b992cbef2de22ca418e78eaa5d00b75d,https://github.com/netty/netty/commit/d25d6667b992cbef2de22ca418e78eaa5d00b75d,IoUring: Keep on reading data until nothing is left to read on a socket (#14668)  Motivation:  We need to keep on submitting reads until there is nothing left to read from the socket. Otherwise we will suffer from performance problems as we will do more fireChannelReadComplete() calls then required and so hurt batching.  Modifications:  Use our own supplier that ensures we read all data.  Result:  Fix performance problems
netty,netty,5637bbe6d346bb8ee576637f0fed11d1e8524a55,https://github.com/netty/netty/commit/5637bbe6d346bb8ee576637f0fed11d1e8524a55,Make JMH executor threads look like event loop threads (#14444) (#14445)  Motivation: Our buffer allocators make specific optimisations for event loop threads. Thus it makes sense that our benchmark executors look as much like event loops as possible  to trigger those optimisations.  Modification: Make sure that JMH executor threads add themselves to the `ThreadExecutorMap` so that they look like event loop threads.  Result: The various `ByteBufAllocator*Benchmarks` show improved performance because they now go through event-loop optimised code paths.  When needed  it's still possible to disable this behavior by disabling the custom harness executor. That functionality has been part of our custom JMH harness for a long time.  Co-authored-by: Chris Vest <christianvest_hansen@apple.com>
netty,netty,48687be377c1e0b94acddca4ae54eff996c1ee2a,https://github.com/netty/netty/commit/48687be377c1e0b94acddca4ae54eff996c1ee2a,Replace ArrayDeque::pollFirst with ArrayDeque::pollLast for Recycler.… (#14331)  …LocalPool batch. (#14268) (#14292)  Motivation:  Improve performance by replacing ArrayDeque::pollFirst with ArrayDeque::pollLast.  Modifications:  Replaced ArrayDeque::pollFirst with ArrayDeque::pollLast for LocalPool batch.  Result: - The new version shows the best overall performance in benchmarks such as plainNew  recyclerGetAndRecycle  and unguardedProducerConsumer. - Overall  the new version shows a significant reduction in GC time  improving memory efficiency across most tests.  Fixes #14268  Co-authored-by: Obolrom <65775868+Obolrom@users.noreply.github.com>
netty,netty,50e7b2d0e3c25adc9479b85acd0f7ed1571bc045,https://github.com/netty/netty/commit/50e7b2d0e3c25adc9479b85acd0f7ed1571bc045,Switch to AdaptiveByteBufAllocator by default (#14271)  Motivation:  The AdaptiveByteBufAllocator reduce the memory overhead quite a lot while still provide kind of the same performance characteristics. Let's use it as the default allocator  Modifications:  Switch default allocator from PooledByteBufAllocator to AdaptiveByteBufAllocatore. Users can use -Dio.netty.allocator.type=pooled to switch the old default if wanted  Result:  Less memory overhead by default
netty,netty,723539fefaf8572706869ba8e3d13b2f31e33894,https://github.com/netty/netty/commit/723539fefaf8572706869ba8e3d13b2f31e33894,Explicit module support for Netty (#14267)  Motivation:  Although Netty remains compatible with old version of Java  the Java runtime can often be a recent JVM. The JEP [Integrity by default](https://openjdk.org/jeps/8305968) will restrict how Netty operates in the future (JNI  Unsafe). Supporting explicit modules will help to mitigate this by making Netty a clear identified consumer of these features  letting users configure Netty to use them (e.g. `--enable-native-access=io.netty.xxx`). This also allows to use tool like jlink to create runtime image with a smaller application footprint.  In addition it gives the opportunity to libraries (such as Vert.x) and framework to support explicit modules as well.  The supported dependencies of Netty (those which are not in maintenance mode) provides already explicit modules.  A lot of cleanup and decoupling as already been done in previous pull requests  this is the final step to support declarative JPMS.  Changes:  Drop the support for named automatic modules  a maven module either provides a module-info descriptor class and supports module or does not.  Most maven modules do provide this  the following maven modules `netty-transport-rxtx` `netty-transport-sctp`  `netty-transport-udt` cannot support modularity.  Other modules provides a descriptor in yml format compiled to a module-info class by the module-info maven plugin located in `META-INF/versions` turning jars into multi release jars.  A new testsuite is added (`testsuite-jpms`) that only runs for Java versions supporting Java modules (11 and forward). In addition the testsuite module builds a jlink image with an HTTP server that can be used to perform checks and/or demos. The server supports native SSL as well as native transports.  Result:  Provide a better support for the Java Platform Module System.  Fixes #14176
netty,netty,89c43e464191a8edafacb0ba92b480b884e7d5b8,https://github.com/netty/netty/commit/89c43e464191a8edafacb0ba92b480b884e7d5b8,Add keytool-based self-signed certificate generator (#14198)  Motivation:  On newer Java versions  the JDK-based self-signed certificate generator does not work  so adding bcpkix as a dependency was necessary. This is inconvenient for users that want to use self-signed certs as part of their tests.  Modification:  Add a KeytoolSelfSignedCertGenerator which uses the `keytool` command included in the JDK to generate the key pair and certificate.  Introduce a SelfSignedCertificate.Builder to make the constructor chaos a bit cleaner. Additionally  since the `keytool` generator uses an external RNG  this Builder allows for the in-vm KeyPair generation to happen lazily  improving performance for the keytool path.  The next step I've thought about doing is to move the unnecessary file system saving (SelfSignedCertificate.newSelfSignedCertificate) to be lazy  so that we can avoid writing the key and cert at all in some cases. But that is for another day  it'd require some exception handling changes.  Result:  Users can generate self-signed certificates without additional dependencies.  ---------  Co-authored-by: Norman Maurer <norman_maurer@apple.com>
netty,netty,5fa29ce037adf8683004e4bae5daf4d7b333fa25,https://github.com/netty/netty/commit/5fa29ce037adf8683004e4bae5daf4d7b333fa25,Fix potential DNS cache invalidation across different EventLoops (#14147)  Motivation:  By default  `DnsAddressResolverGroup` sets up a separate `DNSCache` for each `EventLoop`. This can lead to cache invalidation when performing DNS resolution across different EventLoops due to the potential absence of cached DNS records.  Modification:  - initializing the DNSCache in `DnsNameResolverBuilder` during `DnsAddressResolverGroup` initialization to enable sharing of DNSCache among EventLoops under the same `DnsAddressResolverGroup`. - Add unit test: testSharedDNSCacheAcrossEventLoops  Result:  Fixes #14046.  ---------  Co-authored-by: Norman Maurer <norman_maurer@apple.com> Co-authored-by: Bryce Anderson <bryce_anderson@apple.com>
netty,netty,b9552ea9d81427252303352ad7005b8651ea52b7,https://github.com/netty/netty/commit/b9552ea9d81427252303352ad7005b8651ea52b7,JdkSslContext doesn't set endpoint verification algorithm (#14144)  Motivation:  If users configure ALPN  `JdkSslContext.configureAndWrapEngine` skips endpointIdentificationAlgorithm configuration at all. Otherwise  it only sets it in a new `SSLParameters` object but does not apply parameters changes back to the engine. As a result  engine always returns a new `SSLParameters` object without seeing a change in `endpointIdentificationAlgorithm`.  Modifications: - Add `configureEndpointVerification(SSLEngine)` method that properly sets identification algorithm and applies the change for `SSLEngine`. - Move this logic before `JdkApplicationProtocolNegotiator.AllocatorAwareSslEngineWrapperFactory` check. - Add `endpointIdentificationAlgorithm(null)` for tests where necessary.  Result:  Endpoint identification algorithm is correctly set for `SslProvider.JDK`.
netty,netty,86b970dfd3e869f3279c100c17b35380e6dfd6a3,https://github.com/netty/netty/commit/86b970dfd3e869f3279c100c17b35380e6dfd6a3, Avoid unnecessary reflective probes on netty initialization (#14107)  Motivation:  https://github.com/netty/netty/pull/14090 added multiple reflective accesses on netty initialization to probe availability of Unsafe methods which may be removed after java 23 (jep 471).  This may have negative performance impact on application startup (particularly with lower end machines) since reflection is expensive before jvm is warmed up. However  these reflective method probes may be potentially useful only after java 23 - when/if method removals actually materialize.  Modification:  Make reflective accesses on netty initialization to probe availability of Unsafe methods only if java version > 23.  Result:  Unnecessary reflective accesses on netty initialization are avoided for java version <= 23.
netty,netty,4565f942690a782f162d6ff4bb5b3167b1b1338d,https://github.com/netty/netty/commit/4565f942690a782f162d6ff4bb5b3167b1b1338d,Optimize wrap buffer cumulation in SslHandler and don't mutate input buffers  (#14086)  Motivation:  SslHandler doesn't support sharing the input buffers since the input buffers get mutated. This issue has been reported as #14069. Fixes have already been made to support the use case using read-only buffers as input. However  this is not useful for existing Netty applications.  I checked the Netty code once again and noticed that it's possible to optimize the current implementation in SslHandler and avoid the buffer mutation.  The default behavior in SslHandler causes very hard to debug problems. In Pulsar  there have been issues open for multiple years that have been caused by the SslHandler's default behavior of mutating the input buffers (apache/pulsar#22601 is one starting point to the issues).  The current implementation of SslHandler will mutate the input buffers by using the input buffer as a cumulation buffer when there's available capacity. This is not optimal at all. This PR contains an improvement where the cumulation buffer is allocated to the final size immediately so that cumulation is more optimal. That's why I think that this will be more optimal in the end and won't cause performance regressions.  After these changes  it should be possible to pass a slice or duplicate of a buffer and the input buffer doesn't get mutated. The input buffer's readIndex will be modified so multiple threads cannot share the same input buffer without a slice or duplicate.  I created a failing test case that shows that a duplicate buffer doesn't prevent input buffer mutation.  ![image](https://github.com/netty/netty/assets/66864/0abec333-6079-40bd-8bc1-ccfd3df6fae1) It's a very surprising behavior of SslHandler that passing a `.retainedDuplicate()` buffer could result in mutations to the original buffer. That's another reason why I think that SslHandler shouldn't ever mutate the inputs.  Modification:  - add `protected ByteBuf composeFirst(ByteBufAllocator allocator  ByteBuf first  int bufferSize)` to AbstractCoalescingBufferQueue to support allocating an optimal cumulation buffer instead of using the input buffer as a cumulation buffer - remove dead code from `compose` where there was handling for CompositeByteBuf - `composeFirst` will be called before `compose` so this case isn't needed.  Result:  Fixes #14069 without the need to set the input buffer as read-only  ---------  Co-authored-by: Norman Maurer <norman_maurer@apple.com>
doocs,leetcode,220c48352b55bb9342cbc9dc385210b2518094b8,https://github.com/doocs/leetcode/commit/220c48352b55bb9342cbc9dc385210b2518094b8,feat: update solutions to lc problem: No.367 (#3002)  No.0367.Valid Perfect Square
TeamNewPipe,NewPipe,48b200868a455722690881f739bcd4cb96e52c74,https://github.com/TeamNewPipe/NewPipe/commit/48b200868a455722690881f739bcd4cb96e52c74,BF-11894 : Fix the menu disappearing on performing backGesture
TeamNewPipe,NewPipe,fef40014a0da63aa264448c676a519ffa1d00bbf,https://github.com/TeamNewPipe/NewPipe/commit/fef40014a0da63aa264448c676a519ffa1d00bbf,Added not null check for thumbnail URL before performing comparison
SeleniumHQ,selenium,dbe100426c9a1111b4d452e12443ce3975f89e2b,https://github.com/SeleniumHQ/selenium/commit/dbe100426c9a1111b4d452e12443ce3975f89e2b,[java] setter for flag JsonInput.readPerformed (#14921)  Co-authored-by: Puja Jagani <puja.jagani93@gmail.com>
SeleniumHQ,selenium,e7c09a2cb2b0951f2c2c1907b11f732f00257e44,https://github.com/SeleniumHQ/selenium/commit/e7c09a2cb2b0951f2c2c1907b11f732f00257e44,[grid] enable the httpclient to perform async requests #14403 (#14409)  Co-authored-by: Viet Nguyen Duc <nguyenducviet4496@gmail.com>
SeleniumHQ,selenium,6704db0535b1f11ba15b242cb2a6a96f704b4b8f,https://github.com/SeleniumHQ/selenium/commit/6704db0535b1f11ba15b242cb2a6a96f704b4b8f,[java] minor performance improvements and code cleanup (#14054)  * replaced empty string comparison with isEmpty() invoking  * replaced manual array copy with System.arraycopy()  * replaced redundant String.format invoking with printf()  * replaced deprecated setScriptTimeout with scriptTimeout according to instruction "Use scriptTimeout(Duration)"  * replaced iterators with bulk methods invoking  * replaced list creations with List.of()  * there is no need to create mutable lists in tests to only get elements  ---------  Co-authored-by: Puja Jagani <puja.jagani93@gmail.com>
YunaiV,ruoyi-vue-pro,20b4329af65559cd30aef6f8dadc85e3c74694ee,https://github.com/YunaiV/ruoyi-vue-pro/commit/20b4329af65559cd30aef6f8dadc85e3c74694ee,!1350 perf:【INFRA 基础设施】优化一些 todo 提到的问题 Merge pull request !1350 from puhui999/vben5-antd-schema
YunaiV,ruoyi-vue-pro,6982243370fcd5f3234b61a0b6d4dab109b991f3,https://github.com/YunaiV/ruoyi-vue-pro/commit/6982243370fcd5f3234b61a0b6d4dab109b991f3,perf:【INFRA 基础设施】优化一些 todo 提到的问题
YunaiV,ruoyi-vue-pro,b9ffa1820dcaa20b4e8741b4dc5d02d8f03473e0,https://github.com/YunaiV/ruoyi-vue-pro/commit/b9ffa1820dcaa20b4e8741b4dc5d02d8f03473e0,perf:【INFRA 基础设施】优化一些 todo 提到的问题
YunaiV,ruoyi-vue-pro,cd341da674be599301a8fffa3cadc664505c58ad,https://github.com/YunaiV/ruoyi-vue-pro/commit/cd341da674be599301a8fffa3cadc664505c58ad,perf:【INFRA 基础设施】代码生成主子表非 erp 模式时，当子表一对多时更新改为通过 diffList 实现对应的增删改
YunaiV,ruoyi-vue-pro,bc77af09e03039d633aef275a5e729ef1abb5693,https://github.com/YunaiV/ruoyi-vue-pro/commit/bc77af09e03039d633aef275a5e729ef1abb5693,perf:【INFRA 基础设施】代码生成示例 demo 代码格式化
YunaiV,ruoyi-vue-pro,61cfcc283b426dd69e3f479a5600be3c6e1f382b,https://github.com/YunaiV/ruoyi-vue-pro/commit/61cfcc283b426dd69e3f479a5600be3c6e1f382b,perf:【INFRA 基础设施】代码生成配置 delete-batch-enable: false # 是否生成批量删除接口
YunaiV,ruoyi-vue-pro,bb236af6310b852337e501d026d08048fad945bf,https://github.com/YunaiV/ruoyi-vue-pro/commit/bb236af6310b852337e501d026d08048fad945bf,perf: 优化 FileTypeUtils 的 TIKA 创建，提升性能
YunaiV,ruoyi-vue-pro,05bf229a3c91b9e74c11b380703ba5dd42e89fa2,https://github.com/YunaiV/ruoyi-vue-pro/commit/05bf229a3c91b9e74c11b380703ba5dd42e89fa2,perf:【INFRA 基础设施】vben5-antd-schema 主主子表inner代码生成时，可生成批量删除
YunaiV,ruoyi-vue-pro,e10425e049daac247b77b4fc4b193d14298c9e11,https://github.com/YunaiV/ruoyi-vue-pro/commit/e10425e049daac247b77b4fc4b193d14298c9e11,perf:【INFRA 基础设施】vben5-antd-schema 主主子表normal代码生成时，可生成批量删除
YunaiV,ruoyi-vue-pro,ef1e7b312bde4e653da3a7e4b40ef7ff5af04bba,https://github.com/YunaiV/ruoyi-vue-pro/commit/ef1e7b312bde4e653da3a7e4b40ef7ff5af04bba,perf:【INFRA 基础设施】vben5-antd-schema 主主子表erp代码生成时，可生成批量删除
YunaiV,ruoyi-vue-pro,e00bfd02a1008a2a9a832c37b94dec7ac3d0cf2c,https://github.com/YunaiV/ruoyi-vue-pro/commit/e00bfd02a1008a2a9a832c37b94dec7ac3d0cf2c,Merge branch 'master-jdk17' of https://gitee.com/zhijiantianya/ruoyi-vue-pro  # Conflicts: #	pom.xml #	yudao-module-bpm/src/main/java/cn/iocoder/yudao/module/bpm/service/task/BpmProcessInstanceServiceImpl.java #	yudao-module-crm/src/main/java/cn/iocoder/yudao/module/crm/controller/admin/operatelog/CrmOperateLogController.java #	yudao-module-crm/src/main/java/cn/iocoder/yudao/module/crm/controller/admin/statistics/CrmStatisticsPerformanceController.java #	yudao-module-crm/src/main/java/cn/iocoder/yudao/module/crm/controller/admin/statistics/vo/performance/CrmStatisticsPerformanceReqVO.java #	yudao-module-crm/src/main/java/cn/iocoder/yudao/module/crm/service/statistics/CrmStatisticsPerformanceServiceImpl.java #	yudao-module-mall/yudao-module-promotion/pom.xml #	yudao-module-mall/yudao-module-trade/pom.xml #	yudao-module-system/src/main/java/cn/iocoder/yudao/module/system/api/social/SocialUserApi.java #	yudao-module-system/src/main/java/cn/iocoder/yudao/module/system/controller/admin/auth/vo/AuthRegisterReqVO.java #	yudao-module-system/src/main/java/cn/iocoder/yudao/module/system/service/user/AdminUserServiceImpl.java
YunaiV,ruoyi-vue-pro,ce1940e4f36fe9a0bfc9dcc7911c4f695d4ae7af,https://github.com/YunaiV/ruoyi-vue-pro/commit/ce1940e4f36fe9a0bfc9dcc7911c4f695d4ae7af,Merge branch 'master-jdk17' of https://gitee.com/zhijiantianya/ruoyi-vue-pro  # Conflicts: #	yudao-module-crm/yudao-module-crm-biz/src/main/java/cn/iocoder/yudao/module/crm/service/statistics/CrmStatisticsPerformanceServiceImpl.java #	yudao-module-mall/yudao-module-trade-biz/src/main/java/cn/iocoder/yudao/module/trade/controller/app/brokerage/AppBrokerageUserController.java #	yudao-module-mall/yudao-module-trade-biz/src/main/java/cn/iocoder/yudao/module/trade/service/brokerage/BrokerageUserService.java #	yudao-module-mall/yudao-module-trade-biz/src/main/java/cn/iocoder/yudao/module/trade/service/brokerage/BrokerageUserServiceImpl.java #	yudao-module-member/yudao-module-member-biz/src/main/java/cn/iocoder/yudao/module/member/controller/app/social/AppSocialUserController.java #	yudao-module-system/yudao-module-system-api/src/main/java/cn/iocoder/yudao/module/system/api/social/SocialUserApi.java #	yudao-module-system/yudao-module-system-biz/src/main/java/cn/iocoder/yudao/module/system/service/social/SocialClientService.java
apache,kafka,adb76779ed14ef21f61f8dafe9041054e9a32130,https://github.com/apache/kafka/commit/adb76779ed14ef21f61f8dafe9041054e9a32130,KAFKA-19312 Avoiding concurrent execution of onComplete and tryComplete (#19759)  The `onComplete` method in DelayedOperation is guaranteed to run only once  through `forceComplete`  invoked either by `tryComplete` or when operation is expired (`run` method). The invocation of  `tryComplete` is done by attaining `lock` so no concurrent execution of  `tryComplete` happens for same delayed operation. However  there can be  concurrent execution of `tryComplete` and `onComplete` as the  `expiration` thread can trigger a separte run of `onComplete` while  `tryComplete` is still executing. This behaviour is not ideal as there  are parallel runs where 1 threads method execution is wasteful i.e. if  `onComplete` is already invoked by another thread then execution of  `tryComplete` is not required.  I ran some tests and performance is same.  ### After the chages:  ``` --num 10000 --rate 100 --timeout 1000 --pct50 0.5 --pct75 0.75  # latency samples: pct75 = 0  pct50 = 0  min = 0  max = 7 # interval samples: rate = 100.068948  min = 0  max = 129 # enqueue rate (10000 requests): # <elapsed time ms>	<target rate>	<actual rate>	<process cpu time ms>	<G1 Old Generation count> <G1 Young Generation count>	<G1 Old Generation time ms> <G1 Young Generation time ms> 101196	99.809364	99.806376	3240	0 2	0 8 ```  ``` --num 10000 --rate 1000 --timeout 1000 --pct50 0.5 --pct75 0.75  # latency samples: pct75 = 0  pct50 = 0  min = 0  max = 9 # interval samples: rate = 999.371395  min = 0  max = 14 # enqueue rate (10000 requests): # <elapsed time ms>	<target rate>	<actual rate>	<process cpu time ms>	<G1 Old Generation count> <G1 Young Generation count>	<G1 Old Generation time ms> <G1 Young Generation time ms> 11104	989.902990	989.805008	1349	0 2	0 7 ```  ### Before changes:  ``` --num 10000 --rate 100 --timeout 1000 --pct50 0.5 --pct75 0.75  # latency samples: pct75 = 0  pct50 = 0  min = 0  max = 9 # interval samples: rate = 100.020304  min = 0  max = 130 # enqueue rate (10000 requests): # <elapsed time ms>	<target rate>	<actual rate>	<process cpu time ms>	<G1 Old Generation count> <G1 Young Generation count>	<G1 Old Generation time ms> <G1 Young Generation time ms> 102366	98.657274	98.652408	3444	0 2	0 8  --num 10000 --rate 1000 --timeout 1000 --pct50 0.5 --pct75 0.75  # latency samples: pct75 = 0  pct50 = 0  min = 0  max = 8 # interval samples: rate = 997.134236  min = 0  max = 14 # enqueue rate (10000 requests): # <elapsed time ms>	<target rate>	<actual rate>	<process cpu time ms>	<G1 Old Generation count> <G1 Young Generation count>	<G1 Old Generation time ms> <G1 Young Generation time ms> 11218	978.665101	978.665101	1624	0 2	0 7  Reviewers: Jun Rao <junrao@gmail.com>  Andrew Schofield <aschofield@confluent.io>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,bff1602df3af11873e7e8cb3d0dfd2a53d7b0021,https://github.com/apache/kafka/commit/bff1602df3af11873e7e8cb3d0dfd2a53d7b0021,KAFKA-19280: Fix NoSuchElementException in UnifiedLog (#19717)  In FETCH requests and TXN_OFFSET_COMMIT requests  on current trunk we run into a race condition inside UnifiedLog  causing a `NoSuchElementException` in `UnifiedLog.fetchLastStableOffsetMetadata(UnifiedLog.java:651)`.  The cause is that the line a performing an `isPresent` check on a volatile Optional before accessing it in `get`  leaving the door open to a race condition when the optional changes between `isPresent` and `get`. This change takes a copy of the volatile variable first.
apache,kafka,eb3714f022c948e9348b7e281c4bd69a38666013,https://github.com/apache/kafka/commit/eb3714f022c948e9348b7e281c4bd69a38666013,KAFKA-19160;KAFKA-19164; Improve performance of fetching stable offsets (#19497)  When fetching stable offsets in the group coordinator  we iterate over all requested partitions. For each partition  we iterate over the group's ongoing transactions to check if there is a pending transactional offset commit for that partition.  This can get slow when there are a large number of partitions and a large number of pending transactions. Instead  maintain a list of pending transactions per partition to speed up lookups.  Reviewers: Shaan  Dongnuo Lyu <dlyu@confluent.io>  Chia-Ping Tsai <chia7712@gmail.com>  David Jaco <djacot@confluent.io>
apache,kafka,b5c468fd7ced52184daa286f00ea7ce8cf760bc0,https://github.com/apache/kafka/commit/b5c468fd7ced52184daa286f00ea7ce8cf760bc0,KAFKA-18115; Fix for loading big files while performing load tests (#18391)  When performing perf tests  we can specify a payload using the "--payloadFile" flag. This file is utilized during the load/performance testing process. This causes the entire file to get loaded into a String and split using the delimiter. However  if the file is large  it may result in  NegativeArraySizeException error.  Moving the file loading logic to Scanner which doesn't have this issue.  Reviewers: José Armando García Sancio <jsancio@apache.org>  Ken Huang <s7133700@gmail.com>  Zhe Guang <zheguang.zhao@alumni.brown.edu>
apache,kafka,ac9520b92201839796ce46cb5852fff08b7d5878,https://github.com/apache/kafka/commit/ac9520b92201839796ce46cb5852fff08b7d5878,KAFKA-19227: Piggybacked share fetch acknowledgements performance issue (#19612)  The PR fixes the issue when ShareAcknowledgements are piggybacked on ShareFetch. The current default configuration in clients sets `batch size` and `max fetch records` as per the `max.poll.records` config  default 500. Which means all records in a single poll will be fetched and acknowledged. Also the default configuration for inflight records in a partition is 200. Which means prior fetch records has to be acknowledged prior fetching another batch from share partition.  The piggybacked share fetch-acknowledgement calls from KafkaApis are async and later the response is combined. If respective share fetch starts waiting in purgatory because all inflight records are currently full  hence when startOffset is moved as part of acknowledgement  then a trigger should happen which should try completing any pending share fetch requests in purgatory. Else the share fetch requests wait in purgatory for timeout though records are available  which dips the share fetch performance.  The regular fetch has a single criteria to land requests in purgatory  which is min bytes criteria  hence any produce in respective topic partition triggers to check any pending fetch requests. But share fetch can wait in purgatory because of multiple reasons: 1) Min bytes 2) Inflight records exhaustion 3) Share partition fetch lock competition. The trigger already happens for 1 and current PR fixes 2. We will investigate further if there should be any handling required for 3.  Reviewers: Abhinav Dixit <adixit@confluent.io>  Andrew Schofield <aschofield@confluent.io>
apache,kafka,81c3a285a4536aabe71fb8b3f359037309fa8f8d,https://github.com/apache/kafka/commit/81c3a285a4536aabe71fb8b3f359037309fa8f8d,KAFKA-19133: Support fetching for multiple remote fetch topic partitions in a single share fetch request (#19592)  ### About This PR removes the limitation in remote storage fetch for share groups of only performing remote fetch for a single topic partition in a share fetch request. With this PR  share groups can now fetch multiple remote storage topic partitions in a single share fetch request.  ### Testing I have followed the [AK  documentation](https://kafka.apache.org/documentation/#tiered_storage_config_ex) to test my code locally (by adopting `LocalTieredStorage.java`) and verify with the help of logs that remote storage is happening for multiple topic partitions in a single share fetch request. Also  verified it with the help of unit tests.  Reviewers: Jun Rao <junrao@gmail.com>  Apoorv Mittal <apoorvmittal10@gmail.com>
apache,kafka,a8f49999ccf6a33b986bf895884f1a5e9708fb47,https://github.com/apache/kafka/commit/a8f49999ccf6a33b986bf895884f1a5e9708fb47,KAFKA-19019: Add support for remote storage fetch for share groups (#19437)  This PR adds the support for remote storage fetch for share groups.  There is a limitation in remote storage fetch for consumer groups that we can only perform remote fetch for a single topic partition in a fetch request. Since  the logic of share fetch requests is largely based on how consumer groups work  we are following similar logic in implementing remote storage fetch. However  this problem should be addressed as part of KAFKA-19133 which should help us perform fetch for multiple remote fetch topic partition in a single share fetch request.  Reviewers: Jun Rao <junrao@gmail.com>
apache,kafka,2cd733c9b380d17326b1d929097b2716995d6659,https://github.com/apache/kafka/commit/2cd733c9b380d17326b1d929097b2716995d6659,KAFKA-17184: Fix the error thrown while accessing the RemoteIndexCache (#19462)  For segments that are uploaded to remote  RemoteIndexCache caches the fetched offset  timestamp  and transaction index entries on the first invocation to remote  then the subsequent invocations are accessed from local.  The remote indexes that are cached locally gets removed on two cases:  1. Remote segments that are deleted due to breach by retention size/time and start-offset. 2. The number of cached indexes exceed the remote-log-index-cache size limit of 1 GB (default).  There are two layers of locks used in the RemoteIndexCache. First-layer lock on the RemoteIndexCache and the second-layer lock on the RemoteIndexCache#Entry.  **Issue**  1. The first-layer of lock coordinates the remote-log reader and deleter threads. To ensure that the reader and deleter threads are not blocked on each other  we only take `lock.readLock()` when accessing/deleting the cached index entries. 2. The issue happens when both the reader and deleter threads took the readLock  then the deleter thread marked the index as `markedForCleanup`. Now  the reader thread which holds the `indexEntry` gets an IllegalStateException when accessing it. 3. This is a concurrency issue  where we mark the entry as `markedForCleanup` before removing it from the cache. See RemoteIndexCache#remove  and RemoteIndexCache#removeAll methods. 4. When an entry gets evicted from cache due to breach by maxSize of 1 GB  then the cache remove that entry before calling the evictionListener and all the operations are performed atomically by caffeine cache.  **Solution**  1. When the deleter thread marks an Entry for deletion  then we rename the underlying index files with ".deleted" as suffix and add a job to the remote-log-index-cleaner thread which perform the actual cleanup. Previously  the indexes were not accessible once it was marked for deletion. Now  we allow to access those renamed files (from entry that is about to be removed and held by reader thread) until those relevant files are removed from disk. 2. Similar to local-log index/segment deletion  once the files gets renamed with ".deleted" as suffix then the actual deletion of file happens after `file.delete.delay.ms` delay of 1 minute. The renamed index files gets deleted after 30 seconds. 3. During this time  if the same index entry gets fetched again from remote  then it does not have conflict with the deleted entry as the file names are different.  Reviewers: Satish Duggana <satishd@apache.org>
apache,kafka,434b0d39ae675eb5ac16de81da4a102b32a496a5,https://github.com/apache/kafka/commit/434b0d39ae675eb5ac16de81da4a102b32a496a5,MINOR: use enum map for error counts map (#19314)  Java provides a specialised Map where Enums are the keys  which can provide some performance improvements.  https://docs.oracle.com/javase/8/docs/api/java/util/EnumMap.html  I have updated the Java code where possible to use an EnumMap rather than a HashMap and run the unit tests under the requests directory.  Reviewers: Matthias J. Sax <matthias@confluent.io>  Lianet Magrans <lmagrans@confluent.io>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,a65626b6a806fca960377f9fd5d5383cdcb72946,https://github.com/apache/kafka/commit/a65626b6a806fca960377f9fd5d5383cdcb72946,MINOR: Add functionalinterface to the producer callback  (#19366)  The Callback interface is a perfect example of a place that can use the functionalinterface in Java. Strictly for Java  this isn't "required" since Java will automatically coerce  but for Clojure (and other JVM languages I belive) to interop with Java lambdas it needs the FunctionalInterface annotation.  Since FunctionalInterface doesn't add any overhead and provides compiler-enforced documentation  I don't see any reason *not* to have this. This has already been added into Kafka Streams here: https://github.com/apache/kafka/pull/19234#pullrequestreview-2740742487  I am happy to add it to any other spots in that might be useful too.  Reviewers: Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,d4d9f118165ea22494cd6128050ed733e33d5871,https://github.com/apache/kafka/commit/d4d9f118165ea22494cd6128050ed733e33d5871,KAFKA-18761: [2/N] List share group offsets with state and auth (#19328)  This PR approaches completion of Admin.listShareGroupOffsets() and kafka-share-groups.sh --describe --offsets.  Prior to this patch  kafka-share-groups.sh was only able to describe the offsets for partitions which were assigned to active members. Now  the Admin.listShareGroupOffsets() uses the persister's knowledge of the share-partitions which have initialised state. Then  it uses this list to obtain a complete set of offset information.  The PR also implements the topic-based authorisation checking. If Admin.listShareGroupOffsets() is called with a list of topic-partitions specified  the authz checking is performed on the supplied list  returning errors for any topics to which the client is not authorised. If Admin.listShareGroupOffsets() is called without a list of topic-partitions specified  the list of topics is discovered from the persister as described above  and then the response is filtered down to only show the topics to which the client is authorised. This is consistent with other similar RPCs in the Kafka protocol  such as OffsetFetch.  Reviewers: David Arthur <mumrah@gmail.com>  Sushant Mahajan <smahajan@confluent.io>  Apoorv Mittal <apoorvmittal10@gmail.com>
jeqo,kafka,2723dbf3a007af0a1cafc17ed019aabd46eb6881,https://github.com/jeqo/kafka/commit/2723dbf3a007af0a1cafc17ed019aabd46eb6881,KAFKA-15931: Cancel RemoteLogReader gracefully (#19197)  Reverts commit
jeqo,kafka,269e8892ad26904c82a3bff7b7d7a81c47619e4a.,https://github.com/jeqo/kafka/commit/269e8892ad26904c82a3bff7b7d7a81c47619e4a.,
apache,kafka,f24945b519005c0bc7a28db2db7aae6cec158927,https://github.com/apache/kafka/commit/f24945b519005c0bc7a28db2db7aae6cec158927,
apache,kafka,31e1a57c41cf9cb600751669dc71bcd9596b45f9,https://github.com/apache/kafka/commit/31e1a57c41cf9cb600751669dc71bcd9596b45f9,KAFKA-18989 Optimize FileRecord#searchForOffsetWithSize (#19214)  The `lastOffset` includes the entire batch header  so we should check `baseOffset` instead.  To optimize this  we need to update the search logic. The previous approach simply checked whether each batch's `lastOffset()` was greater than or equal to the target offset. Once it found the first batch that met this condition  it returned that batch immediately.  Now that we are using `baseOffset()`  we need to handle a special case: if the `targetOffset` falls between the `lastOffset` of the previous batch and the `baseOffset` of the matching batch  we should select the matching batch. The updated logic is structured as follows:  1. First  if baseOffset exactly equals targetOffset  return immediately. 2. If we find the first batch with baseOffset greater than targetOffset - Check if the previous batch contains the target - If there's no previous batch  return the current batch or the previous batch doesn't contain the target  return the current batch 5. After iterating through all batches  check if the last batch contains the target offset.  This code path is not thread-safe  so we need to prevent `EOFException`. To avoid this exception  I am still using an early return. In this scenario  `lastOffset` is still used within the loop  but it should be executed at most once within the loop.  Therefore  in the new implementation  `lastOffset` will be executed at most once. In most cases  this results in an optimization.  Test: Verifying Memory Usage Improvement To evaluate whether this optimization helps  I followed the steps below to monitor memory usage:  1. Start a Standalone Kafka Server ```sh KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)" bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties bin/kafka-server-start.sh config/server.properties ```  2. Use Performance Console Tools to Produce and Consume Records  **Produce Records:** ```sh ./kafka-producer-perf-test.sh \ --topic test-topic \ --num-records 1000000000 \ --record-size 100 \ --throughput -1 \ --producer-props bootstrap.servers=localhost:9092 ``` **Consume Records:** ```sh ./bin/kafka-consumer-perf-test.sh \ --topic test-topic \ --messages 1000000000 \ --bootstrap-server localhost:9092 ``` It can be observed that memory usage has significantly decreased. trunk: ![CleanShot 2025-03-16 at 11 53 31@2x](https://github.com/user-attachments/assets/eec26b1d-38ed-41c8-8c49-e5c68643761b) this PR: ![CleanShot 2025-03-16 at 17 41 56@2x](https://github.com/user-attachments/assets/c8d4c234-18c2-4642-88ae-9f96cf54fccc)  Reviewers: Kirk True <kirk@kirktrue.pro>  TengYao Chi <kitingiao@gmail.com>  David Arthur <mumrah@gmail.com>  Jun Rao <junrao@gmail.com>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,da46cf6e79afbbed1da2bae831e0f70992e85f9b,https://github.com/apache/kafka/commit/da46cf6e79afbbed1da2bae831e0f70992e85f9b,KAFKA-17565 Move MetadataCache interface to metadata module (#18801)  ### Changes  * Move MetadataCache interface to metadata module and change Scala function to Java. * Remove functions `getTopicPartitions`  `getAliveBrokers`  `topicNamesToIds`  `topicIdInfo`  and `getClusterMetadata` from MetadataCache interface  because these functions are only used in test code.  ### Performance  * ReplicaFetcherThreadBenchmark ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.fetcher.ReplicaFetcherThreadBenchmark ``` * trunk ``` Benchmark (partitionCount) Mode Cnt Score Error Units ReplicaFetcherThreadBenchmark.testFetcher 100 avgt 2 4775.490 ns/op ReplicaFetcherThreadBenchmark.testFetcher 500 avgt 2 25730.790 ns/op ReplicaFetcherThreadBenchmark.testFetcher 1000 avgt 2 55334.206 ns/op ReplicaFetcherThreadBenchmark.testFetcher 5000 avgt 2 488427.547 ns/op ``` * branch ``` Benchmark (partitionCount) Mode Cnt Score Error Units ReplicaFetcherThreadBenchmark.testFetcher 100 avgt 2 4825.219 ns/op ReplicaFetcherThreadBenchmark.testFetcher 500 avgt 2 25985.662 ns/op ReplicaFetcherThreadBenchmark.testFetcher 1000 avgt 2 56056.005 ns/op ReplicaFetcherThreadBenchmark.testFetcher 5000 avgt 2 497138.573 ns/op ```  * KRaftMetadataRequestBenchmark ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.metadata.KRaftMetadataRequestBenchmark ``` * trunk ``` Benchmark (partitionCount) (topicCount) Mode Cnt Score Error Units KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 500 avgt 2 884933.558 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 1000 avgt 2 1910054.621 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 5000 avgt 2 21778869.337 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 500 avgt 2 1537550.670 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 1000 avgt 2 3168237.805 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 5000 avgt 2 29699652.466 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 500 avgt 2 3501483.852 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 1000 avgt 2 7405481.182 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 5000 avgt 2 55839670.124 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 500 avgt 2 333.667 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 1000 avgt 2 339.685 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 5000 avgt 2 334.293 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 500 avgt 2 329.899 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 1000 avgt 2 347.537 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 5000 avgt 2 332.781 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 500 avgt 2 327.085 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 1000 avgt 2 325.206 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 5000 avgt 2 316.758 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 500 avgt 2 7.569 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 1000 avgt 2 7.565 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 5000 avgt 2 7.574 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 500 avgt 2 7.568 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 1000 avgt 2 7.557 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 5000 avgt 2 7.585 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 500 avgt 2 7.560 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 1000 avgt 2 7.554 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 5000 avgt 2 7.574 ns/op ``` * branch ``` Benchmark (partitionCount) (topicCount) Mode Cnt Score Error Units KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 500 avgt 2 910337.770 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 1000 avgt 2 1902351.360 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 10 5000 avgt 2 22215893.338 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 500 avgt 2 1572683.875 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 1000 avgt 2 3188560.081 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 20 5000 avgt 2 29984751.632 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 500 avgt 2 3413567.549 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 1000 avgt 2 7303174.254 ns/op KRaftMetadataRequestBenchmark.testMetadataRequestForAllTopics 50 5000 avgt 2 54293721.640 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 500 avgt 2 318.335 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 1000 avgt 2 331.386 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 10 5000 avgt 2 332.944 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 500 avgt 2 340.322 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 1000 avgt 2 330.294 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 20 5000 avgt 2 342.154 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 500 avgt 2 341.053 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 1000 avgt 2 335.458 ns/op KRaftMetadataRequestBenchmark.testRequestToJson 50 5000 avgt 2 322.050 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 500 avgt 2 7.538 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 1000 avgt 2 7.548 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 10 5000 avgt 2 7.545 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 500 avgt 2 7.597 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 1000 avgt 2 7.567 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 20 5000 avgt 2 7.558 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 500 avgt 2 7.559 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 1000 avgt 2 7.615 ns/op KRaftMetadataRequestBenchmark.testTopicIdInfo 50 5000 avgt 2 7.562 ns/op ```  * PartitionMakeFollowerBenchmark ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.partition.PartitionMakeFollowerBenchmark ``` * trunk ``` Benchmark Mode Cnt Score Error Units PartitionMakeFollowerBenchmark.testMakeFollower avgt 2 158.816 ns/op ``` * branch ``` Benchmark Mode Cnt Score Error Units PartitionMakeFollowerBenchmark.testMakeFollower avgt 2 160.533 ns/op ```  * UpdateFollowerFetchStateBenchmark ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.partition.UpdateFollowerFetchStateBenchmark ``` * trunk ``` Benchmark Mode Cnt Score Error Units UpdateFollowerFetchStateBenchmark.updateFollowerFetchStateBench avgt 2 4975.261 ns/op UpdateFollowerFetchStateBenchmark.updateFollowerFetchStateBenchNoChange avgt 2 4880.880 ns/op ``` * branch ``` Benchmark Mode Cnt Score Error Units UpdateFollowerFetchStateBenchmark.updateFollowerFetchStateBench avgt 2 5020.722 ns/op UpdateFollowerFetchStateBenchmark.updateFollowerFetchStateBenchNoChange avgt 2 4878.855 ns/op ```   * CheckpointBench ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.server.CheckpointBench ``` * trunk ``` Benchmark (numPartitions) (numTopics) Mode Cnt Score Error Units CheckpointBench.measureCheckpointHighWatermarks 3 100 thrpt 2 0.997 ops/ms CheckpointBench.measureCheckpointHighWatermarks 3 1000 thrpt 2 0.703 ops/ms CheckpointBench.measureCheckpointHighWatermarks 3 2000 thrpt 2 0.486 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 100 thrpt 2 1.038 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 1000 thrpt 2 0.734 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 2000 thrpt 2 0.637 ops/ms ``` * branch ``` Benchmark (numPartitions) (numTopics) Mode Cnt Score Error Units CheckpointBench.measureCheckpointHighWatermarks 3 100 thrpt 2 0.990 ops/ms CheckpointBench.measureCheckpointHighWatermarks 3 1000 thrpt 2 0.659 ops/ms CheckpointBench.measureCheckpointHighWatermarks 3 2000 thrpt 2 0.508 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 100 thrpt 2 0.923 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 1000 thrpt 2 0.736 ops/ms CheckpointBench.measureCheckpointLogStartOffsets 3 2000 thrpt 2 0.637 ops/ms ```  * PartitionCreationBench ``` ./jmh-benchmarks/jmh.sh -f 1 -i 2 -wi 2 org.apache.kafka.jmh.server.PartitionCreationBench ``` * trunk ``` Benchmark (numPartitions) (useTopicIds) Mode Cnt Score Error Units PartitionCreationBench.makeFollower 20 false avgt 2 5.997 ms/op PartitionCreationBench.makeFollower 20 true avgt 2 6.961 ms/op ``` * branch ``` Benchmark (numPartitions) (useTopicIds) Mode Cnt Score Error Units PartitionCreationBench.makeFollower 20 false avgt 2 6.212 ms/op PartitionCreationBench.makeFollower 20 true avgt 2 7.005 ms/op ```  Reviewers: Ismael Juma <ismael@juma.me.uk>  David Arthur <mumrah@gmail.com>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,485699a1877561aa777686cc7ebd6feb83ceded2,https://github.com/apache/kafka/commit/485699a1877561aa777686cc7ebd6feb83ceded2,MINOR: Delete DeleteGroupsResult class. (#19057)  In this PR  we perform this refactor as the class is not needed since there is no need to refer to child classes by common ref and the duplicated code is minimal.  Reviewers: Andrew Schofield <aschofield@confluent.io>
apache,kafka,f20f2994928fbae53541a2b821515427ee2679a2,https://github.com/apache/kafka/commit/f20f2994928fbae53541a2b821515427ee2679a2,KAFKA-18839: Drop EAGER rebalancing support in Kafka Streams (#18988)  In 3.1 we deprecated the eager rebalancing protocol and marked it for removal in a later release. We aim to officially drop support and remove the protocol from Streams in 4.0.  The effect of this PR is that it will no longer be possible to perform a live upgrade Kafka Streams directly to 4.0 from version 2.3 or below. Users will have to go through a bridge release between 2.4 - 3.9 instead.  Reviewers: Matthias J. Sax <matthias@confluent.io>
apache,kafka,102de21355e465771ddd0ba1464cac389abb0e01,https://github.com/apache/kafka/commit/102de21355e465771ddd0ba1464cac389abb0e01,KAFKA-17379: Fix inexpected state transition from ERROR to PENDING_SHUTDOWN (#18765)  The exception stack trace shown in the the ticket can happen when we are concurrently closing the producer because of an error and doing a regular close. This is not a bug in the test  but a real race condition that can happen.  The sequence is this:  Thread1: Enter PENDING_ERROR Thread2: Check if state is already ERROR Thread1: Transition to ERROR Thread2: Check if state is already PENDING_ERROR Thread2: Transition to PENDING_SHUTDOWN  One idea to fix this would be to synchronize the sequence performed by Thread1 using the state lock. However  this would require more changes  since we cannot use the normal state transition method `setState` while owning the lock  as it calls user-defined callbacks  which may create deadlocks. Do avoid adding more synchronization  we can also fix it by first attempting to transition to PENDING_SHUTDOWN  and _then_ checking whether another thread is already attempting to shut down (states PENDING_SHUTDOWN  PENDING_ERROR  ERROR  NOT_RUNNING). Since we never transition from a shutdown state back to a non-shutdown state.  Reviewers: Matthias J. Sax <matthias@confluent.io>
apache,kafka,ab8ef87c7f920894566c25234e9ed2d8f1f9cce2,https://github.com/apache/kafka/commit/ab8ef87c7f920894566c25234e9ed2d8f1f9cce2,KAFKA-18654 [1/2]: Transaction Version 2 performance regression due to early return (#18720)  https://issues.apache.org/jira/browse/KAFKA-18575 solved a critical race condition by returning with CONCURRENT_TRANSACTIONS early when the transaction was still completing. In testing  it was discovered that this early return could cause performance regressions.  Prior to KIP-890 the addpartitions call was a separate call from the producer. There was a previous change https://issues.apache.org/jira/browse/KAFKA-5477 that decreased the retry backoff to 20ms. With KIP-890 and making the call through the produce path  we go back to the default retry backoff which takes longer. Prior to 18575 we introduce a slight delay when sending to the coordinator  so prior to 18575  we are less likely to return quickly and get stuck in this backoff. However  based on results from produce benchmarks  we can still run into the default backoff in some scenarios.  This PR reverts KAFKA-18575  and doesn't return early and wait until the coordinator for checking if a transaction is ongoing. Instead  it will fix the handling with the verification guard so we don't hit the edge condition.  Also cleans up some of the verification text that was unclear.  Reviewers: Jeff Kim <jeff.kim@confluent.io>  Artem Livshits <alivshits@confluent.io>
apache,kafka,6cf54c4dab9ff39f21d7f2c9bd241b938b5651b2,https://github.com/apache/kafka/commit/6cf54c4dab9ff39f21d7f2c9bd241b938b5651b2,KAFKA-17182: Consumer fetch sessions are evicted too quickly with AsyncKafkaConsumer (#17700)  This change reduces fetch session cache evictions on the broker for AsyncKafkaConsumer by altering its logic to determine which partitions it includes in fetch requests.  Background Consumer implementations fetch data from the cluster and temporarily buffer it in memory until the user next calls Consumer.poll(). When a fetch request is being generated  partitions that already have buffered data are not included in the fetch request.  The ClassicKafkaConsumer performs much of its fetch logic and network I/O in the application thread. On poll()  if there is any locally-buffered data  the ClassicKafkaConsumer does not fetch any new data and simply returns the buffered data to the user from poll().  On the other hand  the AsyncKafkaConsumer consumer splits its logic and network I/O between two threads  which results in a potential race condition during fetch. The AsyncKafkaConsumer also checks for buffered data on its application thread. If it finds there is none  it signals the background thread to create a fetch request. However  it's possible for the background thread to receive data from a previous fetch and buffer it before the fetch request logic starts. When that occurs  as the background thread creates a new fetch request  it skips any buffered data  which has the unintended result that those partitions get added to the fetch request's "to remove" set. This signals to the broker to remove those partitions from its internal cache.  This issue is technically possible in the ClassicKafkaConsumer too  since the heartbeat thread performs network I/O in addition to the application thread. However  because of the frequency at which the AsyncKafkaConsumer's background thread runs  it is ~100x more likely to happen.  Options The core decision is: what should the background thread do if it is asked to create a fetch request and it discovers there's buffered data. There were multiple proposals to address this issue in the AsyncKafkaConsumer. Among them are:  The background thread should omit buffered partitions from the fetch request as before (this is the existing behavior) The background thread should skip the fetch request generation entirely if there are any buffered partitions The background thread should include buffered partitions in the fetch request  but use a small “max bytes” value The background thread should skip fetching from the nodes that have buffered partitions Option 4 won out. The change is localized to AbstractFetch where the basic idea is to skip fetch requests to a given node if that node is the leader for buffered data. By preventing a fetch request from being sent to that node  it won't have any "holes" where the buffered partitions should be.  Reviewers: Lianet Magrans <lmagrans@confluent.io>  Jeff Kim <jeff.kim@confluent.io>  Jun Rao <junrao@gmail.com>
apache,kafka,b51b31ed9c9cede1ae0ffdf8ca4b471925f72a64,https://github.com/apache/kafka/commit/b51b31ed9c9cede1ae0ffdf8ca4b471925f72a64,KAFKA-18428: Measure share consumers performance (#18415)  Reviewers: Andrew Schofield <aschofield@confluent.io>
apache,kafka,c4840f5e933baaf9c45867940d2f17cc378988f4,https://github.com/apache/kafka/commit/c4840f5e933baaf9c45867940d2f17cc378988f4,KAFKA-16446: Improve controller event duration logging (#15622)  There are times when the controller has a high event processing time  such as during startup  or when creating a topic with many partitions. We can see these processing times in the p99 metric (kafka.controller:type=ControllerEventManager name=EventQueueProcessingTimeMs)  however it's difficult to see exactly which event is causing high processing time.  With DEBUG logs  we see every event along with its processing time. Even with this  it's a bit tedious to find the event with a high processing time.  This PR logs all events which take longer than 2 seconds at ERROR level. This will help identify events that are taking far too long  and which could be disruptive to the operation of the controller. The slow event logging looks like this:  ``` [2024-12-20 15:03:39 754] ERROR [QuorumController id=1] Exceptionally slow controller event createTopics took 5240 ms.  (org.apache.kafka.controller.EventPerformanceMonitor) ```  Also  every 60 seconds  it logs some event time statistics  including average time  maximum time  and the name of the event which took the longest. This periodic message looks like this:  ``` [2024-12-20 15:35:04 798] INFO [QuorumController id=1] In the last 60000 ms period  333 events were completed  which took an average of 12.34 ms each. The slowest event was handleCommit[baseOffset=0]  which took 41.90 ms. (org.apache.kafka.controller.EventPerformanceMonitor) ```  An operator can disable these logs by adding the following to their log4j config:  ``` org.apache.kafka.controller.EventPerformanceMonitor=OFF ```  Reviewers: Colin P. McCabe <cmccabe@apache.org>
apache,kafka,a0291a8d50d116c536333007185f185281b1bd6c,https://github.com/apache/kafka/commit/a0291a8d50d116c536333007185f185281b1bd6c,MINOR: Fix flaky state updater test (#18253)  The tests are flaky because the tests end before the verified calls are executed. This happens because the state updater thread executes the verified calls  but the thread that executes the tests with the verifications is a different thread.  This commit fixes the flaky tests by enusring that the calls were performed by the state updater by either shutting down the state updater or waiting for the condition.  Reviewers: Lucas Brutschy <lbrutschy@confluent.io>  Matthias J. Sax <matthias@confluent.io>
apache,kafka,671cbedc1bd2050b859fe983374e8414c4a3497a,https://github.com/apache/kafka/commit/671cbedc1bd2050b859fe983374e8414c4a3497a,KAFKA-18219 Use INFO level instead of ERROR after successfully performing an unclean leader election (#18159)  Reviewers: Kuan-Po Tseng <brandboat@gmail.com>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,1f26b9607ed58400dffaddb16181bff66a7ffed5,https://github.com/apache/kafka/commit/1f26b9607ed58400dffaddb16181bff66a7ffed5,MINOR: Perf improvement in share state batch combiner. (#18090)  Change from ArrayList to LinkedList following performance analysis  Reviewers: Andrew Schofield <aschofield@confluent.io>
apache,kafka,0ff55c316aab9b54adcb1cfc4d3d3dbedc279270,https://github.com/apache/kafka/commit/0ff55c316aab9b54adcb1cfc4d3d3dbedc279270,KAFKA-18106: Generate LeaderAndIsrUpdates on unclean shutdown (#18045)  Generate LeaderAndISR change records when a broker re-registers and the quorum controller detects an unclean shutdown.  This is necessary to ensure that we perform the expected partition state transitions  eg: bumping leader epochs and so on.  Reviewers: Colin P. McCabe <cmccabe@apache.org>
apache,kafka,a592912ec94b021a2bd7485abb9ebda02dc01766,https://github.com/apache/kafka/commit/a592912ec94b021a2bd7485abb9ebda02dc01766,KAFKA-17663 Add metadata caching in PartitionLeaderStrategy (#17367)  Admin API operations have two phases: lookup and fulfilment. The lookup phase involves a METADATA request whose details depend upon the operation being performed.  For some operations  the METADATA request can be quite expensive to serve. For example  if the user calls Admin.listOffsets for 1000 topics  the METADATA request will include all 1000 topics and the response will contain the leader information for all of these topics. And then the actual fulfilment phase does the real work of the operation.  In cases where a long-running application is performing repeated admin operations which need the same metadata information about partition leadership  it is not necessary to send the METADATA request for every single admin operation.  This PR adds a cache of the mapping from topic-partition to leader id to the admin client. The cache doesn't need to be very sophisticated because the admin client will retry if the information becomes stale  and the cache can be updated as a result of the retry.  Reviewers: Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,571f50817c0c3e81a8f767396e485bc23a0731ba,https://github.com/apache/kafka/commit/571f50817c0c3e81a8f767396e485bc23a0731ba,KAFKA-17411: Create local state Standbys on start (#16922)  Instead of waiting until Tasks are assigned to us  we pre-emptively create a StandbyTask for each non-empty Task directory found on-disk.  We do this before starting any StreamThreads  and on our first assignment (after joining the consumer group)  we recycle any of these StandbyTasks that were assigned to us  either as an Active or a Standby.  We can't just use these "initial Standbys" as-is  because they were constructed outside the context of a StreamThread  so we first have to update them with the context (log context  ChangelogReader  and source topics) of the thread that it has been assigned to.  The motivation for this is to (in a later commit) read StateStore offsets for unowned Tasks from the StateStore itself  rather than the .checkpoint file  which we plan to deprecate and remove.  There are a few additional benefits:  Initializing these Tasks on start-up  instead of on-assignment  will reduce the time between a member joining the consumer group and beginning processing. This is especially important when active tasks are being moved over  for example  as part of a rolling restart.  If a Task has corrupt data on-disk  it will be discovered on startup and wiped under EOS. This is preferable to wiping the state after being assigned the Task  because another instance may have non-corrupt data and would not need to restore (as much).  There is a potential performance impact: we open all on-disk Task StateStores  and keep them all open until we have our first assignment. This could require large amounts of memory  in particular when there are a large number of local state stores on-disk.  However  since old local state for Tasks we don't own is automatically cleaned up after a period of time  in practice  we will almost always only be dealing with the state that was last assigned to the local instance.  Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>  Bruno Cadonna <cadonna@apache.org>  Matthias Sax <mjsax@apache.org>
apache,kafka,33147d1089397ea20ba46bd0afb83174b45b506c,https://github.com/apache/kafka/commit/33147d1089397ea20ba46bd0afb83174b45b506c,KAFKA-17863: share consumer max poll records soft limit (#17592)  Make max.poll.records a soft limit so that record batch boundaries can be respected in records returned by ShareConsumer.poll. This gives a significant performance gain because the broker is much more efficient at handling batches which have not been split.  Reviewers: Apoorv Mittal <apoorvmittal10@gmail.com>   Manikumar Reddy <manikumar.reddy@gmail.com>
apache,kafka,cb3b03377d530cb4815d2fbfd5bf3d9436822ece,https://github.com/apache/kafka/commit/cb3b03377d530cb4815d2fbfd5bf3d9436822ece,KAFKA-17742: Move DelayedShareFetchPurgatory declaration to ReplicaManager (#17437)  Declare the delayed share fetch purgatory inside ReplicaManager along with the existing purgatories.  Check the share fetch purgatory when a replica becomes the follower or a replica is deleted from a broker through ReplicaManager.  Perform a checkAndComplete for share fetch when HWM is updated.  Reviewers:  Andrew Schofield <aschofield@confluent.io>   Apoorv Mittal <apoorvmittal10@gmail.com>  Jun Rao <junrao@gmail.com>
apache,kafka,017da210999c72789ce3b720af04c1a834b80a5c,https://github.com/apache/kafka/commit/017da210999c72789ce3b720af04c1a834b80a5c,KAFKA-17710; Rework uniform heterogeneous assignor to improve perf (#17385)  Rework the uniform heterogeneous assignor to improve performance  while preserving the high level ideas and structure from the existing implementation: * The assignor works in 3 stages: importing the previous assignment for stickiness  assigning unassigned partitions and iteratively reassigning partitions to improve balance. * Unassigned partitions are assigned to the subscribers with the least number of partitions. This maximizes balance within a single topic. * During the iterative rebalancing phase  partitions are reassigned to their previous owner if it improves balance (stickiness restoration). * During the iterative rebalancing phase  partitions are reassigned to the subscriber with the least number of partitions to improve balance.  A non-exhaustive list of changes is: * The assignment of unassigned partitions and iterative reassignment stages now works through partitions topic by topic. Previously partitions from topics with the same number of partitions per subscriber would be interleaved. Since we iterate topic by topic  we can reuse data about topic subscribers. * Instead of maintaining TreeSets to find the least loaded subscribers  we sort an ArrayList of subscribers once per topic and start filling up subscribers  least loaded first. In testing  this approach was found to be faster than maintaining PriorityQueues. * Implement stickiness restoration by creating a mapping of partitions to previous owner and checking against that mapping  instead of tracking partition movements during iterative reassignment. * Track member partition counts using a plain int array  to avoid overhead from boxing and HashMap lookups. Member partition counts are accessed very frequently and this needs to be fast. As a consequence  we have to number members 0 to M - 1. * Bound the iterative reassignment stage to a fixed number of iterations. Under some uncommon subscription patterns  the iterative reassignment stage converges slowly. In these cases  the iterative reassignment stage terminates without producing an optimally balanced assignment anyway (see javadoc for balanceTopics). * Re-use Maps from the previous assignment where possible  ie. introduce a copy-on-write mechanism while computing the new assignment.  Reviewers: David Jacot <djacot@confluent.io>
apache,kafka,99e1d8fbb30c8c132cd9baec016efb833b6036ec,https://github.com/apache/kafka/commit/99e1d8fbb30c8c132cd9baec016efb833b6036ec,MINOR: Cache topic resolution in TopicIds set (#17285)  Looking up topics in a TopicsImage is relatively slow. Cache the results in TopicIds to improve assignor performance. In benchmarks  we see a noticeable improvement in performance in the heterogeneous case.  Before ``` Benchmark                                       (assignmentType)  (assignorType)  (isRackAware)  (memberCount)  (partitionsToMemberRatio)  (subscriptionType)  (topicCount)  Mode  Cnt    Score   Error  Units ServerSideAssignorBenchmark.doAssignment             INCREMENTAL           RANGE          false          10000                         10         HOMOGENEOUS          1000  avgt    5   36.400 ± 3.004  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL           RANGE          false          10000                         10       HETEROGENEOUS          1000  avgt    5  158.340 ± 0.825  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL         UNIFORM          false          10000                         10         HOMOGENEOUS          1000  avgt    5    1.329 ± 0.041  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL         UNIFORM          false          10000                         10       HETEROGENEOUS          1000  avgt    5  382.901 ± 6.203  ms/op ```  After ``` Benchmark                                       (assignmentType)  (assignorType)  (isRackAware)  (memberCount)  (partitionsToMemberRatio)  (subscriptionType)  (topicCount)  Mode  Cnt    Score   Error  Units ServerSideAssignorBenchmark.doAssignment             INCREMENTAL           RANGE          false          10000                         10         HOMOGENEOUS          1000  avgt    5   36.465 ± 1.954  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL           RANGE          false          10000                         10       HETEROGENEOUS          1000  avgt    5  114.043 ± 1.424  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL         UNIFORM          false          10000                         10         HOMOGENEOUS          1000  avgt    5    1.454 ± 0.019  ms/op ServerSideAssignorBenchmark.doAssignment             INCREMENTAL         UNIFORM          false          10000                         10       HETEROGENEOUS          1000  avgt    5  342.840 ± 2.744  ms/op ```  ---  Based heavily on https://github.com/apache/kafka/pull/16527.  Reviewers: David Arthur <mumrah@gmail.com>  David Jacot <djacot@confluent.io>
apache,kafka,d0ad84df5d52fdc55b590356cd0bba62c2de6a8e,https://github.com/apache/kafka/commit/d0ad84df5d52fdc55b590356cd0bba62c2de6a8e,MINOR: producer perf improvements (#17348)  Adding some missing input checks and fixing a formatting issue.  Signed-off-by: Federico Valeri <fedevaleri@gmail.com>  Reviewers: Luke Chen <showuon@gmail.com>
apache,kafka,f8acfa5257f6ba6c229884b638053d52b1b3e68a,https://github.com/apache/kafka/commit/f8acfa5257f6ba6c229884b638053d52b1b3e68a,KAFKA-17621; Reduce logging verbosity on ConsumerGroupHeartbeat path (#17288)  While running large scale performance tests  we noticed that the logging on the ConsumerGroupHeartbeat path took a significant amount of CPU. It is mainly due to the very large data structures that we print out. I made a pass on those logs and I switched some of them to debug.  Reviewers: Lianet Magrans <lianetmr@gmail.com>
apache,kafka,6744a718c2c177c7d462b231ea5e476d98f6eb38,https://github.com/apache/kafka/commit/6744a718c2c177c7d462b231ea5e476d98f6eb38,KAFKA-17066 new consumer updateFetchPositions all in background thread (#16885)  Fix for the known issue that the logic for updating fetch positions in the new consumer was being performed partly in the app thread  party in the background thread  potentially leading to race conditions on the subscription state.  This PR moves the logic for updateFetchPositions to the background thread as a single event (instead of triggering separate events to validate  fetchOffsets  listOffsets). A new UpdateFetchPositionsEvent is triggered from the app thread and processed in the background  where it performs those same operations and updates the subscription state accordingly  without blocking the background thread.  This PR maintains the existing logic for keeping a pendingOffsetFetchRequest that does not complete within the lifetime of the updateFetchPositions attempt  and may be used on the next call to updateFetchPositions.  Reviewers: Andrew Schofield <aschofield@confluent.io>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,4a3ab89f95aba294bb536af55548522d946d1ee3,https://github.com/apache/kafka/commit/4a3ab89f95aba294bb536af55548522d946d1ee3,KAFKA-17386 Remove broker-list  threads and num-fetch-threads in ConsumerPerformance (#16983)  Reviewers: Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,f5439864c6a3cf8275c951a5a09a0d500789d869,https://github.com/apache/kafka/commit/f5439864c6a3cf8275c951a5a09a0d500789d869,KAFKA-15406: Add the ForwardingManager metrics from KIP-938 (#16904)  Implement the remaining ForwardingManager metrics from KIP-938: Add more metrics for measuring KRaft performance:  kafka.server:type=ForwardingManager name=QueueTimeMs.p99 kafka.server:type=ForwardingManager name=QueueTimeMs.p999 kafka.server:type=ForwardingManager name=QueueLength kafka.server:type=ForwardingManager name=RemoteTimeMs.p99 kafka.server:type=ForwardingManager name=RemoteTimeMs.p999  Reviewers: Colin P. McCabe <cmccabe@apache.org>
apache,kafka,94f5039350432e25191d708fd8500c6e285f7bd9,https://github.com/apache/kafka/commit/94f5039350432e25191d708fd8500c6e285f7bd9,KAFKA-17378 Fixes for performance testing (#16942)  Reviewers: Apoorv Mittal <apoorvmittal10@gmail.com>  Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,42715654095d019effc478ab343c0132e02aa70f,https://github.com/apache/kafka/commit/42715654095d019effc478ab343c0132e02aa70f,KAFKA-16900 kafka-producer-perf-test reports error when using transaction. (#16646)  Currently  users need to set --transaction-duration-ms to enable transactions in kafka-producer-perf-test  which is not straightforward. A better approach is to enable transactions when a transaction ID is provided.  This PR allows enabling transaction in kafka-producer-perf-test by either  - set transaction.id=<id> via --producer-props or - set transaction.id=<id> in config file via --producer.config or - set --transaction-id <id> or - set --transaction-duration-ms=<ms>  Reviewers: Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,0cbc5e083a936025a85f127102dc1032f6cf4fd9,https://github.com/apache/kafka/commit/0cbc5e083a936025a85f127102dc1032f6cf4fd9,KAFKA-17217: Batched acknowledge requests per node in ShareConsumeRequestManager (#16727)  In ShareConsumeRequestManager  currently every time we perform a commitSync/commitAsync/acknowledgeOnClose we create one AcknowledgeRequestState for each call. But this can be optimised further as we can batch up the acknowledgements to be sent to the same node before the next poll() is invoked.  This will ensure that between 2 polls  the acknowledgements are accumulated in one request per node and then sent during poll  resulting in lesser RPC calls.  To achieve this  we are storing a pair of acknowledge request states for every node  the first value denotes the requestState for commitAsync() and the second value denotes the requestState for commitSync() and acknowledgeOnClose(). All the acknowledgements to be sent to a particular node are stored in the corresponding acknowledgeRequestState based on whether it was synchronous or asynchronous.  Reviewers:  Andrew Schofield <aschofield@confluent.io>   Manikumar Reddy <manikumar.reddy@gmail.com>
apache,kafka,5a602b2f86935f799e94918c7143820b3d46142f,https://github.com/apache/kafka/commit/5a602b2f86935f799e94918c7143820b3d46142f,KAFKA-17235 system test test_performance_service.py failed (#16789)  related to https://issues.apache.org/jira/browse/KAFKA-17235  The root cause of this issue is a change we introduced in KAFKA-16879  where we modified the PushHttpMetricsReporter constructor to use Time.System [1]. However  Time.System doesn't exist in Kafka versions 0.8.2 and 0.9.  In test_performance_services.py  we have system tests for Kafka versions 0.8.2 and 0.9 [2]. These tests always use the tools JAR from the trunk branch  regardless of the Kafka version being tested [3]  while the client JAR aligns with the Kafka version specified in the test suite [4]. This discrepancy is what causes the issue to arise.  To resolve this issue  we have a few options:  1) Add Time.System to Kafka 0.8.2 and 0.9: This isn't practical  as we no longer maintain these versions. 2) Modify the PushHttpMetricsReporter constructor to use new SystemTime() instead of Time.System: This would contradict the intent of KAFKA-16879  which aims to make SystemTime a singleton. 3) Implement Time in PushHttpMetricsReporter use the time to get current time 4) Remove system tests for Kafka 0.8.2 and 0.9 from test_performance_services.py  Given that we no longer maintain Kafka 0.8.2 and 0.9  and altering the constructor goes against the design goals of KAFKA-16879  option 4 appears to be the most feasible solution. However  I'm not sure whether it's acceptable to remove these old version tests. Maybe someone else has a better solution  "We'll proceed with option 3 since support for versions 0.8 and 0.9 is still required  meaning we can't remove those Kafka versions from the system tests."  Reviewers: Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,6ff51bc388dac55e04343fe4247b1080f1241dab,https://github.com/apache/kafka/commit/6ff51bc388dac55e04343fe4247b1080f1241dab,KAFKA-17210: Broker fixes for smooth concurrent fetches on share partition (#16711)  Identified a couple of reliability issues with broker code for share groups  1. Broker seems to get stuck at times when using multiple share consumers due to a corner case where the second last fetch request did not contain any topic partition to fetch  because of which the broker could never complete the last request. This results in a share fetch request getting stuck.  2. Since persister would not perform any business logic around sending state batches for a share partition  there could be scenarios where it sends state batches with no AVAILABLE records. This could cause a breach on the limit of in-flight messages we have configured  and hence broker would never be able to complete the share fetch requests.  Reviewers:  Andrew Schofield <aschofield@confluent.io>  Apoorv Mittal <apoorvmittal10@gmail.com>   Manikumar Reddy <manikumar.reddy@gmail.com>
apache,kafka,68070c94a6bd5141c720aaff85fa545da3a7a1b5,https://github.com/apache/kafka/commit/68070c94a6bd5141c720aaff85fa545da3a7a1b5,KAFKA-16724: Added support for fractional throughput and monotonic payload in kafka-producer-perf-test.sh  Added support for fractional throughput and monotonic payload in kafka-producer-perf-test.sh. https://cwiki.apache.org/confluence/display/KAFKA/KIP-932%3A+Queues+for+Kafka#KIP932:QueuesforKafka-kafka-producer-perf-test.sh  Reviewers: Andrew Schofield <aschofield@confluent.io>  Manikumar Reddy <manikumar.reddy@gmail.com>
apache,kafka,3835515feaf7cb5bb7de3c4d63794e79100eb62a,https://github.com/apache/kafka/commit/3835515feaf7cb5bb7de3c4d63794e79100eb62a,KAFKA-16541 Fix potential leader-epoch checkpoint file corruption (#15993)  A patch for KAFKA-15046 got rid of fsync on LeaderEpochFileCache#truncateFromStart/End for performance reason  but it turned out this could cause corrupted leader-epoch checkpoint file on ungraceful OS shutdown  i.e. OS shuts down in the middle when kernel is writing dirty pages back to the device.  To address this problem  this PR makes below changes: (1) Revert LeaderEpochCheckpoint#write to always fsync (2) truncateFromStart/End now call LeaderEpochCheckpoint#write asynchronously on scheduler thread (3) UnifiedLog#maybeCreateLeaderEpochCache now loads epoch entries from checkpoint file only when current cache is absent  Reviewers: Jun Rao <junrao@gmail.com>
apache,kafka,078dd9a311a5e9ddb9d0b49a1164c50dadde3a27,https://github.com/apache/kafka/commit/078dd9a311a5e9ddb9d0b49a1164c50dadde3a27,KAFKA-16821; Member Subscription Spec Interface (#16068)  This patch reworks the `PartitionAssignor` interface to use interfaces instead of POJOs. It mainly introduces the `MemberSubscriptionSpec` interface that represents a member subscription and changes the `GroupSpec` interfaces to expose the subscriptions and the assignments via different methods.  The patch does not change the performance.  before: ``` Benchmark                                     (memberCount)  (partitionsToMemberRatio)  (topicCount)  Mode  Cnt  Score   Error  Units TargetAssignmentBuilderBenchmark.build                10000                         10           100  avgt    5  3.462 ± 0.687  ms/op TargetAssignmentBuilderBenchmark.build                10000                         10          1000  avgt    5  3.626 ± 0.412  ms/op JMH benchmarks done ```  after: ``` Benchmark                                     (memberCount)  (partitionsToMemberRatio)  (topicCount)  Mode  Cnt  Score   Error  Units TargetAssignmentBuilderBenchmark.build                10000                         10           100  avgt    5  3.677 ± 0.683  ms/op TargetAssignmentBuilderBenchmark.build                10000                         10          1000  avgt    5  3.991 ± 0.065  ms/op JMH benchmarks done ```  Reviewers: David Jacot <djacot@confluent.io>
apache,kafka,979f8d9aa3e8840951a151ab83eb006f8e7c1314,https://github.com/apache/kafka/commit/979f8d9aa3e8840951a151ab83eb006f8e7c1314,MINOR: Small refactor in TargetAssignmentBuilder (#16174)  This patch is a small refactoring which mainly aims at avoid to construct a copy of the new target assignment in the TargetAssignmentBuilder because the copy is not used by the caller. The change relies on the exiting tests and it does not really have an impact on performance (e.g. validated with TargetAssignmentBuilderBenchmark).  Reviewers: Chia-Ping Tsai <chia7712@gmail.com>
apache,kafka,c8af740bd44dae92bbe68254114c0fd7f7c32345,https://github.com/apache/kafka/commit/c8af740bd44dae92bbe68254114c0fd7f7c32345,Improve producer ID expiration performance (#16075)  Skip using stream when expiring the producer ID. This can improve the performance significantly when the count is high. Before  Benchmark                                        (numProducerIds)  Mode  Cnt      Score       Error  Units ProducerStateManagerBench.testDeleteExpiringIds             10000  avgt    3    101.253 ±    28.031  us/op ProducerStateManagerBench.testDeleteExpiringIds            100000  avgt    3   2297.219 ±  1690.486  us/op ProducerStateManagerBench.testDeleteExpiringIds           1000000  avgt    3  30688.865 ± 16348.768  us/op After  Benchmark                                        (numProducerIds)  Mode  Cnt     Score     Error  Units ProducerStateManagerBench.testDeleteExpiringIds             10000  avgt    3    39.122 ±   1.151  us/op ProducerStateManagerBench.testDeleteExpiringIds            100000  avgt    3   464.363 ±  98.857  us/op ProducerStateManagerBench.testDeleteExpiringIds           1000000  avgt    3  5731.169 ± 674.380  us/op Also  made a change to the JMH testing which excludes the producer ID populating from the testing.  Reviewers: Artem Livshits <alivshits@confluent.io>  Justine Olshan <jolshan@confluent.io>
apache,kafka,8d243dfbd415b5b34515695b72379d1489297a23,https://github.com/apache/kafka/commit/8d243dfbd415b5b34515695b72379d1489297a23,KAFKA-15045: (KIP-924 pt. 12) Wiring in new assignment configs and logic (#16074)  This PR creates the new public config of KIP-924 in StreamsConfig and uses it to instantiate user-created TaskAssignors. If such a TaskAssignor is found and successfully created we then use that assignor to perform the task assignment  otherwise we revert back to the pre KIP-924 world with the internal task assignors.  Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>  Almog Gavra <almog@responsive.dev>
chinabugotech,hutool,d556f5b62f327133c627b9a88f2bf4ecdf2e6ecc,https://github.com/chinabugotech/hutool/commit/d556f5b62f327133c627b9a88f2bf4ecdf2e6ecc,!1228  perf(core): 时间格式化工具支持自定义设置单位 Merge pull request !1228 from 蒋小小/v5-dev
chinabugotech,hutool,24a4c479c457011b154fbc3a358055a1d35d87a0,https://github.com/chinabugotech/hutool/commit/24a4c479c457011b154fbc3a358055a1d35d87a0,🔨 perf(core): 时间格式化工具支持自定义设置单位
keycloak,keycloak,abc448e4d19638cdba028fe0e435e71f16100c65,https://github.com/keycloak/keycloak/commit/abc448e4d19638cdba028fe0e435e71f16100c65,fix: performing inline user import for multi-file  closes: #38251  Signed-off-by: Steve Hawkins <shawkins@redhat.com>
keycloak,keycloak,24910d9e1c76a1e953936c558ab02a27eb91820e,https://github.com/keycloak/keycloak/commit/24910d9e1c76a1e953936c558ab02a27eb91820e,addresses slow import/export performance by limiting persistence context size (#37926)  * fix: addresses slow import/export performance with more batching  closes: #37991  Signed-off-by: Steve Hawkins <shawkins@redhat.com>  * removing flush/detach manipulation  Signed-off-by: Steve Hawkins <shawkins@redhat.com>  * refining the doc note about using multiple files for larger user counts  Signed-off-by: Steve Hawkins <shawkins@redhat.com>  * adding doc note about useExistingSession method removal  and expanding javadocs  Signed-off-by: Steve Hawkins <shawkins@redhat.com>  ---------  Signed-off-by: Steve Hawkins <shawkins@redhat.com>
keycloak,keycloak,491b7861ed1c47a45003cb92ddbf2fc3cae1bdc6,https://github.com/keycloak/keycloak/commit/491b7861ed1c47a45003cb92ddbf2fc3cae1bdc6,[PERF] Determine whether OS is Windows  Closes #33953  Signed-off-by: Martin Bartoš <mabartos@redhat.com>
keycloak,keycloak,a43b65281d6f3631366b71858cad25676aaf635d,https://github.com/keycloak/keycloak/commit/a43b65281d6f3631366b71858cad25676aaf635d,Search user by id and fallback to username when needed - prevents performance issues when reading policies as users are always stored by id.  Closes #35796  Signed-off-by: Stefan Guilhen <sguilhen@redhat.com>
keycloak,keycloak,af434d6bc1ae904da2538f207bf5313098757114,https://github.com/keycloak/keycloak/commit/af434d6bc1ae904da2538f207bf5313098757114,Add checks to prevent GroupLDAPStorageMapper from performing operations on groups it does not manage  Closes #11008 Closes #17593 Closes #19652  Signed-off-by: Stefan Guilhen <sguilhen@redhat.com>
keycloak,keycloak,637ca2e13848c74444d98604918670217247bf64,https://github.com/keycloak/keycloak/commit/637ca2e13848c74444d98604918670217247bf64,[PERF] OpenTelemetry is initialized even when disabled (#34031)  Change host reference in logging guide  Fixes #33948  Signed-off-by: Martin Bartoš <mabartos@redhat.com>
keycloak,keycloak,cf8905efe87276a672dabea249bdd797c6243e9d,https://github.com/keycloak/keycloak/commit/cf8905efe87276a672dabea249bdd797c6243e9d,Fix for Client secret is visable in Admin event representation when Credentials Reset action performed for the Client. (#32067)  * Stripping secrets for the credential representation  Signed-off-by: kaustubh B <kbawanka@redhat.com>
keycloak,keycloak,d534860e2bf4fd908502c39ce0fddf0377cc92e8,https://github.com/keycloak/keycloak/commit/d534860e2bf4fd908502c39ce0fddf0377cc92e8,fix: admin cli client should set the content when performing a merge (#30539)  closes: #29878  Signed-off-by: Steve Hawkins <shawkins@redhat.com>
keycloak,keycloak,13ef6fb1c8564aa22003cf4d1507d982f63e2f5f,https://github.com/keycloak/keycloak/commit/13ef6fb1c8564aa22003cf4d1507d982f63e2f5f,fix(operator): Scale statefulset to 0 to prepare for update (#30450)  When performing a keycloak update  the operator is supposed to make sure that potential database migrations are run with only one pod active. This change makes the operator scale down the stateful set to zero pods in preparation for the update. The next reconciliation loop will scale the stateful set back up and change the image  making sure migrations are being run on the first pod that is brought up. This also makes sure that the rollover works even if the infinispan versions are incompatible. (ref: #30449)  Signed-off-by: Schmidt  Sascha (sasschmidt) <sascha.schmidt@breuninger.de>
keycloak,keycloak,7d05a7a013495a8c59c3bdc71a04f743d3391b34,https://github.com/keycloak/keycloak/commit/7d05a7a013495a8c59c3bdc71a04f743d3391b34,Logout from all clients after IdP logout is performed Closes #25234  Signed-off-by: rmartinc <rmartinc@redhat.com>
apache,incubator-seata,3d6cf39c9a442c146e128eb5aa1dad4ae4f4e6ea,https://github.com/apache/incubator-seata/commit/3d6cf39c9a442c146e128eb5aa1dad4ae4f4e6ea,optimize: raft mode performs transaction size check in advance (#7344)
apache,incubator-seata,b530e30d3da56b89dd01b4ce00a10f4572765b0a,https://github.com/apache/incubator-seata/commit/b530e30d3da56b89dd01b4ce00a10f4572765b0a,optimize: Use shared `EventLoop` for TM and RM clients to reduce thread overhead and improve performance (#7179)
apache,incubator-seata,c1515acbe33193304c814aeadfcf630d19a22710,https://github.com/apache/incubator-seata/commit/c1515acbe33193304c814aeadfcf630d19a22710,optimize: deserialize performance optimize (#6727)
apache,flink,77edff620326d999e793459ab7c86034e943372f,https://github.com/apache/flink/commit/77edff620326d999e793459ab7c86034e943372f,[FLINK-37213][table-runtime] Improve performance of unbounded OVER aggregations  Instead of sorting all of the records based on the row time explicilty use timers to achieve the same thing.  This version vs the previous one register a timer for each record  as opposed to just one timer per key. However since we are using RocksDB for timers  this is a minor problem. In exchange  we: - don't have to iterate over all of the state for each timer - we are firing timers only when needed  vs for each watermark for each key. For example if watermarks are fire every 200ms and for a given key  we have only one record that should be fired 20s into the future  the previous version would be firing a timer for that key for each watermark unnecessarily without doing any work.
apache,flink,82031921de01c66ee0543e85764663373b05144b,https://github.com/apache/flink/commit/82031921de01c66ee0543e85764663373b05144b,[FLINK-36823][Checkpointing] Drain state requests after the user function perform snapshotState (#25715)
apache,flink,94c3b86a368d545a0aa3ff6b5c42f6f8ec3e11de,https://github.com/apache/flink/commit/94c3b86a368d545a0aa3ff6b5c42f6f8ec3e11de,[FLINK-36530][state] Fix S3 performance issue with uncompressed state restore
apache,skywalking,3559e85f36f860382d751dde03ae8fc8ac6e15a1,https://github.com/apache/skywalking/commit/3559e85f36f860382d751dde03ae8fc8ac6e15a1,Improve the performance of OTEL metrics handler. (#12645)  Benchmark for a single node k8s monitoring.  | metrics (avg)            | before | after | | ------------------------ | ------ | ----- | | cpu                      | 19     | 16    | | gc count                 | 16     | 1     | | gc time                  | 38.8   | 5.1   | | otel metrics latency P50 | 125    | 8     | | otel metrics latency P90 | 333.3  | 22.5  | | otel metrics latency P99 | 666.6  | 166.6 |
apache,skywalking,9644f3fda9a7313e5aef9a3b581006710b8369ac,https://github.com/apache/skywalking/commit/9644f3fda9a7313e5aef9a3b581006710b8369ac,BanyanDB: stream sort-by `time` query  use internal time-series rather than `index` to improve the query performance. (#12486)
apache,skywalking,39ee6026f4c1608b56b6686cfcf6da7d798c2588,https://github.com/apache/skywalking/commit/39ee6026f4c1608b56b6686cfcf6da7d798c2588,BanyanDB: Zipkin Module set service as Entity for improving the query performance (#12474)
bazelbuild,bazel,9c86087aaf2a335db1243592b8c9d63cc2d40feb,https://github.com/bazelbuild/bazel/commit/9c86087aaf2a335db1243592b8c9d63cc2d40feb,Mark SkyframeLookups abandoned if there are errors or state eviction.  SkyframeLookups are ListenableFutures and deserialization may add listeners to them. However  the future requires a corresponding Skyframe lookup to be performed to complete. The SkyframeLookups are sometimes retained in SkyKeyComputeState and can be evicted. They are sometimes skipped because there is a previous error. This change marks SkyframeLookups abandoned in both of those cases  allowing listeners to progress  rather than hanging.  In the case where the SkyframeLookupCollector fails fast  marks any enqueued lookups abandoned  and marks any subsequently added lookups abandoned without enqueuing.  PiperOrigin-RevId: 759614205 Change-Id: I6e03bcbcd9dd289211979efa9901aab539ca191a
bazelbuild,bazel,32a816ee37f332ecc39553de528c302d4cb8b2e8,https://github.com/bazelbuild/bazel/commit/32a816ee37f332ecc39553de528c302d4cb8b2e8,Attempt invalidation lookup on all existing deserialized nodes before analysis.  This is a key step towards incremental builds with a remote analysis cache service.  This introduces a new method in `RemoteAnalysisCachingDependenciesProvider` to invalidate `SkyValue`s using their fingerprinted `SkyKey`s. Doing the remote lookup may add more latency on every build before analysis. Current experiments show that the performance may not be a huge concern (sub-second invalidation for O(10000) keys).  This also requires a few changes to the `SkyframeExecutor`  because it needs to persist the deserialized keys across command invocations for invalidation.  PiperOrigin-RevId: 753993489 Change-Id: I06cb244c04e469ba3bcfd226348a7523601e8d6a
bazelbuild,bazel,113aacc83c146d2eec52d37fab909201671118ba,https://github.com/bazelbuild/bazel/commit/113aacc83c146d2eec52d37fab909201671118ba,Implement an ActionCache trim() method.  This is the method responsible for garbage collecting entries that haven't been used in a while. Garbage collection operates by copying all entries below a given cutoff age into a new cache; atomically replacing the old on-disk representation with the new; then reloading the cache from the new representation. This requires minimal additional logic  is safe in the event of interruption (either intentionally due to a subsequent command  or unintentionally due to a crash)  and performs well enough even for the largest caches in use at Google.  The code is currently unused; a followup CL will wire it into an idle task.  Progress on #22515.  PiperOrigin-RevId: 753153798 Change-Id: Ia2d7e2dbe96586635ed0c9255e6538ecb96a3783
bazelbuild,bazel,edc876200bbc51206559e38811221dd41562a641,https://github.com/bazelbuild/bazel/commit/edc876200bbc51206559e38811221dd41562a641,Pass LibraryToLink into createLtoArtifacts  Implement _maybe_do_lto_indexing and create_lto_artifacts_and_lto_indexing_action using LibraryToLink (instead of LegacyLinkerInput).  Works towards removing LegacyLinkerInputs. The change is a no-op.  - libraries_to_link (list[LibraryToLink]) are passed into _maybe_do_lto_indexing. - static libraries_to_link are filtered out (using a new native method to keep the same performance) - _maybe_do_lto_indexing and create_lto_artifacts_and_lto_indexing_action is implemented using static_libraries_to_link. Interface and dynamic libraries play no role here. - libraries (list[LegacyLinkerInput]) is kept to pass into finalize_link_action. It will be removed in followup cl-s.  PiperOrigin-RevId: 751453530 Change-Id: I6a7839ef8213a41636b6e9430a1333de526a4b75
bazelbuild,bazel,68c612942338aad5c64d8011a368063c4f51240e,https://github.com/bazelbuild/bazel/commit/68c612942338aad5c64d8011a368063c4f51240e,Part 1 of making CcStaticCompilationHelper methods static (in this order):  * getOutputNameBaseWith() * collectPerFileCopts() * getCopts() * shouldPassPropellerProfiles() * getAuxiliaryFdoInputs() * configureFdoBuildVariables()  PiperOrigin-RevId: 747881687 Change-Id: Id275d729e2ac3b9173f3a4da542fce80f0e8357e
bazelbuild,bazel,dc74fd9ff4d58bbe0a99553f4a3dd87f4d2dc605 upload builds with modified workspaces are crashing but this,Since https://github.com/bazelbuild/bazel/commit/dc74fd9ff4d58bbe0a99553f4a3dd87f4d2dc605 upload builds with modified workspaces are crashing but this,Skycache: Fix crash with modified workspace in upload mode 
bazelbuild,bazel,9e2cd2510f0cbc828c0c0128e370a3a3fff15ddf,https://github.com/bazelbuild/bazel/commit/9e2cd2510f0cbc828c0c0128e370a3a3fff15ddf,
bazelbuild,bazel,03dfb497c16f0ff5a79410fe4f6bb1a154e0335a,https://github.com/bazelbuild/bazel/commit/03dfb497c16f0ff5a79410fe4f6bb1a154e0335a,Plumb the metadata for files referenced by Filesets through ExpandedArtifact.  The benefit is that these won't be considered local files anymore for the purposes of uploading files. In addition to a minor performance improvement  this also allows us to remove the mutation of ActionInputMap from ArtifactValueFileSystem.  That  in turn  will make it possible for ArtifactValueFileSystem to use an InputMetadataProvider instead of an ActionInputMap  which in turn enable putting a Skyframe-based InputMetadataProvider in ActionFileSystem  which will then unlock removing the logic to do Skyframe restarts from ActionFileSystem.  RELNOTES: None. PiperOrigin-RevId: 743648043 Change-Id: Ie61fbbb1f107b106f56d36ab90786411b83e45dc
bazelbuild,bazel,ae39adc27b6cb59f169e2c69867df71edc8a105f,https://github.com/bazelbuild/bazel/commit/ae39adc27b6cb59f169e2c69867df71edc8a105f,Always use an execroot-relative path when a fileset symlink pointing to a generated file is represented as an `ActionInput`.  Some places were using absolute paths while others were using execroot-relative paths  leading to `ActionInputMetadataProvider` having to support metadata lookups both ways.  Note that we still use absolute paths for fileset symlinks pointing to a source file  since there is such a thing as a build with multiple source roots.  An additional performance benefit is that we use the `PathFragment` directly from `FilesetOutputSymlink` instead of prepending the exec root and creating a new string. We've seen severe memory issues from this before  which were mitigated with interning.  PiperOrigin-RevId: 736533456 Change-Id: If3c5797288b3a85687afc5b59368e0bbe57b6a07
bazelbuild,bazel,85d9cc9cc6e1bc46116fb6163d43cad9aa17ffc7,https://github.com/bazelbuild/bazel/commit/85d9cc9cc6e1bc46116fb6163d43cad9aa17ffc7,Retry build when `RemoteActionFileSystem` encounters a missing digest  Cache evictions encountered during reads of remote files in `RemoteActionFileSystem` now result in the build being retried when `--experimental_remote_cache_eviction_retries` is set to a positive value (the default).  This is enabled by implementing the `checkForLostInputs` method on the `RemoteOutputService`   building on the refactoring performed in https://github.com/bazelbuild/bazel/pull/25396.  Closes #25358.  PiperOrigin-RevId: 736064349 Change-Id: Idc70d55138c3366d22ece7f0579b242b3c217da8
bazelbuild,bazel,11eb0c52b8779799608ce7cc598de240807eefcb,https://github.com/bazelbuild/bazel/commit/11eb0c52b8779799608ce7cc598de240807eefcb,Refactor lost input ownership calculation into `ActionRewindStrategy` and simplify it.  Lost input ownership calculation is important for action rewinding - in order to rewind properly  we need to know which special artifacts (runfiles  tree artifact  fileset) that a lost input is a part of.  Historically  lost input ownership calculation was performed by `ActionExecutionFunction` by sharing the same code that handles inputs for regular execution. It accomplished this via an interface `ActionInputMapSink` which was implemented by `ActionInputMap` (for regular execution) and `ActionInputDepOwnerMap` (for lost input ownership).  This solution was chosen to guarantee fidelity between the two. However  it is not necessary to accommodate arbitrary ownership mappings between artifacts - there is a well-defined  finite set of possible owner relationships. By using knowledge of the possible owner relationships  we can simplify and improve the performance of the calculation. In this change  we handle the three types of possible owners as follows:  * Runfiles - iterate over all `RunfilesTree`s in the action's inputs and expand them. * Tree artifact - simply call `hasParent()`/`getParent()` on lost input artifacts. * Fileset (no change from previous) - assume that fileset ownership is tracked by the spawn strategy that threw the original lost inputs exception. A future change will add fileset ownership handling to `ActionRewindStrategy`  making spawn-strategy-side ownership accounting optional.  Notably  we no longer re-flatten and re-process all of the action's inputs  which should improve rewinding performance.  PiperOrigin-RevId: 735759787 Change-Id: I21fbaea40810cc8c6da79c5d8c6d245318be5252
bazelbuild,bazel,813497d745d3720f80187b30e142957ce849d1d8,https://github.com/bazelbuild/bazel/commit/813497d745d3720f80187b30e142957ce849d1d8,Skip a skyframe evaluation per top-level target's configuration key when checking environment constraints.  The configured target was already analyzed  so the corresponding configuration value must exist in the graph. Performing a skyframe evaluation is heavyweight.  PiperOrigin-RevId: 733757263 Change-Id: I12abfb892e68c28dedb74c352c5ea3719078937a
bazelbuild,bazel,e12fdd93a3e22163d37209634ad5546f9fccd3bf,https://github.com/bazelbuild/bazel/commit/e12fdd93a3e22163d37209634ad5546f9fccd3bf,Build the label to target map without performing skyframe evaluations.  After parsing target patterns we build a `Map<Label  Target>` containing all top-level targets. The code was looking up packages of top-level targets by performing a `MemoizingEvaluator#evaluate` call for each  one at a time  in sequence. A skyframe evaluation is relatively heavyweight given that we can just look up the `PackageIdentifier` in the graph since we already evaluated it on the current build.  PiperOrigin-RevId: 733065574 Change-Id: Ib012c6fd60d8bf8e171b7ab07d4622ed73caaa43
bazelbuild,bazel,3e115b937d7f6eff5c819667c6f6226c8fdba466,https://github.com/bazelbuild/bazel/commit/3e115b937d7f6eff5c819667c6f6226c8fdba466,Fix a severe performance bug for builds with many top-level targets.  This one-line fix speeds up some real-life builds by 90%  saving over 20 minutes!  We only need to remove labels from the weak interner when a package is freshly computed (`directDeps != null`)  not when it is already up-to-date as a top-level key in a skyframe evaluation. Without this guard  the removal code is being superfluously executed (in sequence) for each top-level target's package (without deduplication) twice during `BuildView#update`: once when computing the `labelToTargets` map  and again during `checkTargetEnvironmentRestrictions()`.  PiperOrigin-RevId: 733025147 Change-Id: Ief68c375a5d8444df1869968b4008af4a314b9c9
bazelbuild,bazel,3f36f64df2135cb4be9a0ba917d4d77813772177,https://github.com/bazelbuild/bazel/commit/3f36f64df2135cb4be9a0ba917d4d77813772177,Accept `Duration` in `SpawnMetrics.Builder` methods.  `SpawnMetrics` represents durations using primitive `int` fields for performance reasons. Callers of its `Builder` usually have to convert from `Duration` to `int`. Add `Builder` methods to do this conversion.  PiperOrigin-RevId: 732145629 Change-Id: Ifaaa18450b7fcffe5ab0a52ec4b4d30a8f76ffba
bazelbuild,bazel,70b0afd4e814e49fad93729324519174fe321190,https://github.com/bazelbuild/bazel/commit/70b0afd4e814e49fad93729324519174fe321190,ArgumentProcessor performance tuning:  * Add StarlarkCallable to Starlark.callViaArgumentProcessor() to remove getCallable() in thread.push() call. * Minimize calls between StarlarkFunction.ArgumentProcessor and StarlarkFunction: inline 3 methods from StarlarkFunction and one method from StarlarkFunction.ArgumentProcessor into StarlarkFunction.ArgumentProcessor.call().  PiperOrigin-RevId: 724604311 Change-Id: I18793004800d6fb28d5f5973fd9908f542e0860c
bazelbuild,bazel,ee12906c5d9a48924db6fc3aba36ccd6d5c7f69e,https://github.com/bazelbuild/bazel/commit/ee12906c5d9a48924db6fc3aba36ccd6d5c7f69e,Prevent use of BuiltinFunction.ArgumentProcessor in Eval.evalCall.  This is a temporary fix for a performance regression that the use of BuiltinFunction.ArgumentProcessor caused.  PiperOrigin-RevId: 723413626 Change-Id: I395052c8e05047223dc8b342fbe5c7d6b57369cc
bazelbuild,bazel,e6e8ffaa6dadf45f7b668dc887d7cc81af6a49ff,https://github.com/bazelbuild/bazel/commit/e6e8ffaa6dadf45f7b668dc887d7cc81af6a49ff,Automated rollback of commit 56bf54716094bf6b687366d20b577435213681d5.  *** Reason for rollback ***  performance regression b/392290938  *** Original change description ***  Remove use of fastcall() from Eval.java  PiperOrigin-RevId: 723396402 Change-Id: Icc69adbd41a5148349464544b918d2d7190755e7
bazelbuild,bazel,5f1aa38b2ea824a4b10e83c0baddfcd17fb418a2,https://github.com/bazelbuild/bazel/commit/5f1aa38b2ea824a4b10e83c0baddfcd17fb418a2,Automated rollback of commit b17a37bce15dded96868310616cd3c1c63f36c38.  *** Reason for rollback ***  performance regression b/392290938  *** Original change description ***  Remove fastcall() and related code from BuiltinFunction  PiperOrigin-RevId: 723375619 Change-Id: Ib1dd60269a09472436fe3a03f3fab80402865840
bazelbuild,bazel,936692fb27cb265ff1c832c571a7231157d0b6dd,https://github.com/bazelbuild/bazel/commit/936692fb27cb265ff1c832c571a7231157d0b6dd,Automated rollback of commit 2b928efcd651fe9e42d6d09956365f58b95ce248.  *** Reason for rollback ***  performance regression b/392290938  *** Original change description ***  Remove fastcall() from StarlarkCallable.java and Starlark.java  PiperOrigin-RevId: 723363896 Change-Id: Ic0fd4f5642b106d1e2d3843555f0ebc1fdf7b140
bazelbuild,bazel,0fd3ae4f66e7fb0e1449d1a736693a00b78a3439,https://github.com/bazelbuild/bazel/commit/0fd3ae4f66e7fb0e1449d1a736693a00b78a3439,Delete analyze-profile command  Not quite pulling its weight as a standalone command  and to the best of our knowledge it's not widely used  especially compared to the alternatives documented at https://bazel.build/advanced/performance/json-trace-profile.  PiperOrigin-RevId: 721383891 Change-Id: Ief393c8acdfb40cfe60437798440d95a6366f37b
bazelbuild,bazel,316de03a821e9ad2c19b13437090bb2c42c6816d,https://github.com/bazelbuild/bazel/commit/316de03a821e9ad2c19b13437090bb2c42c6816d,Remove positionalOnlyCall() override in StarlarkFunction - the ArgumentProcessor based default in StarlarkCallable should work perfectly now.  PiperOrigin-RevId: 720095256 Change-Id: I4a0dcaeb5d923e1cdd00e21324b758c7c7039589
bazelbuild,bazel,e66d61d542c3377f79641bc0c8dcde6e63a0c4db,https://github.com/bazelbuild/bazel/commit/e66d61d542c3377f79641bc0c8dcde6e63a0c4db,Fix potential race in `Spawn#getLocalResources()`  Both implementations performed two reads of the nullable instance variable  which could be reordered so that the first read returns a non-null value and the second read returns null.  Closes #24985.  PiperOrigin-RevId: 719418766 Change-Id: Ib01b0a86c7d0e61f3e2f14bc6520c375b456d086
bazelbuild,bazel,b704fdb18b8201ddfc017ab91b9d0a63892a4cd2,https://github.com/bazelbuild/bazel/commit/b704fdb18b8201ddfc017ab91b9d0a63892a4cd2,Introduce FileOpNodeMemoizingLookup for tracking file dependencies.  This change introduces `FileOpNodeMemoizingLookup`  a new class responsible for accurately determining the file dependencies of both analysis (configured target) and execution (action) SkyValues.  **Key Changes:**  *   **FileOpNodeMemoizingLookup:** This new class efficiently computes and caches file dependencies by performing a concurrent  memoized  and recursive traversal of Skyframe edges. It distinguishes between analysis and execution file dependencies  storing them separately within `FileOpNode` (previously `FileSystemOperationNode`).  *   **Refined `AbstractNestedFileOpNodes`:** The `AbstractNestedFileOpNodes` class (which is used to store a set of `FileOpNode`s) is now split into two subclasses: *   `NestedFileOpNodes`: Represents a set of analysis file dependencies *without* any source file dependencies. *   `NestedFileOpNodesWithSources`: Represents a set of analysis file dependencies that *includes* immediate source file dependencies.  This separation allows a more precise representation of dependencies  distinguishing between files depended on during analysis and those depended on during execution. Source dependencies  which correspond to `InputFileConfiguredTarget` instances  are declared during analysis but are only truly depended upon by the actions owned by the corresponding configured targets  not the configured targets themselves. This distinction optimizes storage by allowing both configured targets and actions to share the same underlying file dependency graph structure.  *   **Consolidated `FileOpNodeOrFuture`:** The `FileSystemOperationNode` class (and related classes) have been renamed to `FileOpNode` and moved into the newly added `FileOpNodeOrFuture` interface. This consolidation improves code readability by centralizing the hierarchical structure of file operation nodes.  *   **Renamed `FileOpNode`:** The old name  `FileSystemOperationNode`  was becoming cumbersome. The new name  `FileOpNode`  is more concise.  PiperOrigin-RevId: 717556933 Change-Id: I7376f1d5ffb30e520af0e15140d00159f2a78d6d
bazelbuild,bazel,cf701ebeab2565e910453cad8b1b950553596d83,https://github.com/bazelbuild/bazel/commit/cf701ebeab2565e910453cad8b1b950553596d83,Fix performance regression on builds that change a test configuration flag.  Instead of `BuildOptionsScopeFunction` requesting `PrecomputedValue.BASELINE_CONFIGURATION`  request it in `BuildConfigurationKeyProducer` only after we know that we truly need it. This way the dependency is only requested if scopes are being applied.  The performance concern still exists if starlark flag scoping is enabled.  PiperOrigin-RevId: 716785874 Change-Id: Iabd7c11440c70c909f111f1dcafafa3b98610be9
bazelbuild,bazel,33a6b9a8477f70b31441d99758ff3cede9d0c926,https://github.com/bazelbuild/bazel/commit/33a6b9a8477f70b31441d99758ff3cede9d0c926,Fix performance regression in CriticalPathComputer.  Previously  when actions are change pruned  we add their entire transitive closure to critical path graph. This could slow down incremental builds if the action graph is large and only a small portion of it is invalidated.  This CL improves that by only adding actions that are invalidated (including change pruned).  We achieve that by updating Skyframe to emit change pruning event. These event are only emitted for nodes that were invalidated by dirtiness check during incremental builds. So that the CriticalPathComputer now operates on a subset of action graph that only contains dirty actions. This is more correct and performant for incremental builds.  PiperOrigin-RevId: 716146196 Change-Id: I71add4240458b5d4087b1af81a41202f98fee947
bazelbuild,bazel,293be8a64f41d2aa5e2d5fc655d0fe0d33c700e0,https://github.com/bazelbuild/bazel/commit/293be8a64f41d2aa5e2d5fc655d0fe0d33c700e0,Reduce retained memory of `UnresolvedSymlinkAction`  The required conversion to `PathFragment` can be performed during execution  which avoids retaining a new `PathFragment` instance per action.  Closes #24831.  PiperOrigin-RevId: 713577830 Change-Id: I2781ea979a3c97abf5a519acd5d3d6a668dd1f77
bazelbuild,bazel,4e8ae1fd1fb547588ce058478da173348515a1b7,https://github.com/bazelbuild/bazel/commit/4e8ae1fd1fb547588ce058478da173348515a1b7,Improve ConfiguredTargetKeyValueSharingCodec performance.  Use sharing for its BuildConfigurationKey. Inlining it is expensive.  PiperOrigin-RevId: 703309002 Change-Id: Ie12a33f1e131612b068dbbdfd66997ab450f01af
bazelbuild,bazel,a298190ff22435f32dc3a3ddc6507ddc72e0e503,https://github.com/bazelbuild/bazel/commit/a298190ff22435f32dc3a3ddc6507ddc72e0e503,Reduce verbosity of BulkTransferException  BulkTransferException was sometimes overly verbose in the console  particularly when many transfers failed for the same reason.  This commit shortens and normalizes BulkTransferException messages to improve the efficiency of existing event deduplication  such as that performed by _RemoteExecutionService.report()_.  Normalization involves not mentioning individual digests in exception messages from ByteStreamUploader. Instead  we now wrap StatusRuntimeException directly in an IOException  which is a common pattern in the GrpcCacheClient.  Closes #23490.  PiperOrigin-RevId: 699081591 Change-Id: I1d02e19200e4a84b7c36813a1705fb039a1f60a9
bazelbuild,bazel,e8b2790c2904ef775f34b67e0070f1e9c2bbe330,https://github.com/bazelbuild/bazel/commit/e8b2790c2904ef775f34b67e0070f1e9c2bbe330,Make Bazel not crash if a runfiles tree is in a runfiles tree.  There was a test for this  but it didn't check for the exit code  only the error message and the message printed on the crash coincidentally contained the expected message.  My best understanding is that this was broken in unknown commit. The root cause of why that change was imperfect is probably the odd behavior that if one posts an error to the event handler in ActionExecutionContext  the action doesn't automatically fail.  RELNOTES: None. PiperOrigin-RevId: 697564109 Change-Id: I0847bd7f090af44ffd37880b4957755bca511ad1
bazelbuild,bazel,7a4416f114ee5c02fc159f44f71b57ee3e67d29d after it was rolled back due to a,This is a rollforward of https://github.com/bazelbuild/bazel/commit/7a4416f114ee5c02fc159f44f71b57ee3e67d29d after it was rolled back due to a,Add output of top level aspects to master log 
bazelbuild,bazel,1565bbb9d65f6ea93767904a3a4156440fd81ced,https://github.com/bazelbuild/bazel/commit/1565bbb9d65f6ea93767904a3a4156440fd81ced,
bazelbuild,bazel,e17b5f7b716c1238c5477fa0d708b932022ff45b,https://github.com/bazelbuild/bazel/commit/e17b5f7b716c1238c5477fa0d708b932022ff45b,Fold MetadataInjector into OutputMetadataStore.  The only awkwardness this causes is that THROWING_OUTPUT_METADATA_STORE_FOR_ACTIONFS is now much more boilerplate than its predecessor. I was considering just replacing that object with null since its job seems to be to always throw when called  which null would perfectly well accomplish  but the additional error messages seem to have some value.  RELNOTES: None. PiperOrigin-RevId: 691431466 Change-Id: Ib58eb5846c19071a8c0939ddd5af06f2fd3bee98
bazelbuild,bazel,a0a72265ea37270c110630f528127e18939f4016,https://github.com/bazelbuild/bazel/commit/a0a72265ea37270c110630f528127e18939f4016,Performance optimization for isSymbolicLink() calls on Windows  Current implementation for WindowsFileSystem.isSymbolicLink() makes unnecessary system calls which significantly affect the performance. Original code goes through the following: - AbstractFileSystemWithCustomStat.isSymbolicLink - WindowsFileSystem.stat - WindowsFileSystem.getIoFile - JavaIoFileSystem.getNioPath - Files.readAttributes - WindowsFileSystem.fileIsSymbolicLink - WindowsFileOperations.getLastChangeTime This implementation skips most of them.  Closes #24047.  PiperOrigin-RevId: 690964117 Change-Id: I2b407d9c69af62e770684d868d04e60d7ce1773e
bazelbuild,bazel,4f7c859f6dfb3402a1adfe9346228695caa1b2b2,https://github.com/bazelbuild/bazel/commit/4f7c859f6dfb3402a1adfe9346228695caa1b2b2,Add NestedFileSystemOperationNodes  canonical wire format and round-tripping.  Since NestedFileSystemOperationNodes are identified by a fingerprint of their serialized representation uses a custom  canonical wire format. As per https://protobuf.dev/programming-guides/serialization-not-canonical/  protos are not canonical.  Performs some renamings for consistency. * In some places renames Directory to Listing. This emphasizes that it's the listing that matters for invalidation. * For consistency  some places that were DirectoryListing are also renamed as Listing. * GetDependenciesResult is renamed to GetFileDependenciesResult for consistency.  PiperOrigin-RevId: 688712493 Change-Id: I0456a3d7ab0a8f4077d057d9ded3f1061fd85d11
bazelbuild,bazel,8f7ffcfd9c866396d6709a971100ed098171b302,https://github.com/bazelbuild/bazel/commit/8f7ffcfd9c866396d6709a971100ed098171b302,Add FileDependencySerializer.  Performs serialization of leaf-level FileValue and DirectoryListingValue dependencies using the schema defined in file_invalidation_data.proto.  The logical flow mirrors that of FileFunction.  This is intended for use during serialization  where there is a high degree of concurrency and many nodes may share the same file dependencies.  PiperOrigin-RevId: 683855840 Change-Id: I148cf394f65fd0a87018367927e03ddfc3ce186b
bazelbuild,bazel,fd67506b0c73eed53089de5df339e6f4e2d810de,https://github.com/bazelbuild/bazel/commit/fd67506b0c73eed53089de5df339e6f4e2d810de,Introduce `FilesetOutputTree`.  This is a mechanical refactoring to replace `ImmutableList<FilesetOutputSymlink>` with an extra abstraction layer called `FilesetOutputTree`.  In this change  `FilesetOutputTree` only contains `ImmutableList<FilesetOutputSymlink>`. In a subsequent change  we will also store a `boolean` indicating whether the fileset has any relative symlinks. If the fileset does not have any relative symlinks (expected to be a common case)  we can make some significant performance optimizations.  PiperOrigin-RevId: 681116734 Change-Id: If49f8acf15122f6287cd426cb36a081952424b84
bazelbuild,bazel,c72af54fcf692ae553071071adda9d29ffad3cdc,https://github.com/bazelbuild/bazel/commit/c72af54fcf692ae553071071adda9d29ffad3cdc,Use a LongAdder instead of an AtomicLong.  Unlikely to matter  but technically has better performance under contention.  PiperOrigin-RevId: 680917850 Change-Id: I1a26f01d9e092427634f725bd2211c272500aecc
bazelbuild,bazel,a716fdf5c03c918daad557a0bec9ace74400ec2f,https://github.com/bazelbuild/bazel/commit/a716fdf5c03c918daad557a0bec9ace74400ec2f,Make the cache in `PlatformMappingValue` strong instead of weak.  Clear the cache after each command. The overhead of a weak cache's online cleanup performed during lookups shows up in cpu profiles  and we don't need intra-command cleanup.  PiperOrigin-RevId: 680791811 Change-Id: Ibbd062baac40c4c4311c236cc211269a9a3f87b1
bazelbuild,bazel,31b412581eb0c564f98c57e6aa5aa4c4de63c771,https://github.com/bazelbuild/bazel/commit/31b412581eb0c564f98c57e6aa5aa4c4de63c771,Add rule class definitions to query proto output  As rules are starlarkified and migrated out of Bazel  tools cannot rely any longer on the (undocumented and deprecated) `info build-language` output to find the definitions of rule classes. Moreover  the name of the rule class is no longer unique: two different .bzl files might define different rule classes with the same name - and both might be used in the same package.  For starlarkified rules  tools could use `starlark_doc_extract` targets to get the rule class definition in proto form. However  this requires (1) knowing in advance which .bzl file to introspect (2) adding a `starlark_doc_extract` target to perform this introspection.  This is obviously not ideal when a tool needs to introspect targets without changing the state of a repo.  The solution is to add `starlark_doc_extract`-like rule class definitions to `query` command output. Since a RuleInfo proto (the rule class definition as output by starlark_doc_extract and Stardoc) is large and expensive to generate  we want to * only output it optionally  when --proto:rule_classes flag is set * only output each rule class definition once per output stream - in the first rule target having a given rule class key. * add a rule class key  unique per rule class definition (not just rule class name!)  so that later rule targets in the stream with the same rule class definition can be connected with the rule class definition provided in the first target.  RELNOTES: If --proto:rule_classes flag is enabled  query proto output will contain rule class definitions in Stardoc proto format. PiperOrigin-RevId: 680642590 Change-Id: Ibdeb9c8acb7368b510f62d945c361a86cdb6f447
bazelbuild,bazel,af38f3bcdbd52457b42c8fcd1c75bb2ed2fa8494,https://github.com/bazelbuild/bazel/commit/af38f3bcdbd52457b42c8fcd1c75bb2ed2fa8494,Add execution info to action conflict error message  The execution info is part of the action  so including it in the error message makes the reported error easier to diagnose when two targets performing the same action have different execution info.  PiperOrigin-RevId: 680633760 Change-Id: I6a93bf3584b5306456b712c84351145939c119b8
bazelbuild,bazel,33a2bb79c6d1b03adb3b9c46c658fe18438cc5d1,https://github.com/bazelbuild/bazel/commit/33a2bb79c6d1b03adb3b9c46c658fe18438cc5d1,Merge TargetDefinitionContext into TargetRegistrationEnvironment  The only thing in TargetDefinitionContext is NameConflictException and its subclass. Moving it into TargetRegistrationEnvironment matches how the name conflict checking is now performed there anyway.  A subsequent CL may reintroduce TargetDefinitionContext as a branched file from Package.java  factoring with it a number of methods and fields from Package.Builder.  Work toward #19922.  PiperOrigin-RevId: 678410292 Change-Id: I8e21076a5736a10d8d7053bf39e650e5c7b86a6e
bazelbuild,bazel,33aca3fe8803ca55838af5467fa801d03e28ffd8,https://github.com/bazelbuild/bazel/commit/33aca3fe8803ca55838af5467fa801d03e28ffd8,Implement core garbage collection logic for the disk cache.  The garbage collection policy is defined by a maximum target size and a maximum age of individual cache entries  both of which may be simultaneously provided. I/O operations are parallelized to improve performance for large caches or slow filesystems.  PiperOrigin-RevId: 677860078 Change-Id: Ib342ad5e80ef4ef4af237aae243a300d13caaa06
bazelbuild,bazel,9f0a47101ad71c6b9cc018feb99fbb8e987a22c1,https://github.com/bazelbuild/bazel/commit/9f0a47101ad71c6b9cc018feb99fbb8e987a22c1,Performance improvements in `BuildConfigurationKeyMapProducer`.  PiperOrigin-RevId: 673396624 Change-Id: Iafd228f0e626b24da4de0087754c60862e5d27a6
bazelbuild,bazel,56fc0998e21dbc4fd38ae8aea036e9c2e3d25bfe,https://github.com/bazelbuild/bazel/commit/56fc0998e21dbc4fd38ae8aea036e9c2e3d25bfe,Perform frontier serialization after the build command completes  if enabled.  This moves the callsite from `DumpCommand` into an `afterCommand` hook for the serialization module  which triggers with a non-empty `--serialize_frontier_profile` (now) build option.  PiperOrigin-RevId: 671227197 Change-Id: I8665d18069393102e59db13316f27d468f48075d
bazelbuild,bazel,6fabb1fc6869a204373e5ee0adde696a659415dd,https://github.com/bazelbuild/bazel/commit/6fabb1fc6869a204373e5ee0adde696a659415dd,No longer eagerly fetch labels in repo rule attributes  Eager fetching of all labels listed in repo rule attributes was introduced as a performance optimization to avoid costly restarts.  Now that restarts are gone by default  this is no longer a benefit as it can cause unnecessary fetches and also create cycles where there wouldn't be any without this behavior (e.g. when two repos write each others labels into a file without resolving them).  Work towards #19055  Closes #23371.  PiperOrigin-RevId: 665744319 Change-Id: Ia27f207793a2da3fb8e37743b328483f9d45192c
bazelbuild,bazel,37df41df54a68e05b0f6c7dafadf1ae745eba968,https://github.com/bazelbuild/bazel/commit/37df41df54a68e05b0f6c7dafadf1ae745eba968,Implements a generic request-batching mechanism.  Purpose: Optimize interactions with services that offer batch request interfaces  significantly reducing overhead compared to individual unary requests. (For example  sending one batched RPC instead of 1000 individual requests)  User Experience: Clients continue to use a simple unary interface  with batching handled seamlessly in the background.  Implementation Overview:  The solution queues requests and distributes them to workers. Each worker asynchronously performs a cycle of:  1. Gathering queued requests. 2. Sending them as a batch to the service. 3. Awaiting the batch response.  Performance Impact: Preliminary testing on a frontier serialization dump operation shows a roughly 50x speed improvement compared to a non-batching approach.  PiperOrigin-RevId: 664822660 Change-Id: I5efa6f33ff1f0704c771f2ba1fc13150ed7ed253
bazelbuild,bazel,ffdf41acfd4d82bdfd3f855b8f4aa3840d918eba,https://github.com/bazelbuild/bazel/commit/ffdf41acfd4d82bdfd3f855b8f4aa3840d918eba,Change rewinding's `ArtifactNestedSetKey` search strategy to improve worst-case performance.  Instead of doing a search for every (lost artifact  `ArtifactNestedSetKey`) pair  do a single "bulk" search looking for all lost artifacts. When there are many lost artifacts  this is much more efficient. Additionally  we can prune visitations of nodes shared by multiple `NestedSet`s.  PiperOrigin-RevId: 663981449 Change-Id: Ic06f65e4f1dc9ca16a9bdd4a9c520b3ecd44be57
bazelbuild,bazel,242adde185ba10799d03be228466943750e1d5e9,https://github.com/bazelbuild/bazel/commit/242adde185ba10799d03be228466943750e1d5e9,Report errors for all files with Error Prone diagnostics  not just the first one  This removes logic that was skipping Error Prone of compilations with errors. The intent was to skip analyzing compilations with javac errors  but it also meant that if Error Prone reported an error analyzing a file all subsequent files were skipped.  As of unknown commit  `ErrorProneAnalyzer` handles skipping analysis of compilations with errors (and distinguishing between javac and Error Prone errors)  so it's safe for JavaBuilder to unconditionally run ErrorProne and let it decided whether or not to proceed.  This change also removes some bookkeeping about the number of 'flow' events javac processes and skips  which was used for testing. Since it no longer skips flow events  there isn't a good way to perform the assertion that was done previously. Since that logic was added we have better end to end integration tests for the Error Prone JavaBuilder integration  which reduces the risk of accidentally turning Error Prone off.  PiperOrigin-RevId: 641283846 Change-Id: I539f0713879835f031cb13a015b45dfa90e20c24
bazelbuild,bazel,642b571962ca9e6cf40ce94a8f53b8618f12080a,https://github.com/bazelbuild/bazel/commit/642b571962ca9e6cf40ce94a8f53b8618f12080a,Disable SkyframeStats collection unless specifically requested by flag. Removes the logic to limit on the number of reported types  if you're requesting the stats already you just get all of them.  SkyframeStats generation can be very expensive on large build graphs and can add significantly to latency when completing a build.  Disabling it entirely unless specifically requested mitigates that performance hit  unless the user specifically opts into the additional cost.  PiperOrigin-RevId: 640358760 Change-Id: I8bfa0cfaa5dcb019c541d240687c6d8a9b61aab8
bazelbuild,bazel,1c0135cf88bf16d9ffddf8f687ded797f07960b1,https://github.com/bazelbuild/bazel/commit/1c0135cf88bf16d9ffddf8f687ded797f07960b1,Add option to track sandbox stashes in memory  This change introduces the flag `--experimental_inmemory_sandbox_stashes` to track in memory the contents of the stashes stored with `--reuse_sandbox_directories=true`  With the old behavior Bazel has to perform a lot of I/O to read the contents of each stash before reusing it in a new action. Essentially  it checks  every directory and subdirectory in the stashed sandbox to find out which files are different to the inputs of the new action about to be executed.  With in-memory stashes we associate each stash to the symlinks that needed to be created for that action. We also store the time at which these symlinks were created. In a background thread after the action has finished executing we stat every directory and for the ones that have changed (this should be rare) we update the contents. Because we only read the contents of the directories that have changed we do much less I/O than before.  If an action purposefully changes a sandbox symlink in-place  this won't be detected by statting the directory. I don't know any use case for this since the symlink itself is an implementation detail to achieve sandboxing. For this reason   manipulation of sandbox symlinks in-place is not supported.  Depending on the build this change might have a significant effect on memory. It should generally improve wall time further in builds where `--reuse_sandbox_directories` already improved wall time.  This change also introduces a minor optimization which is to associate each stash with the target that it was originally created for. When a new action wants to reuse a stash and there is more than one available  it will take the stash whose target is closest to its own. This is done with the assumption that targets that are closer together in the graph will have more inputs in common.  Fixes #22309 .  Closes #22559.  PiperOrigin-RevId: 640142481 Change-Id: Iece2d718df47f403e2fe91c1bd887604eceee8ee
bazelbuild,bazel,81a0294cca18d25c5089776c87fee1ced79453cf,https://github.com/bazelbuild/bazel/commit/81a0294cca18d25c5089776c87fee1ced79453cf,Refactor `ImportantOutputHandler` to accept unexpanded artifacts.  The `ImportantOutputHandler` is now responsible for performing expansion of directory artifacts (tree artifacts and filesets) using the provided `ArtifactExpander`.  PiperOrigin-RevId: 639867123 Change-Id: I831d8794e6d8dda1184253af72debd8206a9badd
libgdx,libgdx,fa96aafbc6b3205ccfd51ad515d0bc358b787377,https://github.com/libgdx/libgdx/commit/fa96aafbc6b3205ccfd51ad515d0bc358b787377,Add option to use Box2D native ContactFilter for performance optimization (#7578)
libgdx,libgdx,3b2ae2b4a304fcf27d5449584197ec2da78f04c3,https://github.com/libgdx/libgdx/commit/3b2ae2b4a304fcf27d5449584197ec2da78f04c3,Optmize JNI performance of Box2D Body methods (#7579)
libgdx,libgdx,bfe255a2727377b910be20af48d40867c588a8a3,https://github.com/libgdx/libgdx/commit/bfe255a2727377b910be20af48d40867c588a8a3,Optimization for SpriteBatch when running non VertexArray VertexDataModes. (GL30 default) (#7346)  * SpriteBatch optimizations to preupload full indices data for the size of the batch to prevent doing this each frame. Increase the performance of default SpriteBatch in gl30 where this performs worse than gl2 vertex array  * Apply formatter  * Add message for waiting for data  * Changelog  ---------  Co-authored-by: GitHub Action <action@github.com>
jenkinsci,jenkins,f45ba02af86d07caa3928010e384015babc2ccdb,https://github.com/jenkinsci/jenkins/commit/f45ba02af86d07caa3928010e384015babc2ccdb,[JENKINS-33704] Limit scope of `Jenkins#updateComputerList` to improve performance at scale (#10494)  * [JENKINS-33704] Limit scope of `Jenkins#updateComputerList`  Along the same line as #5882  but for computers  Adding/updating/removing a node to the system was causing all nodes retention strategies to be checked. If you have a lot of nodes  this quickly adds up to an extreme amount of checks.  This change makes the calls local  by only affecting the related nodes.  * Clarify variable name  * Fix logic  * Avoid dealing with `null`
jenkinsci,jenkins,a6f723cef725ea66c992f286516e872857dc6d9d,https://github.com/jenkinsci/jenkins/commit/a6f723cef725ea66c992f286516e872857dc6d9d,Introduce `SaveableListener#onDeleted` (#9743)  * Introduce SaveableListener#onDeleted  Usually `Saveable` objects are written  but it can happen on occasion that they get deleted  and it wasn't generating an event for every case.  This provides a more fine-grained event that can be handled by implemented listeners.  In my case  I have a use case in CloudBees CI where I need to clear a cache entry when a Saveable gets deleted from disk.  * Spotless  * Explicitly test that the user that has performed the change can be obtained from the Saveable listener.
Anuken,Mindustry,ab0a47a837628f8bb70802fd7ef6b581005e5113,https://github.com/Anuken/Mindustry/commit/ab0a47a837628f8bb70802fd7ef6b581005e5113,Conveyor progress (#10620)  * sensor progressssssssssssssssssssssssss  * for duct aswell  * better setprop for conveyors  * slight performance improvement i assume  * * import  * check if len is 0
Anuken,Mindustry,a5de1317117e6e9822ebc1e097faf21794532758,https://github.com/Anuken/Mindustry/commit/a5de1317117e6e9822ebc1e097faf21794532758,Trapezoidal jumps  I haven't checked this yet  but it seems like it also reduces the complexity (runtime) from ≈O(n^2) to ≈O(k^2 + n) (n = number of jump curves) (k = number of unique destinations)  The worst case  that is k = n  is still O(n^2) but generally it's much more (scalably) performant.  Thanks Arkanic for suggesting this! https://discord.com/channels/391020510269669376/663854113418641429/1256161815503704104  Co-authored-by: Arkanic <50847107+Arkanic@users.noreply.github.com>
OpenAPITools,openapi-generator,40894382fc9fe959f3beacd20cfd6421eaf840b0,https://github.com/OpenAPITools/openapi-generator/commit/40894382fc9fe959f3beacd20cfd6421eaf840b0,[php-symfony] fix handling of endpoints with "text/plain" or "image/png" response type (#21261)  * [php-symfony] Never return 406 when user accepts */*  When a query has header "Accept" set to "*/*" it means it accepts everything. It is hence weird to return a 406. This patch ensures it does not occur: when the query accepts everything then we take any produced type.  This fixes #13334. This also partly makes the open PR #15560 obsolete (or at least  it provides a workaround)  * [php-symfony] Don't crash at runtime on null convertFormat  $this->convertFormat may return "null". When it's the case we end up calling  ...->serialize($data  null);  but this crashes at runtime because that serialize method declares that the 2nd parameter is of type "string" (so null is not accepted).  With this patch we avoid having an error 500. Instead we return something that makes perfect sense when the OpenApi specification declares a content of type "text/plain" and that the returned value is for instance a string  an int  or a boolean.  * [php Symfony] fix return type for non json/xml api  This fixes the generated returned type of controller methods for endpoint with a response declared like  content: text/plain: schema: type: <boolean|string|integer|number>  or for  content: image/png: schema: type: string format: binary  Without this commit the generated method *had to* return a value that matched "array|object|null"  which does not work in this case. This commit makes it possible to return the proper type.
OpenAPITools,openapi-generator,f9f5af5ed99e58a68ee7c55107942d670b962a99,https://github.com/OpenAPITools/openapi-generator/commit/f9f5af5ed99e58a68ee7c55107942d670b962a99,[JAVA][FEIGN] Removing hardcoded HTTP Client which is causing performance issues (#21085)  * [JAVA][FEIGN] Removing hardcoded HTTP Client  Fixing performance issues  * Updating samples for Java Feign performance betterment changes  * added APIClient.java for feign-hc5
redisson,redisson,67f03e214eb0e04b5cdc92d0473ac37240b30dc3,https://github.com/redisson/redisson/commit/67f03e214eb0e04b5cdc92d0473ac37240b30dc3,Improvement - performance optimization for Apache Tomcat Session management
google,gson,d4d6744f8c225709e1adcc38eaae4e968c476125,https://github.com/google/gson/commit/d4d6744f8c225709e1adcc38eaae4e968c476125,Improve `JsonWriter#value(Number)` performance (#2702)  * Improve `JsonWriter#value(Number)` performance  For JDK number types other than `Float` and `Double` there is no need to check if the number string is NaN or Infinity.  * Refactor boolean expression
google,gson,99cc4cb11f73a6d672aa6381013d651b7921e00f,https://github.com/google/gson/commit/99cc4cb11f73a6d672aa6381013d651b7921e00f,Migrate all tests to Truth & use `assertThrows` (#2687)  * Use `assertThrows` for expected exceptions in tests  * Use Truth for unit tests in `proto` and `extras` module  * Perform assertions on some currently unused test variables  * Improve tests  * Improve non-finite floating point tests  Because there is only `Gson.toJson(Object)` (and no primitive specific overloads) the tests were previously testing the adapter for the boxed object  e.g. Double  twice.  * Improve tests  * Remove explicit type arguments from tests where they can be inferred  This works because unlike the main sources the tests are compiled with Java 11.
conductor-oss,conductor,b4e7442d5c8393e19af456ee164c2b2aaebb3566,https://github.com/conductor-oss/conductor/commit/b4e7442d5c8393e19af456ee164c2b2aaebb3566,Address postgresql popmessage query not  limiting update to messages on the specific queue and only non popped messages. Switching to CTE for - better performance at scale - more conistent query planning at scale
yuliskov,SmartTube,274c583d04d44af7868ea0bb00eeec12f94258c5,https://github.com/yuliskov/SmartTube/commit/274c583d04d44af7868ea0bb00eeec12f94258c5,atv channels: performance tweaks
iBotPeaches,Apktool,c41a5272140f51d41c0de015e6f0103498266781,https://github.com/iBotPeaches/Apktool/commit/c41a5272140f51d41c0de015e6f0103498266781,perf: improve doNotCompress lookups (#3872)  We passed doNotCompress to zipDir as a List. The longer the list  the slower the lookups. Convert to a Set for buildApkFile and reuse for all zipDir calls.
iBotPeaches,Apktool,5a1bd06031e8df6821b15194fd6be127d44947cd,https://github.com/iBotPeaches/Apktool/commit/5a1bd06031e8df6821b15194fd6be127d44947cd,refactor: convert meta classes to pojo  rework ARSC & rework cli parsing (#3868)  * refactor: convert meta classes to pojo  Wanted to make that change back when I converted Config class to POJO  finishing the job now. This will guarantee safe access to all meta classes (moved from brut.androlib.apk to brut.androlib.meta to make more sense). All non-primitive fields are guaranteed to never be null. Outputs clean apktool.yml without useless fields that don't affect the rebuild procedure. Still backwards compatible  just cleaner output  especially for JARs.  Cleaned some logic in a few classes  especially related to updateApkInfo and loading of libraries/frameworks. It was a mess that was difficult to make sense of.  Test issue1235 removed  it's a aapt1 duplicate of aapt2's issue2328/debuggable-false.  Misc style tweaks for consistency.  Tested on a ROM decompile/recompile  no regression detected.  * fix accessors  * fix flaw in net sec conf  * refactor ResConfigFlags and isInvalid logic  * git windows case issue (1)  * git windows case issue (2)  * style  * checker  wake up  * reorganize some classes and minimize reading signed data from arsc  Move: * brut.util.AaptManager -> brut.androlib.AaptManager * brut.androlib.BackgroundWorker -> brut.util.BackgroundWorker * brut.androlib.meta.Yaml* -> brut.yaml  Removed useless "throws" on signatures of Yaml functions  none of the Yaml functions throw any managed exceptions  it's a perfectly generic set of classes that deserves a dedicated module.  Most of data in arsc is encoded as unsigned integers  so normally we'd use readUnsignedByte/Short where possible (both return int) to ensure the values are never negative. Java doesn't have any valid unsigned counterpart for readInt  so we'll have to live with it  it works for now. Java was never good for dealing with raw binary data.  * remove useless semicolon  * cases ordered by value  * comment correction  * unneeded import  * use readBytes for clarity  * config options larping as flags  * config options larping as flags (2)  * refactor: command line parsing and Config  Make command line commands and options great again. Each command has its own valid options  verified properly like any good CLI application. Instead of checking hasOption(shortName/longName)  we do hasOption(Option) on the option object  which maintains sanity and very easy to manage. "allOptions" collection is no longer needed.
iBotPeaches,Apktool,2b2efb1ad485e51f152f6ecea00cac8d51dccb11,https://github.com/iBotPeaches/Apktool/commit/2b2efb1ad485e51f152f6ecea00cac8d51dccb11,refactor: migrate tests to aapt2 + related fixes (#3865)  * refactor: migrate tests to aapt2 + related fixes  All tests have been migrated to aapt2  no tests left for aapt1  as we phase out support.  Fixed shared library support using a custom option '-l' or '-lib'  that can be specified many times for multiple libs. On decoding  the dynamic_ref_table is used to determine which shared libraries were used  and when done they are stored in ascending order in apktool.yml as a usesLibrary list. The order within usesLibrary determines the package IDs that will be dynamically assigned to shared resources on recompile. Works perfectly with aapt2  doesn't seem to work with aapt1 since it's garbage and never worked properly. SharedLibraryTest was updated accordingly.  "isFrameworkApk" and "sharedLibrary" flags are obsolete. We decide aapt options to use by package ID.  values-godzilla dir omitted for now  currently aapt2 doesn't support it properly (needs more binary patching).  drawableXhdpiTest commented out for now until we decide what to do with those old Samsung and HTC 9patch drawables. aapt2 stores them with ".9" or ".r" appended to resource name  and the decoded files are named ".9.9.qmg" or ".r.r.9.png".  Some variable name tweaks for consistency.  * organize tests logically to minimize imports  * Use camelCase for test methods  * remove old proprietary drawables and restore drawableXhdpiTest
apache,rocketmq,ae7179d75e11f469d68be05fbf556fde42c8a795,https://github.com/apache/rocketmq/commit/ae7179d75e11f469d68be05fbf556fde42c8a795,[ISSUE #8765] fix low performance of delay message when enable rocksdb consume queue (#8766)  * #7538 fix wrong cachedMsgSize if msg body is changed in consumer callback * [ISSUE #8765] fix low performance of delay message when enable rocksdb consume queue * remove prefetch
apache,rocketmq,95b88ff8fcfecbd1942e1a35460f7417ee620673,https://github.com/apache/rocketmq/commit/95b88ff8fcfecbd1942e1a35460f7417ee620673,[ISSUE #8442][RIP-70-3] Extract adaptive lock mechanism (#8663)  * extract the adaptive lock  * extract the adaptive lock  * feat(): perfect the adaptive lock  * feat(): perfect the adaptive lock  * Optimized code type  * Optimized code type  * Optimized code type  * fix fail test  * Optimize the adaptive locking mechanism logic  * Optimize the adaptive locking mechanism logic  * feat:Adaptive locking mechanism adjustment  * feat:Adaptive locking mechanism adjustment  * feat:Adaptive locking mechanism adjustment  * Optimize the adaptive locking mechanism logic  * Optimize the adaptive locking mechanism logic  * Optimize the adaptive locking mechanism logic  * feat:Supports the hot activation of ABS locks  * feat:Supports the hot activation of ABS locks  * feat:Supports the hot activation of ABS locks  * feat:Supports the hot activation of ABS locks  * Optimize code style  * Optimize code style  * Optimize code style  * Optimize code style  * Optimize code style  * Optimize code style  * Updated the locking mechanism name  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * delete unused import  * Optimize the logic of switching to spin locks  * Revert "Optimize the logic of switching to spin locks"  This reverts commit 1d7bac5c2fea0531af01d4c57c843084ba4fea61.  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimize the logic of switching to spin locks  * Optimized locking logic  * Optimized locking logic  * Optimized locking logic  * fix test  * fix test  * fix test  * fix test  * Optimize code style  * Optimize code style  * fix test  * fix test  * optimize client rebalancing logic  ---------  Co-authored-by: wanghuaiyuan <wanghuaiyuan@xiaomi.com>
apache,rocketmq,4f5f705f16faeb7d491b25679d1c100f38264bb9,https://github.com/apache/rocketmq/commit/4f5f705f16faeb7d491b25679d1c100f38264bb9,[ISSUE #8780] Implement asynchronous storage of ack/ck messages in pop consume to enhance performance (#8727)  * Pop consume asynchronization  * Pass UTs and ITs  * Pass the checkstyle  * Fix LocalGrpcIT can not pass  * Fix the UT can not pass  * Simplify duplicate methods in EscapeBridge
apache,rocketmq,3aa5d1936f1162afefccdf03fcc55bf0f7ee642c,https://github.com/apache/rocketmq/commit/3aa5d1936f1162afefccdf03fcc55bf0f7ee642c,Adjust the default value of ackMessageThreadPoolNums to 16 to prevent performance bottlenecks during high traffic. (#8337)
elunez,eladmin,31b033afe89356950c2cf19672047455b99a8d1b,https://github.com/elunez/eladmin/commit/31b033afe89356950c2cf19672047455b99a8d1b,perf: 优化用户缓存管理，统一转换为小写 close https://github.com/elunez/eladmin/issues/866
elunez,eladmin,db63c953d49e100b38f65d13b3fc17ead5a831ec,https://github.com/elunez/eladmin/commit/db63c953d49e100b38f65d13b3fc17ead5a831ec,perf: 添加权限检查，优化角色缓存及命名
elunez,eladmin,0a91748fd246e566d09beac1b9593dcd532e17c8,https://github.com/elunez/eladmin/commit/0a91748fd246e566d09beac1b9593dcd532e17c8,perf: 优化Token生成
elunez,eladmin,fb422c6a9a4e9a41d1745ad935b245ca74bdda5f,https://github.com/elunez/eladmin/commit/fb422c6a9a4e9a41d1745ad935b245ca74bdda5f,perf: 优化系统日志参数获取，优化在线用户Token管理，优化SQl日志打印
openjdk,jdk,d9b6e4b13200684b69a161e288b9883ff0d96bec,https://github.com/openjdk/jdk/commit/d9b6e4b13200684b69a161e288b9883ff0d96bec,8352642: Set zipinfo-time=false when constructing zipfs FileSystem in com.sun.tools.javac.file.JavacFileManager$ArchiveContainer for better performance  Reviewed-by: liach  jpai  jlahoda  lancea
openjdk,jdk,84458ec18ce33295636f7b26b8e3ff25ecb349f2,https://github.com/openjdk/jdk/commit/84458ec18ce33295636f7b26b8e3ff25ecb349f2,8353013: java.net.URI.create(String) may have low performance to scan the host/domain name from URI string when the hostname starts with number  Reviewed-by: michaelm  xpeng
openjdk,jdk,d684867066edb886bc444c864ef9db3eff318c34,https://github.com/openjdk/jdk/commit/d684867066edb886bc444c864ef9db3eff318c34,8346230: [perf] scalability issue for the specjvm2008::xml.transform workload  Reviewed-by: joehw  jbhateja
openjdk,jdk,5481021ee64fd457279ea7083be0f977c7ce3e3c,https://github.com/openjdk/jdk/commit/5481021ee64fd457279ea7083be0f977c7ce3e3c,8321591: (fs) Improve String -> Path conversion performance (win)  Reviewed-by: alanb
openjdk,jdk,8b0602dbed2f7ced190ec81753defab8a4bc316d,https://github.com/openjdk/jdk/commit/8b0602dbed2f7ced190ec81753defab8a4bc316d,8319447: Improve performance of delayed task handling  Reviewed-by: vklang  alanb
openjdk,jdk,bbd5b174c50346152a624317b6bd76ec48f7e551,https://github.com/openjdk/jdk/commit/bbd5b174c50346152a624317b6bd76ec48f7e551,8339280: jarsigner -verify performs cross-checking between CEN and LOC  Reviewed-by: mullan  weijun  lancea
openjdk,jdk,b499c827a512fb209a806d95b97df0f5932a29c0,https://github.com/openjdk/jdk/commit/b499c827a512fb209a806d95b97df0f5932a29c0,8349383: (fs) FileTreeWalker.next() superfluous null check of visit() return value  Reviewed-by: djelinski
openjdk,jdk,250ff86dc86f73dbf7c944d9b5a792c4bdfeef0d,https://github.com/openjdk/jdk/commit/250ff86dc86f73dbf7c944d9b5a792c4bdfeef0d,8349000: Performance improvement for Currency.isPastCutoverDate(String)  Reviewed-by: naoto  aturbanov
openjdk,jdk,10d08dbc81aa14499410f0a7a64d0b3243b660f1,https://github.com/openjdk/jdk/commit/10d08dbc81aa14499410f0a7a64d0b3243b660f1,8346142: [perf] scalability issue for the specjvm2008::xml.validation workload  Reviewed-by: joehw
openjdk,jdk,9a60f4457bb56d0f5039a97e6b943e62a8a2c3ee,https://github.com/openjdk/jdk/commit/9a60f4457bb56d0f5039a97e6b943e62a8a2c3ee,8345668: ZoneOffset.ofTotalSeconds performance regression  Reviewed-by: rriggs  aturbanov
openjdk,jdk,06c44dd568d91e1bd68f60fd3e57abcbe97e5dca,https://github.com/openjdk/jdk/commit/06c44dd568d91e1bd68f60fd3e57abcbe97e5dca,8345465: Fix performance regression on x64 after JDK-8345120  Reviewed-by: mcimadamore
openjdk,jdk,5958463cadb04560ec85d9af972255bfe6dcc2f2,https://github.com/openjdk/jdk/commit/5958463cadb04560ec85d9af972255bfe6dcc2f2,8343377: Performance regression in reflective invocation of native methods  Reviewed-by: mchung
openjdk,jdk,d49f21043b84ebcc8b9176de3a84621ca7bca8fb,https://github.com/openjdk/jdk/commit/d49f21043b84ebcc8b9176de3a84621ca7bca8fb,8342040: Further improve entry lookup performance for multi-release JARs  Co-authored-by: Claes Redestad <redestad@openjdk.org> Reviewed-by: redestad
openjdk,jdk,83dcb02d776448aa04f3f41df489bd4355443a4d,https://github.com/openjdk/jdk/commit/83dcb02d776448aa04f3f41df489bd4355443a4d,8340079: Modify rearrange/selectFrom Vector API methods to perform wrapIndexes instead of checkIndexes  Reviewed-by: jbhateja  psandoz
openjdk,jdk,81ff91ef27a6a856ae2c453a9a9b8333b91da3ab,https://github.com/openjdk/jdk/commit/81ff91ef27a6a856ae2c453a9a9b8333b91da3ab,8339531: Improve performance of MemorySegment::mismatch  Reviewed-by: mcimadamore
openjdk,jdk,6be927260a84b1d7542167e526ff41f7dc26cab0,https://github.com/openjdk/jdk/commit/6be927260a84b1d7542167e526ff41f7dc26cab0,8338591: Improve performance of MemorySegment::copy  Reviewed-by: mcimadamore
openjdk,jdk,7a418fc07464fe359a0b45b6d797c65c573770cb,https://github.com/openjdk/jdk/commit/7a418fc07464fe359a0b45b6d797c65c573770cb,8338967: Improve performance for MemorySegment::fill  Reviewed-by: mcimadamore  psandoz
openjdk,jdk,ab8071d28027ecbf5e8984c30b35fa1c2d934de7,https://github.com/openjdk/jdk/commit/ab8071d28027ecbf5e8984c30b35fa1c2d934de7,8338146: Improve Exchanger performance with VirtualThreads  Reviewed-by: alanb
openjdk,jdk,75bea280b9adb6dac9fefafbb3f4b212f100fbb5,https://github.com/openjdk/jdk/commit/75bea280b9adb6dac9fefafbb3f4b212f100fbb5,8333867: SHA3 performance can be improved  Reviewed-by: kvn  valeriep
openjdk,jdk,a941397327972f130e683167a1b429f17603df46,https://github.com/openjdk/jdk/commit/a941397327972f130e683167a1b429f17603df46,8329031: CPUID feature detection for Advanced Performance Extensions (Intel® APX)  Reviewed-by: sviswanathan  kvn
openjdk,jdk,d826127970bd2ae8bf4cacc3c55634dc5af307c4,https://github.com/openjdk/jdk/commit/d826127970bd2ae8bf4cacc3c55634dc5af307c4,8333462: Performance regression of new DecimalFormat() when compare to jdk11  Reviewed-by: liach  naoto  jlu
oracle,graal,323edb21d56e6f5efd844ba1f78d2cb750daa5b6,https://github.com/oracle/graal/commit/323edb21d56e6f5efd844ba1f78d2cb750daa5b6,Make sure truffle context is entered when performing operations that require so
oracle,graal,4c4677ecfe9354cbc6764910fc22b70576264718,https://github.com/oracle/graal/commit/4c4677ecfe9354cbc6764910fc22b70576264718,Implement JSObject field accesses using @JS  Every access generates a method call to a dynamically generated method that is (implicitly) annotated with @JS(...) to perform the access on the JS side.  This replaces the more clunky ForeignCall-based implementation that emitted all the necessary conversion/coercion code in-line at every access.
oracle,graal,66b50d5943e742a7f217710941fd718d5bacff48,https://github.com/oracle/graal/commit/66b50d5943e742a7f217710941fd718d5bacff48,[GR-64745] Adapt JDK-8348638: Performance regression in Math.tanh  PullRequest: graal/20726
oracle,graal,db9fde6c18ace56bbb05b5612096db70ceb5c23f,https://github.com/oracle/graal/commit/db9fde6c18ace56bbb05b5612096db70ceb5c23f,Adapt JDK-8348638: Performance regression in Math.tanh
oracle,graal,72c4ce0bc7574825ddf51df9024fe8ab4e5c3a5d,https://github.com/oracle/graal/commit/72c4ce0bc7574825ddf51df9024fe8ab4e5c3a5d,[GR-64420] Use initial system properties for jvmstat performance counters.  PullRequest: graal/20587
oracle,graal,e3f68dfa84f63a458a6f9bcc67ed95063a30b772,https://github.com/oracle/graal/commit/e3f68dfa84f63a458a6f9bcc67ed95063a30b772,Use initial system properties for jvmstat performance counters.
oracle,graal,0a5253bd7aad2f78489fcf16e7276246fd26a7e7,https://github.com/oracle/graal/commit/0a5253bd7aad2f78489fcf16e7276246fd26a7e7,[JDK-8350376] Adapt JDK-8319447: Improve performance of delayed task handling  PullRequest: graal/20121
oracle,graal,74fbcaf69b17c4da76bcde126abd2596a19d0e30,https://github.com/oracle/graal/commit/74fbcaf69b17c4da76bcde126abd2596a19d0e30,svm: adopt "JDK-8319447: Improve performance of delayed task handling"
oracle,graal,25391bbc98a27313ed31eef1028c6148dc586236,https://github.com/oracle/graal/commit/25391bbc98a27313ed31eef1028c6148dc586236,Perform glob validation in GlobUtils without constructing a trie
oracle,graal,b8693d33f51ca09e60c04a3dcd0e90a12bf6e3b6,https://github.com/oracle/graal/commit/b8693d33f51ca09e60c04a3dcd0e90a12bf6e3b6,[GR-50017] TruffleStrings: internal refactoring for better performance in statically compiled code.  PullRequest: graal/19999
oracle,graal,ff566b6fb87f685ccd3561c75508ba819d29a35b,https://github.com/oracle/graal/commit/ff566b6fb87f685ccd3561c75508ba819d29a35b,[GR-62951] Add Truffle boundary for NFI Panama downcall  * The downcall MethodHandle is in general not safe to PE  looking at compiler graphs. * It was also caught by the Blocklisted methods check. * However performance significantly suffers with just `@TruffleBoundary`  the default. * Performance is the same with `@TruffleBoundary(allowInlining = true)`.
oracle,graal,cdb038c687694c9c86d88ae67de9ba91c1e0e0f6,https://github.com/oracle/graal/commit/cdb038c687694c9c86d88ae67de9ba91c1e0e0f6,[GR-61853] Fix perf regression in Polyglot.castWithGenerics.  PullRequest: graal/20159
oracle,graal,8c14da3ed5e5b83a6a7a6eb534955045b9d9a33a,https://github.com/oracle/graal/commit/8c14da3ed5e5b83a6a7a6eb534955045b9d9a33a,revert change to limit in cached specialization due to perf regression
oracle,graal,18cba17dc74246f8846c9e1c92552b113132aa92,https://github.com/oracle/graal/commit/18cba17dc74246f8846c9e1c92552b113132aa92,Fix TruffleSafepoint.poll(...) must not fail if performed on unrelated threads while thread local actions are performed on a different thread. (Fixes GR-61393)
oracle,graal,eae546c63aee4d33e93f241151668d4d40adab71,https://github.com/oracle/graal/commit/eae546c63aee4d33e93f241151668d4d40adab71,Perform bounds checks on bulk memory operations  Now  all memory operations are guarded by bounds checks and don't need to be guarded at the call site.  Out-of-bounds memory accesses no longer invalidate compiled code.
oracle,graal,fb360eb27f2b456e639523e2ac63b3b9a756c29c,https://github.com/oracle/graal/commit/fb360eb27f2b456e639523e2ac63b3b9a756c29c,[GR-60423] Improve FileSystems.isNormalized(Path) performance.  PullRequest: graal/19576
oracle,graal,54dfa944de5d80f0399e1c9daad820385e417d33,https://github.com/oracle/graal/commit/54dfa944de5d80f0399e1c9daad820385e417d33,Perform static field initialization during init  not prepare
oracle,graal,3632022ed910b7f0d4d6302a4ec277adb52cff2e,https://github.com/oracle/graal/commit/3632022ed910b7f0d4d6302a4ec277adb52cff2e,[GR-60423] Improve FileSystems.isNormalized(Path) performance.
oracle,graal,45253c7be4d04047e07583ae349e8cf046114026,https://github.com/oracle/graal/commit/45253c7be4d04047e07583ae349e8cf046114026,Refactor LibGraalClassLoader to be usable as customLoader  diff --git a/compiler/mx.compiler/suite.py b/compiler/mx.compiler/suite.py index 6fa9c61e2f5..7c4d8918601 100644 --- a/compiler/mx.compiler/suite.py +++ b/compiler/mx.compiler/suite.py @@ -465 6 +465 22 @@ suite = { "workingSets" : "Graal Test"  "graalCompilerSourceEdition": "ignore"  }  + +    "jdk.graal.compiler.libgraal" : { +      "subDir" : "src"  +      "sourceDirs" : ["src"]  +      "workingSets" : "Graal"  +      "javaCompliance" : "21+"  +      "dependencies" : [ +        "jdk.graal.compiler"  +      ]  +      "requiresConcealed" : { +        "java.base" : [ +          "jdk.internal.module"  +          "jdk.internal.jimage"  +        ]  +      }  +    }  }   "distributions" : { @@ -581 6 +597 7 @@ suite = { "GRAAL_VERSION"  ]  "distDependencies" : [ +        "sdk:NATIVEIMAGE"  "sdk:COLLECTIONS"  "sdk:WORD"  "sdk:NATIVEIMAGE"  @@ -691 6 +708 17 @@ suite = { "graalCompilerSourceEdition": "ignore"  }   +    "GRAAL_LIBGRAAL" : { +      "subDir": "src"  +      "dependencies" : [ +        "jdk.graal.compiler.libgraal"  +      ]  +      "distDependencies" : [ +        "GRAAL"  +      ]  +      "maven": False  +    }  + "GRAAL_PROFDIFF_TEST" : { "subDir" : "src"  "dependencies" : [ diff --git a/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalClassLoader.java b/compiler/src/jdk.graal.compiler.libgraal/src/jdk/graal/compiler/hotspot/libgraal/LibGraalClassLoader.java similarity index 70% rename from substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalClassLoader.java rename to compiler/src/jdk.graal.compiler.libgraal/src/jdk/graal/compiler/hotspot/libgraal/LibGraalClassLoader.java index aaa171c742a..2c1e53e6850 100644 --- a/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalClassLoader.java +++ b/compiler/src/jdk.graal.compiler.libgraal/src/jdk/graal/compiler/hotspot/libgraal/LibGraalClassLoader.java @@ -22 7 +22 7 @@ * or visit www.oracle.com if you need additional information or have any * questions. */ -package com.oracle.svm.graal.hotspot.libgraal; +package jdk.graal.compiler.hotspot.libgraal;  import java.io.ByteArrayInputStream; import java.io.IOException; @@ -38 7 +38 6 @@ import java.nio.ByteBuffer; import java.nio.file.Path; import java.security.ProtectionDomain; import java.util.ArrayList; -import java.util.Collections; import java.util.Enumeration; import java.util.HashMap; import java.util.List; @@ -47 50 +46 49 @@ import java.util.NoSuchElementException; import java.util.Objects; import java.util.Set;  -import com.oracle.svm.core.SubstrateUtil; -import com.oracle.svm.core.util.VMError; +import org.graalvm.nativeimage.Platform; +import org.graalvm.nativeimage.Platforms; +import org.graalvm.nativeimage.hosted.Feature;  -import com.oracle.svm.util.ModuleSupport; +import jdk.graal.compiler.debug.GraalError; import jdk.internal.jimage.BasicImageReader; import jdk.internal.jimage.ImageLocation; -import org.graalvm.nativeimage.Platform; -import org.graalvm.nativeimage.Platforms; +import jdk.internal.module.Modules;  /** * A classloader  that reads class files and resources from a jimage file at image build time. */ -public class LibGraalClassLoader extends ClassLoader { +@Platforms(Platform.HOSTED_ONLY.class) +final class HostedLibGraalClassLoader extends ClassLoader { + +    private static final String JAVA_HOME_PROPERTY_KEY = "libgraal.javahome"; +    private static final String JAVA_HOME_PROPERTY_VALUE = System.getProperty(JAVA_HOME_PROPERTY_KEY  System.getProperty("java.home"));  /** * Reader for the image. */ -    @Platforms(Platform.HOSTED_ONLY.class) // private final BasicImageReader imageReader;  /** * Map from the name of a resource (without module qualifier) to its path in the image. */ -    @Platforms(Platform.HOSTED_ONLY.class) // private final Map<String  String> resources = new HashMap<>();  /** * Map from the {@linkplain Class#forName(String) name} of a class to the image path of its * class file. */ -    @Platforms(Platform.HOSTED_ONLY.class) // private final Map<String  String> classes = new HashMap<>();  /** * Map from a service name to a list of providers. */ -    @Platforms(Platform.HOSTED_ONLY.class) // private final Map<String  List<String>> services = new HashMap<>();  /** * Map from the {@linkplain Class#forName(String) name} of a class to the name of its enclosing * module. */ -    @Platforms(Platform.HOSTED_ONLY.class) // private final Map<String  String> modules;  /** @@ -109 17 +107 22 @@ public class LibGraalClassLoader extends ClassLoader { ClassLoader.registerAsParallelCapable(); }  -    /** -     * @param imagePath path to the runtime image of a Java installation -     */ -    @Platforms(Platform.HOSTED_ONLY.class) -    LibGraalClassLoader(Path imagePath) { -        super("LibGraalClassLoader"  null); +    public final Path libGraalJavaHome; + +    public HostedLibGraalClassLoader() { +        super(LibGraalClassLoader.LOADER_NAME  Feature.class.getClassLoader()); +        libGraalJavaHome = Path.of(JAVA_HOME_PROPERTY_VALUE); + Map<String  String> modulesMap = new HashMap<>(); try { -            // Need access to jdk.internal.jimage -            ModuleSupport.accessPackagesToClass(ModuleSupport.Access.EXPORT  getClass()  false  -                            "java.base"  "jdk.internal.jimage"); +            /* +             * Access to jdk.internal.jimage classes is needed by this Classloader implementation. +             */ +            Modules.addExports(Object.class.getModule()  "jdk.internal.jimage"  HostedLibGraalClassLoader.class.getModule()); + +            Modules.addExports(Object.class.getModule()  "jdk.internal.misc"  HostedLibGraalClassLoader.class.getModule()); + +            Path imagePath = libGraalJavaHome.resolve(Path.of("lib"  "modules")); this.imageReader = BasicImageReader.open(imagePath); for (var entry : imageReader.getEntryNames()) { int secondSlash = entry.indexOf('/'  1); @@ -152 15 +155 28 @@ public class LibGraalClassLoader extends ClassLoader {  /** * Gets an unmodifiable map from the {@linkplain Class#forName(String) name} of a class to the -     * name of its enclosing module. +     * name of its enclosing module. Reflectively accessed by +     * {@code LibGraalFeature.OptionCollector#afterAnalysis(AfterAnalysisAccess)}. */ +    @SuppressWarnings("unused") public Map<String  String> getModules() { return modules; }  +    @SuppressWarnings("unused") +    public List<Class<?>> getSystemClasses() { +        Class<?> clazz = null; +        try { +            clazz = loadClass("jdk.graal.compiler.hotspot.libgraal.NewLibGraalEntryPoints"); +        } catch (ClassNotFoundException e) { +            throw GraalError.shouldNotReachHere("FIXME"); +        } +        return List.of(clazz); +    } + @Override protected Class<?> loadClass(String name  boolean resolve) throws ClassNotFoundException { -        if (!SubstrateUtil.HOSTED || !classes.containsKey(name)) { +        if (!classes.containsKey(name)) { return super.loadClass(name  resolve); } synchronized (getClassLoadingLock(name)) { @@ -172 40 +188 18 @@ public class LibGraalClassLoader extends ClassLoader { } }  -    @Platforms(Platform.HOSTED_ONLY.class) -    public Class<?> loadClassOrFail(Class<?> c) { -        if (c.getClassLoader() == this) { -            return c; -        } -        if (c.isArray()) { -            return loadClassOrFail(c.getComponentType()).arrayType(); -        } -        return loadClassOrFail(c.getName()); -    } - -    @Platforms(Platform.HOSTED_ONLY.class) -    public Class<?> loadClassOrFail(String name) { -        try { -            return loadClass(name); -        } catch (ClassNotFoundException e) { -            throw VMError.shouldNotReachHere("%s unable to load class '%s'"  getName()  name); -        } -    } - @Override protected Class<?> findClass(final String name) throws ClassNotFoundException { -        if (SubstrateUtil.HOSTED) { -            String path = name.replace('.'  '/').concat(".class"); - -            String pathInImage = resources.get(path); -            if (pathInImage != null) { -                ImageLocation location = imageReader.findLocation(pathInImage); -                if (location != null) { -                    ByteBuffer bb = Objects.requireNonNull(imageReader.getResourceBuffer(location)); -                    ProtectionDomain pd = null; -                    return super.defineClass(name  bb  pd); -                } +        String path = name.replace('.'  '/').concat(".class"); + +        String pathInImage = resources.get(path); +        if (pathInImage != null) { +            ImageLocation location = imageReader.findLocation(pathInImage); +            if (location != null) { +                ByteBuffer bb = Objects.requireNonNull(imageReader.getResourceBuffer(location)); +                ProtectionDomain pd = null; +                return super.defineClass(name  bb  pd); } } throw new ClassNotFoundException(name); @@ -222 35 +216 32 @@ public class LibGraalClassLoader extends ClassLoader { */ private static final String RESOURCE_PROTOCOL = "resource";  -    @Platforms(Platform.HOSTED_ONLY.class) // private URLStreamHandler serviceHandler;  @Override protected URL findResource(String name) { -        if (SubstrateUtil.HOSTED) { -            URLStreamHandler handler = this.serviceHandler; -            if (handler == null) { -                this.serviceHandler = handler = new ImageURLStreamHandler(); -            } -            if (name.startsWith("META-INF/services/")) { -                String service = name.substring("META-INF/services/".length()); -                if (services.containsKey(service)) { -                    try { -                        var uri = new URI(SERVICE_PROTOCOL  service  null); -                        return URL.of(uri  handler); -                    } catch (URISyntaxException | MalformedURLException e) { -                        return null; -                    } +        URLStreamHandler handler = this.serviceHandler; +        if (handler == null) { +            this.serviceHandler = handler = new ImageURLStreamHandler(); +        } +        if (name.startsWith("META-INF/services/")) { +            String service = name.substring("META-INF/services/".length()); +            if (services.containsKey(service)) { +                try { +                    var uri = new URI(SERVICE_PROTOCOL  service  null); +                    return URL.of(uri  handler); +                } catch (URISyntaxException | MalformedURLException e) { +                    return null; } -            } else { -                String path = resources.get(name); -                if (path != null) { -                    try { -                        var uri = new URI(RESOURCE_PROTOCOL  name  null); -                        return URL.of(uri  handler); -                    } catch (URISyntaxException | MalformedURLException e) { -                        return null; -                    } +            } +        } else { +            String path = resources.get(name); +            if (path != null) { +                try { +                    var uri = new URI(RESOURCE_PROTOCOL  name  null); +                    return URL.of(uri  handler); +                } catch (URISyntaxException | MalformedURLException e) { +                    return null; } } } @@ -259 9 +250 6 @@ public class LibGraalClassLoader extends ClassLoader {  @Override protected Enumeration<URL> findResources(String name) throws IOException { -        if (!SubstrateUtil.HOSTED) { -            return Collections.emptyEnumeration(); -        } return new Enumeration<>() { private URL next = findResource(name);  @@ -284 9 +272 8 @@ public class LibGraalClassLoader extends ClassLoader {  /** * A {@link URLStreamHandler} for use with URLs returned by -     * {@link LibGraalClassLoader#findResource(java.lang.String)}. +     * {@link HostedLibGraalClassLoader#findResource(java.lang.String)}. */ -    @Platforms(Platform.HOSTED_ONLY.class) private class ImageURLStreamHandler extends URLStreamHandler { @Override public URLConnection openConnection(URL u) { @@ -307 7 +294 6 @@ public class LibGraalClassLoader extends ClassLoader { } }  -    @Platforms(Platform.HOSTED_ONLY.class) private static class ImageURLConnection extends URLConnection { private final byte[] bytes; private InputStream in; @@ -341 4 +327 23 @@ public class LibGraalClassLoader extends ClassLoader { return "application/octet-stream"; } } + +    /** +     * @return instance of ClassLoader that should be seen at image-runtime if a class was loaded at +     *         image-buildtime by this classloader. +     */ +    @SuppressWarnings("unused") +    public static ClassLoader getRuntimeClassLoader() { +        return LibGraalClassLoader.singleton; +    } +} + +public final class LibGraalClassLoader extends ClassLoader { + +    static final String LOADER_NAME = "LibGraalClassLoader"; +    static final LibGraalClassLoader singleton = new LibGraalClassLoader(); + +    private LibGraalClassLoader() { +        super(LOADER_NAME  null); +    } } diff --git a/compiler/src/jdk.graal.compiler/src/jdk/graal/compiler/hotspot/libgraal/NewLibGraalEntryPoints.java b/compiler/src/jdk.graal.compiler/src/jdk/graal/compiler/hotspot/libgraal/NewLibGraalEntryPoints.java new file mode 100644 index 00000000000..e5a5ab499ef --- /dev/null +++ b/compiler/src/jdk.graal.compiler/src/jdk/graal/compiler/hotspot/libgraal/NewLibGraalEntryPoints.java @@ -0 0 +1 36 @@ +package jdk.graal.compiler.hotspot.libgraal; + +import java.util.function.BooleanSupplier; + +import org.graalvm.nativeimage.IsolateThread; +import org.graalvm.nativeimage.c.function.CEntryPoint; + +import jdk.graal.compiler.core.phases.EconomyLowTier; + +public class NewLibGraalEntryPoints { + +    private static class LoadedByLibGraalClassLoader implements BooleanSupplier { +        @Override +        public boolean getAsBoolean() { +            ClassLoader loader = NewLibGraalEntryPoints.class.getClassLoader(); +            return loader != null && "LibGraalClassLoader".equals(loader.getName()); +        } +    } + +    @CEntryPoint(name = "runMain"  include = LoadedByLibGraalClassLoader.class) +    public static int runMain(@SuppressWarnings("unused") IsolateThread thread) { +        System.out.println("Hello from jdk.graal.compiler.hotspot.libgraal.NewLibGraalEntryPoints.runMain"); +        System.out.println("EconomyLowTier.class.getClassLoader() = " + EconomyLowTier.class.getClassLoader()); +        try { +            CompilerConfig.main(new String[]{"CompilerConfig.out"}); +        } catch (Exception e) { +            e.printStackTrace(); +        } +        return 0; +    } + +    @CEntryPoint(name = "java_check_heap"  include = LoadedByLibGraalClassLoader.class) +    protected static int checkHeap(@SuppressWarnings("unused") IsolateThread thread) { +        return 0; +    } +} \ No newline at end of file diff --git a/compiler/src/jdk.graal.compiler/src/jdk/graal/compiler/hotspot/libgraal/NewLibGraalFeature.java b/compiler/src/jdk.graal.compiler/src/jdk/graal/compiler/hotspot/libgraal/NewLibGraalFeature.java new file mode 100644 index 00000000000..0fbc82d734d --- /dev/null +++ b/compiler/src/jdk.graal.compiler/src/jdk/graal/compiler/hotspot/libgraal/NewLibGraalFeature.java @@ -0 0 +1 27 @@ +package jdk.graal.compiler.hotspot.libgraal; + +import java.io.PrintStream; + +import org.graalvm.nativeimage.hosted.Feature; + +import jdk.graal.compiler.core.phases.CommunityCompilerConfiguration; +import jdk.graal.compiler.debug.GraalError; + +public class NewLibGraalFeature implements Feature { +    @Override +    public void afterRegistration(AfterRegistrationAccess access) { +        System.out.println("access.getApplicationClassLoader() = " + access.getApplicationClassLoader()); +        ClassLoader loader = NewLibGraalFeature.class.getClassLoader(); +        System.out.println("CommunityCompilerConfiguration.class.getClassLoader() = " + CommunityCompilerConfiguration.class.getClassLoader()); +        if (loader == null || !"HostedLibGraalClassLoader".equals(loader.getClass().getSimpleName())) { +            throw GraalError.shouldNotReachHere("NewLibGraalFeature was not loaded by HostedLibGraalClassLoader"); +        } +    } + +    @Override +    public void beforeAnalysis(BeforeAnalysisAccess access) { +        access.registerSubtypeReachabilityHandler((duringAnalysisAccess  aClass) -> { +            System.out.println(aClass.getName() + " reachable"); +        }  PrintStream.class); +    } +} diff --git a/substratevm/mx.substratevm/mx_substratevm.py b/substratevm/mx.substratevm/mx_substratevm.py index e685f649bc8..d930dbb8220 100644 --- a/substratevm/mx.substratevm/mx_substratevm.py +++ b/substratevm/mx.substratevm/mx_substratevm.py @@ -1468 6 +1468 7 @@ mx_sdk_vm.register_graalvm_component(mx_sdk_vm.GraalVMSvmMacro( libgraal_jar_distributions = [ 'sdk:NATIVEBRIDGE'  'sdk:JNIUTILS'  +    'compiler:GRAAL_LIBGRAAL'  'substratevm:LIBGRAAL_LIBRARY']  def allow_build_path_in_libgraal(): diff --git a/substratevm/mx.substratevm/suite.py b/substratevm/mx.substratevm/suite.py index 12feca34223..58b4a8b610d 100644 --- a/substratevm/mx.substratevm/suite.py +++ b/substratevm/mx.substratevm/suite.py @@ -298 6 +298 7 @@ suite = { "sun.util.resources"  "jdk.internal.access"  "jdk.internal.event"  +                    "jdk.internal.jimage"  "jdk.internal.loader"  "jdk.internal.logger"  "jdk.internal.misc"  @@ -1386 7 +1387 6 @@ suite = { ]  "requiresConcealed" : { "java.base" : [ -                    "jdk.internal.jimage"  "jdk.internal.misc"  ]  "jdk.internal.vm.ci" : [ diff --git a/substratevm/src/com.oracle.svm.core/src/com/oracle/svm/core/hub/ClassForNameSupport.java b/substratevm/src/com.oracle.svm.core/src/com/oracle/svm/core/hub/ClassForNameSupport.java index 7420631d361..120fe5a356e 100644 --- a/substratevm/src/com.oracle.svm.core/src/com/oracle/svm/core/hub/ClassForNameSupport.java +++ b/substratevm/src/com.oracle.svm.core/src/com/oracle/svm/core/hub/ClassForNameSupport.java @@ -32 12 +32 12 @@ import java.util.Objects; import org.graalvm.collections.EconomicMap; import org.graalvm.nativeimage.ImageSingletons; import org.graalvm.nativeimage.Platform; +import org.graalvm.nativeimage.Platform.HOSTED_ONLY; import org.graalvm.nativeimage.Platforms; import org.graalvm.nativeimage.impl.ConfigurationCondition;  import com.oracle.svm.core.configure.ConditionalRuntimeValue; import com.oracle.svm.core.configure.RuntimeConditionSet; -import com.oracle.svm.core.feature.AutomaticallyRegisteredImageSingleton; import com.oracle.svm.core.layeredimagesingleton.LayeredImageSingletonBuilderFlags; import com.oracle.svm.core.layeredimagesingleton.MultiLayeredImageSingleton; import com.oracle.svm.core.layeredimagesingleton.UnsavedSingleton; @@ -45 9 +45 18 @@ import com.oracle.svm.core.reflect.MissingReflectionRegistrationUtils; import com.oracle.svm.core.util.ImageHeapMap; import com.oracle.svm.core.util.VMError;  -@AutomaticallyRegisteredImageSingleton public final class ClassForNameSupport implements MultiLayeredImageSingleton  UnsavedSingleton {  +    private ClassLoader customLoader; + +    public ClassForNameSupport(ClassLoader customLoader) { +        setCustomLoader(customLoader); +    } + +    public void setCustomLoader(ClassLoader customLoader) { +        this.customLoader = customLoader; +    } + public static ClassForNameSupport singleton() { return ImageSingletons.lookup(ClassForNameSupport.class); } @@ -115 12 +124 9 @@ public final class ClassForNameSupport implements MultiLayeredImageSingleton  Un } }  -    private static boolean isLibGraalClass(Class<?> clazz) { -        var loader = clazz.getClassLoader(); -        if (loader == null) { -            return false; -        } -        return "LibGraalClassLoader".equals(loader.getName()); +    @Platforms(HOSTED_ONLY.class) +    private boolean isLibGraalClass(Class<?> clazz) { +        return customLoader != null && clazz.getClassLoader() == customLoader; }  public static ConditionalRuntimeValue<Object> updateConditionalValue(ConditionalRuntimeValue<Object> existingConditionalValue  Object newValue  diff --git a/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalFeature.java b/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalFeature.java index 0c341c9ca9b..bb3d3bcd51f 100644 --- a/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalFeature.java +++ b/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalFeature.java @@ -58 22 +58 23 @@ import com.oracle.graal.pointsto.meta.AnalysisType; import com.oracle.graal.pointsto.meta.ObjectReachableCallback; import com.oracle.graal.pointsto.reports.CallTreePrinter; import com.oracle.svm.core.SubstrateTargetDescription; -import com.oracle.svm.core.option.HostedOptionKey; +import com.oracle.svm.core.hub.ClassForNameSupport; import com.oracle.svm.core.util.VMError; import com.oracle.svm.graal.hotspot.GetCompilerConfig; import com.oracle.svm.graal.hotspot.GetJNIConfig; +import com.oracle.svm.hosted.ClassLoaderFeature; import com.oracle.svm.hosted.FeatureImpl.AfterAnalysisAccessImpl; import com.oracle.svm.hosted.FeatureImpl.BeforeAnalysisAccessImpl; import com.oracle.svm.hosted.FeatureImpl.DuringAnalysisAccessImpl; import com.oracle.svm.hosted.FeatureImpl.DuringSetupAccessImpl; import com.oracle.svm.util.ModuleSupport; +import com.oracle.svm.util.ModuleSupport.Access; import com.oracle.svm.util.ReflectionUtil;  import jdk.graal.compiler.debug.DebugContext; import jdk.graal.compiler.hotspot.CompilerConfigurationFactory; import jdk.graal.compiler.hotspot.libgraal.BuildTime; import jdk.graal.compiler.nodes.graphbuilderconf.GeneratedInvocationPlugin; -import jdk.graal.compiler.options.Option; import jdk.graal.compiler.options.OptionDescriptor; import jdk.graal.compiler.options.OptionKey; import jdk.graal.compiler.serviceprovider.LibGraalService; @@ -87 7 +88 7 @@ import jdk.vm.ci.code.TargetDescription; * <p> * This feature is composed of these key classes: * <ul> - * <li>{@link LibGraalClassLoader}</li> + * <li>{@code HostedLibGraalClassLoader}</li> * <li>{@link LibGraalEntryPoints}</li> * <li>{@link LibGraalSubstitutions}</li> * </ul> @@ -95 14 +96 6 @@ import jdk.vm.ci.code.TargetDescription; @Platforms(Platform.HOSTED_ONLY.class) public final class LibGraalFeature implements Feature {  -    static class Options { -        @Option(help = "The value of the java.home system property reported by the Java " + -                        "installation that includes the Graal classes in its runtime image " + -                        "from which libgraal will be built. If not provided  the java.home " + -                        "of the Java installation running native-image will be used.") // -        public static final HostedOptionKey<Path> LibGraalJavaHome = new HostedOptionKey<>(Path.of(System.getProperty("java.home"))); -    } - public static final class IsEnabled implements BooleanSupplier { @Override public boolean getAsBoolean() { @@ -120 7 +113 7 @@ public final class LibGraalFeature implements Feature { /** * Loader used for loading classes from the guest GraalVM. */ -    LibGraalClassLoader loader; +    ClassLoader loader;  /** * Handle to {@link BuildTime} in the guest. @@ -143 14 +136 32 @@ public final class LibGraalFeature implements Feature {  MethodHandle handleGlobalAtomicLongGetInitialValue;  -    public LibGraalClassLoader getLoader() { +    public ClassLoader getLoader() { return loader; }  +    public Class<?> loadClassOrFail(Class<?> c) { +        if (c.getClassLoader() == loader) { +            return c; +        } +        if (c.isArray()) { +            return loadClassOrFail(c.getComponentType()).arrayType(); +        } +        return loadClassOrFail(c.getName()); +    } + +    public Class<?> loadClassOrFail(String name) { +        try { +            return loader.loadClass(name); +        } catch (ClassNotFoundException e) { +            throw new AssertionError("%s unable to load class '%s'".formatted(loader.getName()  name)); +        } +    } + /** * Performs tasks once this feature is registered. * <ul> -     * <li>Create the {@link LibGraalClassLoader} instance.</li> +     * <li>Create the {@code HostedLibGraalClassLoader} instance.</li> * <li>Get a handle to the {@link BuildTime} class in the guest.</li> * <li>Initializes the options in the guest.</li> * <li>Initializes some state needed by {@link LibGraalSubstitutions}.</li> @@ -173 9 +184 10 @@ public final class LibGraalFeature implements Feature { // org.graalvm.nativeimage.impl.IsolateSupport accessModulesToClass(ModuleSupport.Access.EXPORT  LibGraalFeature.class  "org.graalvm.nativeimage");  -        loader = new LibGraalClassLoader(Options.LibGraalJavaHome.getValue().resolve(Path.of("lib"  "modules"))); +        loader = createHostedLibGraalClassLoader(access); +        ImageSingletons.lookup(ClassForNameSupport.class).setCustomLoader(loader);  -        buildTimeClass = loader.loadClassOrFail("jdk.graal.compiler.hotspot.libgraal.BuildTime"); +        buildTimeClass = loadClassOrFail("jdk.graal.compiler.hotspot.libgraal.BuildTime");  // Guest JVMCI and Graal need access to some JDK internal packages String[] basePackages = {"jdk.internal.misc"  "jdk.internal.util"  "jdk.internal.vm"}; @@ -186 7 +198 7 @@ public final class LibGraalFeature implements Feature { * Get GlobalAtomicLong.getInitialValue() method from LibGraalClassLoader for * LibGraalGraalSubstitutions.GlobalAtomicLongAddressProvider FieldValueTransformer */ -            handleGlobalAtomicLongGetInitialValue = mhl.findVirtual(loader.loadClassOrFail("jdk.graal.compiler.serviceprovider.GlobalAtomicLong")  +            handleGlobalAtomicLongGetInitialValue = mhl.findVirtual(loadClassOrFail("jdk.graal.compiler.serviceprovider.GlobalAtomicLong")  "getInitialValue"  methodType(long.class));  } catch (Throwable e) { @@ -194 6 +206 13 @@ public final class LibGraalFeature implements Feature { } }  +    @SuppressWarnings("unchecked") +    private static ClassLoader createHostedLibGraalClassLoader(AfterRegistrationAccess access) { +        var hostedLibGraalClassLoaderClass = access.findClassByName("jdk.graal.compiler.hotspot.libgraal.HostedLibGraalClassLoader"); +        ModuleSupport.accessPackagesToClass(Access.EXPORT  hostedLibGraalClassLoaderClass  false  "java.base"  "jdk.internal.module"); +        return ReflectionUtil.newInstance((Class<ClassLoader>) hostedLibGraalClassLoaderClass); +    } + private static void accessModulesToClass(ModuleSupport.Access access  Class<?> accessingClass  String... moduleNames) { for (String moduleName : moduleNames) { var module = getBootModule(moduleName); @@ -208 17 +227 25 @@ public final class LibGraalFeature implements Feature {  @Override public void duringSetup(DuringSetupAccess access) { + +        /* +         * HostedLibGraalClassLoader provides runtime-replacement loader instance. Make sure +         * HostedLibGraalClassLoader gets replaced by customRuntimeLoader instance in image. +         */ +        ClassLoader customRuntimeLoader = ClassLoaderFeature.getCustomRuntimeClassLoader(loader); +        access.registerObjectReplacer(obj -> obj == loader ? customRuntimeLoader : obj); + try { -            var basePhaseStatisticsClass = loader.loadClassOrFail("jdk.graal.compiler.phases.BasePhase$BasePhaseStatistics"); -            var lirPhaseStatisticsClass = loader.loadClassOrFail("jdk.graal.compiler.lir.phases.LIRPhase$LIRPhaseStatistics"); +            var basePhaseStatisticsClass = loadClassOrFail("jdk.graal.compiler.phases.BasePhase$BasePhaseStatistics"); +            var lirPhaseStatisticsClass = loadClassOrFail("jdk.graal.compiler.lir.phases.LIRPhase$LIRPhaseStatistics"); MethodType statisticsCTorType = methodType(void.class  Class.class); var basePhaseStatisticsCTor = mhl.findConstructor(basePhaseStatisticsClass  statisticsCTorType); var lirPhaseStatisticsCTor = mhl.findConstructor(lirPhaseStatisticsClass  statisticsCTorType); newBasePhaseStatistics = new StatisticsCreator(basePhaseStatisticsCTor)::create; newLIRPhaseStatistics = new StatisticsCreator(lirPhaseStatisticsCTor)::create;  -            basePhaseClass = loader.loadClassOrFail("jdk.graal.compiler.phases.BasePhase"); -            lirPhaseClass = loader.loadClassOrFail("jdk.graal.compiler.lir.phases.LIRPhase"); +            basePhaseClass = loadClassOrFail("jdk.graal.compiler.phases.BasePhase"); +            lirPhaseClass = loadClassOrFail("jdk.graal.compiler.lir.phases.LIRPhase");  ImageSingletons.add(LibGraalCompilerSupport.class  new LibGraalCompilerSupport()); } catch (Throwable e) { @@ -229 7 +256 7 @@ public final class LibGraalFeature implements Feature { accessImpl.registerClassReachabilityListener(this::registerPhaseStatistics); optionCollector = new OptionCollector(LibGraalEntryPoints.vmOptionDescriptors); accessImpl.registerObjectReachableCallback(OptionKey.class  optionCollector::doCallback); -        accessImpl.registerObjectReachableCallback(loader.loadClassOrFail(OptionKey.class.getName())  optionCollector::doCallback); +        accessImpl.registerObjectReachableCallback(loadClassOrFail(OptionKey.class.getName())  optionCollector::doCallback); GetJNIConfig.register(loader); }  @@ -240 7 +267 7 @@ public final class LibGraalFeature implements Feature { * {@link OptionKey} instances reached by the static analysis. The VM options are instances of * {@link OptionKey} loaded by the {@link com.oracle.svm.hosted.NativeImageClassLoader} and * compiler options are instances of {@link OptionKey} loaded by the -     * {@link LibGraalClassLoader}. +     * {@code HostedLibGraalClassLoader}. */ private class OptionCollector implements ObjectReachableCallback<Object> { private final Set<Object> options = Collections.newSetFromMap(new ConcurrentHashMap<>()); @@ -298 7 +325 7 @@ public final class LibGraalFeature implements Feature { try { MethodType mt = methodType(Iterable.class  List.class  Object.class  Map.class); MethodHandle mh = mhl.findStatic(buildTimeClass  "finalizeLibgraalOptions"  mt); -                Map<String  String> modules = loader.getModules(); +                Map<String  String> modules = ReflectionUtil.invokeMethod(ReflectionUtil.lookupMethod(loader.getClass()  "getModules")  loader); Iterable<Object> values = (Iterable<Object>) mh.invoke(compilerOptions  compilerOptionsInfo  modules); for (Object descriptor : values) { VMError.guarantee(access.isReachable(descriptor.getClass())  "%s"  descriptor.getClass()); @@ -347 11 +374 11 @@ public final class LibGraalFeature implements Feature { var bb = impl.getBigBang();  /* Contains static fields that depend on HotSpotJVMCIRuntime */ -        RuntimeClassInitialization.initializeAtRunTime(loader.loadClassOrFail("jdk.vm.ci.hotspot.HotSpotModifiers")); -        RuntimeClassInitialization.initializeAtRunTime(loader.loadClassOrFail("jdk.vm.ci.hotspot.HotSpotCompiledCodeStream")); -        RuntimeClassInitialization.initializeAtRunTime(loader.loadClassOrFail("jdk.vm.ci.hotspot.HotSpotCompiledCodeStream$Tag")); +        RuntimeClassInitialization.initializeAtRunTime(loadClassOrFail("jdk.vm.ci.hotspot.HotSpotModifiers")); +        RuntimeClassInitialization.initializeAtRunTime(loadClassOrFail("jdk.vm.ci.hotspot.HotSpotCompiledCodeStream")); +        RuntimeClassInitialization.initializeAtRunTime(loadClassOrFail("jdk.vm.ci.hotspot.HotSpotCompiledCodeStream$Tag")); /* ThreadLocal in static field jdk.graal.compiler.debug.DebugContext.activated */ -        RuntimeClassInitialization.initializeAtRunTime(loader.loadClassOrFail("jdk.graal.compiler.debug.DebugContext")); +        RuntimeClassInitialization.initializeAtRunTime(loadClassOrFail("jdk.graal.compiler.debug.DebugContext"));  /* Needed for runtime calls to BoxingSnippets.Templates.getCacheClass(JavaKind) */ RuntimeReflection.registerAllDeclaredClasses(Character.class); @@ -377 7 +404 7 @@ public final class LibGraalFeature implements Feature {  List<Class<?>> guestServiceClasses = new ArrayList<>(); List<Class<?>> serviceClasses = impl.getImageClassLoader().findAnnotatedClasses(LibGraalService.class  false); -            serviceClasses.stream().map(c -> loader.loadClassOrFail(c.getName())).forEach(guestServiceClasses::add); +            serviceClasses.stream().map(c -> loadClassOrFail(c.getName())).forEach(guestServiceClasses::add);  // Transfer libgraal qualifier (e.g. "PGO optimized") from host to guest. String nativeImageLocationQualifier = CompilerConfigurationFactory.getNativeImageLocationQualifier(); @@ -392 13 +419 16 @@ public final class LibGraalFeature implements Feature { String.class  // nativeImageLocationQualifier byte[].class // encodedGuestObjects )); -            GetCompilerConfig.Result configResult = GetCompilerConfig.from(Options.LibGraalJavaHome.getValue()  bb.getOptions()); +            Path libGraalJavaHome = ReflectionUtil.readField(loader.getClass()  "libGraalJavaHome"  loader); +            GetCompilerConfig.Result configResult = GetCompilerConfig.from(libGraalJavaHome  bb.getOptions()); for (var e : configResult.opens().entrySet()) { for (String source : e.getValue()) { ModuleSupport.accessPackagesToClass(ModuleSupport.Access.OPEN  buildTimeClass  false  e.getKey()  source); } }  +            ModuleSupport.accessPackagesToClass(ModuleSupport.Access.OPEN  buildTimeClass  false  "org.graalvm.word"  "org.graalvm.word.impl"); + configureGraalForLibGraal.invoke(arch  guestServiceClasses  registerAsInHeap  @@ -420 7 +450 7 @@ public final class LibGraalFeature implements Feature {  @SuppressWarnings("unchecked") private void initializeTruffle() throws Throwable { -        Class<?> truffleBuildTimeClass = loader.loadClassOrFail("jdk.graal.compiler.hotspot.libgraal.truffle.BuildTime"); +        Class<?> truffleBuildTimeClass = loadClassOrFail("jdk.graal.compiler.hotspot.libgraal.truffle.BuildTime"); MethodHandle getLookup = mhl.findStatic(truffleBuildTimeClass  "initializeLookup"  methodType(Map.Entry.class  Lookup.class  Class.class  Class.class)); Map.Entry<Lookup  Class<?>> truffleLibGraal = (Map.Entry<Lookup  Class<?>>) getLookup.invoke(mhl  TruffleFromLibGraalStartPoints.class  NativeImageHostEntryPoints.class); ImageSingletons.add(LibGraalTruffleToLibGraalEntryPoints.class  new LibGraalTruffleToLibGraalEntryPoints(truffleLibGraal.getKey()  truffleLibGraal.getValue())); diff --git a/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalFieldsOffsetsFeature.java b/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalFieldsOffsetsFeature.java index cf55ea2b1f8..13810ffb231 100644 --- a/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalFieldsOffsetsFeature.java +++ b/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalFieldsOffsetsFeature.java @@ -62 7 +62 7 @@ import jdk.internal.misc.Unsafe; public final class LibGraalFieldsOffsetsFeature implements InternalFeature {  private final MethodHandles.Lookup mhl = MethodHandles.lookup(); -    private LibGraalClassLoader loader; +    private LibGraalFeature libGraalFeature;  private Class<?> fieldsClass; private Class<?> edgesClass; @@ -119 20 +119 20 @@ public final class LibGraalFieldsOffsetsFeature implements InternalFeature { @Override public void duringSetup(DuringSetupAccess a) { DuringSetupAccessImpl access = (DuringSetupAccessImpl) a; -        loader = ImageSingletons.lookup(LibGraalFeature.class).loader; - -        fieldsClass = loader.loadClassOrFail("jdk.graal.compiler.core.common.Fields"); -        edgesClass = loader.loadClassOrFail("jdk.graal.compiler.graph.Edges"); -        edgesTypeClass = loader.loadClassOrFail("jdk.graal.compiler.graph.Edges$Type"); -        nodeClass = loader.loadClassOrFail("jdk.graal.compiler.graph.Node"); -        nodeClassClass = loader.loadClassOrFail("jdk.graal.compiler.graph.NodeClass"); -        lirInstructionClass = loader.loadClassOrFail("jdk.graal.compiler.lir.LIRInstruction"); -        lirInstructionClassClass = loader.loadClassOrFail("jdk.graal.compiler.lir.LIRInstructionClass"); -        compositeValueClass = loader.loadClassOrFail("jdk.graal.compiler.lir.CompositeValue"); -        compositeValueClassClass = loader.loadClassOrFail("jdk.graal.compiler.lir.CompositeValueClass"); -        fieldIntrospectionClass = loader.loadClassOrFail("jdk.graal.compiler.core.common.FieldIntrospection"); -        inputEdgesClass = loader.loadClassOrFail("jdk.graal.compiler.graph.InputEdges"); -        successorEdgesClass = loader.loadClassOrFail("jdk.graal.compiler.graph.SuccessorEdges"); +        libGraalFeature = ImageSingletons.lookup(LibGraalFeature.class); + +        fieldsClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.core.common.Fields"); +        edgesClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.graph.Edges"); +        edgesTypeClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.graph.Edges$Type"); +        nodeClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.graph.Node"); +        nodeClassClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.graph.NodeClass"); +        lirInstructionClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.lir.LIRInstruction"); +        lirInstructionClassClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.lir.LIRInstructionClass"); +        compositeValueClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.lir.CompositeValue"); +        compositeValueClassClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.lir.CompositeValueClass"); +        fieldIntrospectionClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.core.common.FieldIntrospection"); +        inputEdgesClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.graph.InputEdges"); +        successorEdgesClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.graph.SuccessorEdges");  try { fieldsClassGetOffsetsMethod = mhl.findVirtual(fieldsClass  "getOffsets"  MethodType.methodType(long[].class)); @@ -171 7 +171 7 @@ public final class LibGraalFieldsOffsetsFeature implements InternalFeature { public void beforeAnalysis(BeforeAnalysisAccess access) { MethodHandle getInputEdgesOffsets; MethodHandle getSuccessorEdgesOffsets; -        var buildTimeClass = loader.loadClassOrFail("jdk.graal.compiler.hotspot.libgraal.BuildTime"); +        var buildTimeClass = libGraalFeature.loadClassOrFail("jdk.graal.compiler.hotspot.libgraal.BuildTime"); try { MethodType offsetAccessorSignature = MethodType.methodType(long[].class  Object.class); getInputEdgesOffsets = mhl.findStatic(buildTimeClass  "getInputEdgesOffsets"  offsetAccessorSignature); diff --git a/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalSubstitutions.java b/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalSubstitutions.java index 2c2fb99e2fb..c885d19b899 100644 --- a/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalSubstitutions.java +++ b/substratevm/src/com.oracle.svm.graal.hotspot.libgraal/src/com/oracle/svm/graal/hotspot/libgraal/LibGraalSubstitutions.java @@ -30 11 +30 6 @@ import java.lang.ref.ReferenceQueue; import java.util.Map; import java.util.function.Supplier;  -import com.oracle.svm.core.heap.GCCause; -import com.oracle.svm.core.heap.Heap; -import com.oracle.svm.core.jdk.JDKLatest; -import com.oracle.svm.core.log.FunctionPointerLogHandler; -import com.oracle.svm.graal.hotspot.LibGraalJNIMethodScope; import org.graalvm.jniutils.JNI; import org.graalvm.jniutils.JNIExceptionWrapper; import org.graalvm.jniutils.JNIMethodScope; @@ -61 7 +56 12 @@ import com.oracle.svm.core.annotate.TargetClass; import com.oracle.svm.core.annotate.TargetElement; import com.oracle.svm.core.c.CGlobalData; import com.oracle.svm.core.c.CGlobalDataFactory; +import com.oracle.svm.core.heap.GCCause; +import com.oracle.svm.core.heap.Heap; +import com.oracle.svm.core.jdk.JDKLatest; +import com.oracle.svm.core.log.FunctionPointerLogHandler; import com.oracle.svm.core.util.VMError; +import com.oracle.svm.graal.hotspot.LibGraalJNIMethodScope;  class LibGraalJVMCISubstitutions {  @@ -461 7 +461 7 @@ public class LibGraalSubstitutions { class LibGraalClassLoaderSupplier implements Supplier<ClassLoader> { @Override public ClassLoader get() { -        LibGraalClassLoader loader = ImageSingletons.lookup(LibGraalFeature.class).loader; +        ClassLoader loader = ImageSingletons.lookup(LibGraalFeature.class).loader; VMError.guarantee(loader != null); return loader; } diff --git a/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/ClassForNameSupportFeature.java b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/ClassForNameSupportFeature.java new file mode 100644 index 00000000000..521e2fe4265 --- /dev/null +++ b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/ClassForNameSupportFeature.java @@ -0 0 +1 23 @@ +package com.oracle.svm.hosted; + +import org.graalvm.nativeimage.ImageSingletons; + +import com.oracle.svm.core.feature.AutomaticallyRegisteredFeature; +import com.oracle.svm.core.feature.InternalFeature; +import com.oracle.svm.core.hub.ClassForNameSupport; +import com.oracle.svm.core.layeredimagesingleton.FeatureSingleton; +import com.oracle.svm.core.layeredimagesingleton.LoadedLayeredImageSingletonInfo; +import com.oracle.svm.core.layeredimagesingleton.UnsavedSingleton; +import com.oracle.svm.hosted.FeatureImpl.AfterRegistrationAccessImpl; + +@AutomaticallyRegisteredFeature +public final class ClassForNameSupportFeature implements InternalFeature  FeatureSingleton  UnsavedSingleton { +    @Override +    public void afterRegistration(AfterRegistrationAccess access) { +        if (ImageSingletons.lookup(LoadedLayeredImageSingletonInfo.class).handledDuringLoading(ClassForNameSupport.class)) { +            return; +        } +        ClassLoader customLoader = ((AfterRegistrationAccessImpl) access).getImageClassLoader().classLoaderSupport.getCustomLoader(); +        ImageSingletons.add(ClassForNameSupport.class  new ClassForNameSupport(customLoader)); +    } +} \ No newline at end of file diff --git a/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/ClassLoaderFeature.java b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/ClassLoaderFeature.java index 6685997b488..085c67d7273 100644 --- a/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/ClassLoaderFeature.java +++ b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/ClassLoaderFeature.java @@ -24 6 +24 7 @@ */ package com.oracle.svm.hosted;  +import java.lang.reflect.Method; import java.util.concurrent.ConcurrentHashMap; import java.util.function.Function;  @@ -34 6 +35 7 @@ import com.oracle.svm.core.feature.InternalFeature; import com.oracle.svm.core.fieldvaluetransformer.FieldValueTransformerWithAvailability; import com.oracle.svm.core.imagelayer.ImageLayerBuildingSupport; import com.oracle.svm.core.util.VMError; +import com.oracle.svm.hosted.FeatureImpl.DuringSetupAccessImpl; import com.oracle.svm.hosted.imagelayer.CrossLayerConstantRegistry; import com.oracle.svm.hosted.imagelayer.ObjectToConstantFieldValueTransformer; import com.oracle.svm.hosted.jdk.HostedClassLoaderPackageManagement; @@ -118 6 +120 17 @@ public class ClassLoaderFeature implements InternalFeature {  var config = (FeatureImpl.DuringSetupAccessImpl) access; if (ImageLayerBuildingSupport.firstImageBuild()) { +            ClassLoader customLoader = ((DuringSetupAccessImpl) access).imageClassLoader.classLoaderSupport.getCustomLoader(); +            if (customLoader != null) { +                ClassLoader customRuntimeLoader = getCustomRuntimeClassLoader(customLoader); +                if (customRuntimeLoader != null) { +                    /* +                     * CustomLoader provides runtime-replacement ClassLoader instance. Make sure +                     * customLoader gets replaced by customRuntimeLoader instance in image. +                     */ +                    access.registerObjectReplacer(obj -> obj == customLoader ? customRuntimeLoader : obj); +                } +            } access.registerObjectReplacer(this::runtimeClassLoaderObjectReplacer); if (ImageLayerBuildingSupport.buildingInitialLayer()) { config.registerObjectReachableCallback(ClassLoader.class  (a1  classLoader  reason) -> { @@ -133 6 +146 12 @@ public class ClassLoaderFeature implements InternalFeature { } }  +    public static ClassLoader getCustomRuntimeClassLoader(ClassLoader customLoader) { +        Class<? extends ClassLoader> customLoaderClass = customLoader.getClass(); +        Method getRuntimeClassLoaderMethod = ReflectionUtil.lookupMethod(true  customLoaderClass  "getRuntimeClassLoader"); +        return getRuntimeClassLoaderMethod != null ? ReflectionUtil.invokeMethod(getRuntimeClassLoaderMethod  null) : null; +    } + @Override public void beforeAnalysis(BeforeAnalysisAccess access) { var packagesField = ReflectionUtil.lookupField(ClassLoader.class  "packages"); diff --git a/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/FeatureHandler.java b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/FeatureHandler.java index 418e0d584e2..250f5abf07f 100644 --- a/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/FeatureHandler.java +++ b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/FeatureHandler.java @@ -48 8 +48 8 @@ import com.oracle.svm.core.feature.AutomaticallyRegisteredFeature; import com.oracle.svm.core.feature.AutomaticallyRegisteredFeatureServiceRegistration; import com.oracle.svm.core.feature.InternalFeature; import com.oracle.svm.core.option.APIOption; -import com.oracle.svm.core.option.HostedOptionKey; import com.oracle.svm.core.option.AccumulatingLocatableMultiOptionValue; +import com.oracle.svm.core.option.HostedOptionKey; import com.oracle.svm.core.option.SubstrateOptionsParser; import com.oracle.svm.core.util.InterruptImageBuilding; import com.oracle.svm.core.util.UserError; @@ -183 11 +183 24 @@ public class FeatureHandler { }  for (String featureName : Options.userEnabledFeatures()) { -            Class<?> featureClass; -            try { -                featureClass = Class.forName(featureName  true  loader.getClassLoader()); -            } catch (ClassNotFoundException e) { -                throw UserError.abort("Feature %s class not found on the classpath. Ensure that the name is correct and that the class is on the classpath."  featureName); +            ClassLoader customLoader = loader.classLoaderSupport.getCustomLoader(); +            List<ClassLoader> featureClassLoaders; +            if (customLoader != null) { +                featureClassLoaders = List.of(customLoader  loader.getClassLoader()); +            } else { +                featureClassLoaders = List.of(loader.getClassLoader()); +            } +            Class<?> featureClass = null; +            for (ClassLoader featureClassLoader : featureClassLoaders) { +                try { +                    featureClass = Class.forName(featureName  true  featureClassLoader); +                    break; +                } catch (ClassNotFoundException e) { +                    /* Ignore */ +                } +            } +            if (featureClass == null) { +                throw UserError.abort("User-enabled Feature %s class not found. Ensure that the name is correct and that the class is on the class- or module-path."  featureName); } registerFeature(featureClass  specificClassProvider  access); } diff --git a/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/ImageClassLoader.java b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/ImageClassLoader.java index c55efe36d59..fc87c58847d 100644 --- a/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/ImageClassLoader.java +++ b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/ImageClassLoader.java @@ -99 7 +99 7 @@ public final class ImageClassLoader { classLoaderSupport.reportBuilderClassesInApplication(); }  -    private void findSystemElements(Class<?> systemClass) { +    void findSystemElements(Class<?> systemClass) { Method[] declaredMethods = null; try { declaredMethods = systemClass.getDeclaredMethods(); diff --git a/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/NativeImageClassLoaderSupport.java b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/NativeImageClassLoaderSupport.java index e243c31a216..9611c625370 100644 --- a/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/NativeImageClassLoaderSupport.java +++ b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/NativeImageClassLoaderSupport.java @@ -118 6 +118 7 @@ public class NativeImageClassLoaderSupport { private final ConcurrentHashMap<String  LinkedHashSet<String>> serviceProviders;  private final NativeImageClassLoader classLoader; +    private final ClassLoader customLoader;  public final ModuleFinder upgradeAndSystemModuleFinder; public final ModuleLayer moduleLayerForImageBuild; @@ -187 6 +188 23 @@ public class NativeImageClassLoaderSupport {  classLoader = new NativeImageClassLoader(imagecp  configuration  defaultSystemClassLoader);  +        String customLoaderPropertyKey = "org.graalvm.nativeimage.experimental.loader"; +        String customLoaderPropertyValue = System.getProperty(customLoaderPropertyKey); +        if (customLoaderPropertyValue != null) { +            try { +                Class<?> customLoaderClass = Class.forName(customLoaderPropertyValue  true  classLoader); +                customLoader = (ClassLoader) ReflectionUtil.newInstance(customLoaderClass); +            } catch (ClassNotFoundException e) { +                throw VMError.shouldNotReachHere("Custom ClassLoader " + customLoaderPropertyValue + +                                " set via system property " + customLoaderPropertyKey + " could not be found."  e); +            } catch (ClassCastException e) { +                throw VMError.shouldNotReachHere("Custom ClassLoader " + customLoaderPropertyValue + +                                " set via system property " + customLoaderPropertyKey + " does not extend class ClassLoader."  e); +            } +        } else { +            customLoader = null; +        } + ModuleLayer moduleLayer = ModuleLayer.defineModules(configuration  List.of(ModuleLayer.boot())  ignored -> classLoader).layer(); adjustBootLayerQualifiedExports(moduleLayer); moduleLayerForImageBuild = moduleLayer; @@ -219 6 +237 10 @@ public class NativeImageClassLoaderSupport { return classLoader; }  +    public ClassLoader getCustomLoader() { +        return customLoader; +    } + private static Path stringToPath(String path) { return Path.of(Path.of(path).toAbsolutePath().toUri().normalize()); } @@ -246 6 +268 16 @@ public class NativeImageClassLoaderSupport { includeAllFromClassPath = IncludeAllFromClassPath.getValue(parsedHostedOptions);  new LoadClassHandler(executor  imageClassLoader).run(); +        if (customLoader != null) { +            Class<? extends ClassLoader> customLoaderClass = customLoader.getClass(); +            Method getSystemClassesMethod = ReflectionUtil.lookupMethod(true  customLoaderClass  "getSystemClasses"); +            if (getSystemClassesMethod != null) { +                List<Class<?>> customSystemClasses = ReflectionUtil.invokeMethod(getSystemClassesMethod  customLoader); +                for (Class<?> systemClass : customSystemClasses) { +                    imageClassLoader.findSystemElements(systemClass); +                } +            } +        } }  private static void missingFromSetOfEntriesError(Object entry  Collection<?> allEntries  String typeOfEntry  diff --git a/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/reflect/ReflectionFeature.java b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/reflect/ReflectionFeature.java index 5d80ffe544e..bcbf1ccb606 100644 --- a/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/reflect/ReflectionFeature.java +++ b/substratevm/src/com.oracle.svm.hosted/src/com/oracle/svm/hosted/reflect/ReflectionFeature.java @@ -63 7 +63 6 @@ import com.oracle.svm.core.feature.InternalFeature; import com.oracle.svm.core.fieldvaluetransformer.FieldValueTransformerWithAvailability; import com.oracle.svm.core.graal.meta.KnownOffsets; import com.oracle.svm.core.hub.ClassForNameSupport; -import com.oracle.svm.core.hub.ClassForNameSupportFeature; import com.oracle.svm.core.hub.DynamicHub; import com.oracle.svm.core.meta.MethodPointer; import com.oracle.svm.core.meta.SharedMethod; @@ -73 6 +72 7 @@ import com.oracle.svm.core.reflect.SubstrateConstructorAccessor; import com.oracle.svm.core.reflect.SubstrateMethodAccessor; import com.oracle.svm.core.reflect.target.ReflectionSubstitutionSupport; import com.oracle.svm.core.util.VMError; +import com.oracle.svm.hosted.ClassForNameSupportFeature; import com.oracle.svm.hosted.FallbackFeature; import com.oracle.svm.hosted.FeatureImpl; import com.oracle.svm.hosted.FeatureImpl.BeforeCompilationAccessImpl;
oracle,graal,77d04343a7bbc4c21e6d05c3463ba9ee7fe8fe5f,https://github.com/oracle/graal/commit/77d04343a7bbc4c21e6d05c3463ba9ee7fe8fe5f,Revert PublishWritesNode simplification logic  Benchmark has shown that this does not result in a measurable improvement in performance or memory usage. In the future we could try to a chieve a similar result by postponing lowering of CommitAllocationNodes.
oracle,graal,09e0d06debdde2ff41ac958d935201af949be730,https://github.com/oracle/graal/commit/09e0d06debdde2ff41ac958d935201af949be730,[GR-57969] Update PosixPerfMemoryProvider to JDK 24+13 and annotate with @BasedOnJDKFile.  PullRequest: graal/18801
oracle,graal,84be97d55464ce4741f25e62bea880666f3b0fe0,https://github.com/oracle/graal/commit/84be97d55464ce4741f25e62bea880666f3b0fe0,Update PosixPerfMemoryProvider to JDK 24+13 and annotate with @BasedOnJDKFile.
oracle,graal,dc09603757e836cab633f92fa63ff7508ceba341,https://github.com/oracle/graal/commit/dc09603757e836cab633f92fa63ff7508ceba341,[GR-57385] Improve performance of interop invoke member.  PullRequest: graal/18668
oracle,graal,6145da095b8c2abc6b007cba65c7060d12737e51,https://github.com/oracle/graal/commit/6145da095b8c2abc6b007cba65c7060d12737e51,[GR-57289] Fix type checks need to always be performed to avoid heap corruption.  PullRequest: graal/18587
oracle,graal,2141d727416ce6b384433da8b9053e3f62069db4,https://github.com/oracle/graal/commit/2141d727416ce6b384433da8b9053e3f62069db4,Fix type checks need to always be performed to avoid heap corruption.
oracle,graal,b24d9c7330aa0c0ffb3444dfcd1470c0d48fb9d8,https://github.com/oracle/graal/commit/b24d9c7330aa0c0ffb3444dfcd1470c0d48fb9d8,[GR-56337] Fix performance regressions due to inlining and JavaC.  PullRequest: graal/18503
oracle,graal,e3377572e554cba58141397948d710e6a6886101,https://github.com/oracle/graal/commit/e3377572e554cba58141397948d710e6a6886101,Only perform initial memory 0 bounds check if the wasm function uses memory 0.
oracle,graal,983982e4ee044215b08a01cccb1e43ecd57d6394,https://github.com/oracle/graal/commit/983982e4ee044215b08a01cccb1e43ecd57d6394,Taints frames on modification through espresso scope  use this information to perform a copy of te frame on an unwind from a continuable.
oracle,graal,dfc4db490068947ee60a85c004a914267e97184b,https://github.com/oracle/graal/commit/dfc4db490068947ee60a85c004a914267e97184b,Fixed a minor performance issue in the serial GC. Various fixes for @Uninterruptible.
brettwooldridge,HikariCP,e58966bc09a8c4b7862728ce0fcedc0e96ec7ba1,https://github.com/brettwooldridge/HikariCP/commit/e58966bc09a8c4b7862728ce0fcedc0e96ec7ba1,Fix inconsistency between `isWrapperFor` and `unwrap` (#2243)  Closes GH-2237
apache,shardingsphere,dfe831264086467c31708d06cd3bf165a439312c,https://github.com/apache/shardingsphere/commit/dfe831264086467c31708d06cd3bf165a439312c,Add database type parameter to SQLBindEngine.bind method (#35511)  * Optimize MySQL multi-statements handling  - Extract DatabaseType instance as a private field to improve performance - Update SQLParserEngine initialization to use the pre-loaded DatabaseType - Modify SQLBindEngine invocation to include DatabaseType as a parameter  * Add database type parameter to SQLBindEngine.bind method - Update the SQLBindEngine.bind method call to include the databaseType parameter - This change ensures that the database type is properly passed during SQL statement binding
apache,shardingsphere,df7fe751e0b9419692f003f6860205920b9b7741,https://github.com/apache/shardingsphere/commit/df7fe751e0b9419692f003f6860205920b9b7741,Optimize MySQL multi-statements handling (#35510)  - Extract DatabaseType instance as a private field to improve performance - Update SQLParserEngine initialization to use the pre-loaded DatabaseType - Modify SQLBindEngine invocation to include DatabaseType as a parameter
apache,shardingsphere,8c7c2fce787b1d16679045a7e4f8bad4f1b776d6,https://github.com/apache/shardingsphere/commit/8c7c2fce787b1d16679045a7e4f8bad4f1b776d6,Optimize MySQLComFieldListPacketExecutor with DatabaseType (#35509)  - Add a private final DatabaseType field initialized with "MySQL" - Replace inline DatabaseType retrieval with the new field for better readability and performance
apache,shardingsphere,7eb3176b02d5a786a590c5dcfb3c82454ab9c2eb,https://github.com/apache/shardingsphere/commit/7eb3176b02d5a786a590c5dcfb3c82454ab9c2eb,Add high frequency invocation annotation for tableless route engine (#35415)  - Add @HighFrequencyInvocation annotation to TablelessDataSourceUnicastRouteEngine class - This change helps identify the class as frequently used  potentially for performance monitoring or optimization
apache,shardingsphere,8f5b7a8d12551f2cacf22a6c5279707f3fe07996,https://github.com/apache/shardingsphere/commit/8f5b7a8d12551f2cacf22a6c5279707f3fe07996,Replace ThreadLocalRandom with SecureRandom for better security (#35411)  - Use SecureRandom instead of ThreadLocalRandom for generating random numbers - This change improves the security of the random selection process - The performance impact is negligible for this specific use case
apache,shardingsphere,dd3ee66539a422ea05aeb6aae6237d2c56b2297e,https://github.com/apache/shardingsphere/commit/dd3ee66539a422ea05aeb6aae6237d2c56b2297e,Optimize execution prepare engine for high frequency invocation (#35352)  - Add @HighFrequencyInvocation annotation to relevant classes - Improve data structure usage for better performance - Refactor method names for consistency and clarity
apache,shardingsphere,1c452cbf2646a6ce1213c26a4ebbd49e9570128b,https://github.com/apache/shardingsphere/commit/1c452cbf2646a6ce1213c26a4ebbd49e9570128b,Merge pull request #33361 from strongduanmu/dev-1023  Fix SQL performance issues caused by repeated subquery fetches
apache,shardingsphere,2cfd72981a16e1eaef88cbbe86e0fe4d1e751cff,https://github.com/apache/shardingsphere/commit/2cfd72981a16e1eaef88cbbe86e0fe4d1e751cff,Fix SQL performance issues caused by repeated subquery fetches
dataease,dataease,a959070b47c0ecfbe07c6117a31937953a003170,https://github.com/dataease/dataease/commit/a959070b47c0ecfbe07c6117a31937953a003170,perf: 系统设置-保存系统参数接口逻辑
dataease,dataease,d7c2ec52ca29240e155de5b24dcb545787b310b0,https://github.com/dataease/dataease/commit/d7c2ec52ca29240e155de5b24dcb545787b310b0,perf: 优化菜单查询接口查询速度
dataease,dataease,db694b12613a655726191a73fce8dbb7bb6264ea,https://github.com/dataease/dataease/commit/db694b12613a655726191a73fce8dbb7bb6264ea,perf(X-Pack): 优化还原社区版本对水印数据的处理
dataease,dataease,05235f883e1709c0ace1b7b19b4a1b1713afccd5,https://github.com/dataease/dataease/commit/05235f883e1709c0ace1b7b19b4a1b1713afccd5,perf(X-Pack): OAuth2 支持 client_secret_jwt 方式认证
dataease,dataease,1f29ce93b6cc7c49810adb8ffa008c0d29322f6c,https://github.com/dataease/dataease/commit/1f29ce93b6cc7c49810adb8ffa008c0d29322f6c,perf(X-Pack): 定时报告下载附件错误
dataease,dataease,50bf66348baf43e6ab6dd3235c8b2107ef8afc76,https://github.com/dataease/dataease/commit/50bf66348baf43e6ab6dd3235c8b2107ef8afc76,perf(X-Pack): 组织管理页面排序
dataease,dataease,1e02dfb39ff1202679cbeaa2980a07dc47d5f793,https://github.com/dataease/dataease/commit/1e02dfb39ff1202679cbeaa2980a07dc47d5f793,perf(X-Pack): 组织管理页面增加分页以及懒加载
dataease,dataease,7db41bd8bb88d4d9561baf81422380effa013bed,https://github.com/dataease/dataease/commit/7db41bd8bb88d4d9561baf81422380effa013bed,perf(API 文档): 优化数据源管理 API 描述信息
dataease,dataease,96e505779fc671858b59af04d26d21ff8bd5d1a1,https://github.com/dataease/dataease/commit/96e505779fc671858b59af04d26d21ff8bd5d1a1,perf(API 文档): 优化图表管理的 API 文档信息
dataease,dataease,7f580e7d0c223e7c43e146063dbb62e8915e2eb5,https://github.com/dataease/dataease/commit/7f580e7d0c223e7c43e146063dbb62e8915e2eb5,perf(X-Pack): 优化 X-Pack 相关 API 文档信息
dataease,dataease,b4b33e67cf6c52c5fe32e515eaa7fb73a80a9ce7,https://github.com/dataease/dataease/commit/b4b33e67cf6c52c5fe32e515eaa7fb73a80a9ce7,perf(X-Pack): 根据账号获取用户信息接口
dataease,dataease,6567c09acd88470e13b836b44fe5f728e56c5115,https://github.com/dataease/dataease/commit/6567c09acd88470e13b836b44fe5f728e56c5115,perf: 国际化默认语言设置
dataease,dataease,38f4f96901c9272bf74c364b68a73ba8fe49870a,https://github.com/dataease/dataease/commit/38f4f96901c9272bf74c364b68a73ba8fe49870a,perf: 删除存在 SQL 注入风险的代码
dataease,dataease,e13a594d45d58f24247a5663793e3361d53f9ace,https://github.com/dataease/dataease/commit/e13a594d45d58f24247a5663793e3361d53f9ace,perf: 增强url特殊字符攻击检测
dataease,dataease,0d5154e92136dd87651f91e90357b0b57f54291f,https://github.com/dataease/dataease/commit/0d5154e92136dd87651f91e90357b0b57f54291f,perf(X-Pack): 阈值告警-定时任务国际化
dataease,dataease,af6c1d7492208c8ecc31a4c2c5d27ec96a852de0,https://github.com/dataease/dataease/commit/af6c1d7492208c8ecc31a4c2c5d27ec96a852de0,perf(仪表板): 公共链接api白名单
dataease,dataease,4e7f8450e5335cbfcade9b2a832cf9d50a4895e8,https://github.com/dataease/dataease/commit/4e7f8450e5335cbfcade9b2a832cf9d50a4895e8,perf: 社区版使用默认密码无法登录
dataease,dataease,f91e5a1071b89aa78ce5abe1c18c2a1f287eff2d,https://github.com/dataease/dataease/commit/f91e5a1071b89aa78ce5abe1c18c2a1f287eff2d,perf(X-Pack): 修改初始密码逻辑在 MFA 之前
dataease,dataease,8688d8aa92e54360713375893bd95ab4e3b786b1,https://github.com/dataease/dataease/commit/8688d8aa92e54360713375893bd95ab4e3b786b1,perf(X-Pack): Webhhok 切换 Ssl无效
dataease,dataease,62f1554ba5472ae25d93c2da2c6c704d22a30819,https://github.com/dataease/dataease/commit/62f1554ba5472ae25d93c2da2c6c704d22a30819,perf(X-Pack): 用户管理-批量导入国际化
dataease,dataease,4868a778ccd508406a0079d37ff91327eda0e458,https://github.com/dataease/dataease/commit/4868a778ccd508406a0079d37ff91327eda0e458,perf(X-Pack): 阈值告警-后端国际化
dataease,dataease,1927e9fbf7f17c68aeb503dcf42faad7b6965fc4,https://github.com/dataease/dataease/commit/1927e9fbf7f17c68aeb503dcf42faad7b6965fc4,perf(仪表板): 解决冲突合并代码
dataease,dataease,a1a0780ed4c8c1cffec0589b4e5d25b4f4b06fd4,https://github.com/dataease/dataease/commit/a1a0780ed4c8c1cffec0589b4e5d25b4f4b06fd4,perf(仪表板): 分享 Ticket增加分页机制
dataease,dataease,5bc800cb60cd2c2d4c2512c30833a8faf31a0a95,https://github.com/dataease/dataease/commit/5bc800cb60cd2c2d4c2512c30833a8faf31a0a95,perf(X-Pack): 嵌入式管理页面优化-增加分页组件
dataease,dataease,1dbc9e62e66f3652503bc014edd3bc188631f75c,https://github.com/dataease/dataease/commit/1dbc9e62e66f3652503bc014edd3bc188631f75c,perf(X-Pack): OIDC 认证配置支持字段映射配置
dataease,dataease,87bed21277ea0d4f7cf3e0dfc52f2341f38014e2,https://github.com/dataease/dataease/commit/87bed21277ea0d4f7cf3e0dfc52f2341f38014e2,perf(X-Pack): MFA 页面交互优化
dataease,dataease,ef4e27240155b6fc5d33fa597fe1b8938865495a,https://github.com/dataease/dataease/commit/ef4e27240155b6fc5d33fa597fe1b8938865495a,Merge pull request #13951 from dataease/pr@dev-v2@perf_ds_excel_upload  perf(数据源): Excel 数据源上传失败
dataease,dataease,6b6376e4bce3610a9f3a84a1aff8b1b30b811e2b,https://github.com/dataease/dataease/commit/6b6376e4bce3610a9f3a84a1aff8b1b30b811e2b,Merge pull request #13953 from dataease/pr@dev-v2@perf_community_language  perf: 社区版语言切换
dataease,dataease,aa39d52a554db503e5ec82d539e8b20466cf34b2,https://github.com/dataease/dataease/commit/aa39d52a554db503e5ec82d539e8b20466cf34b2,perf: 社区版语言切换
dataease,dataease,1c400d4440b71a700f4d5c3e89e9dfa9dbe76c80,https://github.com/dataease/dataease/commit/1c400d4440b71a700f4d5c3e89e9dfa9dbe76c80,perf(数据源): Excel 数据源上传失败
dataease,dataease,de7f98a48b7533619cbbd6c7fec0f36b474ef2d8,https://github.com/dataease/dataease/commit/de7f98a48b7533619cbbd6c7fec0f36b474ef2d8,Merge pull request #13942 from dataease/pr@dev-v2@perf_user_bind_mfa  perf(X-Pack): 用户绑定 MFA
dataease,dataease,3a62fe5eebdc45936352126a50d86c8c842bcd5d,https://github.com/dataease/dataease/commit/3a62fe5eebdc45936352126a50d86c8c842bcd5d,perf(X-Pack): 用户绑定 MFA
dataease,dataease,765c6bbec19db46a04bfe0f1384ea20bf0477f29,https://github.com/dataease/dataease/commit/765c6bbec19db46a04bfe0f1384ea20bf0477f29,Merge pull request #13902 from dataease/pr@dev-v2@perf_local_mfa_login  perf(X-Pack): 本地用户登录 MFA 二次校验
dataease,dataease,6f9728d15bd01de74a08d2c3ab6ab720f099f95b,https://github.com/dataease/dataease/commit/6f9728d15bd01de74a08d2c3ab6ab720f099f95b,perf(X-Pack): 本地用户登录 MFA 二次校验
dataease,dataease,1b5c1c7f19e903f12b639ab57db7b4b07be83e4f,https://github.com/dataease/dataease/commit/1b5c1c7f19e903f12b639ab57db7b4b07be83e4f,Merge pull request #13888 from dataease/pr@dev-v2@perf_fma_login  perf(X-Pack): 登录增加 MFA 机制
dataease,dataease,a9233f306263cfb66763a425656887ee23e2ddd7,https://github.com/dataease/dataease/commit/a9233f306263cfb66763a425656887ee23e2ddd7,perf(X-Pack): 登录增加 MFA 机制
dataease,dataease,204258736c128c4a23f67f2c5687418fb25fd220,https://github.com/dataease/dataease/commit/204258736c128c4a23f67f2c5687418fb25fd220,Merge pull request #13878 from dataease/pr@dev-v2@perf_person_info_mfa  Pr@dev v2@perf person info mfa
dataease,dataease,32f5b5d4582a82bcd9e80312c0b6e66cfc0eb6c7,https://github.com/dataease/dataease/commit/32f5b5d4582a82bcd9e80312c0b6e66cfc0eb6c7,perf(X-Pack): 用户个人信息增加 MFA 开关
dataease,dataease,5dd3d556b57b802e77ea6ed25c52895f37da3484,https://github.com/dataease/dataease/commit/5dd3d556b57b802e77ea6ed25c52895f37da3484,Merge pull request #13874 from dataease/pr@dev-v2@perf_security_page  perf(X-Pack): 安全设置-MFA-api文案
dataease,dataease,53aafdd3c888cdf4255b0387600e8f02e2c17f73,https://github.com/dataease/dataease/commit/53aafdd3c888cdf4255b0387600e8f02e2c17f73,perf(X-Pack): 安全设置-MFA-api文案
dataease,dataease,4b1e9b27a9cc0f0578169306bb3e1e01fdc12a56,https://github.com/dataease/dataease/commit/4b1e9b27a9cc0f0578169306bb3e1e01fdc12a56,Merge pull request #13873 from dataease/pr@dev-v2@perf_security_page  perf(X-Pack): 安全设置页面
dataease,dataease,9176f5680c8a041573ca66275e8228ba7fce3c11,https://github.com/dataease/dataease/commit/9176f5680c8a041573ca66275e8228ba7fce3c11,perf(X-Pack): 安全设置页面
dataease,dataease,cdeec6633e1129d5b177465594c3892b04385329,https://github.com/dataease/dataease/commit/cdeec6633e1129d5b177465594c3892b04385329,Merge pull request #13809 from dataease/pr@dev-v2@perf_report_export  perf(X-Pack): 定时报告-导出的数据仅有第一页
dataease,dataease,aac0756865e75424ce9c2a150c48b5d73f4ab9ab,https://github.com/dataease/dataease/commit/aac0756865e75424ce9c2a150c48b5d73f4ab9ab,perf(X-Pack): 定时报告-导出的数据仅有第一页
dataease,dataease,83adc9c2d71a9611e58a96bc25f5e515474dd5c1,https://github.com/dataease/dataease/commit/83adc9c2d71a9611e58a96bc25f5e515474dd5c1,Merge branch 'dev-v2' into pr@dev-v2@perf_i18n
dataease,dataease,be22cdf966b51ce99497779c99c782264ee77263,https://github.com/dataease/dataease/commit/be22cdf966b51ce99497779c99c782264ee77263,Merge pull request #13758 from dataease/pr@dev-v2@perf_threshold_number  perf(X-Pack): 阈值告警-使用数值动态值选择等于不会触发
dataease,dataease,0f88dbf5ffc6eeaf907b452a891d3534295a651d,https://github.com/dataease/dataease/commit/0f88dbf5ffc6eeaf907b452a891d3534295a651d,perf(X-Pack): 阈值告警-使用数值动态值选择等于不会触发
dataease,dataease,40404f29c0e9ed96d3b0825afa1a8a43ef8c88b7,https://github.com/dataease/dataease/commit/40404f29c0e9ed96d3b0825afa1a8a43ef8c88b7,Merge pull request #13725 from dataease/pr@dev-v2@perf_auth_weight  perf: 社区版默认资源权重
dataease,dataease,bc777cac252569c78d6dcab3021bfe308fdc6845,https://github.com/dataease/dataease/commit/bc777cac252569c78d6dcab3021bfe308fdc6845,perf: 社区版默认资源权重
dataease,dataease,a3e5fd3c0c7e5a7e0751a06ee7c19ae1e59aac0d,https://github.com/dataease/dataease/commit/a3e5fd3c0c7e5a7e0751a06ee7c19ae1e59aac0d,Merge pull request #13705 from dataease/pr@dev-v2@perf_ds_count  perf(X-Pack): 数据源应用数量控制
dataease,dataease,bb01077806ce28ebb6bd414cd71f9cc2a253a4c9,https://github.com/dataease/dataease/commit/bb01077806ce28ebb6bd414cd71f9cc2a253a4c9,perf(X-Pack): 数据源应用数量控制
dataease,dataease,310fd0151f5f6b2dec2ed567ff4bd25736a9b417,https://github.com/dataease/dataease/commit/310fd0151f5f6b2dec2ed567ff4bd25736a9b417,Merge pull request #13704 from dataease/pr@dev-v2@perf_embedded_count  perf(X-Pack): 嵌入式应用数量控制
dataease,dataease,73c45908afb3891482871707992852cafd2b7d0f,https://github.com/dataease/dataease/commit/73c45908afb3891482871707992852cafd2b7d0f,perf(X-Pack): 嵌入式应用数量控制
dataease,dataease,557b19874c07221e60e3d70d679d62715e5ea7c7,https://github.com/dataease/dataease/commit/557b19874c07221e60e3d70d679d62715e5ea7c7,Merge pull request #13686 from dataease/pr@dev-v2@perf_cas_api_auth  perf(X-Pack): CAS方式登录api鉴权
dataease,dataease,1b59f3f78562b09d33ee07c9dea19e5dcf700c83,https://github.com/dataease/dataease/commit/1b59f3f78562b09d33ee07c9dea19e5dcf700c83,perf(X-Pack): CAS方式登录api鉴权
dataease,dataease,46c66b2395ec26c18c794989c33d7e97886a7418,https://github.com/dataease/dataease/commit/46c66b2395ec26c18c794989c33d7e97886a7418,Merge pull request #13633 from dataease/pr@dev-v2@perf_del_auto_sync  perf: 删除自动同步游离资源逻辑
dataease,dataease,3b95c6a94510e94f16dd88e328f83c3e3805cf4b,https://github.com/dataease/dataease/commit/3b95c6a94510e94f16dd88e328f83c3e3805cf4b,perf: 删除自动同步游离资源逻辑
dataease,dataease,1186bb7f8947dde1194233e994d498794bef0ce7,https://github.com/dataease/dataease/commit/1186bb7f8947dde1194233e994d498794bef0ce7,Merge pull request #13624 from dataease/pr@dev-v2@perf_cors_config  feat: 增加是否严格校验跨域配置
dataease,dataease,ae402bdda58b93cc67f0ee8a14ad9e37ce3322cb,https://github.com/dataease/dataease/commit/ae402bdda58b93cc67f0ee8a14ad9e37ce3322cb,Merge pull request #13613 from dataease/pr@dev-v2@perf_free_resource_auth  perf(X-Pack): 游离资源迁移后同步权限
dataease,dataease,e54a0ff03d1ec26431e47f7a58e329f5306be753,https://github.com/dataease/dataease/commit/e54a0ff03d1ec26431e47f7a58e329f5306be753,perf(X-Pack): 游离资源迁移后同步权限
dataease,dataease,7a87a511ce13aca69f4e43ca6ac50973f2a77935,https://github.com/dataease/dataease/commit/7a87a511ce13aca69f4e43ca6ac50973f2a77935,Merge pull request #13559 from dataease/pr@dev-v2@perf_login_validate  Pr@dev v2@perf login validate
dataease,dataease,162b5f318b99b71b622b3fe56065fb06d9e0ffcd,https://github.com/dataease/dataease/commit/162b5f318b99b71b622b3fe56065fb06d9e0ffcd,perf(X-Pack): 游离资源血缘关系图
dataease,dataease,1d788750fb74f98d5aff762df561504b4dc939cc,https://github.com/dataease/dataease/commit/1d788750fb74f98d5aff762df561504b4dc939cc,Merge pull request #13516 from dataease/pr@dev-v2@perf_free_resource_delete  perf(X-Pack): 游离资源删除
dataease,dataease,c4818a2a7a808747fc348cbf65f4cf73e8130c46,https://github.com/dataease/dataease/commit/c4818a2a7a808747fc348cbf65f4cf73e8130c46,perf(X-Pack): 游离资源删除
dataease,dataease,dce592671807d1d09fc9063586434cfbba66a58e,https://github.com/dataease/dataease/commit/dce592671807d1d09fc9063586434cfbba66a58e,Merge pull request #13509 from dataease/pr@dev-v2@perf_free_resource  perf(X-Pack): 游离资源序列化
dataease,dataease,513ea684ec3cf7e7737da7a3eab0a5863920111e,https://github.com/dataease/dataease/commit/513ea684ec3cf7e7737da7a3eab0a5863920111e,perf(X-Pack): 游离资源序列化
dataease,dataease,87a7a1b8bb97b975bc0c1587a5c95068ada530ff,https://github.com/dataease/dataease/commit/87a7a1b8bb97b975bc0c1587a5c95068ada530ff,Merge pull request #13508 from dataease/pr@dev-v2@perf_community  perf(X-Pack): 官方社区版仅展示游离资源
dataease,dataease,904aa2c3351f94f394054d3cd6e56b277775b942,https://github.com/dataease/dataease/commit/904aa2c3351f94f394054d3cd6e56b277775b942,perf(X-Pack): 官方社区版仅展示游离资源
dataease,dataease,5832bb8e427a2ca117df5d0d0cd7fd19fd7e0175,https://github.com/dataease/dataease/commit/5832bb8e427a2ca117df5d0d0cd7fd19fd7e0175,Merge pull request #13487 from dataease/pr@dev-v2@perf_free_resource  perf(X-Pack): 游离资源管理页面
dataease,dataease,3fb3b97d9e29871f4619041e3c2fd8f17c8d02aa,https://github.com/dataease/dataease/commit/3fb3b97d9e29871f4619041e3c2fd8f17c8d02aa,Merge branch 'dev-v2' into pr@dev-v2@perf_free_resource
dataease,dataease,9a5ef82558feb35f3ac3f50f878efcedda02b7f2,https://github.com/dataease/dataease/commit/9a5ef82558feb35f3ac3f50f878efcedda02b7f2,perf(X-Pack): 游离资源管理页面
dataease,dataease,9ee7d49db2f2386ba28ee9cc9d63a01ff37b1ebc,https://github.com/dataease/dataease/commit/9ee7d49db2f2386ba28ee9cc9d63a01ff37b1ebc,Merge pull request #13406 from dataease/pr@dev-v2@perf_login_limit  perf(X-Pack): 系统参数-基础设置增加登录设置项
dataease,dataease,3eb6bca5ac8df01988d30e187a058885f804e00b,https://github.com/dataease/dataease/commit/3eb6bca5ac8df01988d30e187a058885f804e00b,perf(X-Pack): 系统参数-基础设置增加登录设置项
dataease,dataease,7951d15a2d130c62362bf77b563b2ce2eb2f3b56,https://github.com/dataease/dataease/commit/7951d15a2d130c62362bf77b563b2ce2eb2f3b56,Merge pull request #13394 from dataease/pr@dev-v2@perf_log_client  perf(X-Pack): 日志区分客户端类型
dataease,dataease,7b4a6dadb9d3948449b4c5f5a2e881609cb9b66c,https://github.com/dataease/dataease/commit/7b4a6dadb9d3948449b4c5f5a2e881609cb9b66c,perf(X-Pack): 日志区分客户端类型
dataease,dataease,42b2537a2eca75232e9553a8fae07a979b74899d,https://github.com/dataease/dataease/commit/42b2537a2eca75232e9553a8fae07a979b74899d,Merge pull request #13388 from dataease/pr@dev-v2@perf_embedded_secret_length  perf(X-Pack): 嵌入式应用指定密钥长度
dataease,dataease,0ab3e6f510090eeb9a8b0dcff11a5015ced84015,https://github.com/dataease/dataease/commit/0ab3e6f510090eeb9a8b0dcff11a5015ced84015,perf(X-Pack): 嵌入式应用指定密钥长度
dataease,dataease,b859c99ea394cda5e9ff173e45163fa1b4b8acd1,https://github.com/dataease/dataease/commit/b859c99ea394cda5e9ff173e45163fa1b4b8acd1,Merge pull request #13367 from dataease/pr@dev-v2@export_data_permission  perf(X-Pack): 仪表板导出权限页面控制
dataease,dataease,8eb801dbc0f4b272a03d61b29cfaa6441c1bcf33,https://github.com/dataease/dataease/commit/8eb801dbc0f4b272a03d61b29cfaa6441c1bcf33,perf(X-Pack): 仪表板导出权限页面控制
dataease,dataease,dae7d6c021e901cecb5a169f9cf53c0aa96f685f,https://github.com/dataease/dataease/commit/dae7d6c021e901cecb5a169f9cf53c0aa96f685f,Merge pull request #13364 from dataease/pr@dev-v2@perf_export_auth  perf(X-Pack): 导出权限独立控制
dataease,dataease,9da46043e197aefe2b59c3c4babf531a0e786130,https://github.com/dataease/dataease/commit/9da46043e197aefe2b59c3c4babf531a0e786130,perf(X-Pack): 导出权限独立控制
dataease,dataease,f62efef775495c86b1459a6ba141c1577f5ce3d8,https://github.com/dataease/dataease/commit/f62efef775495c86b1459a6ba141c1577f5ce3d8,Merge pull request #13348 from dataease/pr@dev-v2@perf_msg_fill_menu  perf(X-Pack): 填报消息菜单设置
dataease,dataease,5e219563517ea63f12a2c4fa9c1631fa6e942490,https://github.com/dataease/dataease/commit/5e219563517ea63f12a2c4fa9c1631fa6e942490,perf(X-Pack): 填报消息菜单设置
dataease,dataease,e8bb2d6fd6f91c084723ab062201b7af76263c33,https://github.com/dataease/dataease/commit/e8bb2d6fd6f91c084723ab062201b7af76263c33,Merge pull request #13267 from dataease/pr@dev-v2@perf_auth_ext  Pr@dev v2@perf auth ext
dataease,dataease,36c6d84298ea6ef17fed714c4dac1cc9bc356430,https://github.com/dataease/dataease/commit/36c6d84298ea6ef17fed714c4dac1cc9bc356430,perf: typos拼写错误
dataease,dataease,1ca5e8935a3213f42b1015cc8e9c1a85e1b9088a,https://github.com/dataease/dataease/commit/1ca5e8935a3213f42b1015cc8e9c1a85e1b9088a,Merge pull request #13212 from dataease/pr@dev-v2@perf_person_ip  perf: 社区版获取客户端IP信息
dataease,dataease,ed51826bae09b9483b29be969c4522194b41e5c9,https://github.com/dataease/dataease/commit/ed51826bae09b9483b29be969c4522194b41e5c9,perf: 社区版获取客户端IP信息
dataease,dataease,86aa3302b593d8f014447da1d5fba2ab78828ed6,https://github.com/dataease/dataease/commit/86aa3302b593d8f014447da1d5fba2ab78828ed6,Merge pull request #13156 from dataease/pr@dev-v2@perf_share_exp_require  perf(仪表板): 分享-设置有效期必填后分享链接关闭有效期链接依然有效
dataease,dataease,f8bec0ae0840522ffbcbc86e5c6a80ee20129c77,https://github.com/dataease/dataease/commit/f8bec0ae0840522ffbcbc86e5c6a80ee20129c77,perf(仪表板): 分享-设置有效期必填后分享链接关闭有效期链接依然有效
dataease,dataease,5f17ed6c37dee9d669c2f0d9c285f74b582e6f75,https://github.com/dataease/dataease/commit/5f17ed6c37dee9d669c2f0d9c285f74b582e6f75,perf(X-Pack): 权限配置-增加独立权限节点
dataease,dataease,8c7501bada38529e89aee5fceeba42d511425001,https://github.com/dataease/dataease/commit/8c7501bada38529e89aee5fceeba42d511425001,Merge pull request #13038 from dataease/pr@dev-v2@perf_redis_cache  perf: 集群环境redis缓存优化
dataease,dataease,b7368a236ae29aca86072cf6ddb3e179e857e9b9,https://github.com/dataease/dataease/commit/b7368a236ae29aca86072cf6ddb3e179e857e9b9,perf: 集群环境redis缓存优化
dataease,dataease,41bb1510fc33b3aa27629e60f8776839a61145de,https://github.com/dataease/dataease/commit/41bb1510fc33b3aa27629e60f8776839a61145de,Merge pull request #12976 from dataease/pr@dev-v2@perf_share  perf(仪表板): 分享功能增加全局禁用以及有效期密码必填设置 #12815 #12816
dataease,dataease,273c8ae27587e5a0793c11ff2e9051338926e66f,https://github.com/dataease/dataease/commit/273c8ae27587e5a0793c11ff2e9051338926e66f,Merge branch 'dev-v2' into pr@dev-v2@perf_share
dataease,dataease,e7d2cc82b92286ffdf7196878def87feb5d27b24,https://github.com/dataease/dataease/commit/e7d2cc82b92286ffdf7196878def87feb5d27b24,perf(仪表板): 分享功能增加全局禁用以及有效期密码必填设置 #12815 #12816
dataease,dataease,278fdeae5507e20b66ff529ab3a8c0c73b89390f,https://github.com/dataease/dataease/commit/278fdeae5507e20b66ff529ab3a8c0c73b89390f,Merge pull request #12881 from dataease/pr@dev-v2@perf_threshold_org_deafult_admin  perf(X-Pack): 阈值告警-仅组织默认管理员可查看组织下所有告警
dataease,dataease,628f4e36821ba0d79b19474716cd34f5a4709f4d,https://github.com/dataease/dataease/commit/628f4e36821ba0d79b19474716cd34f5a4709f4d,perf(X-Pack): 阈值告警-仅组织默认管理员可查看组织下所有告警
dataease,dataease,65ad3dd7507d2b36c69d33e5c0154f3cb627a24c,https://github.com/dataease/dataease/commit/65ad3dd7507d2b36c69d33e5c0154f3cb627a24c,Merge pull request #12810 from dataease/pr@dev-v2@perf_threshold_dynamic_date_perf  perf(X-Pack): 阈值告警-日期类型字段动态值
dataease,dataease,74f3f25ddeb8e9827eb12f8c7285938a16620339,https://github.com/dataease/dataease/commit/74f3f25ddeb8e9827eb12f8c7285938a16620339,perf(X-Pack): 阈值告警-日期类型字段动态值
dataease,dataease,9d86e9b6494a6853b77493dcf7bd1b0db96269c3,https://github.com/dataease/dataease/commit/9d86e9b6494a6853b77493dcf7bd1b0db96269c3,Merge pull request #12795 from dataease/pr@dev-v2@perf@api_description  perf: api文档描述文案
dataease,dataease,ffbe0e77e4f90d50ead3c882d90fdcd38ada0671,https://github.com/dataease/dataease/commit/ffbe0e77e4f90d50ead3c882d90fdcd38ada0671,perf: api文档描述文案
dataease,dataease,f7406ad4869df288855b53097b82101ceb301632,https://github.com/dataease/dataease/commit/f7406ad4869df288855b53097b82101ceb301632,Merge pull request #12790 from dataease/pr@dev-v2@perf_threshold_dynamic_date  perf(X-Pack): 阈值告警-告警规则日期类型字段增加动态值支持 #12612
dataease,dataease,1ebea375efb5de832f8c416c57df104133995d5e,https://github.com/dataease/dataease/commit/1ebea375efb5de832f8c416c57df104133995d5e,perf(X-Pack): 阈值告警-告警规则日期类型字段增加动态值支持 #12612
dataease,dataease,5d5cdf0be683bf5ff9eee9198a83b3aba6e28563,https://github.com/dataease/dataease/commit/5d5cdf0be683bf5ff9eee9198a83b3aba6e28563,Merge pull request #12695 from dataease/pr@dev-v2@perf_redis_cache  perf(X-Pack): 集群环境中redis缓存清除机制
dataease,dataease,c4fe12f71f0de4ba87fc94c6825d2fcca318c52b,https://github.com/dataease/dataease/commit/c4fe12f71f0de4ba87fc94c6825d2fcca318c52b,perf(X-Pack): 集群环境中redis缓存清除机制
dataease,dataease,abfae548be73433de45c3c46245bad491d17eadc,https://github.com/dataease/dataease/commit/abfae548be73433de45c3c46245bad491d17eadc,Merge pull request #12682 from dataease/pr@dev-v2@perf_token  perf(X-Pack): 优化社区版token验证机制
dataease,dataease,b28b00850afbd38458f4f3510fea95e4d939575d,https://github.com/dataease/dataease/commit/b28b00850afbd38458f4f3510fea95e4d939575d,perf(X-Pack): 优化社区版token验证机制
dataease,dataease,7f116c027168151797f28d8e00682372a1ea836f,https://github.com/dataease/dataease/commit/7f116c027168151797f28d8e00682372a1ea836f,Merge pull request #12550 from dataease/pr@dev-v2@perf_community_token  perf: 优化社区版token机制
dataease,dataease,b3bb62b12362e5a1879a26428f710a4c0c0b44ca,https://github.com/dataease/dataease/commit/b3bb62b12362e5a1879a26428f710a4c0c0b44ca,perf: 优化社区版token机制
dataease,dataease,bec1873704ccc2de12f0143a0cdc02b10b56ceb5,https://github.com/dataease/dataease/commit/bec1873704ccc2de12f0143a0cdc02b10b56ceb5,Merge pull request #12538 from dataease/pr@dev-v2@perf_token  perf: 社区版token机制
dataease,dataease,e755248d59543bcd668ace495f293ff735fa82e9,https://github.com/dataease/dataease/commit/e755248d59543bcd668ace495f293ff735fa82e9,perf: 社区版token机制
dataease,dataease,982f833d546220fda49781a24af2191b9ca14a54,https://github.com/dataease/dataease/commit/982f833d546220fda49781a24af2191b9ca14a54,Merge pull request #12493 from dataease/pr@dev-v2@perf_xpack_request  perf(X-Pack): 社区版本取消xpack请求 #12477
dataease,dataease,992120f8aa605469e476a9f974a63e7e6506baa1,https://github.com/dataease/dataease/commit/992120f8aa605469e476a9f974a63e7e6506baa1,perf(X-Pack): 社区版本取消xpack请求
dataease,dataease,0d3e8b051032ab794474a12ea4a58beed99d3f83,https://github.com/dataease/dataease/commit/0d3e8b051032ab794474a12ea4a58beed99d3f83,Merge pull request #12400 from dataease/pr@dev-v2@perf_front_request  perf(X-Pack): 前端同步请求优化合并
dataease,dataease,87523d54f202c721964454979b1beaa743b50a86,https://github.com/dataease/dataease/commit/87523d54f202c721964454979b1beaa743b50a86,perf(X-Pack): 前端同步请求优化合并
dataease,dataease,186b87c5a4cb544aa943d3ffb7103dba2264c35e,https://github.com/dataease/dataease/commit/186b87c5a4cb544aa943d3ffb7103dba2264c35e,Merge pull request #12383 from dataease/pr@dev-v2@perf_export_panel  perf(X-Pack): 后台导出仪表板并发参数可配置
dataease,dataease,a60d481c0d80eb39f379c003ebba11659af46623,https://github.com/dataease/dataease/commit/a60d481c0d80eb39f379c003ebba11659af46623,perf(X-Pack): 后台导出仪表板并发参数可配置
dataease,dataease,9fee9c9f1e52fa91cd35283fdd35fb15205eba0e,https://github.com/dataease/dataease/commit/9fee9c9f1e52fa91cd35283fdd35fb15205eba0e,Merge pull request #12270 from dataease/pr@dev-v2@perf_dataset_this_call  perf(数据集): 视图获取数据集方式优化
dataease,dataease,1f07d4a93cb75b208d57756839fce8cdc96c7e2a,https://github.com/dataease/dataease/commit/1f07d4a93cb75b208d57756839fce8cdc96c7e2a,perf(数据集): 视图获取数据集方式优化
dataease,dataease,a43e4c979607d6661846e950bc374cfa0e851e83,https://github.com/dataease/dataease/commit/a43e4c979607d6661846e950bc374cfa0e851e83,Merge pull request #12211 from dataease/pr@dev-v2@perf_oauth2_validate  perf(X-Pack): OAuth2校验机制
dataease,dataease,82aa669977a7ceba9df7ede011c65d16e4add951,https://github.com/dataease/dataease/commit/82aa669977a7ceba9df7ede011c65d16e4add951,perf(X-Pack): OAuth2校验机制
dataease,dataease,f16d893120bbfb6c618146cc2f2a5dc8361e66d6,https://github.com/dataease/dataease/commit/f16d893120bbfb6c618146cc2f2a5dc8361e66d6,Merge pull request #12090 from dataease/pr@dev-v2@perf_api  perf: 数据集接口文档参数描述不准确
dataease,dataease,cf9e9d7f8b18ba0f545db9c8b8da9ba08dc32bbd,https://github.com/dataease/dataease/commit/cf9e9d7f8b18ba0f545db9c8b8da9ba08dc32bbd,perf: 数据集接口文档参数描述不准确
dataease,dataease,cea7b58a0c0d2fb70b5b6edbb02b06d4080f3ece,https://github.com/dataease/dataease/commit/cea7b58a0c0d2fb70b5b6edbb02b06d4080f3ece,Merge pull request #12087 from dataease/pr@dev-v2@perf_auth_save_api  perf: 保存权限接口参数文档必填项描述不准确
dataease,dataease,0a35fd1e9a6ff925932a56fe263ede08c2a75595,https://github.com/dataease/dataease/commit/0a35fd1e9a6ff925932a56fe263ede08c2a75595,perf: 保存权限接口参数文档必填项描述不准确
dataease,dataease,6dbd7d92e821da5c63855f3f1c4b024a8589a609,https://github.com/dataease/dataease/commit/6dbd7d92e821da5c63855f3f1c4b024a8589a609,Merge pull request #12070 from dataease/pr@dev-v2@perf_threshold_time_field  perf(X-Pack): 阈值告警-时间字段规则
dataease,dataease,c12a7dc00da93aa444638eeaf1e2aedca7858877,https://github.com/dataease/dataease/commit/c12a7dc00da93aa444638eeaf1e2aedca7858877,perf(X-Pack): 阈值告警-时间字段规则
dataease,dataease,ce0bf5e8ba832ade738545480b12f3016b846fb7,https://github.com/dataease/dataease/commit/ce0bf5e8ba832ade738545480b12f3016b846fb7,Merge pull request #12034 from dataease/pr@dev-v2@perf_chart_exception  fix(X-Pack): 阈值告警-图表数据异常导致的NPE
dataease,dataease,4787d2857bec0e6ee5f7e944c4762ae464d843da,https://github.com/dataease/dataease/commit/4787d2857bec0e6ee5f7e944c4762ae464d843da,Merge pull request #12032 from dataease/pr@dev-v2@perf_threshold_npe  fix(X-Pack): 阈值告警-可能存在的NPE
dataease,dataease,05a8935baca6bff2880c04d7ca88ce1bd9f51a1a,https://github.com/dataease/dataease/commit/05a8935baca6bff2880c04d7ca88ce1bd9f51a1a,Merge pull request #12018 from dataease/pr@dev-v2@perf_share_ticket_exp  fix(仪表板): 分享链接ticket的有效期没有随着刷新ticket重置
dataease,dataease,d00d165fc47d0495144ea603fd0e8424f6ba1ee9,https://github.com/dataease/dataease/commit/d00d165fc47d0495144ea603fd0e8424f6ba1ee9,Merge pull request #11996 from dataease/pr@dev-v2@perf_threshold_task_log  Pr@dev v2@perf threshold task log
dataease,dataease,789ed0ad10761110059ba70299f4a81875012978,https://github.com/dataease/dataease/commit/789ed0ad10761110059ba70299f4a81875012978,perf(X-Pack): 阈值告警任务日志
dataease,dataease,8e1e9d6f6c9c3bfda1dd834ff7c6c3702c05e9e1,https://github.com/dataease/dataease/commit/8e1e9d6f6c9c3bfda1dd834ff7c6c3702c05e9e1,Merge pull request #11992 from dataease/pr@dev-v2@perf_threshold_chart_exclude  fix(X-Pack): 阈值告警-混合类型图表禁用
dataease,dataease,d7b77af7f2db0267eb76768029591d1dccb22806,https://github.com/dataease/dataease/commit/d7b77af7f2db0267eb76768029591d1dccb22806,Merge pull request #11981 from dataease/pr@dev-v2@perf_threshold_null_value  perf(X-Pack): 阈值告警图表数据包含空值无效
dataease,dataease,7cb63778d3849e0421ddb37a9b0116ac6a1001a5,https://github.com/dataease/dataease/commit/7cb63778d3849e0421ddb37a9b0116ac6a1001a5,perf(X-Pack): 阈值告警图表数据包含空值无效
dataease,dataease,94dd0f17174bc04767cc0fd40e0a98061e71b2ec,https://github.com/dataease/dataease/commit/94dd0f17174bc04767cc0fd40e0a98061e71b2ec,Merge pull request #11947 from dataease/pr@dev-v2@perf_threshold_task  perf(X-Pack): 阈值告警任务
dataease,dataease,05cf5324975e6bc221c9f85489ae977962cfc988,https://github.com/dataease/dataease/commit/05cf5324975e6bc221c9f85489ae977962cfc988,perf(X-Pack): 阈值告警任务
dataease,dataease,dac355bd907cf6ec27d37df83ce3345e4da5bab1,https://github.com/dataease/dataease/commit/dac355bd907cf6ec27d37df83ce3345e4da5bab1,Merge pull request #11945 from dataease/pr@dev-v2@perf_threshold_view  perf(X-Pack): 阈值告警-兼容流向地图、热力图
dataease,dataease,af91d3adcc60e3d9b53f6d5203cb48937cf42abf,https://github.com/dataease/dataease/commit/af91d3adcc60e3d9b53f6d5203cb48937cf42abf,perf(X-Pack): 阈值告警-兼容流向地图、热力图
dataease,dataease,ca2cc0949d215d210e9d63c3a48e6acc2168780f,https://github.com/dataease/dataease/commit/ca2cc0949d215d210e9d63c3a48e6acc2168780f,Merge pull request #11870 from dataease/pr@dev-v2@perf_threshold_instance_clean  perf(X-Pack): 阈值告警记录定时清理
dataease,dataease,ee3125a6e4734ff3f812e54b94efdf522f673915,https://github.com/dataease/dataease/commit/ee3125a6e4734ff3f812e54b94efdf522f673915,perf(X-Pack): 阈值告警记录定时清理
dataease,dataease,aa7c0b36dc334c76b07ee9e224aa4f062a339cd8,https://github.com/dataease/dataease/commit/aa7c0b36dc334c76b07ee9e224aa4f062a339cd8,Merge pull request #11825 from dataease/pr@dev-v2@perf_report_task_msg  perf(X-Pack): 定时报告接入企业微信和钉钉消息
dataease,dataease,027ced0aa99e338f497e7a819014241033da3055,https://github.com/dataease/dataease/commit/027ced0aa99e338f497e7a819014241033da3055,perf(X-Pack): 定时报告接入企业微信和钉钉消息
dataease,dataease,e1d4ccaaee67c33db86520b8329d2d3f3193b582,https://github.com/dataease/dataease/commit/e1d4ccaaee67c33db86520b8329d2d3f3193b582,Merge pull request #11802 from dataease/pr@dev-v2@perf_communicate  perf(X-Pack): 企业微信消息通知
dataease,dataease,80b9a3f4f7af8d5cc6bc07cd888f62e0c30a2dd0,https://github.com/dataease/dataease/commit/80b9a3f4f7af8d5cc6bc07cd888f62e0c30a2dd0,perf(X-Pack): 企业微信消息通知
dataease,dataease,10d0421d54c80af3376336e679095c9928592b5c,https://github.com/dataease/dataease/commit/10d0421d54c80af3376336e679095c9928592b5c,Merge pull request #11767 from dataease/pr@dev-v2@perf_threshold_dynamic_rule  perf(X-Pack): 阈值告警规则动态值
dataease,dataease,6a243f38cd36f8ceab05fb0938421f28930e230a,https://github.com/dataease/dataease/commit/6a243f38cd36f8ceab05fb0938421f28930e230a,perf(X-Pack): 阈值告警规则动态值
dataease,dataease,c759aa93104d1e9256043b5d1a2f49f0de76dfd3,https://github.com/dataease/dataease/commit/c759aa93104d1e9256043b5d1a2f49f0de76dfd3,Merge pull request #11741 from dataease/pr@dev-v2@perf_threshold_email_msg  perf(X-Pack): 邮件发送阈值告警信息
dataease,dataease,d19f83e38dbac583a935e1d1a5840c99aedfbaeb,https://github.com/dataease/dataease/commit/d19f83e38dbac583a935e1d1a5840c99aedfbaeb,perf(X-Pack): 邮件发送阈值告警信息
dataease,dataease,10ecaf449217aa9c1813260855a4a1a1d746057c,https://github.com/dataease/dataease/commit/10ecaf449217aa9c1813260855a4a1a1d746057c,Merge pull request #11693 from dataease/pr@dev-v2@perf_threshold_screen  perf(X-Pack): 大屏增加阈值告警
dataease,dataease,546d2b23acec4af4768575cdbd063aa61bb79575,https://github.com/dataease/dataease/commit/546d2b23acec4af4768575cdbd063aa61bb79575,perf(X-Pack): 大屏增加阈值告警
dataease,dataease,af9aca15aca79627586e23cc4ff09181c3afad05,https://github.com/dataease/dataease/commit/af9aca15aca79627586e23cc4ff09181c3afad05,Merge pull request #11683 from dataease/pr@dev-v2@perf_threshold  perf(X-Pack): 阈值告警跟随视图删除
dataease,dataease,f2e5a9dee4188a4f32f2e78654500ef7e59db582,https://github.com/dataease/dataease/commit/f2e5a9dee4188a4f32f2e78654500ef7e59db582,perf(X-Pack): 阈值告警跟随视图删除
dataease,dataease,c4ccd5954b427f039b81efdf3af01d5b788e5c6a,https://github.com/dataease/dataease/commit/c4ccd5954b427f039b81efdf3af01d5b788e5c6a,Merge pull request #11676 from dataease/pr@dev-v2@perf_threshold_new_chart  perf(X-Pack): 新增视图未保存情况下设置阈值告警错误
dataease,dataease,d3c26d00dcc15a724f21ea22d765a2c54b3c7760,https://github.com/dataease/dataease/commit/d3c26d00dcc15a724f21ea22d765a2c54b3c7760,Merge branch 'dev-v2' into pr@dev-v2@perf_threshold_new_chart
dataease,dataease,c7a2027b8902f0789255567747604f90611811ae,https://github.com/dataease/dataease/commit/c7a2027b8902f0789255567747604f90611811ae,perf(X-Pack): 新增视图未保存情况下设置阈值告警错误
dataease,dataease,3e73b7ccf8ee97779a96c7e5c1cfe7fad172e664,https://github.com/dataease/dataease/commit/3e73b7ccf8ee97779a96c7e5c1cfe7fad172e664,Merge pull request #11661 from dataease/pr@dev-v2@perf_threshold  perf(X-Pack): 阈值告警关注指标
dataease,dataease,43bc442eaf65928a8c6e1fc6067cf1d797c15d39,https://github.com/dataease/dataease/commit/43bc442eaf65928a8c6e1fc6067cf1d797c15d39,perf(X-Pack): 阈值告警关注指标
dataease,dataease,141cf09dfd68186919a2a71a117dbde9fb55d58f,https://github.com/dataease/dataease/commit/141cf09dfd68186919a2a71a117dbde9fb55d58f,Merge pull request #11634 from dataease/pr@dev-v2@fix_chart_threshold  perf(X-Pack): 阈值告警漏提代码
dataease,dataease,90b7508734d7701fd711b7e51fc07a57e4206fdf,https://github.com/dataease/dataease/commit/90b7508734d7701fd711b7e51fc07a57e4206fdf,perf(X-Pack): 阈值告警漏提代码
dataease,dataease,42189856151b4fc8febe639b86e69642884986b8,https://github.com/dataease/dataease/commit/42189856151b4fc8febe639b86e69642884986b8,Merge pull request #11618 from dataease/pr@dev-v2@perf_threshold_condition_field  perf(X-Pack): 阈值告警条件字段限定视图字段
dataease,dataease,dc3d9b643cb63c4a1c3e4507453219addf0f666a,https://github.com/dataease/dataease/commit/dc3d9b643cb63c4a1c3e4507453219addf0f666a,perf(X-Pack): 阈值告警条件字段限定视图字段
dataease,dataease,a6f254a735a55e8d91c339880656340fd394ef0c,https://github.com/dataease/dataease/commit/a6f254a735a55e8d91c339880656340fd394ef0c,Merge pull request #11614 from dataease/pr@dev-v2@perf_threshold_preview_msg  perf(X-Pack): 阈值告警预览页面优化
dataease,dataease,e9db5a1121cd0334cefa9b5d505f21918fe083e8,https://github.com/dataease/dataease/commit/e9db5a1121cd0334cefa9b5d505f21918fe083e8,perf(X-Pack): 阈值告警预览页面优化
dataease,dataease,94d06b0045dce5672d2dc1712da2973edda545d9,https://github.com/dataease/dataease/commit/94d06b0045dce5672d2dc1712da2973edda545d9,Merge pull request #11589 from dataease/pr@dev-v2@perf_threshold_preview  perf(X-Pack): 阈值告警预览页面
dataease,dataease,378fbd502005a34015577e35adf7e32bae6202d7,https://github.com/dataease/dataease/commit/378fbd502005a34015577e35adf7e32bae6202d7,perf(X-Pack): 阈值告警预览页面
dataease,dataease,201c0e6cd4213bc701a4fac3ed529c5c03865ff2,https://github.com/dataease/dataease/commit/201c0e6cd4213bc701a4fac3ed529c5c03865ff2,Merge pull request #11560 from dataease/pr@dev-v2@perf_threshold_form  perf(X-Pack): 阈值告警表单信息回填
dataease,dataease,2235f22f45dd32b86760876380e4094210a8f938,https://github.com/dataease/dataease/commit/2235f22f45dd32b86760876380e4094210a8f938,perf(X-Pack): 阈值告警表单信息回填
dataease,dataease,3d7c2cbce076c5d96719f5edce09b5dc1a084648,https://github.com/dataease/dataease/commit/3d7c2cbce076c5d96719f5edce09b5dc1a084648,Merge pull request #11558 from dataease/pr@dev-v2@perf_threshold_task  perf(X-Pack): 阈值告警任务
dataease,dataease,5b055801cb55d6709e99ee5aa6b62973a194b3f2,https://github.com/dataease/dataease/commit/5b055801cb55d6709e99ee5aa6b62973a194b3f2,perf(X-Pack): 阈值告警任务
dataease,dataease,8570118226678fc36a0ea773c5220d279e532edf,https://github.com/dataease/dataease/commit/8570118226678fc36a0ea773c5220d279e532edf,perf(X-Pack): 阈值告警任务
dataease,dataease,cf7755e7393527193a7dccdbe604edde2ff7557b,https://github.com/dataease/dataease/commit/cf7755e7393527193a7dccdbe604edde2ff7557b,Merge pull request #11551 from dataease/pr@dev-v2@perf_threshold_batch_reci  perf(X-Pack): 阈值告警批量设置接收人
dataease,dataease,8d33cd3bd2b748fd3f803b778801a2dedfa41b36,https://github.com/dataease/dataease/commit/8d33cd3bd2b748fd3f803b778801a2dedfa41b36,perf(X-Pack): 阈值告警批量设置接收人
dataease,dataease,4196eff0dbbd665a18b1b9b410aefb971d6292dd,https://github.com/dataease/dataease/commit/4196eff0dbbd665a18b1b9b410aefb971d6292dd,Merge pull request #11530 from dataease/pr@dev-v2@perf_threshold  perf(X-Pack): 阈值告警
dataease,dataease,8c2525ce6adca254f2bc1e041a64500fb4ba5330,https://github.com/dataease/dataease/commit/8c2525ce6adca254f2bc1e041a64500fb4ba5330,perf(X-Pack): 阈值告警
dataease,dataease,dd1eb07be298f187e8c06f0714efab7c61a05d00,https://github.com/dataease/dataease/commit/dd1eb07be298f187e8c06f0714efab7c61a05d00,Merge pull request #11507 from dataease/pr@dev-v2@perf_threshold_task  perf(X-Pack): 阈值告警定时任务
dataease,dataease,f250417851d5c4aff2eb59c518b1900659b7e61d,https://github.com/dataease/dataease/commit/f250417851d5c4aff2eb59c518b1900659b7e61d,perf(X-Pack): 阈值告警定时任务
dataease,dataease,3bb5dba524fac521100f272e61f70d6b9d4039fa,https://github.com/dataease/dataease/commit/3bb5dba524fac521100f272e61f70d6b9d4039fa,Merge pull request #11491 from dataease/pr@dev-v2@perf_threshold_manage_page  perf(X-Pack): 阈值告警管理页面
dataease,dataease,a391a5db29bf34f36174840f7aeae346a04d1aef,https://github.com/dataease/dataease/commit/a391a5db29bf34f36174840f7aeae346a04d1aef,perf(X-Pack): 阈值告警管理页面
dataease,dataease,32bf7d278ccf1cb407e351ff6063077017d7e4ab,https://github.com/dataease/dataease/commit/32bf7d278ccf1cb407e351ff6063077017d7e4ab,Merge pull request #11462 from dataease/pr@dev-v2@perf_data_fill_auth  perf(X-Pack): 权限矩阵增加数据填报
dataease,dataease,46fc59f2a6ba68e67430c0060c26974dc28d23f0,https://github.com/dataease/dataease/commit/46fc59f2a6ba68e67430c0060c26974dc28d23f0,perf(X-Pack): 权限矩阵增加数据填报
dataease,dataease,596abb0401a91da16825fdaeca20ce0429e0aff4,https://github.com/dataease/dataease/commit/596abb0401a91da16825fdaeca20ce0429e0aff4,Merge pull request #11288 from dataease/pr@dev-v2@perf_report_retry  perf(X-Pack): 定时报告-重试任务从本地文件读取报告信息
dataease,dataease,6c053251ec38658dc07973095b79d645b3edf67b,https://github.com/dataease/dataease/commit/6c053251ec38658dc07973095b79d645b3edf67b,perf(X-Pack): 定时报告-重试任务从本地文件读取报告信息
dataease,dataease,fc92aa6e8360257cf64bc94e147a6a85aa90b698,https://github.com/dataease/dataease/commit/fc92aa6e8360257cf64bc94e147a6a85aa90b698,Merge pull request #11270 from dataease/pr@dev-v2@perf_report  perf(X-Pack): 定时报告-失败重试禁止重复生成截图
dataease,dataease,c46cfb8a19bb912740fc9c7393c060f9302ee4a3,https://github.com/dataease/dataease/commit/c46cfb8a19bb912740fc9c7393c060f9302ee4a3,perf(X-Pack): 定时报告-失败重试禁止重复生成截图
dataease,dataease,57481725d59f095da9d027916b51d7785595801a,https://github.com/dataease/dataease/commit/57481725d59f095da9d027916b51d7785595801a,Merge pull request #11219 from dataease/pr@dev-v2@perf_plugin_driver  perf(插件): 卸载数据源插件驱动文件相关操作
dataease,dataease,b4574fdb5ede5be08e6ecd0e122fd07b56149a46,https://github.com/dataease/dataease/commit/b4574fdb5ede5be08e6ecd0e122fd07b56149a46,perf(插件): 卸载数据源插件驱动文件相关操作
dataease,dataease,e91dfb9a12b13f3be0b182e8c4ea81a5ecd548ef,https://github.com/dataease/dataease/commit/e91dfb9a12b13f3be0b182e8c4ea81a5ecd548ef,Merge pull request #11124 from dataease/pr@dev-v2@perf_plugin_ds_driver  perf(X-Pack): 数据源插件驱动相关
dataease,dataease,4c8dc204b3b516bc68e4e09636f32498e24e2326,https://github.com/dataease/dataease/commit/4c8dc204b3b516bc68e4e09636f32498e24e2326,perf(X-Pack): 数据源插件驱动相关
dataease,dataease,89ee108ba8142e22bf6221ee44acf767486a0e7b,https://github.com/dataease/dataease/commit/89ee108ba8142e22bf6221ee44acf767486a0e7b,Merge pull request #11040 from dataease/pr@dev-v2@perf_report_retry_error  perf(X-Pack): 定时报告失败重试无效
dataease,dataease,098bc2fd82d5e0d821b940cf085eddc215708022,https://github.com/dataease/dataease/commit/098bc2fd82d5e0d821b940cf085eddc215708022,perf(X-Pack): 定时报告失败重试无效
dataease,dataease,3fca8653721b4cc53c06408cc633aca912b29017,https://github.com/dataease/dataease/commit/3fca8653721b4cc53c06408cc633aca912b29017,Merge pull request #10978 from dataease/pr@dev-v2@perf_api_traffic  perf: api限流相关flyway
dataease,dataease,c7038a233ddf633bc43bdcf4d3454b2efc7af969,https://github.com/dataease/dataease/commit/c7038a233ddf633bc43bdcf4d3454b2efc7af969,perf: api限流相关flyway
dataease,dataease,8917c5b2bb49b1046713994807ad606e254c6333,https://github.com/dataease/dataease/commit/8917c5b2bb49b1046713994807ad606e254c6333,Merge pull request #10870 from dataease/pr@dev-v2@perf_delete_retry_task  perf: 项目启动时删除所有重试任务
dataease,dataease,b046d03357a035bf555605cec3dd560edd71eff4,https://github.com/dataease/dataease/commit/b046d03357a035bf555605cec3dd560edd71eff4,perf: 项目启动时删除所有重试任务
dataease,dataease,94f6bff735490864525ce51c75a2226f9e575365,https://github.com/dataease/dataease/commit/94f6bff735490864525ce51c75a2226f9e575365,Merge pull request #10448 from dataease/pr@dev-v2@perf_share_ticket  perf(仪表板): 漏提代码
dataease,dataease,71a6573df0b227827512da67614f0d1c83f4b18e,https://github.com/dataease/dataease/commit/71a6573df0b227827512da67614f0d1c83f4b18e,perf(仪表板): 漏提代码
dataease,dataease,230637b0fdf11b34d6dff7587633c3792fe108f6,https://github.com/dataease/dataease/commit/230637b0fdf11b34d6dff7587633c3792fe108f6,Merge pull request #10412 from dataease/pr@dev-v2@perf_plugin_static_load  perf(X-Pack): 插件管理-前端分布式加载静态资源
dataease,dataease,54d254e29012719dbf3ae8ba398c53b28700306c,https://github.com/dataease/dataease/commit/54d254e29012719dbf3ae8ba398c53b28700306c,perf(X-Pack): 插件管理-前端分布式加载静态资源
dataease,dataease,c43233cce8df3776b9fe53107ea7b5b95782bdea,https://github.com/dataease/dataease/commit/c43233cce8df3776b9fe53107ea7b5b95782bdea,perf(X-Pack): 优化插件操作
dataease,dataease,43f421d3e954d76ed3be1f34ce64fff1f08c206f,https://github.com/dataease/dataease/commit/43f421d3e954d76ed3be1f34ce64fff1f08c206f,perf(X-Pack): 优化插件操作
dataease,dataease,f699e8a3db66add1dfeb9e81a6043cacc2073876,https://github.com/dataease/dataease/commit/f699e8a3db66add1dfeb9e81a6043cacc2073876,Merge pull request #10352 from dataease/pr@dev-v2@perf_plugin  perf(X-Pack): 优化插件加载机制
dataease,dataease,7b3aa92a33bac242a7908cd14a1add1c7831d45f,https://github.com/dataease/dataease/commit/7b3aa92a33bac242a7908cd14a1add1c7831d45f,perf(X-Pack): 优化插件加载机制
dataease,dataease,bfb80323980b134340258b7f950d3d49fe892fd4,https://github.com/dataease/dataease/commit/bfb80323980b134340258b7f950d3d49fe892fd4,Merge pull request #10325 from dataease/pr@dev-v2@perf_plugin_view  Pr@dev v2@perf plugin view
dataease,dataease,fd769ddb81c1a32b97ec3eb0cab1a35a756ac4d5,https://github.com/dataease/dataease/commit/fd769ddb81c1a32b97ec3eb0cab1a35a756ac4d5,Merge pull request #10092 from dataease/pr@dev-v2@perf_link_iframe  perf: 优化公共链接iframe嵌入
dataease,dataease,c0d84fe024a753508fa17346f34b1f66128674bd,https://github.com/dataease/dataease/commit/c0d84fe024a753508fa17346f34b1f66128674bd,perf: 优化公共链接iframe嵌入
dataease,dataease,871c55560f9c5e6748cb3efc8431d9215edd8e6d,https://github.com/dataease/dataease/commit/871c55560f9c5e6748cb3efc8431d9215edd8e6d,Merge branch 'dev-v2' into pr@dev-v2@perf_report_view_selector
dataease,dataease,be63e55fca2bf15e619ff11d5ead21e024647731,https://github.com/dataease/dataease/commit/be63e55fca2bf15e619ff11d5ead21e024647731,Merge pull request #10073 from dataease/pr@dev-v2@perf_report_lark_group  perf(X-Pack): 定时报告飞书群消息
dataease,dataease,8f816214ffa6e10717d59998980edde86f23e7eb,https://github.com/dataease/dataease/commit/8f816214ffa6e10717d59998980edde86f23e7eb,perf(X-Pack): 定时报告飞书群消息
dataease,dataease,f8d3a6910e1cc063e09cfe3f0511bbf854688eb9,https://github.com/dataease/dataease/commit/f8d3a6910e1cc063e09cfe3f0511bbf854688eb9,Merge pull request #9998 from dataease/pr@dev-v2@perf_platform_qr  perf(X-Pack): 第三方平台扫码登录接口删除secret字段
dataease,dataease,febec1e80bc20857d555f75ec0c6f3c89e52e125,https://github.com/dataease/dataease/commit/febec1e80bc20857d555f75ec0c6f3c89e52e125,perf(X-Pack): 第三方平台扫码登录接口删除secret字段
dataease,dataease,98eca50cbeca4793ef1154b9ceb5cbcec90a6cb8,https://github.com/dataease/dataease/commit/98eca50cbeca4793ef1154b9ceb5cbcec90a6cb8,Merge pull request #9987 from dataease/pr@dev-v2@perf_report_ws_refresh  perf(X-Pack): 定时报告使用websocket机制实时刷新页面
dataease,dataease,636e18edd47c8d005b86cd9f14e5e435df995721,https://github.com/dataease/dataease/commit/636e18edd47c8d005b86cd9f14e5e435df995721,Merge branch 'dev-v2' into pr@dev-v2@perf_report_ws_refresh
dataease,dataease,67af3a7eeb072c2e50a80de5e84dafeba947029d,https://github.com/dataease/dataease/commit/67af3a7eeb072c2e50a80de5e84dafeba947029d,perf(X-Pack): 定时报告使用websocket机制实时刷新页面
dataease,dataease,913b99131d1a77001e5d36f93a10353e9d2dd08a,https://github.com/dataease/dataease/commit/913b99131d1a77001e5d36f93a10353e9d2dd08a,Merge pull request #9897 from dataease/pr@dev-v2@perf_div_embedded_cors  perf: div嵌入式跨域设置
dataease,dataease,cae7cd0064a2cebb6d99f6c4487644fb9b6e7cb4,https://github.com/dataease/dataease/commit/cae7cd0064a2cebb6d99f6c4487644fb9b6e7cb4,perf: div嵌入式跨域设置
thingsboard,thingsboard,265e4181b7dc4ae5132fbb23b68bf94c6e80f603,https://github.com/thingsboard/thingsboard/commit/265e4181b7dc4ae5132fbb23b68bf94c6e80f603,improved performance
thingsboard,thingsboard,139c0c1d0433eed08b36eacb56208edf83b31d5c,https://github.com/thingsboard/thingsboard/commit/139c0c1d0433eed08b36eacb56208edf83b31d5c,Type casting was performed in the Test class  Signed-off-by: Oleksandra Matviienko <al.zzzeebra@gmail.com>
thingsboard,thingsboard,86569c312e96e297ad1de55b31c681bf32c36a28,https://github.com/thingsboard/thingsboard/commit/86569c312e96e297ad1de55b31c681bf32c36a28,added ability to perform calculations on the last records
thingsboard,thingsboard,1499f6fdf43f6ea31015996b62138b7fe04d34db,https://github.com/thingsboard/thingsboard/commit/1499f6fdf43f6ea31015996b62138b7fe04d34db,Merge pull request #11666 from thingsboard/fix/alarms-unassign  Housekeeper: performance improvements for alarms unassigning
thingsboard,thingsboard,dd3936ca666dc9b9ba867b002e553bb5d971c16e,https://github.com/thingsboard/thingsboard/commit/dd3936ca666dc9b9ba867b002e553bb5d971c16e,Performance improvements for processing alarms unassigning task
thingsboard,thingsboard,911232cdd167561b11c6c746d0ef69fdbd3e9002,https://github.com/thingsboard/thingsboard/commit/911232cdd167561b11c6c746d0ef69fdbd3e9002,Merge pull request #11498 from YevhenBondarenko/hotfix/ws-unsubscribe-optimization-3.7.0  DefaultTbLocalSubscriptionService WS single lock refactored by tenantId; TbEntityLocalSubsInfo performance optimizations for remove subscription
thingsboard,thingsboard,bff310b6913ab750fb506884bb29515098177bc0,https://github.com/thingsboard/thingsboard/commit/bff310b6913ab750fb506884bb29515098177bc0,DefaultTbLocalSubscriptionService WS single lock refactored by tenantId; TbEntityLocalSubsInfo performance optimizations for remove subscription
kestra-io,kestra,b0f93e19459c1950a8d3922acef022e81376a88e,https://github.com/kestra-io/kestra/commit/b0f93e19459c1950a8d3922acef022e81376a88e,chore(system): skip sleep in the JdbcQueue when at max poll size  When a queue is at max poll size  this means that it is at full capacity. In this case skip the sleep and process immediatly the next batch of message. This improve latency at high thoughput without adding too much load to the database.  We can even go further by skipping sleep each time the poll returns messages but this would imply database cost so for now we balance performance and database cost by only skipping sleep when at max capacity.
kestra-io,kestra,11a7e68e93e6723e3aa0ece35e83acb3c7ecbb6c,https://github.com/kestra-io/kestra/commit/11a7e68e93e6723e3aa0ece35e83acb3c7ecbb6c,feat(core)!: make tenant id required (#8460)  * feat(core)!: WIP make tenant id required  * feat(core)!: WIP make tenant id required  * test(core)!: WIP fix storage unit test  * build(deps): bump com.google.guava:guava from 33.4.7-jre to 33.4.8-jre  Bumps [com.google.guava:guava](https://github.com/google/guava) from 33.4.7-jre to 33.4.8-jre. - [Release notes](https://github.com/google/guava/releases) - [Commits](https://github.com/google/guava/commits)  --- updated-dependencies: - dependency-name: com.google.guava:guava dependency-version: 33.4.8-jre dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump io.micronaut.platform:micronaut-platform  Bumps [io.micronaut.platform:micronaut-platform](https://github.com/micronaut-projects/micronaut-platform) from 4.8.0 to 4.8.2. - [Release notes](https://github.com/micronaut-projects/micronaut-platform/releases) - [Commits](https://github.com/micronaut-projects/micronaut-platform/compare/v4.8.0...v4.8.2)  --- updated-dependencies: - dependency-name: io.micronaut.platform:micronaut-platform dependency-version: 4.8.2 dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump flyingSaucerVersion from 9.11.6 to 9.12.0  Bumps `flyingSaucerVersion` from 9.11.6 to 9.12.0.  Updates `org.xhtmlrenderer:flying-saucer-core` from 9.11.6 to 9.12.0 - [Release notes](https://github.com/flyingsaucerproject/flyingsaucer/releases) - [Changelog](https://github.com/flyingsaucerproject/flyingsaucer/blob/main/CHANGELOG.md) - [Commits](https://github.com/flyingsaucerproject/flyingsaucer/compare/v9.11.6...v9.12.0)  Updates `org.xhtmlrenderer:flying-saucer-pdf` from 9.11.6 to 9.12.0 - [Release notes](https://github.com/flyingsaucerproject/flyingsaucer/releases) - [Changelog](https://github.com/flyingsaucerproject/flyingsaucer/blob/main/CHANGELOG.md) - [Commits](https://github.com/flyingsaucerproject/flyingsaucer/compare/v9.11.6...v9.12.0)  --- updated-dependencies: - dependency-name: org.xhtmlrenderer:flying-saucer-core dependency-version: 9.12.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: org.xhtmlrenderer:flying-saucer-pdf dependency-version: 9.12.0 dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump software.amazon.awssdk:bom from 2.31.21 to 2.31.25  Bumps software.amazon.awssdk:bom from 2.31.21 to 2.31.25.  --- updated-dependencies: - dependency-name: software.amazon.awssdk:bom dependency-version: 2.31.25 dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump com.github.oshi:oshi-core from 6.8.0 to 6.8.1  Bumps [com.github.oshi:oshi-core](https://github.com/oshi/oshi) from 6.8.0 to 6.8.1. - [Release notes](https://github.com/oshi/oshi/releases) - [Changelog](https://github.com/oshi/oshi/blob/master/CHANGELOG.md) - [Commits](https://github.com/oshi/oshi/compare/oshi-parent-6.8.0...oshi-parent-6.8.1)  --- updated-dependencies: - dependency-name: com.github.oshi:oshi-core dependency-version: 6.8.1 dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump org.opensearch.client:opensearch-java  Bumps [org.opensearch.client:opensearch-java](https://github.com/opensearch-project/opensearch-java) from 2.22.0 to 2.23.0. - [Release notes](https://github.com/opensearch-project/opensearch-java/releases) - [Changelog](https://github.com/opensearch-project/opensearch-java/blob/v2.23.0/CHANGELOG.md) - [Commits](https://github.com/opensearch-project/opensearch-java/compare/v2.22.0...v2.23.0)  --- updated-dependencies: - dependency-name: org.opensearch.client:opensearch-java dependency-version: 2.23.0 dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump co.elastic.logging:logback-ecs-encoder  Bumps [co.elastic.logging:logback-ecs-encoder](https://github.com/elastic/ecs-logging-java) from 1.6.0 to 1.7.0. - [Release notes](https://github.com/elastic/ecs-logging-java/releases) - [Commits](https://github.com/elastic/ecs-logging-java/compare/v1.6.0...v1.7.0)  --- updated-dependencies: - dependency-name: co.elastic.logging:logback-ecs-encoder dependency-version: 1.7.0 dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * tests(system): isolate SchedulerScheduleTest tests with tenantId  * Feat/storage outputs (#8361)  * feat(executions): Store outputs inside the internal storage (1/2)  * feat(executions): Store outputs inside the internal storage (2/2)  * feat(test): allow passing tenantId to tests  ---------  Co-authored-by: Ludovic DEHON <tchiot.ludo@gmail.com>  * chore(deps): regular dependency update (#8484)  Performing a weekly round of dependency updates in the NPM ecosystem to keep everything up to date.  * fix(ui): full view height for single task logs (#8042)  Co-authored-by: Miloš Paunović <paun992@hotmail.com>  * feat: synchronize task edition with editor (#8433)  * fix(controls): adjust bottom position of contorls in multiPanelsEditor (#8465)  * fix(executions): unqueing execution must remove the execution queued  When an execution is queued in the JDBC backend  a record is inserted inside the execution_queued table  we must remove this record when we unqeue an execution.  Fixes #8448  * chore(core): localize to languages other than english (#8485)  Extended localization support by adding translations for multiple languages using English as the base. This enhances accessibility and usability for non-English-speaking users while keeping English as the source reference.  Co-authored-by: GitHub Action <actions@github.com>  * tests(webserver): fix flaky test which could query previous tests tasks  * tests(system): debug flaky error of shouldPauseExecutionByQueryRunningFlows  * tests(system): debug flaky error of shouldPauseExecutionByQueryRunningFlows  * tests(system): try with different task id  * tests(system): bump sleep-short sleep time to 10s  * fix(execution)*: decode and hide nested inputs of type SECRET  Fixes #7964  * refactor(core): pass the dynamic concurrency schema to no code editor (#8488)  There was an issue with passing hard-coded concurrency schema to be rendered in No Code editor  which is now amended and we're passing down the previously fetched one  * chore(deps): update gradle version  * doc(basic.md): add link to configuration for kestra property variables (#8490)  * fix(flows): properly check average duration for dashboard graphs (#8457)  There was a problem on flows view with the main chart not showing proper data until user clicks on duration toggle.  Closes https://github.com/kestra-io/kestra/issues/8435. Closes https://github.com/kestra-io/kestra-ee/issues/3499.  * docs(core-pause): update pauseDuration properties  titles  descriptions (#8495)  * fix(system): restrict the JdbcConcurrencyLimitService to the JDBC runner  * fix(core): fix indexer metric description (#8500)  * Add examples with expression and trimmed values (#6154)  * chore(ui): improvement to drilldown for Default and Custom Charts. (#7885)  * chore(ui): improvement to drilldown for Default and Custom Charts.  * minor tweak  * test: fix the Barchart stories to test drilldown  ---------  Co-authored-by: Bart Ledoux <bledoux@kestra.io>  * fix(ui): restart trigger position for backfill column (#8246)  Co-authored-by: Miloš Paunović <paun992@hotmail.com> Co-authored-by: Barthélémy Ledoux <ledouxb@me.com> Co-authored-by: Bart Ledoux <bledoux@kestra.io>  * feat(plugin): add a way to provide additional type of plugins  Provide a way for plugins to define a new type of plugins. To do that  a plugin must provide both an abstract base class that extends AdditionalPlugin and a set of concret classes. Both the abstract base class and the concrete classes mut be inside the same plugin. This is a limitation that we may work on later by providing  for example  an SPI to add base classes to the application classloader.  * fix(ui): save existing flow after making changes (#8378)  Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  * fix(core): failing DocumentationGeneratorTest.returnDoc()  * chore(core): refactor  component to composition API  structure and with some styling (#8504)  * feat(flows): add validation for use of inputs and outputs with '-' in the name (#8379)  * test(core): fix breaking change in local flow repository (#8517)  Co-authored-by: nKwiatkowski <nkwiatkowski@kestra.io>  * feat(system)!: remove the SQLServer runner  Part-of: https://github.com/kestra-io/kestra-ee/issues/3504  * chore(build): add Postgres stat extension  * feat(plugins): add Langchain4J plugins  * chore(system): add warn log when emit logQueue failed (#8432)  * feat(plugins): add Go Script plugin  * fix(jdbc): add service_id index on service_instance table  * chore(flows): improve the blueprints view within the flow editing panels (#7983)  Changes here consist of removing the tags from blueprint view on Multi Panel flow editor  along with couple of other UI improvements.  Closes https://github.com/kestra-io/kestra/issues/7881.  Co-authored-by: Miloš Paunović <paun992@hotmail.com>  * fix(flows): properly load blueprints in multi panel view (#8524)  While using the new Multi Panel view blueprints were not loading properly due to wrong paramtere being sent to action. now that's sorted.  Closes https://github.com/kestra-io/kestra/issues/8523.  * feat(flows): improve the display of array inputs when running an execution (#7953)  This PR is introducing a change of how the `array` inputs are displayed inside the flow run dialog  to be more user-friendly.  Closes https://github.com/kestra-io/kestra/issues/6947.  ---------  Co-authored-by: Miloš Paunović <paun992@hotmail.com>  * fix(system): change default config values for liveness  Change kestra.server.liveness.interval from 5s to 10s to be less agressive on liveness check. Align other default liveness configs with kafka implementation.  * fix(ui): amend Absolute date filter's looks (#8501)  * chore(core): localize to languages other than english (#8528)  Extended localization support by adding translations for multiple languages using English as the base. This enhances accessibility and usability for non-English-speaking users while keeping English as the source reference.  Co-authored-by: GitHub Action <actions@github.com>  * docs(flow-trigger): add note about no Pebble in conditions  * fix(ui): remove parts of filter using backspace (#8105)  Co-authored-by: Miloš Paunović <paun992@hotmail.com> Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  * fix(ui): open link in markdown in other tab. (#8258)  * fix(ui): open link in markdown in other tab.  * chore(core): restrict attribute to external links.  Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  ---------  Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  * fix(system): load OpenTelemetry lib in the app classloader  Without that  as we have it here  plugins may have class loading issue if they use OpenTelemetry internally (like in the Elasticsearch client).  * feat(ui): Add search in internal docs (#8458)  Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  * fix(triggers): inject default later inside the Scheduler  Today  as they are injected eagerly  they are done even if no trigger exists. This is counter-performant  and in case the flow is an error will log each seconds. Doing it a little later will be better.  * build(deps): bump software.amazon.awssdk:bom from 2.31.25 to 2.31.30  Bumps software.amazon.awssdk:bom from 2.31.25 to 2.31.30.  --- updated-dependencies: - dependency-name: software.amazon.awssdk:bom dependency-version: 2.31.30 dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump org.wiremock:wiremock-jetty12 from 3.12.1 to 3.13.0  Bumps [org.wiremock:wiremock-jetty12](https://github.com/wiremock/wiremock) from 3.12.1 to 3.13.0. - [Release notes](https://github.com/wiremock/wiremock/releases) - [Commits](https://github.com/wiremock/wiremock/compare/3.12.1...3.13.0)  --- updated-dependencies: - dependency-name: org.wiremock:wiremock-jetty12 dependency-version: 3.13.0 dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * build(deps): bump jacksonVersion from 2.18.3 to 2.19.0  Bumps `jacksonVersion` from 2.18.3 to 2.19.0.  Updates `com.fasterxml.jackson:jackson-bom` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-bom/compare/jackson-bom-2.18.3...jackson-bom-2.19.0)  Updates `com.fasterxml.jackson.core:jackson-core` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-core/compare/jackson-core-2.18.3...jackson-core-2.19.0)  Updates `com.fasterxml.jackson.core:jackson-databind` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson/commits)  Updates `com.fasterxml.jackson.core:jackson-annotations` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson/commits)  Updates `com.fasterxml.jackson.module:jackson-module-parameter-names` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-modules-java8/compare/jackson-modules-java8-2.18.3...jackson-modules-java8-2.19.0)  Updates `com.fasterxml.jackson.dataformat:jackson-dataformat-yaml` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-dataformats-text/compare/jackson-dataformats-text-2.18.3...jackson-dataformats-text-2.19.0)  Updates `com.fasterxml.jackson.dataformat:jackson-dataformat-smile` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-dataformats-binary/compare/jackson-dataformats-binary-2.18.3...jackson-dataformats-binary-2.19.0)  Updates `com.fasterxml.jackson.dataformat:jackson-dataformat-cbor` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-dataformats-binary/compare/jackson-dataformats-binary-2.18.3...jackson-dataformats-binary-2.19.0)  Updates `com.fasterxml.jackson.dataformat:jackson-dataformat-ion` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-dataformat-ion/commits)  Updates `com.fasterxml.jackson.dataformat:jackson-dataformat-xml` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-dataformat-xml/compare/jackson-dataformat-xml-2.18.3...jackson-dataformat-xml-2.19.0)  Updates `com.fasterxml.jackson.datatype:jackson-datatype-guava` from 2.18.3 to 2.19.0 - [Commits](https://github.com/FasterXML/jackson-datatypes-collections/compare/jackson-datatypes-collections-2.18.3...jackson-datatypes-collections-2.19.0)  Updates `com.fasterxml.jackson.datatype:jackson-datatype-jsr310` from 2.18.3 to 2.19.0  Updates `com.fasterxml.jackson.datatype:jackson-datatype-jdk8` from 2.18.3 to 2.19.0  --- updated-dependencies: - dependency-name: com.fasterxml.jackson:jackson-bom dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.core:jackson-core dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.core:jackson-databind dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.core:jackson-annotations dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.module:jackson-module-parameter-names dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.dataformat:jackson-dataformat-yaml dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.dataformat:jackson-dataformat-smile dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.dataformat:jackson-dataformat-cbor dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.dataformat:jackson-dataformat-ion dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.dataformat:jackson-dataformat-xml dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.datatype:jackson-datatype-guava dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.datatype:jackson-datatype-jsr310 dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor - dependency-name: com.fasterxml.jackson.datatype:jackson-datatype-jdk8 dependency-version: 2.19.0 dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * chore(execution): update display names for executions. (#8527)  * fix(core): change incorrectly used search parameter (#8534)  Co-authored-by: nKwiatkowski <nkwiatkowski@kestra.io>  * fix(ui): set isCreating to false when opening flow edit mode (#8549)  * fix(core): safely access section and identifier query params (#8542)  Co-authored-by: Barthélémy Ledoux <ledouxb@me.com>  * fix(ui): update storybook editor tests with provided keys (#8550)  Co-authored-by: Piyush Bhaskar <102078527+Piyush-r-bhaskar@users.noreply.github.com>  * chore(core): localize to languages other than english (#8554)  Extended localization support by adding translations for multiple languages using English as the base. This enhances accessibility and usability for non-English-speaking users while keeping English as the source reference.  Co-authored-by: GitHub Action <actions@github.com>  * fix(triggers): amend broken filtering on triggers tab (#8553)  There was a problem with both namespace and state filters on Triggers page which is now properly sorted.  Closes https://github.com/kestra-io/kestra/issues/8529.  * chore: attempt to fix flaky tests (#8537)  SingleFlowCommandsTest:  The flow Delete -> Create -> Update sequence is weird - delete got HTTP 404. Reworked to Create -> Update -> Delete sequence.  PurgeLogsTest:  The log repository contained the prepared single entry but also might contain additional entries from previously logged messages.  * fix(namespaces): namespaceFiles with same name are wrongly overwritten (#8562)  Co-authored-by: nKwiatkowski <nkwiatkowski@kestra.io>  * feat(core): forward execution labels in Flow Trigger  * chore(triggers)*: properly handle switches for triggers disabled from within flow source (#8106)  It was not clear as to which trigger can not be enabled and why. Now  that is much more clear with the proper tooltips and disabling of switch toggling.  Closes https://github.com/kestra-io/kestra/issues/8011. Closes https://github.com/kestra-io/kestra/issues/5736.  * fix(executions): fix execution failure due to UnsupportedOperationException (#8563)  Fix: #8563  * chore(core): localize to languages other than english (#8568)  Extended localization support by adding translations for multiple languages using English as the base. This enhances accessibility and usability for non-English-speaking users while keeping English as the source reference.  Co-authored-by: GitHub Action <actions@github.com>  * feat(system): add TestSuite model taskFixture impl  * fix: redirect to edit when saving new flow (#8560)  Co-authored-by: Piyush Bhaskar <102078527+Piyush-r-bhaskar@users.noreply.github.com>  * feat(system)*: decrease defaut JDBC queue poll size  Decreasing it from 100 to 50 didn't show any performance hit but should lower the memory consumption now that we process the queue concurrently in the executor.  ## BEFORE - pollSize=100 - 10 tx/s: 150ms - 25 tx/s: 200ms - 50 tx/s: 300ms - 75 tx/s: 5.2s - 100 tx/s: 15s  ## AFTER - pollSize=50 - 10 tx/s: 150ms - 25 tx/s: 200ms - 50 tx/s: 300ms - 75 tx/s: 4.8s - 100 tx/s: 14s  * feat(flows): Allow to define an onPause task on the Pause task  The onPause task will be executed immediatly when the execution is paused. Part-of: #3601  * feat(core)!: WIP make tenant id required  * feat(core)!: WIP make tenant id required  * Feat/storage outputs (#8361)  * feat(executions): Store outputs inside the internal storage (1/2)  * feat(executions): Store outputs inside the internal storage (2/2)  * feat(test): allow passing tenantId to tests  ---------  Co-authored-by: Ludovic DEHON <tchiot.ludo@gmail.com>  * tests(webserver): fix flaky test which could query previous tests tasks  * feat(executions): Add workerId to each worker task attemps  Closes #7799  * Feat/storage outputs (#8361)  * feat(executions): Store outputs inside the internal storage (1/2)  * feat(executions): Store outputs inside the internal storage (2/2)  * feat(test): allow passing tenantId to tests  ---------  Co-authored-by: Ludovic DEHON <tchiot.ludo@gmail.com>  * wip(core): fix unit tests  * test(core): fix unit tests  * test(core): fix unit tests  * test(core): fix unit tests  * feat(core): make tenant id required everywhere  * feat(core): make tenant required in create user command  * feat(core): clean the PR  * feat(core): add tenant id to dashboard controller  * fix(core): tests after merging  * clean(core): fixes after review  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: nKwiatkowski <nkwiatkowski@kestra.io> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: Roman Acevedo <roman.acevedo62@gmail.com> Co-authored-by: Loïc Mathieu <loikeseke@gmail.com> Co-authored-by: Ludovic DEHON <tchiot.ludo@gmail.com> Co-authored-by: Miloš Paunović <paun992@hotmail.com> Co-authored-by: Piyush Bhaskar <102078527+Piyush-r-bhaskar@users.noreply.github.com> Co-authored-by: Barthélémy Ledoux <ledouxb@me.com> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: GitHub Action <actions@github.com> Co-authored-by: AJ Emerich <aemerich@kestra.io> Co-authored-by: ben8t <46634684+Ben8t@users.noreply.github.com> Co-authored-by: Bart Ledoux <bledoux@kestra.io> Co-authored-by: Satvik Kushwaha <59243339+satvik2131@users.noreply.github.com> Co-authored-by: Karuna Tata <karuna.tata@devrev.ai> Co-authored-by: Hashim Khalifa <105060840+hashimzs@users.noreply.github.com> Co-authored-by: lwyang <1670906161@qq.com> Co-authored-by: 杨利伟 <yangliwei@xiaomi.com> Co-authored-by: Florian Hussonnois <fhussonnois@kestra.io> Co-authored-by: yuri <1969yuri1969@gmail.com> Co-authored-by: AJ Emerich <aj-emerich@proton.me> Co-authored-by: rajatsingh23 <48049052+rajatsingh23@users.noreply.github.com>
kestra-io,kestra,2c53a210d7d3677d01a7e46e92111363c2e5cb6b,https://github.com/kestra-io/kestra/commit/2c53a210d7d3677d01a7e46e92111363c2e5cb6b,feat: Performance optimization for handle() method: Filter out executions with non-null next_execution_date in the query method  start a separate scheduled thread for scanning  reporting metrics and logging
kestra-io,kestra,476f34e98689845dfec2478fd81026fe4e76d4dc,https://github.com/kestra-io/kestra/commit/476f34e98689845dfec2478fd81026fe4e76d4dc,feat(system): change the way we concurrently process executor queues  Instead of consuming multiple time the queue  which lead to concurrent queries on the `queues` table  process concurrently via an ExecutorService the messages from the queue. We dind't process a new batch of messages until the existing one is totally process to be sure we process in FIFO the same execution message.  Also  go back to a poll size of 100 to mitiguate the performance hit due to this change.
kestra-io,kestra,3639abb8bb8363af95e29b9183a118f36b195f88,https://github.com/kestra-io/kestra/commit/3639abb8bb8363af95e29b9183a118f36b195f88,chore(system): improve performance of IdUtils.fromParts()  Surprisingly  this method appear in some CPU and allocation profile as having a high cost  especially on the scheduler. Switching to using a StringJoiner brings 4x perf improvements in method execution time (great improvement also on allocation but didn't have a measurement).
kestra-io,kestra,4e602021a830e345a310bcce1f1132cc31f45c32,https://github.com/kestra-io/kestra/commit/4e602021a830e345a310bcce1f1132cc31f45c32,feat(system)*: decrease defaut JDBC queue poll size  Decreasing it from 100 to 50 didn't show any performance hit but should lower the memory consumption now that we process the queue concurrently in the executor.  ## BEFORE - pollSize=100 - 10 tx/s: 150ms - 25 tx/s: 200ms - 50 tx/s: 300ms - 75 tx/s: 5.2s - 100 tx/s: 15s  ## AFTER - pollSize=50 - 10 tx/s: 150ms - 25 tx/s: 200ms - 50 tx/s: 300ms - 75 tx/s: 4.8s - 100 tx/s: 14s
kestra-io,kestra,e19056cad3a59b6341743fff94a5df2a5785f18e,https://github.com/kestra-io/kestra/commit/e19056cad3a59b6341743fff94a5df2a5785f18e,fix(triggers): inject default later inside the Scheduler  Today  as they are injected eagerly  they are done even if no trigger exists. This is counter-performant  and in case the flow is an error will log each seconds. Doing it a little later will be better.
kestra-io,kestra,fa07cbd3b9fce507a4feff0b14f4aa00f986d94a,https://github.com/kestra-io/kestra/commit/fa07cbd3b9fce507a4feff0b14f4aa00f986d94a,feat(core): improve performance of ExecutorService.handleChildWorkerTaskResult  Searching for a retry in all parents is a costly operation  doing it only wgen we are retrying or failing avoid it most of the time.
kestra-io,kestra,5a462043941eb5119a4fd608f9d793892e0c1d0d,https://github.com/kestra-io/kestra/commit/5a462043941eb5119a4fd608f9d793892e0c1d0d,chore(core): small perf improvements to MapUtils
kestra-io,kestra,92ff557514972743c38665c0c4be5ac8fedb8527,https://github.com/kestra-io/kestra/commit/92ff557514972743c38665c0c4be5ac8fedb8527,chore(core): tiny perf improvement in MapUtils.merge()  MapUtils.merge() un-necessary clone the map when there is only one map that is not-null and not-empty which is not needed as the map is not modified but returned immediatly.
kestra-io,kestra,fb9691d67a96c20e7ed0041faa839dc9487822a7,https://github.com/kestra-io/kestra/commit/fb9691d67a96c20e7ed0041faa839dc9487822a7,chore(core): avoid using applicationContext.init() in the RunContext  This will improve performance as a run context is created very often.  Fixes #5492
kestra-io,kestra,34fa6ce9103e11eddebe4cd5640d289020638fb4,https://github.com/kestra-io/kestra/commit/34fa6ce9103e11eddebe4cd5640d289020638fb4,feat(jdbc): Improve execution queued performance  Add date inside the index to speed up order by in case there are a lot of execution queued. Skip locked records when selecting them as if there is a locked records it means you need to pop the next one.
kestra-io,kestra,3bb41e27ba6f19cdd79d5ef00f10162655d98936,https://github.com/kestra-io/kestra/commit/3bb41e27ba6f19cdd79d5ef00f10162655d98936,fix(core webserver): use more performant replace() instead of replaceAll() when possible
kestra-io,kestra,1d7982406c5cfb31a7278b032337822c58a75fd1,https://github.com/kestra-io/kestra/commit/1d7982406c5cfb31a7278b032337822c58a75fd1,feat(jdbc-postgres): improve JSONB performance
JetBrains,intellij-community,6c89ac0dd46f1b5c6b6011b5241a041a4d31ad4a,https://github.com/JetBrains/intellij-community/commit/6c89ac0dd46f1b5c6b6011b5241a041a4d31ad4a,IJPL-184289 [find in files] implement backend Find/Replace All operations  - Add performFindAllOrReplaceAll method to handle backend Find/Replace All - Refactor FindInProject/ReplaceInProject managers to use a unified executor  GitOrigin-RevId: aadde69545afa7abc457793016aa74f8fbf18919
JetBrains,intellij-community,851495e26f63f9c45e15504d35cd56212cec7fda,https://github.com/JetBrains/intellij-community/commit/851495e26f63f9c45e15504d35cd56212cec7fda,IJPL-184289 [find in files] perform replace on frontend if the registry key is enabled  GitOrigin-RevId: dd47e7d76551004e806f7de8279a3cbe5ae0aea0
JetBrains,intellij-community,58676fe01eeaf5b035cb56bff006d0ff25608426,https://github.com/JetBrains/intellij-community/commit/58676fe01eeaf5b035cb56bff006d0ff25608426,fixed java.lang.Throwable: Bus is already disposed at com.intellij.openapi.diagnostic.Logger.error(Logger.java:375) at com.intellij.util.messages.impl.MessageBusConnectionImpl.deliverImmediately(MessageBusConnectionImpl.kt:47) at com.intellij.execution.console.ConsoleExecutionEditor.dispose(ConsoleExecutionEditor.java:200) at com.intellij.openapi.util.ObjectTree.runWithTrace(ObjectTree.java:132) at com.intellij.openapi.util.ObjectTree.executeAll(ObjectTree.java:164) at com.intellij.openapi.util.Disposer.dispose(Disposer.java:211) at com.intellij.openapi.util.Disposer.dispose(Disposer.java:199) at com.intellij.serviceContainer.ComponentManagerImpl.dispose(ComponentManagerImpl.kt:1176) at com.intellij.openapi.project.impl.ProjectImpl.dispose(ProjectImpl.kt:336) at com.intellij.openapi.util.ObjectTree.runWithTrace(ObjectTree.java:132) at com.intellij.openapi.util.ObjectTree.executeAll(ObjectTree.java:164) at com.intellij.openapi.util.Disposer.dispose(Disposer.java:211) at com.intellij.openapi.util.Disposer.dispose(Disposer.java:199) at com.intellij.openapi.project.impl.ProjectManagerImpl.closeProject$lambda$17(ProjectManagerImpl.kt:421) at com.intellij.openapi.application.impl.AppImplKt$runnableUnitFunction$1.invoke(appImpl.kt:137) at com.intellij.openapi.application.impl.AppImplKt$runnableUnitFunction$1.invoke(appImpl.kt:137) at com.intellij.openapi.application.impl.NestedLocksThreadingSupport.runWriteAction(NestedLocksThreadingSupport.kt:866) at com.intellij.openapi.application.impl.ApplicationImpl.runWriteAction(ApplicationImpl.java:1022) at com.intellij.openapi.project.impl.ProjectManagerImpl.closeProject(ProjectManagerImpl.kt:401) at com.intellij.openapi.project.impl.ProjectManagerImpl.closeProject$default(ProjectManagerImpl.kt:326) at com.intellij.openapi.project.impl.ProjectManagerImpl.closeAndDispose(ProjectManagerImpl.kt:431) at com.intellij.openapi.wm.impl.CloseProjectWindowHelper.closeProjectAndShowWelcomeFrameIfNoProjectOpened(CloseProjectWindowHelper.kt:92) at com.intellij.openapi.wm.impl.CloseProjectWindowHelper.windowClosing$lambda$2(CloseProjectWindowHelper.kt:78) at com.intellij.openapi.application.WriteIntentReadAction.lambda$run$0(WriteIntentReadAction.java:24) at com.intellij.openapi.application.impl.AppImplKt$rethrowCheckedExceptions$2.invoke(appImpl.kt:139) at com.intellij.openapi.application.impl.NestedLocksThreadingSupport.runPreventiveWriteIntentReadAction(NestedLocksThreadingSupport.kt:608) at com.intellij.openapi.application.impl.NestedLocksThreadingSupport.runWriteIntentReadAction(NestedLocksThreadingSupport.kt:583) at com.intellij.openapi.application.impl.ApplicationImpl.runWriteIntentReadAction(ApplicationImpl.java:1076) at com.intellij.openapi.application.WriteIntentReadAction.compute(WriteIntentReadAction.java:55) at com.intellij.openapi.application.WriteIntentReadAction.run(WriteIntentReadAction.java:23) at com.intellij.openapi.wm.impl.CloseProjectWindowHelper.windowClosing(CloseProjectWindowHelper.kt:64) at com.intellij.openapi.wm.impl.ProjectFrameHelper.windowClosing(ProjectFrameHelper.kt:561) at com.intellij.openapi.wm.impl.WindowCloseListener.windowClosing(ProjectFrameHelper.kt:608) at java.desktop/java.awt.AWTEventMulticaster.windowClosing(AWTEventMulticaster.java:358) at java.desktop/java.awt.AWTEventMulticaster.windowClosing(AWTEventMulticaster.java:357) at java.desktop/java.awt.Window.processWindowEvent(Window.java:2115) at java.desktop/javax.swing.JFrame.processWindowEvent(JFrame.java:298) at java.desktop/java.awt.Window.processEvent(Window.java:2074) at java.desktop/java.awt.Component.dispatchEventImpl(Component.java:5043) at java.desktop/java.awt.Container.dispatchEventImpl(Container.java:2324) at java.desktop/java.awt.Window.dispatchEventImpl(Window.java:2810) at java.desktop/java.awt.Component.dispatchEvent(Component.java:4871) at java.desktop/java.awt.EventQueue.dispatchEventImpl(EventQueue.java:783) at java.desktop/java.awt.EventQueue$4.run(EventQueue.java:728) at java.desktop/java.awt.EventQueue$4.run(EventQueue.java:722) at java.base/java.security.AccessController.doPrivileged(AccessController.java:400) at java.base/java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:87) at java.base/java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:98) at java.desktop/java.awt.EventQueue$5.run(EventQueue.java:755) at java.desktop/java.awt.EventQueue$5.run(EventQueue.java:753) at java.base/java.security.AccessController.doPrivileged(AccessController.java:400) at java.base/java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:87) at java.desktop/java.awt.EventQueue.dispatchEvent(EventQueue.java:752) at com.intellij.ide.IdeEventQueue.defaultDispatchEvent(IdeEventQueue.kt:586) at com.intellij.ide.IdeEventQueue._dispatchEvent(IdeEventQueue.kt:483) at com.intellij.ide.IdeEventQueue.dispatchEvent$lambda$12$lambda$11$lambda$10$lambda$9(IdeEventQueue.kt:308) at com.intellij.openapi.progress.impl.CoreProgressManager.computePrioritized(CoreProgressManager.java:885) at com.intellij.ide.IdeEventQueue.dispatchEvent$lambda$12$lambda$11$lambda$10(IdeEventQueue.kt:307) at com.intellij.ide.IdeEventQueueKt.performActivity$lambda$3(IdeEventQueue.kt:959) at com.intellij.openapi.application.TransactionGuardImpl.performActivity(TransactionGuardImpl.java:118) at com.intellij.ide.IdeEventQueueKt.performActivity(IdeEventQueue.kt:959) at com.intellij.ide.IdeEventQueue.dispatchEvent$lambda$12(IdeEventQueue.kt:302) at com.intellij.ide.IdeEventQueue.dispatchEvent(IdeEventQueue.kt:342) at java.desktop/java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:207) at java.desktop/java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:128) at java.desktop/java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:117) at java.desktop/java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:113) at java.desktop/java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:105) at java.desktop/java.awt.EventDispatchThread.run(EventDispatchThread.java:92)  GitOrigin-RevId: 3cd9dbf74a34915690b8669b93ae251b28da2263
JetBrains,intellij-community,acb4c276e5f4d7ef730a899eac1c37940bc4eed4,https://github.com/JetBrains/intellij-community/commit/acb4c276e5f4d7ef730a899eac1c37940bc4eed4,unify performAction method name  GitOrigin-RevId: f7e67f6baa07cec91c5e107a27b76e1d58f622ee
JetBrains,intellij-community,cf1d1ab88a947be9663365700840ca6e46df4444,https://github.com/JetBrains/intellij-community/commit/cf1d1ab88a947be9663365700840ca6e46df4444,IJPL-186227 platform: disable debug color markers by-default  The performance impact might be (or not be) exaggerated by the profiler overhead  but there's no reason to keep it for everyone.  GitOrigin-RevId: 31f53201d03b76318da178bfe348f00572c20f14
JetBrains,intellij-community,ce7297003b00cb178c5014516b72fcc2ac73a1d4,https://github.com/JetBrains/intellij-community/commit/ce7297003b00cb178c5014516b72fcc2ac73a1d4,IJPL-186227 editor: do not cache TextAttributes during Editor rendering  We do not save memory by doing that  only make the cache bigger. This also saves CPU cycles on the cache map access.  This became an issue after introducing ComparableColor with slightly slower equality. This fixes 'EditorPaintingPerformanceTest.testScrollingThroughLongTextFile'  GitOrigin-RevId: 09885db0335b96a9adbf6771951986d0827fc1b5
JetBrains,intellij-community,544e626970dc3011bb7486d1aba6a05cb45ab467,https://github.com/JetBrains/intellij-community/commit/544e626970dc3011bb7486d1aba6a05cb45ab467,deprecate ActionUtil.performDumbAwareWithCallbacks  And drop its usages.  GitOrigin-RevId: 9505af7fb37d368509d5ce3b05ef823e39118c93
JetBrains,intellij-community,5963235ef9b2dd4ce3d62b36a436fedd3a7f5c38,https://github.com/JetBrains/intellij-community/commit/5963235ef9b2dd4ce3d62b36a436fedd3a7f5c38,introduce ContentManager.addUiDataProvider  Old API is a performance problem.  GitOrigin-RevId: bcd953d84601c82500e0718a12e044ae6b1675f0
JetBrains,intellij-community,ef6a53e148c8ada1121b11159d2ce6adb873c582,https://github.com/JetBrains/intellij-community/commit/ef6a53e148c8ada1121b11159d2ce6adb873c582,IJPL-185303 Do not spam UiNotifyConnector instances in toolbars  When a toolbar is added/removed many times to a parent that is not currently showing  on every addNotify it would add a listener through UiNotifyConnector. Eventually there can be too many such listeners to handle.  Fix by using launchOnceOnShow instead and saving the returned job. If there's already a job scheduled  simply do nothing. Remove the job to save memory when we're done. This  of course  has a side effect of launching the same job again if the toolbar is removed and then added. That is not an issue because updateActionsFirstTime() is a no-op unless it's a really first update  so subsequent jobs won't do anything. And because there's at most one job  it'll never become a performance problem.  Dispatchers.EDT is needed in launchOnceOnShow because updateActionsImmediately() is a legacy API that requires both EDT and read actions. By default launchOnceOnShow uses Dispatchers.UI that prohibits read actions.  GitOrigin-RevId: 97f77a0d8192a0457c1a7e1e9395c21534705800
JetBrains,intellij-community,33ed91b42323d04eeb09354c86e8453b24fb6ec2,https://github.com/JetBrains/intellij-community/commit/33ed91b42323d04eeb09354c86e8453b24fb6ec2,drop last beforeActionPerformedUpdate usages  GitOrigin-RevId: 096ac37b77c02b04aaf627e97b763c93901df275
JetBrains,intellij-community,45a88340114d878e894071cef0d0ffdd10e2dbda,https://github.com/JetBrains/intellij-community/commit/45a88340114d878e894071cef0d0ffdd10e2dbda,drop beforeActionPerformedUpdate setting  GitOrigin-RevId: 9cce7d035e8cac4dddc64ca2ea95da6d1c825ae5
JetBrains,intellij-community,c850a623760cf0be37afc9284dc9fb6d4b52400b,https://github.com/JetBrains/intellij-community/commit/c850a623760cf0be37afc9284dc9fb6d4b52400b,[debugger] Provide evaluatable context on Pause  * Considered the following potential race: If the future  which should provide an evaluatable context  is cancelled on a timeout  the old fallback Pause should be performed. We make sure that vm.suspend() does not happen twice during a fallback Pause and at the moment MethodEntryRequest is hit concurrently with the timeout expiration. * Do not notify debugSessionListeners about a pause in case of method entry stop  otherwise they are notified twice. * Check in DebuggerSessionTest that only 2 pauses have occurred.  IDEA-368296  GitOrigin-RevId: e923e431eaae35fa2e1519041b45d096c4f8e719
JetBrains,intellij-community,9881b881e5f325ca709053485d3be5460809a952,https://github.com/JetBrains/intellij-community/commit/9881b881e5f325ca709053485d3be5460809a952,IJPL-184075 Validate the terminal editor size when changing the font  The standard editor implementation doesn't work because it has a bug: it doesn't update the size immediately  because it calls invalidate() and not validateSize(). This is not a big deal for the editor  as it almost never uses a point that's close to the bottom  and it's actually beneficial in terms of performance. But it's a deal breaker for the terminal  as increasing the font size never scrolls correctly. Fix by introducing a flag therefore.  GitOrigin-RevId: 9b5ba77a5aa314b9ca8d3a997b5705fc54b7bea7
JetBrains,intellij-community,f1764deb9989dadc95224e97e80d47028a70c147,https://github.com/JetBrains/intellij-community/commit/f1764deb9989dadc95224e97e80d47028a70c147,[driver] Use write-intent read action as the read lock for EDT computations  By default  NO_LOCK semantics leads to the complete absence of lock. We need to execute some runnables under write-intent lock  and we don't want to pollute the API layer by an internal concept. Hence  the combination of OnDispatcher.EDT and LockSemantics.READ_ACTION will lead to the Write-Intent lock on EDT  as a way to perform computations in compatibility mode. We hope that no one wants to run pure read computations on EDT  GitOrigin-RevId: 8ae0706f0aba2f3e6cbef42c6c45b361097f7c19
JetBrains,intellij-community,801affaf9a1f0da5fc6390d1ccaa2b8cb11e612c,https://github.com/JetBrains/intellij-community/commit/801affaf9a1f0da5fc6390d1ccaa2b8cb11e612c,IJPL-184686 Fix hasBeenExpanded JTree -> Tree migration  Turns out there is a proper API to get "have been expanded" paths after all. Not only that  it's more efficient this way because we perform less checks than getExpandedDescendants. For example  isVisible() is not really needed  as we migrate the entire state anyway  not just the visible part.  GitOrigin-RevId: c34d7eb89b0a60a7e1d5eac897fbcc605ba3bd92
JetBrains,intellij-community,9c8193b99f8889ab13d801008eed0cf866cdef04,https://github.com/JetBrains/intellij-community/commit/9c8193b99f8889ab13d801008eed0cf866cdef04,[git] IJPL-182618 Execute fetch in parallel when updating common branch in all roots  Logic inside `updateBranches` is still broken - update for repos with a branch checked out is performed asynchronously. Besides it there might be a mess in notification. However  it seems like the fix can be postponed...  GitOrigin-RevId: 7e300554088a72a96b9643025462f6cbf73130e2
JetBrains,intellij-community,dbe1cf82eca5ae599bead68baed96ff8bf252eab,https://github.com/JetBrains/intellij-community/commit/dbe1cf82eca5ae599bead68baed96ff8bf252eab,[threading] Allow performing `invokeAndWait` with modality but without locks  GitOrigin-RevId: 93fa9c88d762868f0493f64fcd6da620de3be0c2
JetBrains,intellij-community,8cd04eb35c00ebdcdb29cef3e563d0795097cce1,https://github.com/JetBrains/intellij-community/commit/8cd04eb35c00ebdcdb29cef3e563d0795097cce1,[markdown] IJPL-177111 combine multiple RPCs needed for link opening into one  Refactored Markdown link navigation by introducing MarkdownLinkNavigationData. Legacy URI-based handling has been replaced to streamline file and header navigation while eliminating redundant methods. Instead of performing slow findVirtualFile calls on EDT  the implementation now passes a VirtualFile directly to the openFile and navigateToHeader actions. Related services  DTOs  and mappings were updated accordingly to support the new  more efficient approach.  GitOrigin-RevId: 4545b23b17c55fe191e1b3c1f77617b6c409818d
JetBrains,intellij-community,e3e4b05912258ca30cab67d249f250097b280bbd,https://github.com/JetBrains/intellij-community/commit/e3e4b05912258ca30cab67d249f250097b280bbd,[kotlin] Implement `KotlinGlobalSearchScopeMergeStrategy` for `com.intellij.psi.search.UnionScope`  This merge strategy performs flattening of union scopes and optimizes the resulting `GlobalSearchScope` in `KaGlobalSearchScopeMerger`.  ^KT-62474  GitOrigin-RevId: 35724fb2aad7d0a297c86fb67d80929e1a69c280
JetBrains,intellij-community,13f73dcfb6cafb806f356e699b0ece495bdc60d9,https://github.com/JetBrains/intellij-community/commit/13f73dcfb6cafb806f356e699b0ece495bdc60d9,[injection] IJPL-183718 Make injected highlighting pass dumb aware  LanguageInjectionPerformerAdapter is PossiblyDumbAware LanguageInjector is PossiblyDumbAware  GitOrigin-RevId: 4589c1e2b1cc1acfc49eaa16835d3a4dd33daab8
JetBrains,intellij-community,e689bb2f2be66ef96ee09aa2a4faa0ba2c08b011,https://github.com/JetBrains/intellij-community/commit/e689bb2f2be66ef96ee09aa2a4faa0ba2c08b011,[rd debugger] IJPL-182077 Add warning about action perform skip  GitOrigin-RevId: 0e48baa081b2033646e269af95b5a6fbdb90fadb
JetBrains,intellij-community,01ebbc638e4907662d93afda264107fab8d823b0,https://github.com/JetBrains/intellij-community/commit/01ebbc638e4907662d93afda264107fab8d823b0,[indexing-api] SingleTargetRequestResultProcessor: save service in the field  The processor is created per search session in background thread  so it's a relatively short-living thing. It's unlikely that the service should be unloaded during its function. On the other hand  processTextOccurrence might be called too often  which affects the overall search performance. May help IDEA-368981 Renaming package never finishes  stuck "Looking for Usages"  GitOrigin-RevId: f3f9643fd47a948997dbebde05f10b344e63ba53
JetBrains,intellij-community,4ff346caa255ea16965f92d4c85077ee2a918483,https://github.com/JetBrains/intellij-community/commit/4ff346caa255ea16965f92d4c85077ee2a918483,RIDER-124493 perform "smart step into" enter handler on frontend to avoid executing it on backend   (cherry picked from commit 7ded5cb2306edfd12f7cdb8478d6c4c23a8b3faf)  IJ-MR-159874  GitOrigin-RevId: 8af6ca0949e15fd56b578558abb2bb218d591ecc
JetBrains,intellij-community,35953e3d8b0886de60845f9187a3280ca936ff10,https://github.com/JetBrains/intellij-community/commit/35953e3d8b0886de60845f9187a3280ca936ff10,wrap LOG.debug with a guard for performance  GitOrigin-RevId: d102c1ef88db6d76750f55f4991af30771a2eda4
JetBrains,intellij-community,db0c9cc9925803866876bcadc32c7f8c0f08d882,https://github.com/JetBrains/intellij-community/commit/db0c9cc9925803866876bcadc32c7f8c0f08d882,[Java. Code Formatting] Move check about "keep empty lines" before performing any modifications with javadoc.  IDEA-361836  GitOrigin-RevId: 43df47f4016dec918b869340635b8b4ef7682df1
JetBrains,intellij-community,55a9fc8064c4750a2584a517e66927a340e19ef3,https://github.com/JetBrains/intellij-community/commit/55a9fc8064c4750a2584a517e66927a340e19ef3,[wsl][IJPL-181144] introduced `JdkUtil#isCompatible(Path  Project)` and `checkForJdk(Path  Boolean)` marked as obsolete  `checkForJdk(Path  Boolean)` depends on a flag with information about the current execution environment. As a result  this flag could be miss-used and the wrong check would be performed. The method should only check "is it possible to use a JDK instance" no matter to which execution environment it belongs to. `JdkUtil#isCompatible` is a replacement for environment-related logic from `checkForJdk(Path  Boolean)`.  GitOrigin-RevId: d6e9346c669cf2ad95158867c7ce58e51082b4d5
JetBrains,intellij-community,aaa808c3b4bc9d7d3b5a73177c5580948af60cc0,https://github.com/JetBrains/intellij-community/commit/aaa808c3b4bc9d7d3b5a73177c5580948af60cc0,Revert "PY-79480 Resolve attribute reference to ancestor attributes in constructors if they contain type annotations"  Fixes PY-79997 performance regression  GitOrigin-RevId: 9bccec0543fcdf7970311c187bd9fd9b1357c058
JetBrains,intellij-community,b66992f8eac1483b0576dc6b5b1f065427fd1675,https://github.com/JetBrains/intellij-community/commit/b66992f8eac1483b0576dc6b5b1f065427fd1675,IJPL-182127 eliminating potential performance degradation points  GitOrigin-RevId: aecf12a1de8660397e28af5216fe61eb088c3b62
JetBrains,intellij-community,d9c593bfb744289dfdf83881cdd8966cd4790067,https://github.com/JetBrains/intellij-community/commit/d9c593bfb744289dfdf83881cdd8966cd4790067,IJPL-164422 DocumentImpl: do not create new changedPart if it's a String  This code was initially written to avoid dragging along with the changed part some huge chunks of text in case this value is some complicated implementation of CharSequence. But if it's just a String  turning it into a smart immutable char sequence has no benefit but may slow down things.  Notably  this change noticeably improves the performance of LineSet.isSingleLineChange.  GitOrigin-RevId: 86ef29d3ba08dc7eaf7a51127449afcb92ae6d27
JetBrains,intellij-community,4cd3a97b3a91964b37dc0f9cbb03276585aa5afb,https://github.com/JetBrains/intellij-community/commit/4cd3a97b3a91964b37dc0f9cbb03276585aa5afb,IJPL-177555 Use separate components for highlighting renderers and editors  The same fix as for the severity column.  The renderer is reused over and over again to render cells  even while the editor is shown. The editor  on the other hand  exists in one place while the editing is being performed. Using one component for both creates visual glitches  as it can't be in two places at the same time  acting as the active editor and a temporary renderer elsewhere.  Fix by creating two separate instances.  GitOrigin-RevId: b164978380f20db5d62b74be4e953d9dc8a8b424
JetBrains,intellij-community,69ca335f53773a8a7c1bcab80e93cbef03c5f648,https://github.com/JetBrains/intellij-community/commit/69ca335f53773a8a7c1bcab80e93cbef03c5f648,IJPL-177555 Use separate components for severity renderers and editors  The renderer is reused over and over again to render cells  even while the editor is shown. The editor  on the other hand  exists in one place while the editing is being performed. Using one component for both creates visual glitches  as it can't be in two places at the same time  acting as the active editor and a temporary renderer elsewhere.  Fix by creating two separate instances.  GitOrigin-RevId: 23e594c4e02124193d89b482c827a703ff669d68
JetBrains,intellij-community,b0825ccdbb002a9c1e3de6afbff441ec85d48650,https://github.com/JetBrains/intellij-community/commit/b0825ccdbb002a9c1e3de6afbff441ec85d48650,[performance] IJPL-181579 Report triggered slow operation issues to FUS  GitOrigin-RevId: 7187af7dc142f2637a896fd0739dcd4a17f3e916
JetBrains,intellij-community,ccf87d7e638a1ad404ebebd517c403406a98e67a,https://github.com/JetBrains/intellij-community/commit/ccf87d7e638a1ad404ebebd517c403406a98e67a,[performance] IJPL-181579 Report triggered slow operation issues to FUS  GitOrigin-RevId: 6e22bb1b69f75fa0a17f5e48157f696e6eeff067
JetBrains,intellij-community,cc73160dc963d038c002885c2b6f2313290f6b96,https://github.com/JetBrains/intellij-community/commit/cc73160dc963d038c002885c2b6f2313290f6b96,Eel/IJent: refactor RoutingAwareFileSystemProvider: unwrapping depending on path  The previous architecture was unable to perform a call `File.copy(p1  p2)`  where `p1.getFileSystem().provider() == WindowsFileSystemProvider` and `p2.getFileSystem().provider() == TracingFileSystemProvider(WindowsFileSystemProvider)`  GitOrigin-RevId: 5fe84749f3cd5b61354f1ea0b36ad2052eb150a6
JetBrains,intellij-community,b94deb1c91c2aa000f70459e12bde3723792d75e,https://github.com/JetBrains/intellij-community/commit/b94deb1c91c2aa000f70459e12bde3723792d75e,Eel: fix Provider mismatch: WindowsFileSystemProvider != TracingFileSystemProvider(WindowsFileSystemProvider)  This error appeared in tests:  ``` java.lang.IllegalArgumentException: Provider mismatch: sun.nio.fs.WindowsFileSystemProvider@76cf91c9 != TracingFileSystemProvider(sun.nio.fs.WindowsFileSystemProvider@76cf91c9) at com.intellij.platform.core.nio.fs.MultiRoutingFileSystemProvider.getDelegate(MultiRoutingFileSystemProvider.java:197) at com.intellij.platform.core.nio.fs.DelegatingFileSystemProvider.copy(DelegatingFileSystemProvider.java:198) at java.base/java.nio.file.Files.copy(Files.java:1305) at com.intellij.platform.ijent.performance.benchmarks.IJentWslZipFileReadBenchmarkTest$measureReadFile$1.invokeSuspend(IJentWslZipFileReadBenchmarkTest.kt:48) at _COROUTINE._BOUNDARY._(CoroutineDebugging.kt:42) at com.intellij.platform.ijent.performance.BenchmarkKt$runIJentBenchmark$1.invokeSuspend(benchmark.kt:42) Caused by: java.lang.IllegalArgumentException: Provider mismatch: sun.nio.fs.WindowsFileSystemProvider@76cf91c9 != TracingFileSystemProvider(sun.nio.fs.WindowsFileSystemProvider@76cf91c9) at com.intellij.platform.core.nio.fs.MultiRoutingFileSystemProvider.getDelegate(MultiRoutingFileSystemProvider.java:197) at com.intellij.platform.core.nio.fs.DelegatingFileSystemProvider.copy(DelegatingFileSystemProvider.java:198) at java.base/java.nio.file.Files.copy(Files.java:1305) at com.intellij.platform.ijent.performance.benchmarks.IJentWslZipFileReadBenchmarkTest$measureReadFile$1.invokeSuspend(IJentWslZipFileReadBenchmarkTest.kt:48) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33) at kotlinx.coroutines.internal.ScopeCoroutine.afterResume(Scopes.kt:36) at kotlinx.coroutines.AbstractCoroutine.resumeWith(AbstractCoroutine.kt:101) at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:46) at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:100) at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:263) at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:112) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$BuildersKt__BuildersKt(Builders.kt:85) at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:53) at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source) at com.intellij.testFramework.common.TimeoutKt.timeoutRunBlocking-rnQQ1Ag(timeout.kt:25) at com.intellij.testFramework.common.TimeoutKt.timeoutRunBlocking-rnQQ1Ag$default(timeout.kt:16) at com.intellij.platform.ijent.performance.BenchmarkKt.runIJentBenchmark(benchmark.kt:39) at com.intellij.platform.ijent.performance.benchmarks.IJentWslZipFileReadBenchmarkTest.measureReadFile(IJentWslZipFileReadBenchmarkTest.kt:30) at com.intellij.platform.ijent.performance.benchmarks.IJentWslZipFileReadBenchmarkTest.measureZipFileReading(IJentWslZipFileReadBenchmarkTest.kt:56) at com.intellij.platform.ijent.performance.benchmarks.IJentWslZipFileReadBenchmarkTest.java zip file(IJentWslZipFileReadBenchmarkTest.kt:84) at java.base/java.lang.reflect.Method.invoke(Method.java:580) at com.intellij.ide.starter.junit5.CurrentTestMethodArgumentsProvider.intercept(CurrentTestMethodArgumentsProvider.kt:53) at com.intellij.ide.starter.junit5.CurrentTestMethodArgumentsProvider.interceptTestTemplateMethod(CurrentTestMethodArgumentsProvider.kt:23) at com.intellij.testFramework.junit5.impl.TestLoggerInterceptor.intercept$lambda$0(TestLoggerInterceptor.kt:12) at com.intellij.testFramework.TestLoggerKt.recordErrorsLoggedInTheCurrentThreadAndReportThemAsFailures(testLogger.kt:86) at com.intellij.testFramework.junit5.impl.TestLoggerInterceptor.intercept(TestLoggerInterceptor.kt:11) at com.intellij.testFramework.junit5.impl.AbstractInvocationInterceptor.interceptTestTemplateMethod(AbstractInvocationInterceptor.kt:37) at com.intellij.platform.ijent.testFramework.functional.LogTestName.runTest(IjentTestUtil.kt:63) at com.intellij.platform.ijent.testFramework.functional.LogTestName.interceptTestTemplateMethod(IjentTestUtil.kt:30) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708) at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762) at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:1024) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) at java.base/java.util.ArrayList.forEach(ArrayList.java:1596) at java.base/java.util.ArrayList.forEach(ArrayList.java:1596) ```  GitOrigin-RevId: 690ff7ad7dca00f46bf577c8f5819f3e0068b380
JetBrains,intellij-community,cc8e3f725ddef9c5bce05d2a4d3d08d5710b573c,https://github.com/JetBrains/intellij-community/commit/cc8e3f725ddef9c5bce05d2a4d3d08d5710b573c,[platform] Remove uiLess flag for run content descriptor  since actions now can be marked as FrontendOtherwiseBackend  Previously this uiLess flag was introduced for the actions like Stop  which use RunContentManager to check its availability and performing on the backend  but now such actions may work on the frontend as well and they will use frontend's RunContentManager  GitOrigin-RevId: 72ac822d9e6778075f0fa868069e573ea2ad9f96
JetBrains,intellij-community,3c3590110fc9472d365bfa1c441e36345f9455e3,https://github.com/JetBrains/intellij-community/commit/3c3590110fc9472d365bfa1c441e36345f9455e3,do not restart the entire highlighting after the lazy quickfix registered  Instead  re-launch the additional ShowIntentionsPass to include that newly computed quickfix into the current intention info Restarting the entire highlighting was bad for performance and led to endlessly restarting CWM tests  GitOrigin-RevId: 9b86d4631b74b2c9632cd64cc518df3125d84b8c
JetBrains,intellij-community,c3ea5c3050e87530c6849365584724b0d20f2207,https://github.com/JetBrains/intellij-community/commit/c3ea5c3050e87530c6849365584724b0d20f2207,[terminal] IJPL-180853 Use Terminal Engine option instead of registry keys  Implement migration of `terminal.new.ui` and `terminal.new.ui.reworked` registry keys to the Terminal Engine option that will be stored in `TerminalOptionsProvider`. After migration is performed  registry keys will become no-op. It is required to finally make this option synced between backend and frontend in RemDev  because registry values can't be synced.  GitOrigin-RevId: b5d455ea9205e9660a1ee382b9341c6fa9014d5a
JetBrains,intellij-community,aafc2dc34115187c438af8dbfbd9a5d1e69f54e1,https://github.com/JetBrains/intellij-community/commit/aafc2dc34115187c438af8dbfbd9a5d1e69f54e1,IJPL-179246 Highlighting sometimes disappears after typing when Find tool window is open  Sometimes a highlighting pass should be run twice  e.g. when two file editors are opened for the same document. When two GeneralHighlightingPasses for the same document are run reentrantly  they can compete for markup model  causing flicker as pass1 trying to reuse RangeHighlighter which pass2 already reused. Also  running two identical piece of computation is bad performance-wise. So when the document changed  we run highlighting passes for all file editors containing this document  but document-based passes are run only once per document  whereas editor-bound passes are run per file editor. This document-bound pass is run only once  while its instances in all other file editors are waiting for the first instance to complete  instead of calling its own collectInformation(). We assume that ProgressableTextEditorHighlightingPass inheritors are document-bound passes.  GitOrigin-RevId: 85343ab33f53ba8b8d61342d2de3131be7650c6f
JetBrains,intellij-community,e804b7c38c8cee5933ab483e9bab56bd6ebd535a,https://github.com/JetBrains/intellij-community/commit/e804b7c38c8cee5933ab483e9bab56bd6ebd535a,IJPL-164422 Optimized soft wraps for cell grid editors  When outputting large text into a terminal  it spends about 30-40% of the time calculating soft wraps. Most lines  however  don't have inlays  and because characters are aligned to cells  no font metrics is required to calculate soft wraps. We only need to know the number of columns and for every character wither it's a double-width one or not.  Introduce a special soft wrap mode that automatically kicks in as soon as the grid mode is enabled and there are no inlays in the range.  To avoid creating an iterator just to check whether there are inlays  extract inlay calculation from the constructor.  To avoid performance issues caused by repetitive codePointAt() invocations  make a copy of the whole thing using toString() instead. This will create garbage  of course  but the performance benefit seems to be very well worth it.  GitOrigin-RevId: d34e7522e32691faf70e5dd3e921dfb4856bbac7
JetBrains,intellij-community,221a00408e72c88985112764c3d3d77154a5d9bd,https://github.com/JetBrains/intellij-community/commit/221a00408e72c88985112764c3d3d77154a5d9bd,Fix '[aia] Code Generation Evaluation for Java Marker Tests' (npe when shutdown is performed via ShutDownTracker)  GitOrigin-RevId: 0170e0ffbded81fb7ee4607556adc8296a9d56a1
JetBrains,intellij-community,b73d410a7f2c959168ea1e6429ecba97c698be97,https://github.com/JetBrains/intellij-community/commit/b73d410a7f2c959168ea1e6429ecba97c698be97,ui: speedup popup layout  Do not perform 'removeAll'/'addAll' if nothing has changed  as this renderer is not utilizing the layout hack from 'com.intellij.ui.SimpleListCellRenderer'.  GitOrigin-RevId: a7974daf3accf64d90be74003974701d7d9afb7b
JetBrains,intellij-community,c642f82c677aa3705d9e0bc07ce3a30e075e8cca,https://github.com/JetBrains/intellij-community/commit/c642f82c677aa3705d9e0bc07ce3a30e075e8cca,[debugger] Debugger actions to performDebuggerAction  so reshowInlayRunToCursor will be called afterwards  GitOrigin-RevId: 66c47a6bd6fdf8e0b7f81ecdb04b9c326e3f6643
JetBrains,intellij-community,f84c828a80e1506cacfb1dde9dce98c87a6a54af,https://github.com/JetBrains/intellij-community/commit/f84c828a80e1506cacfb1dde9dce98c87a6a54af,Avoid expensive operations if the module build script classpath is the same object  This results in a performance boost the module build classpaths are already interned in the cases where they are the same.  Also caching some jar related operations becuase a file lookup is being done.  closes https://github.com/JetBrains/intellij-community/pull/2957  GitOrigin-RevId: a48310c79379734cd028aa60defbc3f07a15dc80
JetBrains,intellij-community,8c0b09a70e7020a713420507caee43115acd2981,https://github.com/JetBrains/intellij-community/commit/8c0b09a70e7020a713420507caee43115acd2981,IJPL-179075 Implement caching in TreeState for non-async trees  When performing a linear search for the matching child  cache results so that they can be used for the next path.  This reduces the complexity of the search for nodes with a lot of children from O(N^2) to O(N) because on the next search  either the result is already cached  or at least we skip the already-searched part  starting from the next "unexplored" index instead.  GitOrigin-RevId: 7bbf2667649d959ba1d925c24cc2b4db3b5f5531
JetBrains,intellij-community,2412543138635dc137d26a8c320ffdae674fa5eb,https://github.com/JetBrains/intellij-community/commit/2412543138635dc137d26a8c320ffdae674fa5eb,[kotlin] Do not perform expression live templates within the name of named declarations  #KTIJ-33194 Fixed  GitOrigin-RevId: 94ff8b34821b6975f60dfd80f685a7f980271a8b
JetBrains,intellij-community,ef139eb222078afe2ce892075ba1aa32f672aa4e,https://github.com/JetBrains/intellij-community/commit/ef139eb222078afe2ce892075ba1aa32f672aa4e,[platform] IJPL-178354 Run `System.gc()` when IDE becomes idle  `System.gc()` makes the heap to shrink respecting `-XX:MaxHeapFreeRatio` VM option  so some memory is returned to OS. At the same time  it leads to a long GC pause (hundreds of milliseconds). So  when the IDE becomes idle  it looks like the perfect time to invoke `System.gc()`.  "Idle" means: - All IDE windows are deactivated (unfocused) - No modal progress - There aren't any background task progress indicators in any project - Daemon code analyzer is not running in any project - HeavyProcessLatch does not hold any heavy operation  Can be disabled by `ide.idle.memory.cleaner.enabled` registry option  GitOrigin-RevId: ccfdc9d837ec4023cf8b4d2e1d49809a4d9ef700
JetBrains,intellij-community,426831b2deab8fe5c9536a324fe68837fa0f711d,https://github.com/JetBrains/intellij-community/commit/426831b2deab8fe5c9536a324fe68837fa0f711d,fixup! IJPL-165650 [vcs] Introduce `VcsDirtyScopeManager#rootDirty` for requesting update on a specific VCS root  The existing `VcsDirtyScopeManager#dirDirtyRecursively` method marks all internal repositories as dirty  which limits performance optimizations for Git integration.   (cherry picked from commit fc4e357d83412fbd23fb38f21e755973b6321582)  IJ-MR-154527  GitOrigin-RevId: 4a8be33c7f746399f1ff98b52aee1e440e51c4a0
JetBrains,intellij-community,18aa0440e9e814391a275a3be1b09a797b833ff7,https://github.com/JetBrains/intellij-community/commit/18aa0440e9e814391a275a3be1b09a797b833ff7,fixup! IJPL-165650 [vcs] Introduce `VcsDirtyScopeManager#rootDirty` for requesting update on a specific VCS root  The existing `VcsDirtyScopeManager#dirDirtyRecursively` method marks all internal repositories as dirty  which limits performance optimizations for Git integration.   (cherry picked from commit bd36156fa21ca49d59d348a692e0de2ec4321cf2)  IJ-MR-154527  GitOrigin-RevId: c9c0b6eee0e00ec7dd5d0270d4ff900078b27117
JetBrains,intellij-community,dd87909b676ec29cff20bacad0b45a367dd9f5e3,https://github.com/JetBrains/intellij-community/commit/dd87909b676ec29cff20bacad0b45a367dd9f5e3,IJPL-165650 [vcs] Introduce `VcsDirtyScopeManager#rootDirty` for requesting update on a specific VCS root  The existing `VcsDirtyScopeManager#dirDirtyRecursively` method marks all internal repositories as dirty  which limits performance optimizations for Git integration.   (cherry picked from commit 9f5cc85ce2ab10461ed50d276717633253c14069)  IJ-MR-154527  GitOrigin-RevId: 89209a830c41d2c94ce5e27a9a3501b59cadf116
JetBrains,intellij-community,4c9f0c00f21aa7a9199e0134b2356c20a3112d57,https://github.com/JetBrains/intellij-community/commit/4c9f0c00f21aa7a9199e0134b2356c20a3112d57,[core][monitoring][fus] IJPL-178238 report IDE OS-provided memory usage  - Collect both memory metrics - `ramMinusFileMappings` and `ramPlusSwapMinusFileMappings`. - Memory usage is sampled every second - Aggregated into a histogram with predefined buckets - The histogram is reported to FUS hourly - The fus event additionally contains `xmx` field (max heap size). It can be used to divide reports into groups with similar expected memory usage - Reuse a thread from JVMResponsivenessMonitor. Using coroutines seems imperfect because of less predictable user-space scheduling; running another thread seems redundant because JVMResponsivenessMonitor is a perfect match.  GitOrigin-RevId: 360ca47327a22efaf686d933250571b14c8c84be
JetBrains,intellij-community,0d621e44e8ea32d53d140eb6b67302b7c9a810f3,https://github.com/JetBrains/intellij-community/commit/0d621e44e8ea32d53d140eb6b67302b7c9a810f3,[ui] Improve performance in ColorUtil.toHex by avoiding unnecessary string concatenation  GitOrigin-RevId: 53b3ad78f328c2581b941e41532e071db8ace9fb
JetBrains,intellij-community,b2780d60c13ace8e0e03d9b230ee214c6fa492a5,https://github.com/JetBrains/intellij-community/commit/b2780d60c13ace8e0e03d9b230ee214c6fa492a5,[config-import] IJPL-166096 increase update fetching timeout to 7s  Marketplace requires one request to fetch available update ids and then a request per each update id to fetch the update data  It's better to have a timeout because fetching is performed without a UI  GitOrigin-RevId: d7078b0ee45d8e8d89614ec1a1338c7e3f746573
JetBrains,intellij-community,e2fba18524262048f94094d3f77dbf3948dc93fb,https://github.com/JetBrains/intellij-community/commit/e2fba18524262048f94094d3f77dbf3948dc93fb,IJPL-339 IDEA-367535 fix performance degradation  Inferring all classes from scratch is rather slow. Returning the original behaviour for non-multiverse case. Investigation of the proper cache is to be done.  GitOrigin-RevId: e6134ade61f36f458fc3214d1ae54180727bc2b2
JetBrains,intellij-community,1c7a08a850284d0f4cb06e06329c9be7730ca048,https://github.com/JetBrains/intellij-community/commit/1c7a08a850284d0f4cb06e06329c9be7730ca048,[threading] IJPL-148438: Extract performance monitoring from `AnyThreadWriteThreadingSupport`  GitOrigin-RevId: ceb051de5e2e773cf71f561d97334b144b107eb8
JetBrains,intellij-community,6da62a69370974abb6128eb8cea11601bddc8d19,https://github.com/JetBrains/intellij-community/commit/6da62a69370974abb6128eb8cea11601bddc8d19,[debugger] IDEA-366895 Debugger: collect performance statistics on the command execution time  GitOrigin-RevId: 1de07f7dba0a674838012e1ba228c3356febf93e
JetBrains,intellij-community,81a06d7199f7f2c73a265a745b8b2a163a4f4f60,https://github.com/JetBrains/intellij-community/commit/81a06d7199f7f2c73a265a745b8b2a163a4f4f60,IJPL-175562 Only perform char alignment if its slot is wider  The case when the slot is narrower is tricky and shouldn't really occur in practise except with really strange characters or fonts. The rest of the code doesn't work well with negative alignments  so let's start by prohibiting them altogether and then fix issues on a case-by-case basis if they appear.  GitOrigin-RevId: 67ed1f12a919946e131ba815b49655e0a7923af2
JetBrains,intellij-community,629811c6accc0900f4b58e84b0bdf7368b8b185c,https://github.com/JetBrains/intellij-community/commit/629811c6accc0900f4b58e84b0bdf7368b8b185c,IJPL-176667 Perform extensive memory caches cleanup only when double-clicking the Memory Indicator  GitOrigin-RevId: d336b17915e7596ce4634bfeb098e4ea1f800676
JetBrains,intellij-community,806d8e4a90b08447b2b195a0910221c8cc9a1e09,https://github.com/JetBrains/intellij-community/commit/806d8e4a90b08447b2b195a0910221c8cc9a1e09,[vcs] IJPL-176372 Enable non-modal commit by default  Neither reads nor writes to `VcsApplicationSettings.COMMIT_FROM_LOCAL_CHANGES` are performed anymore - toolwindow mode is enforced. However  modal flow is not removed completely and still present for actions like "Fixup" and is activated if non-distributed VCS is present.  GitOrigin-RevId: 5a4bc6c33202b1678db831a75e97a233dd03eb28
JetBrains,intellij-community,87816bdb8613d058230d60089c39193abb3af5dd,https://github.com/JetBrains/intellij-community/commit/87816bdb8613d058230d60089c39193abb3af5dd,IJPL-175562 Cell grid text rendering: complex fragments  first attempt  Grid calculation is based on glyph positions here. It is assumed that one glyph is one character. This will likely have to be improved in the future.  Unlike simple fragments  we start with absolute positions here  not widths. Therefore  when we perform adjustments  we need to take care to shift the following glyphs accordingly  even if those glyphs are not affected by alignment.  GitOrigin-RevId: 8460c1143858235100bb2640ca881b2b39427db2
JetBrains,intellij-community,98857cb8ef510ce5c2f89663fea3ee6808e708e0,https://github.com/JetBrains/intellij-community/commit/98857cb8ef510ce5c2f89663fea3ee6808e708e0,IJPL-175562 Cell grid text rendering: simple fragments  first attempt  Grid calculation is based on character widths here.  First  if the character width is almost-divisible by the cell width  no adjustment is performed. This is a very common case with monospaced fonts  so it's a good optimization.  Then  the number of slots the character takes is calculated based on its width and cell width  dividing these and rounding up  but with a 10% threshold. This threshold allows skipping slightly wider characters just in case.  When painting  characters without alignment are painted as-is  while the aligned characters are painted one-by-one  centering them in their cells.  GitOrigin-RevId: 4e2f3a46eaa8c822e45639f577988c0cad414716
JetBrains,intellij-community,10ffee4720496371ce43e800c4590f4a6a82e2b9,https://github.com/JetBrains/intellij-community/commit/10ffee4720496371ce43e800c4590f4a6a82e2b9,[indexes] Ignore PCE during index initialization  The cancellation here cannot be caused by the outer coroutine  as this code is executed in non-cancellable section on shutdown. The cancellation here means that the application was already disposed  hence the indexes are not needed anymore  ``` at com.intellij.openapi.progress.util.ProgressIndicatorUtils.awaitWithCheckCanceled(ProgressIndicatorUtils.java:403) at com.intellij.openapi.progress.util.ProgressIndicatorUtils.awaitWithCheckCanceled(ProgressIndicatorUtils.java:364) at com.intellij.util.indexing.RegisteredIndexes.waitUntilIndicesAreInitialized(RegisteredIndexes.java:109) at com.intellij.util.indexing.RegisteredIndexes.waitUntilAllIndicesAreInitialized(RegisteredIndexes.java:104) at com.intellij.openapi.progress.impl.CoreProgressManager.lambda$executeNonCancelableSection$3(CoreProgressManager.java:283) at com.intellij.openapi.progress.impl.CoreProgressManager.registerIndicatorAndRun(CoreProgressManager.java:749) at com.intellij.openapi.progress.impl.CoreProgressManager.computeUnderProgress(CoreProgressManager.java:705) at com.intellij.openapi.progress.impl.CoreProgressManager.lambda$computeInNonCancelableSection$4(CoreProgressManager.java:291) at com.intellij.openapi.progress.Cancellation.computeInNonCancelableSection(Cancellation.java:135) at com.intellij.openapi.progress.impl.CoreProgressManager.computeInNonCancelableSection(CoreProgressManager.java:291) at com.intellij.openapi.progress.impl.CoreProgressManager.executeNonCancelableSection(CoreProgressManager.java:282) at com.intellij.util.indexing.FileBasedIndexImpl.performShutdown(FileBasedIndexImpl.java:641) at com.intellij.util.indexing.FileBasedIndexImpl$ShutDownIndexesTask.run(FileBasedIndexImpl.java:1292) at com.intellij.util.indexing.FileBasedIndexDataInitialization$ShutdownTaskAsDisposable.dispose(FileBasedIndexDataInitialization.java:152) ```  GitOrigin-RevId: 31a7f83549104303cb9f90476b53a9ebb22a7d2c
JetBrains,intellij-community,341758ee1cdd8223512fe3b6c292c1fd78e00c1f,https://github.com/JetBrains/intellij-community/commit/341758ee1cdd8223512fe3b6c292c1fd78e00c1f,[java] Don't set protected visibility for final classes in CreateConstructorMatchingSuperFix  #IDEA-365573 Fixed  GitOrigin-RevId: a64e01c13e919b9992aecd556bf0a2963c848114
JetBrains,intellij-community,241efbd4719942cc85b40d88a7a5fc17aff58d83,https://github.com/JetBrains/intellij-community/commit/241efbd4719942cc85b40d88a7a5fc17aff58d83,add ACTION_PERFORM section  GitOrigin-RevId: 4b8d58a15c5b85d27efc2e4ce87626a19415c992
JetBrains,intellij-community,fcc1eda6ff1aa3774fdeac5e0ba1641932c49b14,https://github.com/JetBrains/intellij-community/commit/fcc1eda6ff1aa3774fdeac5e0ba1641932c49b14,IJPL-173321 Refactor ScopeEditorPanel: use selection paths directly  Using rows and then converting them to paths looks off by itself  as we don't use the row indices anywhere. Moreover  in the default Swing implementation  the getSelectionRows() function actually takes the selection paths and then takes some extra effort to convert them to rows  which we then convert back to paths. The performance impact may or may not be significant  but given that we can both get rid of it and simplify the code  it doesn't really matter.  GitOrigin-RevId: dab8e7df75e01eda099768bd9c4fbe2975e66d07
JetBrains,intellij-community,e9109364d70b3721cf9e6c9abe3342f9c369a70c,https://github.com/JetBrains/intellij-community/commit/e9109364d70b3721cf9e6c9abe3342f9c369a70c,IJPL-173671 Rename DialogWrapper.getValidationThreadToUse  Rationale: it's only used for continuous validation  if enabled. The final validation is always performed on the EDT.  There seem to be no external usages  so it looks like a safe change.  GitOrigin-RevId: 66e3d25bbee4ba3ac55516135fe3eb2c98f16231
JetBrains,intellij-community,b16ddb9ac1243354c1bbdf66bf77237d4ef0ad0f,https://github.com/JetBrains/intellij-community/commit/b16ddb9ac1243354c1bbdf66bf77237d4ef0ad0f,[slices] disable for selection  otherwise  it's confusing that action is performed based on caret position and not on selection  ^KTIJ-32633 fixed  GitOrigin-RevId: 34cd9306a8919cb4478f2da168815f8472733824
JetBrains,intellij-community,2f43f915a6dadd33057f7d337beffbf32e148fd2,https://github.com/JetBrains/intellij-community/commit/2f43f915a6dadd33057f7d337beffbf32e148fd2,[core] disable new `FilePageCache`  + `FilePageCacheLockFree` was developed to replace regular `FilePageCache` but that project was postponed for a long time in favor of using memory-mapped files for performance-critical storages. Current storages don't use `FilePageCacheLockFree`  hence it pays off disabling it and yield ~120Mb of native memory to regular `FilePageCache`  GitOrigin-RevId: ad6ceb85f891e6e737972f208c276582789cdb4f
JetBrains,intellij-community,7a256e2136e3874f2d84bffb8444db971ef7d1f3,https://github.com/JetBrains/intellij-community/commit/7a256e2136e3874f2d84bffb8444db971ef7d1f3,[java] CallMapper: optimize mapFirst for UAST  `isMethodNameOneOf` may significantly improve performance for some languages like Kotlin where `methodName` call can be expensive  ^IDEA-316635  GitOrigin-RevId: 33a016ebef048760576e2a41364b0de29341cc75
JetBrains,intellij-community,0a1f02f237462b630825bfb5fab696b5431448e0,https://github.com/JetBrains/intellij-community/commit/0a1f02f237462b630825bfb5fab696b5431448e0,revert converting to "computeIfAbsent" for critical code paths since they started to show up in perf snapshots  GitOrigin-RevId: aa9d0f29f2ceb7fd7f80441eba7f4e34f93f2490
JetBrains,intellij-community,6aedab6f4ccfb601b8fb5d0cfbdd078805aa6d21,https://github.com/JetBrains/intellij-community/commit/6aedab6f4ccfb601b8fb5d0cfbdd078805aa6d21,[performance] IJPL-173892 FUS: add project count  oom error flag and last action to low.memory events  GitOrigin-RevId: 048a9136700b3d9741cd21cb74260e9a8e62a16b
JetBrains,intellij-community,0cc2444ba268291fd625af1e71e6e81058950260,https://github.com/JetBrains/intellij-community/commit/0cc2444ba268291fd625af1e71e6e81058950260,[eel  jps] IJPL-172886: Disable preloading of build process for non-local JPS projects  With preloading enabled  incremental compilation in JPS does not work correctly. I did not research why  but here we can sacrifice possible performance for correctness and observability.  GitOrigin-RevId: ee27f1f40279813c5363fc19eca3c841d23e1cb9
JetBrains,intellij-community,f5a26b2622aa2954cfd518983ef75ddb0f6ac186,https://github.com/JetBrains/intellij-community/commit/f5a26b2622aa2954cfd518983ef75ddb0f6ac186,IJPL-173325 Create JCEF browser via builder with 'createImmediately' flag Wait for 'onLoadEnd' timeout  fix race due to which 'loadURL' request might be skipped if performed during browser initialization  GitOrigin-RevId: 388c6b230a8aa3e05764f3387dd71349655658c6
JetBrains,intellij-community,294c88168e0fc2cc3c386fe57cb4f76dc5c9653a,https://github.com/JetBrains/intellij-community/commit/294c88168e0fc2cc3c386fe57cb4f76dc5c9653a,IJPL-173405 [regression] Degradation in com.intellij.openapi.vfs.encoding.FileEncodingTest.testEncodingReDetectionRequestsOnDocumentChangeAreBatchedToImprovePerformance - encoding re-detect requests  GitOrigin-RevId: 94c465dc3726ee9eaca55399870343a74856de9e
JetBrains,intellij-community,50fa7b031d92f02c63721ce23f995fd48390d54a,https://github.com/JetBrains/intellij-community/commit/50fa7b031d92f02c63721ce23f995fd48390d54a,a bit of black magic to possibly help IJPL-173405 [regression] Degradation in com.intellij.openapi.vfs.encoding.FileEncodingTest.testEncodingReDetectionRequestsOnDocumentChangeAreBatchedToImprovePerformance - encoding re-detect requests  GitOrigin-RevId: 4c12e90c0fb31ae08c1605b48e74ca418f85a617
JetBrains,intellij-community,4fba5ac5007e1b9a3f3819408a68606f1e6b2d20,https://github.com/JetBrains/intellij-community/commit/4fba5ac5007e1b9a3f3819408a68606f1e6b2d20,mark naive recursive psi element visitors as such  to be able to catch more performance errors  GitOrigin-RevId: 3c28a6d1061d6d861dcc78e55b1786742c190d27
JetBrains,intellij-community,2ca1ff02f9feb5d52cbfe1b3a0199aec812d984f,https://github.com/JetBrains/intellij-community/commit/2ca1ff02f9feb5d52cbfe1b3a0199aec812d984f,WI-77238 Replace PhpPathMappingsTreeTable with inheritance from PerFileConfigurableBase  GitOrigin-RevId: 202673134bbaefcc15a80a9ad1c5f5841f6c5d5d
JetBrains,intellij-community,72adf36a7f3aed5ca4f7d81506c962db64dff941,https://github.com/JetBrains/intellij-community/commit/72adf36a7f3aed5ca4f7d81506c962db64dff941,[json] IJPL-172038 Prefer simple thread local cache instead of ReadActionCache to reduce FUS collection performance impact  GitOrigin-RevId: 0788ef02d6384176e0ed6fec8037c3f63b656fb9
JetBrains,intellij-community,462b20c17109b4eb9fc0ea702c2edaecde896404,https://github.com/JetBrains/intellij-community/commit/462b20c17109b4eb9fc0ea702c2edaecde896404,PY-72070 Introduced PythonPackagesInstallerAsync for performing package installations asynchronously using coroutines. Updated the existing PyPackageManagerUI to utilize this new asynchronous method.  GitOrigin-RevId: 80c4c17b9ad7a209be747b10dc5bdbd94cff6b9b
JetBrains,intellij-community,29ae3496e488a6e6c81240c0399815b407e2a8a1,https://github.com/JetBrains/intellij-community/commit/29ae3496e488a6e6c81240c0399815b407e2a8a1,[devkit] remove superfluous suppressions for IntentionDescriptionNotFoundInspection (IJPL-166110)  GitOrigin-RevId: 4faddda64ae65ab6abcd2bde8130ae829957998c
JetBrains,intellij-community,8eb8b59cfd7e4bda78fa37bff0d42bb9f6817fc7,https://github.com/JetBrains/intellij-community/commit/8eb8b59cfd7e4bda78fa37bff0d42bb9f6817fc7,[platform] migrating `EarFacetConfigurator#performAsync` to NIO and deprecating `VfsUtil#toUri(File)` in favor of stdlib  GitOrigin-RevId: 547975a2914fdf7b5352306407e8db06b4a2f3a3
JetBrains,intellij-community,f17ecdb496aa714debc17e48d760c570d06c1cd7,https://github.com/JetBrains/intellij-community/commit/f17ecdb496aa714debc17e48d760c570d06c1cd7,do not try to reload from disk/reparse documents in tests in tearDown  to fix assertions/perf  GitOrigin-RevId: ed21c0972aacc9aa55f120eb8c8ac4ac775e2f20
JetBrains,intellij-community,07a0ec63006afc7cebd6c96083b1a36603c72ef4,https://github.com/JetBrains/intellij-community/commit/07a0ec63006afc7cebd6c96083b1a36603c72ef4,[json + yaml] IJPL-163460 Implement fast exit for yaml deprecation inspection  There is no point in traversing schema if there are no deprecated nodes specified in it. So we first index the in-memory schema instance and perform fast exit in case no deprecation keywords were detected  GitOrigin-RevId: decd0a968fa3e04326b4fdeec3cdab01563439fb
JetBrains,intellij-community,92a193881f5f0c7ced2bc3cc35cbc401ae7eeb9b,https://github.com/JetBrains/intellij-community/commit/92a193881f5f0c7ced2bc3cc35cbc401ae7eeb9b,performance: do not mess with slow regexes in debug log  GitOrigin-RevId: 398ed69ca0154a798bbcd2d8b501a17f97eebc86
JetBrains,intellij-community,e0d0c10a4db908bd993c183ecd99e1ad9e2cb4e8,https://github.com/JetBrains/intellij-community/commit/e0d0c10a4db908bd993c183ecd99e1ad9e2cb4e8,add ACTION_PERFORM section  GitOrigin-RevId: 2e77a9d66f2acdeb68acd4dc1bcd4db8714eed62
JetBrains,intellij-community,d5fdc5e79ed39e5d7ba46006a47b7152551a071c,https://github.com/JetBrains/intellij-community/commit/d5fdc5e79ed39e5d7ba46006a47b7152551a071c,make `actionPerformed` "override-only"  GitOrigin-RevId: ec0778e95bb7446185f68b9904892f7c7d193b5b
JetBrains,intellij-community,2b2ca8722501811365ecaf410fbae5ee74922193,https://github.com/JetBrains/intellij-community/commit/2b2ca8722501811365ecaf410fbae5ee74922193,[platform] Chasing flaky project leak through PerFileElementTypeStubModificationTracker and StubIndexImpl IDEA-360211  GitOrigin-RevId: b3dd7cb4f8dd595dba4c1f9e0c9b543117200d66
JetBrains,intellij-community,37d58601b9a3d125a2eb88a1aedf237850f8ba74,https://github.com/JetBrains/intellij-community/commit/37d58601b9a3d125a2eb88a1aedf237850f8ba74,[performance] IJPL-161370 Detect UI freezes caused by third-party plugins and disable them  GitOrigin-RevId: 57013d91d5767602b1cb6db72d644e83bed2339b
JetBrains,intellij-community,c87e36af30c1aa6773e261e447e743ccc275b3ee,https://github.com/JetBrains/intellij-community/commit/c87e36af30c1aa6773e261e447e743ccc275b3ee,do not throw PCE from toString() to avoid superfluous test failures  GitOrigin-RevId: 9ed8e761347d4f33c33bf9f1b1f686c726a9ac42
JetBrains,intellij-community,cbff17a5a3c1ea10381dc0051374120e728d9c20,https://github.com/JetBrains/intellij-community/commit/cbff17a5a3c1ea10381dc0051374120e728d9c20,[ui] Update UI in PerFileConfigurableBase  #IJPL-163682 Fixed  GitOrigin-RevId: a6e9cd10f59be52a60e72a06377e102cb7c7cb51
JetBrains,intellij-community,e2f88c73c280097aead4088efab63cd915685500,https://github.com/JetBrains/intellij-community/commit/e2f88c73c280097aead4088efab63cd915685500,[javadoc] perform automatic supertype search recursively for interfaces  #IDEA-358073  GitOrigin-RevId: efeda57e732374963dd97dbe32c8fc1d938c5916
JetBrains,intellij-community,27665880bb8208e0f02778c8e045413accb27e73,https://github.com/JetBrains/intellij-community/commit/27665880bb8208e0f02778c8e045413accb27e73,[indexes][cleanup] remove unused lock in ServerStubIndex  + use of `UpdatableIndex.getLock()` in `ServerStubIndex` is superficial -> remove it  GitOrigin-RevId: 7f6eb2c1c854a14d75732b10bdc4af60fdf6ffd1
JetBrains,intellij-community,9765a5c4ceb875b02d72c90b405e958b96dbd01f,https://github.com/JetBrains/intellij-community/commit/9765a5c4ceb875b02d72c90b405e958b96dbd01f,[performance] A lot of memory allocated from InspectionVisitorOptimizer.getTargetPsiClasses for lambdas  GitOrigin-RevId: a7f330d3fd1633ee1d17040ba114a0daf4a42eb1
JetBrains,intellij-community,92996cecaabb27ca4ca96a2a1dfde34a38e87293,https://github.com/JetBrains/intellij-community/commit/92996cecaabb27ca4ca96a2a1dfde34a38e87293,several optimizations for speeding up getFileType() (part of IJPL-162944 speedup find usages in kotlin files):  - Do not query FileTypeOverrider extension point in hot path; store in fileTypeOverriderCache instead - Store FileTypeManager instance in FileTypeRegistry field to avoid indirection in hot path - Use fields to store some services queried in hot path - perform modifications under myPendingInitializationLock.writeLock() to avoid corruption  GitOrigin-RevId: 9a6ec245f31a779542e2e830089573e8ce25333b
JetBrains,intellij-community,31ec0fab5bf50d97e4c30132497f80fa023f55c3,https://github.com/JetBrains/intellij-community/commit/31ec0fab5bf50d97e4c30132497f80fa023f55c3,[kotlin] Implement a combinable scope for source and class roots as a replacement for `ModuleWithDependenciesScope`s and to combine library scopes  - Uncombined (unions of) `ModuleWithDependenciesScope` and `LibraryWithoutSourceScope` are heavily inefficient because the `contains` function requires getting the virtual file's file info for each `contains` call. When we have uncombined scopes  the file info may be requested hundreds of times. A combined scope allows getting the file info only once  and then check the roots. - The new `ModuleSourcesScope` and the existing `LibraryWithoutSourceScope` are easily combinable  since we just have to create a combined roots map. There is no other complex magic going on to construct an efficient combined scope. - Combined scopes have a large positive impact on performance in various test cases where we have a complex module structure with many dependencies. In fact  in some of the test cases we cannot feasibly optimize anything else from snapshots since uncombined scopes are such a huge drag on performance. - Changing `ModuleWithDependenciesScope` and the default scope provided by `Module.moduleProductionSourceScope` and `moduleTestSourceScope` in the platform is difficult  because there are internal and external usages of `ModuleWithDependenciesScope`. It is supposed to be an implementation detail  but various usages cast `GlobalSearchScope`s to `ModuleWithDependenciesScope`s to get the module from the scope. Hence  it is currently much easier to make a change limited to the Kotlin plugin and reap the performance benefits now. - This has the disadvantage that we'll have duplicate implementations for `AbstractVirtualFileRootsScope` and `ModuleWithDependenciesScope`  so we should still push for integration into the platform.  ^KT-57733  GitOrigin-RevId: 0ca6b15f78803af9a47fd229b9b650368f95ca87
JetBrains,intellij-community,4819a59dee826ab50b6ecea4c85d9b0d6d774d87,https://github.com/JetBrains/intellij-community/commit/4819a59dee826ab50b6ecea4c85d9b0d6d774d87,[performance] IJPL-162450 FUS: provide low memory condition event  current size of heap and memory type  GitOrigin-RevId: aa6027c0e5651114edfa86b105af9e00bb1fbeb1
JetBrains,intellij-community,d6b678da1f717450d5b3caa2d16772b0674ed44a,https://github.com/JetBrains/intellij-community/commit/d6b678da1f717450d5b3caa2d16772b0674ed44a,tests stability: do not perform random idempotence check in stress tests  GitOrigin-RevId: 0c45d6d9f90020e0c062b62f09f75ced7311aec5
JetBrains,intellij-community,007db6c60ecb2fec90c270108fdd11116689d9db,https://github.com/JetBrains/intellij-community/commit/007db6c60ecb2fec90c270108fdd11116689d9db,RelaxNg: Avoid NPEs to improve performance when calculating element descriptors  GitOrigin-RevId: 9260e29679da728aeba7119accf2214b5dcfd460
JetBrains,intellij-community,846941b57136ee6264efe2ac325a8cc35b85efd4,https://github.com/JetBrains/intellij-community/commit/846941b57136ee6264efe2ac325a8cc35b85efd4,[matcher] fallback typo-tolerant matcher to regular matcher for long patterns  After some optimizations  for the sake of performance  a matcher stopped matching anything for long patterns. Keeping this optimization  now we fall back to the regular matcher to match at least something  even without tolerance to typos  GitOrigin-RevId: 8c3be92bccd5eb6e382df32394cc27f95113e3e4
JetBrains,intellij-community,96090e5b02f149caf312e988f52a2b4e20501c45,https://github.com/JetBrains/intellij-community/commit/96090e5b02f149caf312e988f52a2b4e20501c45,[devkit] adjust "Must override ActionGroup.canBePerformed [...]" inspection (IJPL-116447)  GitOrigin-RevId: 02077deb11ed9d0a63dcb064192ed16f1dcff1dd
JetBrains,intellij-community,858791cf180e0536e6749c7ad171ee96b8022eb4,https://github.com/JetBrains/intellij-community/commit/858791cf180e0536e6749c7ad171ee96b8022eb4,IDEA-354490 Performance improvements  GitOrigin-RevId: 30221b63ba9b6b8780e0d9451266c0cc032abf4b
JetBrains,intellij-community,2179c51ce683e944801a291a2a188750fa4b2a55,https://github.com/JetBrains/intellij-community/commit/2179c51ce683e944801a291a2a188750fa4b2a55,IJPL-74471 FileTreeModel: make sure 'on change' events do not load unloaded nodes  This makes sure we do not load into VFS parts of FS that are better be left unknown: "/proc/"  STR: (The issue is no longer reproducible after the parallel fix in 60c7c311caeb7c2034fd84d1a9534f50c3029eb6) * Use Linux * Add "-Dfile.system.trace.loading=/proc/1/fd" * Invalidate VFS caches * Use "File | Open" to open the 'old' file chooser * Restart IDE * Use "File | Open" * Observe the file being loaded // BUG * Some time in the future IDE may start freezing while processing VFS events // induced issue  The full event sequence: * The 'Open File' tree is opened  "/" root is always visible * canOpenGradleProject performs 'getChildren()' call on all visible nodes  including "/proc/" * VFS loads "/proc/*" folders into VFS * IDE is restarted * 'Open File' tree is opened again * Initial VFS scan walks all known folders and notices that "/proc/3" was deleted * It fires an VFileDeleted event * FileTreeModel receives the event and loads "/proc/*" nodes into the tree (as if user has expanded then explicitly) * canOpenGradleProject performs 'getChildren()' call on all "/proc/*/*" files  loading them into VFS * Observe the file being loaded // BUG * Later  one of the files gets a VFS event fired * PerModulePackageCacheService performs 'VF.getFileType' on "/proc/3/fd" * FileTypeDetectionService detection is trying to load first 1024 bytes from the file * The read goes through  even though file is a pipe: // BUG  reason unknown The guard fails: "if (SystemInfo.isUnix && file.is(VFileProperty.SPECIAL)) { // avoid opening FIFO files" * The read on a file pipe hangs indefinitely (ex: it was observed reading IDE's own stderr output)  GitOrigin-RevId: 498465fc415985d7754bdea654d32a112e55883f
JetBrains,intellij-community,651151bf6829c889400f09385eb675a2e4f7dd9f,https://github.com/JetBrains/intellij-community/commit/651151bf6829c889400f09385eb675a2e4f7dd9f,Add canceled/applied data for IJPL-55674 Performance metrics: Search Everywhere opening  GitOrigin-RevId: a550694f4c066c09c9ee22f9c511d388a885a8d4
JetBrains,intellij-community,e3bc6676d6a0b20e75b1447fc13ad78f54337040,https://github.com/JetBrains/intellij-community/commit/e3bc6676d6a0b20e75b1447fc13ad78f54337040,Time from the very start till the first item for IJPL-55674 Performance metrics: Search Everywhere opening  GitOrigin-RevId: 45bdbdf055f6814c581ed4134c0ffca5333b77ac
JetBrains,intellij-community,3a2a6caf80d8efabdc5cf5451e236a6fca2f0a01,https://github.com/JetBrains/intellij-community/commit/3a2a6caf80d8efabdc5cf5451e236a6fca2f0a01,Time from the very start till popup disposal for IJPL-55674 Performance metrics: Search Everywhere opening  GitOrigin-RevId: b1aa6bdbc21c8979b2e126438aa5b0525f3476bd
JetBrains,intellij-community,a818ae54ce057d486e900c2a7c3ab8b805461e69,https://github.com/JetBrains/intellij-community/commit/a818ae54ce057d486e900c2a7c3ab8b805461e69,IJPL-161292 IJent: MultiRoutingFsPath extends sun.nio.fs.BasicFileAttributesHolder  This interface is used in VFS refreshing  allowing to avoid some stat system calls. Paths from the default file system implement this interface. When MultiRoutingFileSystem became the default nio filesystem  paths lose this interface  causing a severe performance degradation.  GitOrigin-RevId: 0c53854c1bcfa1461d96fa0e8da16f1650f1e471
JetBrains,intellij-community,286567da080239da842a5a6748e4db0be4e83569,https://github.com/JetBrains/intellij-community/commit/286567da080239da842a5a6748e4db0be4e83569,fix(JavaDoc): Collapsed markdown comments with wrong suffixes  Not perfect at all  it should be able to rely on the commenter API instead.  GitOrigin-RevId: f41d181e9f27905bca1156912b7850f5a96ba943
JetBrains,intellij-community,2039673d1b82a5372bc59ee05c5e322b96add41e,https://github.com/JetBrains/intellij-community/commit/2039673d1b82a5372bc59ee05c5e322b96add41e,[performanceTests] Add metric for afterShown method  For VCS widget the method takes about 1-3 seconds and it makes sense to track it as well  GitOrigin-RevId: d2707c09c2fa96db2f2870a36d14997706357875
JetBrains,intellij-community,5abb2bda0d2ea8d1931dd0d024b2a91e1526047a,https://github.com/JetBrains/intellij-community/commit/5abb2bda0d2ea8d1931dd0d024b2a91e1526047a,[java-analysis] VariableAccessUtils.getVariableReferences: do not query LocalRefUseInfo for non-physical files  Non-physical copies are usually created for a single purpose (completion  or a single ModCommand quick-fix to apply)  so computing all the references is likely not useful and only creates performance overhead. Should fix IDEA-357624 Slow code completion in large Java file in lines being assigned to "final var"  GitOrigin-RevId: c942fb2a91bd3bcbd4cb67d48f2801e256ab64a1
JetBrains,intellij-community,368a0df14cba11bd3978f00a704bbc16a824cf91,https://github.com/JetBrains/intellij-community/commit/368a0df14cba11bd3978f00a704bbc16a824cf91,PY-72690 Slow code analysis for Python code using many TypedDict  Performance snapshot shows that an enormous amount of time is spent calculating hashcodes for types.  GitOrigin-RevId: 47d488ada253c1aa78ae247d45af50cf4a553426
JetBrains,intellij-community,e555a9d95a7bb338e52f6e77b888e12acc72d285,https://github.com/JetBrains/intellij-community/commit/e555a9d95a7bb338e52f6e77b888e12acc72d285,[terminal] report terminal startup performance/responsiveness metrics (IJPL-159892)  GitOrigin-RevId: f4699f7023655a5a2d1e09ad40a9fb43cd0f726e
JetBrains,intellij-community,3d3e4a9dab67ced3a632105bc3e957328ccf2b16,https://github.com/JetBrains/intellij-community/commit/3d3e4a9dab67ced3a632105bc3e957328ccf2b16,IJPL-160678 report "Too many element types registered" only once per IDE run  and perform reporting on BGT  GitOrigin-RevId: 3d32406f9560df9f24b83a46adbee8c67688018b
JetBrains,intellij-community,f78ebb663a76663e3c2579bf17b8dcfc93c772bb,https://github.com/JetBrains/intellij-community/commit/f78ebb663a76663e3c2579bf17b8dcfc93c772bb,Java: Show confirmation dialog on Undo Paste Class performing (IDEA-146073)  GitOrigin-RevId: 4194929bf3b9561fe3b7d184421e5386c244cbf1
JetBrains,intellij-community,341aea199155817c707cc32c27c5adc6037db16e,https://github.com/JetBrains/intellij-community/commit/341aea199155817c707cc32c27c5adc6037db16e,[Kotlin  Java] fix performance in completion by disabling Java-specific `com.intellij.codeInsight.completion.DeprecatedSkipper` in non-Java languages  ^KTIJ-31014 fixed   (cherry picked from commit 5ae85bc4f0f7b27965595605bcf5d0fcb06c996d)  IJ-MR-142759  GitOrigin-RevId: fa8733e26b0d43bd0d9c6735da8ac110e5e2765e
JetBrains,intellij-community,6d4a5323c50905b4abd95472b6149787b7dc5450,https://github.com/JetBrains/intellij-community/commit/6d4a5323c50905b4abd95472b6149787b7dc5450,add ACTION_PERFORM slow-op section 2  Fixes RemDev reports like 28064078  GitOrigin-RevId: 1d702c06823bee048fc6efddaf784d55d4c0c782
JetBrains,intellij-community,0a5b243b2843eb472829bb1f95c97d8bd122243a,https://github.com/JetBrains/intellij-community/commit/0a5b243b2843eb472829bb1f95c97d8bd122243a,IJPL-159657 Fix performance issue with UrlFilter  After b77168e2 (related to KTIJ-29334)  Url filter started using url filter each time  even if file url is present. That makes the performance test of this filter with only file urls slower. Here I've added some checks before using an expensive URL filter  ^IJPL-159657 Fixed  GitOrigin-RevId: 30651ed33375244feb9e34173c6162387241360c
JetBrains,intellij-community,4953b7542bfbf431c3f3666f8ea12007e225d69a,https://github.com/JetBrains/intellij-community/commit/4953b7542bfbf431c3f3666f8ea12007e225d69a,[log] IJPL-159496 Fix right-click handled as left-click when clicking graph in log  Left-click action was performed along with showing the context menu  GitOrigin-RevId: 76f14cb3a4daa0427b2174b7c4ea81e0207949fe
JetBrains,intellij-community,826ef9e8bf52878908c7455de6dba36a12dd7e26,https://github.com/JetBrains/intellij-community/commit/826ef9e8bf52878908c7455de6dba36a12dd7e26,avoid highlighting the same variable twice  for correctness and performance  GitOrigin-RevId: 6fefd372be01af9f89be1ca35020afd23b943cd8
JetBrains,intellij-community,cc26805ea1d6235e921f7e568bf3f11b319eb211,https://github.com/JetBrains/intellij-community/commit/cc26805ea1d6235e921f7e568bf3f11b319eb211,CWM-9331: count leaves again in perform if they aren't in presentation  GitOrigin-RevId: 75df684db663372dd2316c864fe1f222dae0a34e
JetBrains,intellij-community,58bcc6564da78c08882447846ccd36b90ffdc3f9,https://github.com/JetBrains/intellij-community/commit/58bcc6564da78c08882447846ccd36b90ffdc3f9,[performanceTests] Don't close SE on focus lost in perf tests  Otherwise  the command doesn't function properly since the following happens: 1. We open SE  type something  select first file 2. We open SE again but in the meantime file is opened and takes the focus  we close SE  This case can be fixed by subscription to FileEditorManagerListener but the case when we select already opened file can't. Since in this case  there is no listener or callback that will indicate that the focus is moved to the editor.  GitOrigin-RevId: 0e7959443fcd823ff9ddf10b121fa38e257f38cf
JetBrains,intellij-community,d458e5c2da20dc0fc9126507ce2178d3f1a8d607,https://github.com/JetBrains/intellij-community/commit/d458e5c2da20dc0fc9126507ce2178d3f1a8d607,[watcher]Added ability to create span in PerformanceWatcher  GitOrigin-RevId: 6e8471045d166c54eb76ce0b4ad4c528a2b29c0e
JetBrains,intellij-community,42a6d13ae143190949ff9de1cd96a442f09e951f,https://github.com/JetBrains/intellij-community/commit/42a6d13ae143190949ff9de1cd96a442f09e951f,[fus] IJPL-158835 FUS collector checks IO file content on every AnAction performed  GitOrigin-RevId: 0f7df386341f88ba9c36e9751b5a60251706162b
JetBrains,intellij-community,258d39252e13969ba2c514ffb7e5e367e7c449bd,https://github.com/JetBrains/intellij-community/commit/258d39252e13969ba2c514ffb7e5e367e7c449bd,[debugger] Force minimizing coroutines view at the start of the debug session  Now it has significant performance problems and low user value.  GitOrigin-RevId: 3d8c351ad84b16f76ef37ef0f2cd20db86cc60ac
JetBrains,intellij-community,cdf91bf213fd1012cb40626db22f47cb01cb145c,https://github.com/JetBrains/intellij-community/commit/cdf91bf213fd1012cb40626db22f47cb01cb145c,[benchmarks] Renaming PerformanceTest* => Benchmark*  GitOrigin-RevId: 9963b84d51e1062acc262a8d3d3de1409a708e3b
JetBrains,intellij-community,6bc9c0ed292ec0c07f2ab50c146cb7a1e227d098,https://github.com/JetBrains/intellij-community/commit/6bc9c0ed292ec0c07f2ab50c146cb7a1e227d098,Revert "PY-74049 Improve EditorEmbeddedComponentManager to increase perfomance of notebooks"  This reverts commit bf54b00845ce4126bbcceb9dfba79753d26f028f.  GitOrigin-RevId: 3802fef0ac6764e9995ef04cdc24bbb04630e69f
JetBrains,intellij-community,f0d0cfc7820d3d9e51b9fcacf9e7c6d20990cd0c,https://github.com/JetBrains/intellij-community/commit/f0d0cfc7820d3d9e51b9fcacf9e7c6d20990cd0c,IJPL-158803 use the last selected language until restart will be performed  GitOrigin-RevId: 3260dc2acaa9e95b6fce585e17030829c6013106
JetBrains,intellij-community,f3605d50facac4676102ae8d507a566b815173eb,https://github.com/JetBrains/intellij-community/commit/f3605d50facac4676102ae8d507a566b815173eb,PY-74049 Improve EditorEmbeddedComponentManager to increase perfomance of notebooks  GitOrigin-RevId: bf54b00845ce4126bbcceb9dfba79753d26f028f
JetBrains,intellij-community,f67e7eb60cdc318f826c395b84db3caa84f93efd,https://github.com/JetBrains/intellij-community/commit/f67e7eb60cdc318f826c395b84db3caa84f93efd,IJPL-18530 PerFileElementTypeStubModificationTracker: do not process updates on project close  GitOrigin-RevId: c5e0918bc3d5c0f66d28acd87772f76a02516dde
JetBrains,intellij-community,5853af89b570dca1f632c3d4d347de10aede8be9,https://github.com/JetBrains/intellij-community/commit/5853af89b570dca1f632c3d4d347de10aede8be9,[services] explicitly set perfom group for add service action group  GitOrigin-RevId: 76a247979fa4576c7d0726a554587dc538339bf2
JetBrains,intellij-community,1aaaf777817ff764bbd941df776069482d3ea712,https://github.com/JetBrains/intellij-community/commit/1aaaf777817ff764bbd941df776069482d3ea712,[pycharm] restrict analysis in order to improve completion performance  GitOrigin-RevId: 1c2427d1dbb07d88672347311ec5d6f362881847
JetBrains,intellij-community,ee26890170d42fb245091c72d8df48b077c18c64,https://github.com/JetBrains/intellij-community/commit/ee26890170d42fb245091c72d8df48b077c18c64,todo: support searching for todos inside a range  to improve performance (part of IJPL-28717 Todo line lose coloring)  GitOrigin-RevId: e3d44ee80f342985497740fffbe7a76eae5c747e
JetBrains,intellij-community,23fb60afd8efcb968c2e6d6ab5e97c5aec973cc7,https://github.com/JetBrains/intellij-community/commit/23fb60afd8efcb968c2e6d6ab5e97c5aec973cc7,[json] IJPL-63554 Implemented fast exit for json schema validators  - If requested  validation will stop as soon as any error is found. This is extremelly important performance optimisation that plays well with the recenty introduced if-else branch computation. The number of calls to JsonSchemaResolver.isCorrect() increased dramatically  even more json-schema subsystem refactoring was demanded.  The existing API didn't assume any kind of laziness or cancellability. The refactoring is performed in a way to cause minimal number of changes in code and API. It'd be great to rewrite the entire validation code to sequence/analogs once and drop complicated JsonAnnotationsCollectionMode  GitOrigin-RevId: 4e62f7db76ed6b4071accbe1b80151c4b4664342
JetBrains,intellij-community,2d5af22db1fd5ef2dfca99acca88bcb3fdbeb004,https://github.com/JetBrains/intellij-community/commit/2d5af22db1fd5ef2dfca99acca88bcb3fdbeb004,IJPL-77608 svn: remove the hack with a different DataKey used in 'actionPerformed'  It no longer works  as all the keys are being pre-cached by PreCachedDataContext.  GitOrigin-RevId: f013d69bfb248b9a0a7001a70ebf1db5c6343fed
JetBrains,intellij-community,16f513996c01242c7e748796937547d92b385bdf,https://github.com/JetBrains/intellij-community/commit/16f513996c01242c7e748796937547d92b385bdf,[perf-tests] do not print found during compilation warnings  GitOrigin-RevId: 4eaaa7561cdc5a197ef88dad86ec9ea421984471
JetBrains,intellij-community,13f124e4969d1402f5aaf58e4bd70875d0cfb55c,https://github.com/JetBrains/intellij-community/commit/13f124e4969d1402f5aaf58e4bd70875d0cfb55c,[perf_test]Fixed API checker  GitOrigin-RevId: 8a000bdd3ea1b6c395f91d16e1a01676d30a2a95
JetBrains,intellij-community,385c069cbbf7308dce8bc94fddad20634eb39b94,https://github.com/JetBrains/intellij-community/commit/385c069cbbf7308dce8bc94fddad20634eb39b94,[perf_test]Updated StartInlineRenameCommand to be closer to user behavior  GitOrigin-RevId: 89f365c10c5eb737ea9c52dbefac916bc4b6b26e
JetBrains,intellij-community,149838abea9cf773e37bc3daefdbc6040dea9739,https://github.com/JetBrains/intellij-community/commit/149838abea9cf773e37bc3daefdbc6040dea9739,[perf_test_kotlin]AT-695. Implemented move kotlin files test  GitOrigin-RevId: 318c1046a6d66fe0dd851435e3e75f855dc7587c
JetBrains,intellij-community,c8918f228601b1d337b50bcefb78de214a11e4e1,https://github.com/JetBrains/intellij-community/commit/c8918f228601b1d337b50bcefb78de214a11e4e1,introduce `KeepPopupOnPerform`: drop old multi-choice methods  See IJPL-157628 Introduce Keep popups open for toggle items setting  GitOrigin-RevId: c61ca430c3531c862212d215e764b8ce7979213a
JetBrains,intellij-community,47b112785ed21eb239ae8fb95f20f60b787bc258,https://github.com/JetBrains/intellij-community/commit/47b112785ed21eb239ae8fb95f20f60b787bc258,introduce `KeepPopupOnPerform`: migrate old usages  See IJPL-157628 Introduce Keep popups open for toggle items setting  GitOrigin-RevId: ad10b913a2aac3d7dc32fcdac4c3d3d9277c23fb
JetBrains,intellij-community,98e7545cb88e21bd8772cd8b90284fd9f0514082,https://github.com/JetBrains/intellij-community/commit/98e7545cb88e21bd8772cd8b90284fd9f0514082,introduce `KeepPopupOnPerform` to replace `Presentation.isMultiChoice`  A 4-value enum now includes previous "hard" and "soft" semantics without special treatment for `ToggleAction`.  See IJPL-157628 Introduce Keep popups open for toggle items setting  GitOrigin-RevId: 123935b9367dd624b4c810b0dfa066fba23e1672
JetBrains,intellij-community,1e4b60370cbe3deba7d7d20c475e77fe3cf768c2,https://github.com/JetBrains/intellij-community/commit/1e4b60370cbe3deba7d7d20c475e77fe3cf768c2,IJPL-157491 Use the new bulk expand API for TreeUtil.restoreExpandedPaths  The main reason for this change is that the new API actually checks for every path whether it's a leaf. This prevents a bug when we first mark a path as expanded and then later we add it  so it shows as collapsed (because it's newly added)  but can't be expanded either because the tree thinks it's expanded. An example of this is the filtering tree  which tries to re-expand every expanded path after refiltering  even those paths that don't exist because of that refiltering.  A nice side effect is a considerable performance improvement in case there are many expanded paths.  GitOrigin-RevId: a0c41c33ab374e75cb385c72a796abe917e3fdad
JetBrains,intellij-community,52850e21d86aacd01bb1406106af780ad4b043c8,https://github.com/JetBrains/intellij-community/commit/52850e21d86aacd01bb1406106af780ad4b043c8,PY-62208 Include importable names in basic completion results  Previously  such names were visible only on so-called "extended" completion  activated when the hotkey for the basic completion was hit twice. The main reason was that collecting such variants from indexes was a slow process  and we didn't want to harm the responsiveness of completion for basic names. Now it becomes possible thanks to a number of performance optimizations:  * Instead of using three separate indexes for classes  functions and variables  we use one -- PyExportedModuleAttributeIndex. By definition  it includes only top-level "importable" names  so we additionally save time by not filtering out irrelevant entries. Also  it doesn't contain private definitions starting with an underscore. It might bother some users  but given that the previous completion was used extremely rarely  and the new one is going to be visible everywhere  it seems that pruning unlikely entries as much as possible is a fare tradeoff. In the future  we might enable them back on the "extended" completion if there is a demand. Also  this index binds its keys to the project (`traceKeyHashToVirtualFileMapping`)  further eliminating useless index lookups.  * Thanks to the recent fixes in the platform (IJPL-265)  it's now possible to simultaneously iterate over all keys in an index and request values for a given key without deadlocks  which is much faster than eagerly fetching all keys first.  * While scanning through all matching entries from indexes  we terminate the lookup if the number of items exceeds the size of the lookup list. We can further reduce this number by adjusting the "ide.completion.variant.limit" registry value.  * Calculating expensive "canonical" import paths (e.g. "pkg.private.Name" is importable as "pkg.Name") is offloaded to a background thread thanks to the `withExpensiveRenderer` API. We still calculate these paths synchronously  though  for names whose raw qualified names contain components starting with an underscore to decide whether these private names are publicly re-exported and  hence  should be displayed.  The rest of the work has been put into reducing the number of entries on the list  e.g.  * The prefix under caret is now matched from the beginning of a name  e.g. `Bar<caret>` matches `BarBaz`  but not `FooBar`. * We don't suggest imported names clashing with those already available in scope. * Some kinds of definitions are not suggested in specific contexts  e.g. functions and variables are not suggested inside patterns and type hints. * Nothing is suggested at the top-level of a class body  where dangling reference expressions or calls are not normally expected.  Additionally  we don't suggest names from .pyi stubs at the moment  because it pollutes the suggestion list with entries coming from the stubs for third-party packages in Typeshed. We should probably enable them back once we are able to properly disable Typeshed entries for not installed packages.  Some legacy forms of completion are left in the extended mode. In particular  qualified names of classes are offered inside string literals only in this mode. Also  module and package names are suggested only in the extended mode  because top-level packages and modules are already suggested for the basic completion by PyModuleNameCompletionContributor.  A few tests in PyClassNameCompletionTest were updated or removed entirely because * we no longer suggest private names * we no longer suggest names from private modules not re-exported in a public module * we no longer suggest names clashing with those already available in scope * prefix matching policy was changed to start at the beginning of an identifier  The whole feature can be disabled with the option "Suggest importable classes  functions and variables in basic completion" in settings.  GitOrigin-RevId: 0787d42ce337b73b01a60f0bb7aa434fee43e659
JetBrains,intellij-community,ccbae54f399f51ba31738b98e056a88ab2f26223,https://github.com/JetBrains/intellij-community/commit/ccbae54f399f51ba31738b98e056a88ab2f26223,EA-1319508 - NPE: UndoAction.perform  GitOrigin-RevId: 61b74f83c9117c30bec4a5db9cf3755b4657a551
JetBrains,intellij-community,c733f9149e19c01200785fc5e25e7f7cecdfe2ec,https://github.com/JetBrains/intellij-community/commit/c733f9149e19c01200785fc5e25e7f7cecdfe2ec,add missing `ActionWrapperUtil.actionPerformed` call  GitOrigin-RevId: c26565fa1111bfcfab51900c3fa9d1e5918ffc76
JetBrains,intellij-community,ffd202ba96b1864407ee27aa07eae57791cb2696,https://github.com/JetBrains/intellij-community/commit/ffd202ba96b1864407ee27aa07eae57791cb2696,IJPL-157271: performance experiment: avoid `synchronized` in c.i.o.fileTypes.WildcardFileNameMatcher.RegexpMatcher  This will produce more very short-living garbage which looks harmless. Matcher class has about 20 fields inside. Keep aside that some of them are arrays  Matcher probably occupies 100-150 bytes in the memory. I.e. there will be 150-225MB of garbage for mid-size project like idea ultimate (~1.5M files). Looks acceptable  because preliminary experiments showed that this will decrease 2nd scanning by ~10% (10sec -> 9sec) because of better parallelization when indexing on 19 threads. More parallel environments (like dev pods) should probably show even better parallelization.  GitOrigin-RevId: 450ccf3105d31be2703e4da859a59f1ee6351b8e
JetBrains,intellij-community,99fbe25af3718179e1c99454739746184036f7d0,https://github.com/JetBrains/intellij-community/commit/99fbe25af3718179e1c99454739746184036f7d0,remove superfluous constants which also lead to proxy selector being accessed from a static initializer  GitOrigin-RevId: a80bb97292218c2169eaec2b438538d3be3ebcd5
JetBrains,intellij-community,fa7b6baf9d525476d583c49d0ad18a7ea9419fb2,https://github.com/JetBrains/intellij-community/commit/fa7b6baf9d525476d583c49d0ad18a7ea9419fb2,IJPL-155974 searchable options without words - perform stemming in runtime  GitOrigin-RevId: 8524eaf1f8d5fe70943a8684246015fd90f5292b
JetBrains,intellij-community,b6816ded5e97850f758ce6cc4d0e8a5487e71f32,https://github.com/JetBrains/intellij-community/commit/b6816ded5e97850f758ce6cc4d0e8a5487e71f32,deprecate `beforeActionPerformedUpdate`  The method has not been used since 241. Also  tweak some javadocs.  GitOrigin-RevId: 426f412929542712c8455806d42fef56a837d860
JetBrains,intellij-community,56241ab394e9985464f64d383d4d5894d360ce18,https://github.com/JetBrains/intellij-community/commit/56241ab394e9985464f64d383d4d5894d360ce18,PY-73411 Pycharm performance tests failed   Merge-request: IJ-MR-137368 Merged-by: Egor Eliseev <Egor.Eliseev@jetbrains.com>  GitOrigin-RevId: 23a7e9d443606b1a9a028c5e3275c6c408c0d796
JetBrains,intellij-community,b9979d9c4f3e8224ea925a3bf89c2107fdc66f77,https://github.com/JetBrains/intellij-community/commit/b9979d9c4f3e8224ea925a3bf89c2107fdc66f77,[json] IJPL-63554 Refactored existing schema validators  - Some large methods were split into several - Inspected value adapters were reused to ensure correct resolve - Old incorrect if-else validation was removed in favor of if-else expansion; The idea is that after resolve there wouldn't be any if-then-else expressions to process - we have to resolve to some final json schema node in the end  where plain validations would work perfectly without some special treatment. This is now unified with other applicators like oneOf or referenced schema nodes  GitOrigin-RevId: 6ad07cc467965c039ff29184be6c94e3f33d8b01
JetBrains,intellij-community,ef7610038d423f7854f29a3db67fcc011b510129,https://github.com/JetBrains/intellij-community/commit/ef7610038d423f7854f29a3db67fcc011b510129,IJPL-149317 Disable Write Intent Lock for runnables executed on EDT  1. Move wrapping with WriteIntentLock to LaterInvocator 2. Disable wrapping into WriteIntentLock in IdeEventQueue by flipping read-only property. 3. Run Key and Mouse events with WIL. 4. Run WindowEvent under WIL  as it can cause synchronous messages which leads to actions which needs WriteIntentLock. 5. Add WriteIntentReadAction to performActivity(). 6. Add explicit write intent lock to "immediate" execution in FocusManagerImpl. Add comment about the problem with this "immediate" detection.  GitOrigin-RevId: e1eaaff4988cad2df1eb750c6af846bb92e5c904
JetBrains,intellij-community,1068a81106c40b74b48653842ee3bdf8da64af73,https://github.com/JetBrains/intellij-community/commit/1068a81106c40b74b48653842ee3bdf8da64af73,IJPL-797 `intellij.platform.ide.util.io` review internal API: deprecate `OSProcessUtil.getApplicationPid`  Initially  61bf5253ec56238d6f30a5b480272024a351a889 introduced two methods: * `private static int getCurrentProcessId()` * `public static String getApplicationPid()`  The function `getCurrentProcessId` was somehow expensive  and the function `getApplicationPid` was a caching wrapper for `getCurrentProcessId`.  Later  the commit 0b64798eb2e3d2201350b1d608c4e2412394020f made `getCurrentProcessId` public. Since that commit  new usages of both methods appeared internally and externally.  Finally  the commit a6aca19bbf2667071ccd3466c0fc29319d972bd4 replaced internals of `getCurrentProcessId` with a lightweight call.  Nowadays  there's no need in the caching function. Take a look at this JMH benchmark.  Put it into the module `intellij.platform.benchmarks`  ```java package com.intellij.openapi.vfs.newvfs.persistent;  import com.intellij.execution.process.OSProcessUtil; import org.openjdk.jmh.annotations.Benchmark; import org.openjdk.jmh.runner.Runner; import org.openjdk.jmh.runner.RunnerException; import org.openjdk.jmh.runner.options.Options; import org.openjdk.jmh.runner.options.OptionsBuilder;  import java.lang.management.ManagementFactory;  public class OsProcessUtilBench { @Benchmark public void measureGetCurrentProcessId() { OSProcessUtil.getCurrentProcessId(); }  @Benchmark public void measureGetApplicationPid() { OSProcessUtil.getApplicationPid(); }  @Benchmark public void measureInitialGetCurrentProcessId() { try { String name = ManagementFactory.getRuntimeMXBean().getName(); String result = name.split("@")[0]; } catch (Exception e) { String result = "-1"; } }  public static void main(String[] args) throws RunnerException { Options opt = new OptionsBuilder() .include(OsProcessUtilBench.class.getSimpleName()) .threads(1) .forks(0) .build();  new Runner(opt).run(); } } ```  The results are:  MacBook M2 Max  JBR-17.0.9+8-1166.2-nomod:  ``` Benchmark                                              Mode  Cnt           Score           Error  Units OsProcessUtilBench.measureGetApplicationPid           thrpt    5  2331351911 143 ± 401295172 273  ops/s OsProcessUtilBench.measureGetCurrentProcessId         thrpt    5  1840847082 291 ± 638699340 108  ops/s OsProcessUtilBench.measureInitialGetCurrentProcessId  thrpt    5     2818096 290 ±    128628 230  ops/s ```  Windows 11  i9-12900  JBR-17.0.9+8-1166.2-nomod ``` Benchmark                                              Mode  Cnt           Score           Error  Units OsProcessUtilBench.measureGetApplicationPid           thrpt    5  3409145745.079 ± 235584855.493  ops/s OsProcessUtilBench.measureGetCurrentProcessId         thrpt    5  3251065447.737 ±   4455307.411  ops/s OsProcessUtilBench.measureInitialGetCurrentProcessId  thrpt    5     3056345.314 ±      7065.322  ops/s ```  The caching granted 500x performance boost for the initial implementation and only 1.22x boost for the actual implementation. Today the caching version of this rarely used functionality isn't worth it.  GitOrigin-RevId: 84227add27cd59c0a648326ca42e3a49bc5f4c31
JetBrains,intellij-community,2ff134e7ee3303df92056d25b72db259bfa32243,https://github.com/JetBrains/intellij-community/commit/2ff134e7ee3303df92056d25b72db259bfa32243,Merge branch 'kt-master'  # Conflicts: #	.idea/libraries/kotlinc_high_level_api.xml #	.idea/libraries/kotlinc_high_level_api_fe10.xml #	.idea/libraries/kotlinc_high_level_api_fir.xml #	.idea/libraries/kotlinc_high_level_api_fir_tests.xml #	.idea/libraries/kotlinc_high_level_api_impl_base.xml #	.idea/libraries/kotlinc_high_level_api_impl_base_tests.xml #	community/.idea/libraries/kotlinc_high_level_api.xml #	community/.idea/libraries/kotlinc_high_level_api_fe10.xml #	community/.idea/libraries/kotlinc_high_level_api_fir.xml #	community/.idea/libraries/kotlinc_high_level_api_fir_tests.xml #	community/.idea/libraries/kotlinc_high_level_api_impl_base.xml #	community/.idea/libraries/kotlinc_high_level_api_impl_base_tests.xml #	community/android/android-kotlin/idea-android/k2/src/org/jetbrains/kotlin/android/inspection/K2TypeParameterFindViewByIdInspection.kt #	community/android/android-templates/intellij.android.templates.iml #	community/android/compose-designer/src/com/android/tools/idea/compose/annotator/SpringPickerLineMarkerProvider.kt #	community/android/compose-designer/src/com/android/tools/idea/compose/pickers/preview/utils/KotlinUtils.kt #	community/android/compose-designer/src/com/android/tools/idea/compose/pickers/spring/model/SpringPickerPropertiesModel.kt #	community/android/compose-ide-plugin/compiler-hosted-src/androidx/compose/compiler/plugins/kotlin/lower/IrSourcePrinter.kt #	community/android/compose-ide-plugin/src/com/android/tools/compose/ComposeColorLineMarkerProviderDescriptor.kt #	community/android/compose-ide-plugin/src/com/android/tools/compose/ComposePluginUtils.kt #	community/android/compose-ide-plugin/src/com/android/tools/compose/debug/ComposeFunctionBreakpointType.kt #	community/android/project-system-gradle/src/com/android/tools/idea/run/configuration/AndroidBaselineProfileRunLineMarkerContributor.kt #	community/platform/build-scripts/src/org/jetbrains/intellij/build/CommunityLibraryLicenses.kt #	community/plugins/dev/intellij.kotlin.dev/src/internal/KotlinGoodCodeRedVisitor.kt #	community/plugins/devkit/intellij.kotlin.devkit/src/inspections/ForbiddenInSuspectContextMethodInspection.kt #	community/plugins/devkit/intellij.kotlin.devkit/src/inspections/KtAppServiceAsStaticFinalFieldOrPropertyProvider.kt #	community/plugins/devkit/intellij.kotlin.devkit/src/inspections/KtCallingFunctionShouldBeRequiresBlockingContextVisitorProvider.kt #	community/plugins/devkit/intellij.kotlin.devkit/src/inspections/UsePlatformProcessAwaitExitInspection.kt #	community/plugins/kotlin/base/analysis-api/analysis-api-utils/src/org/jetbrains/kotlin/idea/base/analysis/api/utils/CallParameterInfoProvider.kt #	community/plugins/kotlin/base/analysis-api/analysis-api-utils/src/org/jetbrains/kotlin/idea/base/analysis/api/utils/KtSymbolFromIndexProvider.kt #	community/plugins/kotlin/base/analysis-api/analysis-api-utils/src/org/jetbrains/kotlin/idea/base/analysis/api/utils/resolveUtils.kt #	community/plugins/kotlin/base/code-insight/src/org/jetbrains/kotlin/idea/base/codeInsight/ExpectedExpressionMatcher.kt #	community/plugins/kotlin/base/code-insight/src/org/jetbrains/kotlin/idea/base/codeInsight/KotlinCallProcessor.kt #	community/plugins/kotlin/base/code-insight/src/org/jetbrains/kotlin/idea/base/codeInsight/KotlinNameSuggester.kt #	community/plugins/kotlin/base/fir/analysis-api-platform/kotlin.base.fir.analysis-api-platform.iml #	community/plugins/kotlin/base/fir/analysis-api-platform/test/org/jetbrains/kotlin/idea/base/fir/analysisApiPlatform/modificationEvents/KotlinModuleOutOfBlockModificationTest.kt #	community/plugins/kotlin/base/fir/analysis-api-platform/test/org/jetbrains/kotlin/idea/base/fir/analysisApiPlatform/sessions/CyclicDependenciesSymbolResolutionTest.kt #	community/plugins/kotlin/base/scripting/src/org/jetbrains/kotlin/idea/core/script/scriptUtils.kt #	community/plugins/kotlin/code-insight/api/src/org/jetbrains/kotlin/idea/codeinsight/api/applicable/ContextProvider.kt #	community/plugins/kotlin/code-insight/api/src/org/jetbrains/kotlin/idea/codeinsight/api/applicators/fixes/KotlinApplicatorBasedQuickFix.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/ActualAnnotationsNotMatchExpectFixFactory.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/AddDataModifierFixFactory.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/AddSuspendModifierFixFactory.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/ChangeTypeQuickFixFactories.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/InsertDelegationCallFixFactory.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/SuperClassNotInitializedFactories.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/WrapWithSafeLetCallFixFactories.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/WrongPrimitiveLiteralFix.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/imprt/ClassifierImportCandidatesProvider.kt #	community/plugins/kotlin/code-insight/fixes-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/fixes/imprt/ImportQuickFix.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeInsight/IfThenTransformationUtils.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeInsight/KotlinSuperDeclarationsInfoService.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeInsight/KotlinTypeDeclarationProvider.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeinsights/impl/base/JavaArgumentNameCommentUtils.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeinsights/impl/base/StringTemplateUtils.kt #	community/plugins/kotlin/code-insight/impl-base/src/org/jetbrains/kotlin/idea/codeinsights/impl/base/intentions/RemoveArgumentNamesUtils.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/JoinDeclarationAndAssignmentInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/RemoveToStringInStringTemplateInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/ReplaceGetOrSetInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/SelfAssignmentInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/UsePropertyAccessSyntaxInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/KotlinConstantConditionsInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/KotlinFunctionCallInstruction.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/KtControlFlowBuilder.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/KtDfaHelpers.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/KtVariableDescriptor.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/dfa/SmartCastHelpers.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/expressions/ReplaceCallWithBinaryOperatorInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/expressions/ReplaceSizeCheckInspectionBase.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/expressions/WhenWithOnlyElseInspection.kt #	community/plugins/kotlin/code-insight/inspections-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/inspections/jdk2k/Transformation.kt #	community/plugins/kotlin/code-insight/inspections-shared/src/org/jetbrains/kotlin/idea/codeInsight/inspections/shared/RedundantSuspendModifierInspection.kt #	community/plugins/kotlin/code-insight/inspections-shared/src/org/jetbrains/kotlin/idea/codeInsight/inspections/shared/RedundantValueArgumentInspection.kt #	community/plugins/kotlin/code-insight/inspections-shared/src/org/jetbrains/kotlin/idea/codeInsight/inspections/shared/RemoveEmptyParenthesesFromLambdaCallInspection.kt #	community/plugins/kotlin/code-insight/intentions-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/intentions/ConvertLambdaToReferenceIntention.kt #	community/plugins/kotlin/code-insight/intentions-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/intentions/ImportAllMembersIntention.kt #	community/plugins/kotlin/code-insight/intentions-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/intentions/ImportMemberIntention.kt #	community/plugins/kotlin/code-insight/intentions-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/intentions/RemoveExplicitTypeIntention.kt #	community/plugins/kotlin/code-insight/intentions-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/intentions/ReplaceUnderscoreWithTypeArgumentIntention.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/hints/KtParameterHintsProvider.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/hints/KtReferencesTypeHintsProvider.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/hints/KtValuesHintsProvider.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/quickDoc/KotlinDocumentationTarget.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/quickDoc/KotlinIdeDeclarationRenderer.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/quickFixes/createFromUsage/CreateKotlinCallableActionTextBuilder.kt #	community/plugins/kotlin/code-insight/kotlin.code-insight.k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/quickFixes/createFromUsage/K2CreateFunctionFromUsageUtil.kt #	community/plugins/kotlin/code-insight/line-markers/src/org/jetbrains/kotlin/idea/codeInsight/lineMarkers/KotlinRecursiveCallLineMarkerProvider.kt #	community/plugins/kotlin/code-insight/live-templates-k2/src/org/jetbrains/kotlin/idea/liveTemplates/k2/macro/SymbolBasedAnonymousSuperMacro.kt #	community/plugins/kotlin/code-insight/postfix-templates/src/org/jetbrains/kotlin/idea/codeInsight/postfix/KotlinTryPostfixTemplate.kt #	community/plugins/kotlin/code-insight/postfix-templates/src/org/jetbrains/kotlin/idea/codeInsight/postfix/KotlinWhenPostfixTemplate.kt #	community/plugins/kotlin/code-insight/structural-search-k2/src/org/jetbrains/kotlin/idea/k2/codeinsight/structuralsearch/KotlinStructuralSearchUtil.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/EmptinessCheckFunctionUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/FoldIfOrWhenToFunctionCallUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/ImplicitThisUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/InlineUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/InsertExplicitTypeArgumentsUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/KotlinPsiUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/NamedArgumentUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/TypeParameterUtils.kt #	community/plugins/kotlin/code-insight/utils/src/org/jetbrains/kotlin/idea/codeinsight/utils/TypeUtils.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/Completions.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/KotlinFirCompletionContributor.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/FirCallableCompletionContributor.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/FirClassifierCompletionContributor.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/FirNamedArgumentCompletionContributor.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/FirWhenWithSubjectConditionContributor.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/helpers/CallableMetadataProvider.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/contributors/helpers/FirSuperEntriesProvider.kt #	community/plugins/kotlin/completion/impl-k2/src/org/jetbrains/kotlin/idea/completion/impl/k2/weighers/ExpectedTypeWeigher.kt #	community/plugins/kotlin/fir/src/org/jetbrains/kotlin/idea/parameterInfo/KotlinHighLevelTypeArgumentInfoHandler.kt #	community/plugins/kotlin/fir/src/org/jetbrains/kotlin/idea/parameterInfo/utils.kt #	community/plugins/kotlin/highlighting/highlighting-k2/src/org/jetbrains/kotlin/idea/highlighting/KotlinCallHighlighterExtension.kt #	community/plugins/kotlin/highlighting/highlighting-k2/src/org/jetbrains/kotlin/idea/highlighting/KotlinDiagnosticHighlightVisitor.kt #	community/plugins/kotlin/highlighting/highlighting-k2/src/org/jetbrains/kotlin/idea/highlighting/KotlinUnusedSymbolUtil.kt #	community/plugins/kotlin/highlighting/highlighting-k2/src/org/jetbrains/kotlin/idea/highlighting/highlighters/FunctionCallHighlighter.kt #	community/plugins/kotlin/highlighting/highlighting-k2/test/org/jetbrains/kotlin/idea/k2/highlighting/AbstractK2HighlightingMetaInfoWithExtensionTest.kt #	community/plugins/kotlin/injection/k2/src/org/jetbrains/kotlin/idea/k2/injection/K2KotlinLanguageInjectionContributor.kt #	community/plugins/kotlin/intellij.kotlin.plugin.community.main.iml #	community/plugins/kotlin/jvm-debugger/core-fe10/src/org/jetbrains/kotlin/idea/debugger/stepping/smartStepInto/CallableMemberInfo.kt #	community/plugins/kotlin/jvm-debugger/core/src/org/jetbrains/kotlin/idea/debugger/core/KotlinPositionManager.kt #	community/plugins/kotlin/jvm-debugger/coroutines/src/org/jetbrains/kotlin/idea/debugger/coroutine/KotlinVariableNameFinder.kt #	community/plugins/kotlin/jvm-debugger/evaluation/kotlin.jvm-debugger.evaluation.iml #	community/plugins/kotlin/jvm-debugger/evaluation/src/org/jetbrains/kotlin/idea/debugger/evaluate/kotlinExpressionWrappers.kt #	community/plugins/kotlin/k2-fe10-bindings/src/org/jetbrains/kotlin/idea/fir/fe10/KtSymbolBasedKotlinTypes.kt #	community/plugins/kotlin/k2-fe10-bindings/src/org/jetbrains/kotlin/idea/fir/fe10/binding/Fe10BindingScopeProvider.kt #	community/plugins/kotlin/k2-fe10-bindings/src/org/jetbrains/kotlin/idea/fir/fe10/binding/MiscBindingContextValueProvider.kt #	community/plugins/kotlin/k2-fe10-bindings/src/org/jetbrains/kotlin/idea/fir/fe10/binding/ResolvedCallWrappers.kt #	community/plugins/kotlin/k2-fe10-bindings/src/org/jetbrains/kotlin/idea/fir/fe10/binding/ToDescriptorBindingContextValueProviders.kt #	community/plugins/kotlin/kotlin.performanceExtendedPlugin/kotlin.performanceExtendedPlugin.iml #	community/plugins/kotlin/kotlin.searching/src/org/jetbrains/kotlin/idea/searching/inheritors/DirectKotlinClassInheritorsSearcher.kt #	community/plugins/kotlin/kotlin.searching/src/org/jetbrains/kotlin/idea/searching/usages/KotlinK2FindUsagesSupport.kt #	community/plugins/kotlin/kotlin.searching/src/org/jetbrains/kotlin/idea/searching/usages/KotlinK2SearchUsagesSupport.kt #	community/plugins/kotlin/kotlin.searching/src/org/jetbrains/kotlin/idea/searching/usages/findUsagesUtils.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.common/src/org/jetbrains/kotlin/idea/refactoring/rename/AutomaticOverloadsRenamer.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/KotlinChangeSignatureUsageSearcher.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/KotlinParameterInfo.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/KotlinTypeInfo.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/quickFix/ChangeParameterTypeFixFactory.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/quickFix/ChangeSignatureFixFactory.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/changeSignature/usages/KotlinFunctionCallUsage.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/extractFunction/Parameter.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/extractFunction/parametersUtil.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/inline/codeInliner/CodeInliner.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/inline/codeInliner/InlinePreprocessorUtil.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/introduce/K2SemanticMatcher.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/introduce/extractionEngine/ExtractionDataAnalyzer.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/introduce/extractionEngine/KotlinTypeDescriptor.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/introduceParameter/KotlinFirIntroduceParameterHandler.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/util/ConvertReferenceToLambdaUtil.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/util/RedundantExplicitTypeArgumentsUtil.kt #	community/plugins/kotlin/refactorings/kotlin.refactorings.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/utils.kt #	community/plugins/kotlin/refactorings/rename.k2/src/org/jetbrains/kotlin/idea/k2/refactoring/rename/renameConflictUtils.kt #	community/plugins/kotlin/uast/uast-kotlin-fir/src/org/jetbrains/uast/kotlin/FirKotlinUastResolveProviderService.kt #	community/plugins/kotlin/uast/uast-kotlin-fir/src/org/jetbrains/uast/kotlin/internal/firKotlinInternalUastUtils.kt #	community/plugins/kotlin/util/project-model-updater/src/org/jetbrains/tools/model/updater/kotlincLibraries.kt #	fleet/plugins/mercury/backend/compose.plugin/src/org/jetbrains/compose/codeInsight/Common.kt #	fleet/plugins/mercury/backend/src/fleet/backend/mercury/compose/resources/ComposeResourcesUtils.kt #	plugins/frameworks/exposed/exposed-core/src/com/intellij/exposed/utils/ExposedAnalyzerUtils.kt #	plugins/frameworks/ktor/ktor-starter/src/io/ktor/ide/utils/KotlinResolve.kt  GitOrigin-RevId: 00a6e6be4d6f36368bf5b6e649eb209d83da38d2
JetBrains,intellij-community,e7aa14e2f5b3634774b2eac80d33bede8ba04f42,https://github.com/JetBrains/intellij-community/commit/e7aa14e2f5b3634774b2eac80d33bede8ba04f42,[vcs] Moved PsiFiles initialization to doPerformRediff method.  GitOrigin-RevId: 4c4a8a73304c32be7f8f416d2776b22a6e8facf7
JetBrains,intellij-community,c084e60fe0ab36ef31565f31a02a6ec31c78d88f,https://github.com/JetBrains/intellij-community/commit/c084e60fe0ab36ef31565f31a02a6ec31c78d88f,IJPL-485 Performance metrics: navigation to declaration  GitOrigin-RevId: ba0b8c5b084940711be46a7b00f2895f33ae8a1f
JetBrains,intellij-community,28aa440bce4fdc44b85ed32a993db3f219627fbc,https://github.com/JetBrains/intellij-community/commit/28aa440bce4fdc44b85ed32a993db3f219627fbc,IJPL-156230 merge: disable resolve button on performing external resolving  GitOrigin-RevId: bd9a0ec4f4718f15d5b6f6fe48e43a07cbb52635
JetBrains,intellij-community,9347d3b5baa413ca7e8fdaacc8e6dae585295344,https://github.com/JetBrains/intellij-community/commit/9347d3b5baa413ca7e8fdaacc8e6dae585295344,[indexes] IJPL-1365: optimize .processKeys()  + seems like duplicates is not possible by design  hence an alreadyProcessedKeys set is not needed to avoid duplicates -- big performance win  GitOrigin-RevId: d9a6c2b4ed1b495349bb62314a50a5f103769d67
JetBrains,intellij-community,176d69eba6815929274f29ff6ecd1a8352365258,https://github.com/JetBrains/intellij-community/commit/176d69eba6815929274f29ff6ecd1a8352365258,[Project Structure] [IJPL-10393] Speed up the `ModuleEditor.getModule` by removing the unnecessary loop  This loop was joining with the other loop and caused performance issues on project structure dialog open in case there are a lot of modules. See IJPL-10393  However  this loop is NOT needed as it was added for a specific case that doesn't exist anymore. See the comment with the details about why this loop was introduced and why it's not needed anymore.  https://youtrack.jetbrains.com/issue/IJPL-10393/Opening-Project-Structure-in-20k-module-project-causes-freezing#focus=Comments-27-9879534.0-0  GitOrigin-RevId: f8eb0049068f9cc7d0a0d98acd318f2b710df40a
JetBrains,intellij-community,6517ffb79319bc2c04a878bd280a65fa5f6c2bbd,https://github.com/JetBrains/intellij-community/commit/6517ffb79319bc2c04a878bd280a65fa5f6c2bbd,[Java. Code Formatting] Perform a manual lookup of the annotation in the import list in JavaFormatterUtil#isTypeAnnotation  IDEA-353192  GitOrigin-RevId: fa51cf055f554369102a71e9518aee0f5c520406
JetBrains,intellij-community,63f468795fd45b7dc306b8b5d73f563638590ab4,https://github.com/JetBrains/intellij-community/commit/63f468795fd45b7dc306b8b5d73f563638590ab4,[github] do not perform authorized requests to unknown servers  IJPL-155883 CVE-2024-37051  GitOrigin-RevId: 143dc4a103fff32ba8a6d6cded7418858c1a06ef
JetBrains,intellij-community,e4e5826e81f30a318f2f2eb0bcea0f61951033b6,https://github.com/JetBrains/intellij-community/commit/e4e5826e81f30a318f2f2eb0bcea0f61951033b6,[rdct] Do not perform async code formatting on thin client  do it only via smart backend  fix TypingConditionAndIndentTest.addNestingTest  GitOrigin-RevId: 308e969c566e6031e85b4d7d9e992b33e7099a12
JetBrains,intellij-community,4e21b087f3d7ecd0c4dd12f082d5c8df08cb5c59,https://github.com/JetBrains/intellij-community/commit/4e21b087f3d7ecd0c4dd12f082d5c8df08cb5c59,IJPL-48479 Fix slow ops in SelectInTargetPsiWrapper  Not a perfect solution  as it's yet another NBRA seemingly for no reason  but it's the best we can do without changing the API and enforcing strong EDT/BGT contracts on it.  Existing getSelectorInFile() implementations seem to be safe to call from a BGT  there's mainly PSI stuff there and also some access to thread-safe editor models like the caret position.  GitOrigin-RevId: 92bb27afda5989e283bc98d6a577e11666b21579
JetBrains,intellij-community,8b615f06e747c7fa048223861a6d17af5d9ccd5e,https://github.com/JetBrains/intellij-community/commit/8b615f06e747c7fa048223861a6d17af5d9ccd5e,IJPL-155772 Fix UsageViewImpl.MyPanel update thread  It's pure UI code there  working with the tree and its model. It should only be performed on the BGT.  GitOrigin-RevId: 5dc36ea12ada5def3912ee33156001a3bde4d95b
JetBrains,intellij-community,c60f9353cb7a972f0508632fa9d27d1b2ba37621,https://github.com/JetBrains/intellij-community/commit/c60f9353cb7a972f0508632fa9d27d1b2ba37621,IJPL-149878 IJent WSL: introduce TracingFileSystemProvider for controlling performance of WSL access on fs  TracingFileSystemProvider wraps both the original WindowsFileSystemProvider and IjentNioFileSystemProvider. It allows controlling sudden performance degradations in benchmark tests.  Only WSL drives are supposed to be wrapped into TracingFileSystemProvider. Regular windows drives aren't wrapped.  GitOrigin-RevId: 988278e0e88a7d6c9e01422a0dd0713e883cb275
JetBrains,intellij-community,ce6537bbde0531072298a9f9fb5916f82fc5ddb0,https://github.com/JetBrains/intellij-community/commit/ce6537bbde0531072298a9f9fb5916f82fc5ddb0,[profiler] IDEA-353438 Show performance hints only for the thread Run to Cursor was invoked on  bump async-profiler version to 3.0-4 -- support passing java thread IDs (in addition to native IDs) to the agent to profile only desired threads  GitOrigin-RevId: 7d76a8102d6c7eae44f22bfabcf14409e77ef1f9
LMAX-Exchange,disruptor,bfc35ee29a384e2151cf0dec8f19f9ce6f798b75,https://github.com/LMAX-Exchange/disruptor/commit/bfc35ee29a384e2151cf0dec8f19f9ce6f798b75,Merge pull request #483 from nicholassm/master  Add JMH benchmark to measure multi-producer batch publication performance.
dromara,Sa-Token,905f6714e2439e13d7cdd63b1ee28d3c43922766,https://github.com/dromara/Sa-Token/commit/905f6714e2439e13d7cdd63b1ee28d3c43922766,perf: sa-token-redisx 调整 SaTokenDaoOfRedisJson 类保持与 SaTokenDaoForRedisTemplate 相似的处理逻辑
dromara,Sa-Token,caeb4eba1579dafcf765af43be58188fbc3b692e,https://github.com/dromara/Sa-Token/commit/caeb4eba1579dafcf765af43be58188fbc3b692e,perf: sa-token-solon-plugin 移除 dao 下的代码（由具体插件处理）
dromara,Sa-Token,e7694bd6fb76f45a51babc6e725680e53d5471e5,https://github.com/dromara/Sa-Token/commit/e7694bd6fb76f45a51babc6e725680e53d5471e5,perf(sso): sso 示例代码的跨域处理由原生方式改为 Sa-Token 过滤器模式
YunaiV,yudao-cloud,02a074590ef34f2c1f93c826097253c0005a41ce,https://github.com/YunaiV/yudao-cloud/commit/02a074590ef34f2c1f93c826097253c0005a41ce,perf：【SYSTEM 全局】优化 TenantApi 的使用
facebook,fresco,f77f1f772066209007d76572017320285605d530,https://github.com/facebook/fresco/commit/f77f1f772066209007d76572017320285605d530,with checkNotNull] fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/fragments/SettingsFragment.java  Reviewed By: jocelynluizzi13  Differential Revision: D68765666  fbshipit-source-id: 10209ac659157764a07e674d01fd8db5311a1c62
facebook,fresco,a0346f95afe02b2e78fb2eea31f75582553b3d4e,https://github.com/facebook/fresco/commit/a0346f95afe02b2e78fb2eea31f75582553b3d4e,with checkNotNull] fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/fragments/SettingsFragment.java  Reviewed By: jocelynluizzi13  Differential Revision: D68701564  fbshipit-source-id: c7ad6fcb483d2bd07320c83a972772602c927a8c
facebook,fresco,c5aee95096e5592e0028a545cf4bce1d16ddbcd2,https://github.com/facebook/fresco/commit/c5aee95096e5592e0028a545cf4bce1d16ddbcd2,] fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/conf/Config.java  Reviewed By: jocelynluizzi13  Differential Revision: D68700912  fbshipit-source-id: 918184cc949647917a54da84c6f2b394de7c397c
facebook,fresco,81b7d8dfb32a587ec4dc0e61df4afa443f8e3d9c,https://github.com/facebook/fresco/commit/81b7d8dfb32a587ec4dc0e61df4afa443f8e3d9c,with checkNotNull] fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/fragments/SettingsFragment.java  Reviewed By: oprisnik  Differential Revision: D68620457  fbshipit-source-id: 9eb0410edf4d742f86c6e849e49f32253c4c646f
facebook,fresco,6833fbdd09d1347e31f9974e375d8d01c09aa5e5,https://github.com/facebook/fresco/commit/6833fbdd09d1347e31f9974e375d8d01c09aa5e5,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/preferences/SizePreferences.java  Reviewed By: nicholeic  Differential Revision: D67097657  fbshipit-source-id: 37519f3fbce43016dfda97a2dc3a97353661e634
facebook,fresco,0e58040d64ca31ed4be331d1f973f279eb937ac9,https://github.com/facebook/fresco/commit/0e58040d64ca31ed4be331d1f973f279eb937ac9,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/MainActivity.java  Reviewed By: jocelynluizzi13  Differential Revision: D65144736  fbshipit-source-id: 34b77f1db5a1b4b28299e4e54226da5f8d40e14c
facebook,fresco,d5a2a5ea12032b499dcf6968301042163238364c,https://github.com/facebook/fresco/commit/d5a2a5ea12032b499dcf6968301042163238364c,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/instrumentation/Instrumentation.java  Reviewed By: jocelynluizzi13  Differential Revision: D63085153  fbshipit-source-id: e5ca05181cd8424ac49055d06b09de7bd3498106
facebook,fresco,756caad29d532538e5c265679bc9c280b0089149,https://github.com/facebook/fresco/commit/756caad29d532538e5c265679bc9c280b0089149,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/fragments/SettingsFragment.java  Differential Revision: D62651883  fbshipit-source-id: 62f1e0d41e68fc484a4a2922b693fa29fec27009
facebook,fresco,70edbf581fabfc9605539f55da16fa5a098563c0,https://github.com/facebook/fresco/commit/70edbf581fabfc9605539f55da16fa5a098563c0,Create ImagePerfLoggingListener interface  Reviewed By: kartavya-ramnani  Differential Revision: D62182951  fbshipit-source-id: f46de0ede817b94cc019cfceb61f488c73f67a20
facebook,fresco,d99df62f648c592d2a9c93ff472b5c8843beea0a,https://github.com/facebook/fresco/commit/d99df62f648c592d2a9c93ff472b5c8843beea0a,Finalize more builder classes for Android Studio Performance  Reviewed By: edelron  Differential Revision: D61660695  fbshipit-source-id: c9208d33e6c777c78b2d5b30fbfad9473e446bec
facebook,fresco,b4a2923ed58864adf11d346b70460e78560eac5b,https://github.com/facebook/fresco/commit/b4a2923ed58864adf11d346b70460e78560eac5b,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/fragments/MainFragment.java  Reviewed By: jocelynluizzi13  Differential Revision: D61786759  fbshipit-source-id: 5e4fd916717b996bb62ddc49333938e85bc7430c
facebook,fresco,05eb4fa783458aa44d5f30c0df5b16f121312dbf,https://github.com/facebook/fresco/commit/05eb4fa783458aa44d5f30c0df5b16f121312dbf,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/fragments/MainFragment.java  Reviewed By: jocelynluizzi13  Differential Revision: D61721133  fbshipit-source-id: b1d7d3d2c1e5f431c61c7bebcf8b15100ff90a90
facebook,fresco,275af4a08a6c04b83e02d74479f2878253c691b9,https://github.com/facebook/fresco/commit/275af4a08a6c04b83e02d74479f2878253c691b9,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/fragments/MainFragment.java  Reviewed By: steelrooter  Differential Revision: D61197112  fbshipit-source-id: 5e8322d099eb0b6fbf9f589b422c75d374a88e22
facebook,fresco,725bcd7c0c6a983529aa1411e236b45c8a698dc1,https://github.com/facebook/fresco/commit/725bcd7c0c6a983529aa1411e236b45c8a698dc1,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/data/impl/ContentProviderSimpleAdapter.java  Reviewed By: jocelynluizzi13  Differential Revision: D61111715  fbshipit-source-id: 45b446e3cac0b65203dcefdfc2fdda8343bd4dba
facebook,fresco,98a1b6d3385be6aa8a86c57740bd3dce9b05eac5,https://github.com/facebook/fresco/commit/98a1b6d3385be6aa8a86c57740bd3dce9b05eac5,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/data/SimpleAdapter.java  Reviewed By: jocelynluizzi13  Differential Revision: D61111397  fbshipit-source-id: 38c66d5c3b29ff4298a6a7d0d79907aa06f5d406
facebook,fresco,3fdfeea3c06b4e105ac68ab4be97e140a4eeee4a,https://github.com/facebook/fresco/commit/3fdfeea3c06b4e105ac68ab4be97e140a4eeee4a,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/util/UI.java  Reviewed By: jocelynluizzi13  Differential Revision: D61112605  fbshipit-source-id: 588845817710def8b314131a7475e1a5b9769750
facebook,fresco,44710728be5955a24e35730636b67449e3e5d09d,https://github.com/facebook/fresco/commit/44710728be5955a24e35730636b67449e3e5d09d,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/conf/Config.java  Reviewed By: steelrooter  Differential Revision: D61111163  fbshipit-source-id: 9c24403b9f8a582dc2a212f8e498484ec9acafc5
facebook,fresco,5f38338779adb0ea386843dda3ad1b5dbe24fba7,https://github.com/facebook/fresco/commit/5f38338779adb0ea386843dda3ad1b5dbe24fba7,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/instrumentation/InstrumentedVitoView.java  Reviewed By: steelrooter  Differential Revision: D61112262  fbshipit-source-id: 93c982e19b0cdf90ef7cebabff248b2df71a70b0
facebook,fresco,604e0daa2366aa38e232911f82fd2922e876687a,https://github.com/facebook/fresco/commit/604e0daa2366aa38e232911f82fd2922e876687a,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/fragments/SettingsFragment.java  Reviewed By: oprisnik  Differential Revision: D60886076  fbshipit-source-id: a586a25c515b66d65842ccfb5fc49b03912ba373
facebook,fresco,7450e5e6cd763babe1615bc208d865eead50972f,https://github.com/facebook/fresco/commit/7450e5e6cd763babe1615bc208d865eead50972f,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/preferences/SizePreferences.java  Reviewed By: oprisnik  Differential Revision: D60816275  fbshipit-source-id: 248d63e30c5a3a3f0f71d440497b5b3003c7cfd8
facebook,fresco,f3a3b10c3ab9d50dc4d032652aab6ad3868b0f89,https://github.com/facebook/fresco/commit/f3a3b10c3ab9d50dc4d032652aab6ad3868b0f89,Simplify visibility method name for ImagePerfNotifier  Reviewed By: oprisnik  Differential Revision: D60239472  fbshipit-source-id: d487161981e87c4cf81dc5ea71b2c11a7182b8c3
facebook,fresco,8a2f8e76b7eab0a64e4e45a154aa45e35a8ee01e,https://github.com/facebook/fresco/commit/8a2f8e76b7eab0a64e4e45a154aa45e35a8ee01e,fbandroid/libraries/fresco/samples/scrollperf/src/main/java/com/facebook/samples/scrollperf/util/SizeUtil.java  Reviewed By: jocelynluizzi13  Differential Revision: D60223374  fbshipit-source-id: 6b07adfef0b55da7bcd046d7c1a457498b4323d4
facebook,fresco,e6b7e3ae65dba4d5117e3f3ceab84dfcd0ebd6b1,https://github.com/facebook/fresco/commit/e6b7e3ae65dba4d5117e3f3ceab84dfcd0ebd6b1,Migrate Fresco Scrollperf sample app to use Vito  Reviewed By: steelrooter  Differential Revision: D59804050  fbshipit-source-id: 6d86632ce6a3804269296e73ba4a22101df52b45
facebook,fresco,577742315d7c24beefa8250551767ec24f2d7b32,https://github.com/facebook/fresco/commit/577742315d7c24beefa8250551767ec24f2d7b32,Rename ImagePerfControllerListener2 to ImagePerfStateManager  Reviewed By: oprisnik  Differential Revision: D59275922  fbshipit-source-id: b55dd6d2b8930a2c0fa02e33a9aba60d2f2a1b39
facebook,fresco,66145342f80f4416503c866069ea5817ace9f70d,https://github.com/facebook/fresco/commit/66145342f80f4416503c866069ea5817ace9f70d,Remove dead code: ImagePerfControllerListener  Reviewed By: defHLT  kartavya-ramnani  Differential Revision: D59059182  fbshipit-source-id: ee4ed6c1affe896ddb9e2b4f92c174fd02bf83c0
material-components,material-components-android,d16a19364c91bc6c6e2db1161726d82acb561291,https://github.com/material-components/material-components-android/commit/d16a19364c91bc6c6e2db1161726d82acb561291,[MaterialShapeDrawable] Update ShapeAppearanceModel.Builder to not use extra ContextThemeWrapper for shape appearance overlay due to performance concerns  PiperOrigin-RevId: 762498280
ben-manes,caffeine,62f401ce29cfaa2daabceff7353e69642d5261e2,https://github.com/ben-manes/caffeine/commit/62f401ce29cfaa2daabceff7353e69642d5261e2,remove sun.misc.unsafe leftover in the test and benchmark code  Removed the usage from a unit test that requires a predictable ThreadLocalRandom result. This now uses reflection and the runner must explicitly open the module for access.  Removed from a benchmark comparing table lookup mechanisms and the cache currently uses the VarHandles approach   Removed JDK 7's ConcurrentHashMap as this was only useful for judging the performance benefit of the Java 8 rewrite when trying to set baseline expectations during the library's initial development.
ben-manes,caffeine,91a36fb0957c97669989cd383b1dbe1245ac6d10,https://github.com/ben-manes/caffeine/commit/91a36fb0957c97669989cd383b1dbe1245ac6d10,optimize the frequency sketch  In an earlier analysis the block-based sketch was significantly faster than the flat (uniform) one. This was independently confirmed by a C# and Go port  who also observed a 2x speed up. However  when recently adding this benchmark to the CI it showed it as a regression. Therefore some implicit compiler optimizations are now explicit  which allows the block-based sketch to match or exceed the flat-based performance.  - We no longer rely on escape analysis to optimize away the method scoped arrays (count  index). These should have been stack allocated and broken into their components. - The arrays were meant to break a loop data dependency  but it is now faster to keep that. `Math.min` is a single cycle  branch-free instruction that the OOO pipeline seems to prefer. - `increment` is manually loop unrolled like the flat version  which shows a simiar speed up. - Previously  the flat benchmark version implemented the scaffolding interface directly  was pre-allocated  and the init guard was removed. This gave it a large advantage as it improved inlining  branch prediction  etc. The benchmark is now fair. - For jdk11 the block is always faster by at least 10M ops/s. In jdk23 the speedup only occurs as the table size increases  matching the expected gains from better cache effects. It is marginally slower on the small table size due to indexing differences.  The differences are very hardware and compiler dependent  as there are wide variations when running on Intel  Arm  Java versions  and JVMs (Graal vs C2). The user effect will be noise since this was not a performance bottleneck due to the cache's overall design.
ben-manes,caffeine,98895ffbb5e06b345d69fa969fc116c625b70217,https://github.com/ben-manes/caffeine/commit/98895ffbb5e06b345d69fa969fc116c625b70217,add a clairvoyant admission policy to the simulator  Like Bélády's optimal policy for eviction  this estimates the upper bound for an admission policy by using future knowledge to make the best decision. The candidate entry is only admitted if its next access time is less than the victim's.  While the optimal replacement policy is able to dictate the working set  the optimal admission policy is only able to advise whether to keep the given candidate or victim entry. Thus  the eviction policy's choices may result in a sequence where a more naive admission policy outperforms the clairvoyant one. The intent is to show the upper bounds of an adaptive admission policy based on its best guess rather than the upper bound of the overall cache hit rate.
prestodb,presto,b11f358df710e05d78a913f01ce2a12dd9f4e591,https://github.com/prestodb/presto/commit/b11f358df710e05d78a913f01ce2a12dd9f4e591,Update plugin to generate idl thrift file automatically (#25164)  ## Description 1. Automatically generate idl file for 3 classes  taskStatus  taskInfo  and taskUpdateRequest via plugin 2. Requires https://github.com/prestodb/drift/pull/63  ## Motivation and Context <!---Why is this change required? What problem does it solve?--> <!---If it fixes an open issue  please link to the issue here.-->  ## Impact <!---Describe any public API or user-facing feature change or any performance impact-->  ## Test Plan 1.  Build successfully and the generated idl file looks good. <img width="1174" alt="Screenshot 2025-05-20 at 16 16 07" src="https://github.com/user-attachments/assets/3d20b193-9aed-4635-a4f4-f45ccdde8538" />  <img width="1134" alt="Screenshot 2025-05-20 at 17 19 08" src="https://github.com/user-attachments/assets/5fe74591-5f7c-458e-bb25-4616b612ccf0" />  ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.   ``` == NO RELEASE NOTE == ```
prestodb,presto,9f77bed270474fb4f870344735d4624dffd65d4a,https://github.com/prestodb/presto/commit/9f77bed270474fb4f870344735d4624dffd65d4a,Make taskUpdateRequest and taskInfo classes Thrift ready with json fields (#25020)  ## Description 1. We are enabling thrift for task update request  and task info for critical api communication between coordinator and worker. We have two config toggles for task update request sent to worker and the task info returned to coordinator  2. However  there are some classes that are java interface/polymorphic fields. We keep them as json encoding for now and will migrate them in the next step. 3. We are also doing proper change for native worker: #25079  ## Motivation and Context 1. We observed that coordinator can spend too much cpu/heap memory on json serde for taskUpdateRequest.  ## Impact <!---Describe any public API or user-facing feature change or any performance impact-->  ## Test Plan 1. Passed verifier tests  ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.  ``` == RELEASE NOTES ==  General Changes * Improve communication between coordinator and worker with thrift serde. ```
prestodb,presto,60f1ba70c41d5153596d50c36f6bbd3fee363834,https://github.com/prestodb/presto/commit/60f1ba70c41d5153596d50c36f6bbd3fee363834,Add view text hash info to accessControlReferences (#24955)  ## Description <!---Describe your changes in detail--> In addition to the raw SQL string that the user submitted  checkQueryIntegrity needs to have view definitions used inside the of the query to validate against the credentials passed in through the identity.  ## Motivation and Context <!---Why is this change required? What problem does it solve?--> <!---If it fixes an open issue  please link to the issue here.--> This is to address the security vulnerability if a view definition gets changed in between when the approved credential was generated and when the query begins executing.  ## Impact <!---Describe any public API or user-facing feature change or any performance impact--> - Add extra view text fields to AccessControlReferences - change checkQueryintegrity API to take in view text ## Test Plan <!---Please fill in how you tested your change-->  ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.  ``` == RELEASE NOTES ==  General Changes * Add view definitions from Analyzer phase to perform full integrity check on query credentials. * Change checkQueryIntegrity function signature in AccessControl interface to pass in view definitions as params.
prestodb,presto,9f689e91f2102536a9d3116a5855cfc38895cdff,https://github.com/prestodb/presto/commit/9f689e91f2102536a9d3116a5855cfc38895cdff,Add runtime metrics for task time on event loop and log slow execution (#25009)  ## Description 1. Add runtime metrics for task execution time on event loop 2. Add logging for slow execution where the logging threshold can be controlled by config and we will log the query id  task id  and the method name if the runnable is taking too long to finish.  ## Motivation and Context 1. This increases the observability on what can be running slow on event loop  ## Impact <!---Describe any public API or user-facing feature change or any performance impact-->  ## Test Plan 1. running verifier  ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.   ``` == NO RELEASE NOTE == ```
prestodb,presto,931d4a69746f8104a81ff6188e38327a4a8bc91a,https://github.com/prestodb/presto/commit/931d4a69746f8104a81ff6188e38327a4a8bc91a,Check Query integrity in Analyzer Util (#24927)  ## Description <!---Describe your changes in detail--> An ACL check checkQueryIntegrity needs to be moved from DispatchManager to the Analyzer. This PR will pass down the raw query string as well as accessControl down to the Analyzer so that this ACL call can be performed.  ## Motivation and Context <!---Why is this change required? What problem does it solve?--> <!---If it fixes an open issue  please link to the issue here.--> The checkQueryIntegrity call needs to be done after queuing to address a potential security issue with the ACL check. If the query uses views  then the view definition could be changed between dispatch and when view definitions are read in the analyzer. Making this change will reduce the time window of this security gap.  ## Impact <!---Describe any public API or user-facing feature change or any performance impact--> Changes to createQueryExecution and various other interfaces so that accessControl and raw query string can be passed down to Analyzer and AnalyzerUtil.  ## Test Plan <!---Please fill in how you tested your change--> Using debugger in checkAccessPermissions in AnalyzerUtil  the query string and accessControlInfo are passed down properly.  ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.  ``` == RELEASE NOTES ==  General Changes * Improve ACL check by moving checkQueryIntegrity from Dispatch phase to Analyzer phase.
prestodb,presto,a33f73def6f682c48a09a1ba4c78275cf527da8c,https://github.com/prestodb/presto/commit/a33f73def6f682c48a09a1ba4c78275cf527da8c,Re-introduce improving the merging of operator stats (#24921)  ## Description 1. this pr re-introduce the #24414   which cause a sev where written partition was not logged. The bug is a corner case  where while merging only one single non-mergeable operatorInfo  the old code will NOT perform any merge operation (since the add operation will only get invoke when the second operator stats shows up) and give back the operator info itself while #24414 will actually kick off a merge and gives null result. 2. This pr reintroduce #24414 and handles this corner case and also added specific unit tests for this scenario.  ## Motivation and Context 1. re-introduce #24414  ## Impact <!---Describe any public API or user-facing feature change or any performance impact-->  ## Test Plan 1.  verifier runs log written partition correctly: <img width="1469" alt="Screenshot 2025-04-15 at 17 13 08" src="https://github.com/user-attachments/assets/f7c84a8f-7381-411a-95d1-15b075870b83" />   ## Contributor checklist  - [ ] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [ ] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [ ] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [ ] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [ ] Adequate tests were added if applicable. - [ ] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.  ``` == RELEASE NOTES ==  General Changes * Improve how we merge multiple operator stats together. * Improve metrics creation by refactoring local variables to a dedicated class.  ```
prestodb,presto,02a65c0b6f2ed9027b44f657cc87916352c230de,https://github.com/prestodb/presto/commit/02a65c0b6f2ed9027b44f657cc87916352c230de,Reintroduced json_extract to generate canonicalized output (#24879)  ## Description The original pull request [#24614](https://github.com/prestodb/presto/pull/24614) incorrectly compares canonicalizedJsonExtract and legacyJsonCast in the equals function of an object. This issue can be seen in the code [here](https://github.com/prestodb/presto/pull/24614/files#diff-e921c5d186f9d5daa836bc7330f52caf8c1b84d19cf42288d5a8a7c9a6d2a5d5R156).  As a result  whenever a SQL function requires caching  the cache is never hit  leading to the creation of new SQL function objects repeatedly. This behavior eventually causes an OOM error in the JVM metaspace. and eventually this error led to UER SEV.  After the problematic comparison was updated and tested through shadow cluster by @rschlussel   we are confident that the issue has been resolved in this PR. Therefore  we plan to bring back the json canonicalized extract   ## Motivation and Context Reintroduced json_extract to generate canonicalized output  ## Impact <!---Describe any public API or user-facing feature change or any performance impact--> low impact  ## Test Plan <!---Please fill in how you tested your change--> N/A  ## Contributor checklist  - [x] Please make sure your submission complies with our [contributing guide](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md)  in particular [code style](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#code-style) and [commit standards](https://github.com/prestodb/presto/blob/master/CONTRIBUTING.md#commit-standards). - [x] PR description addresses the issue accurately and concisely. If the change is non-trivial  a GitHub Issue is referenced. - [x] Documented new properties (with its default value)  SQL syntax  functions  or other functionality. - [x] If release notes are required  they follow the [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines). - [x] Adequate tests were added if applicable. - [x] CI passed.  ## Release Notes Please follow [release notes guidelines](https://github.com/prestodb/presto/wiki/Release-Notes-Guidelines) and fill in the release notes below.  ``` == NO RELEASE NOTE == ```
prestodb,presto,3e059408ea2e65c590e1242dc7efb2a3a63c43c8,https://github.com/prestodb/presto/commit/3e059408ea2e65c590e1242dc7efb2a3a63c43c8,Include Table Metadata in Operator Logs (#24590)  Summary: Pull Request resolved: https://github.com/prestodb/presto/pull/24590  X-link: https://github.com/facebookexternal/presto-facebook/pull/3126  > Functional Requirement: Log stats in a way such that we can provide Tetris per-Table Warm Storage read metrics (including number of bytes read  and the region of the WS VC we read from)  Adds the Table name and Warm Storage VC fields to the Presto Query Operator Stats logs  which well be set for all Prism Operators which read a Table (and null otherwise).  This is accomplished by adding the query's `planNodeIdMap` to its `QueryCompletedEvent` (in OSS Presto)  and then using this map in `PrismEventListener` to map from Operator stats to information from their respective Plan Nodes.  # Context  Runtime Stats in a Presto feature which conveniently supports aggregating statistics from different sources (in our case  aggregating the number of WS bytes read across all splits by Operator).  In Java  the Runtime Stats already includes the number of WS Bytes read. Becase this diff adds the WS Region we're reading from  and we already know which region the query executes in  this diff is sufficient for logging all the information that Tetris needs.  However  this data is not yet logged in C++. This is tracked here: https://www.internalfb.com/tasks/my_tasks?t=212787929. This diff will be needed in order to provide Tetris everything it needs  and I'll make sure we complete that task shortly.  # Alternatives Considered  There are quite a few places in the code that at first glance appear that they already should provide the information that we need  but just quite don't. That also means there's many different places this change could be made to accomplish the functional requirement.  ## Scope Runtime Stats by Table  My original approach for this: https://www.internalfb.com/diffs/browse?phabricator_diff_id=1099167592009927. "scope" the runtime stats by appending the Table name to the stats themselves.  Even at the time I thought this was a quick-and-dirty hack  but after getting more context on how Runtime Stats are used (in particular  that they are user-facing because they're displayed in the Query UI)  I realized that this approach wasn't acceptable. There just isn't a clean way to make this  a) not hideous  hacky  fragile code (e.g. adding hacks to hide specific Runtime Stats on an adhoc basis  encoding metadata in the Stat name itself  etc) b) presentable to users (since Runtime Stats are displayed in UIs  and therefore are a user-facing feature) c) satisfy the functional requirements (include stats broken down by Table and Region read from)  ## SplitStats  The split stats contain an Info field which includes the Warm Storage file path they read from  which would be sufficient for our purposes!  It would also seem  looking at our code  that we already log SplitStats. However  looking at [Configerator](https://www.internalfb.com/code/fbsource/[3baf3cc56bb1971bfd5c932f63018870821b520f]/fbcode/tupperware/config/presto/include/warehouse_event_listener.cinc?lines=85)  we see it logs to the "blackhole" topic. That is  it's a dummy log. This was set in order to avoid logging too much  so probably shouldn't be messed with.  ## OperatorStats Info/InfoUnion  In addition to the Info field (which may come from one of the Splits)  OperatorStats has the InfoUnion field which should contain all of the Info fields for its corresponding Splits. This would also be sufficient for our purposes.  However  these fields are never set  at least in our codepaths for Prism reads. I didn't look into this enough to be completely confident about why these aren't being populated  but there may be be dead code here.  While this approch is promising  I know there's already concerns around the amount of CPU cycles Coordinators burn manipulating these sets Objects: https://docs.google.com/document/d/1rzJjMcMXebazQy8PEOEAThY4HOl0e3WhA3GIXUKrRDI/edit?tab=t.0. If these stats are being nulled out  then perhaps it's for the best in order to minimize overhead.  Of the alternative approaches I list here  this the second most-promising behind IO Metadata:  ## IO Metadata  https://www.internalfb.com/code/fbsource/fbcode/github/presto-trunk/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryInputMetadata.java  This one is almost perfect: this class includes statistics  broken down by Table  and is included in our Query logs. There's just two problems:  - Currently  we don't actually populate the `statistics` field for this class. This means we can't use this for mapping from Tables to the statistics we need. I haven't tracked this down to determine whether or not this is intentional  but at the very least this would be a larger lift to get working. - This is part of the SPI and OSS Presto. However  we need to be able to log Meta-internal information (e.g. whether the traffic is x-region). This is a solvable problem  but it'd potentially be a hassle to do deal with.  With that being said  if I needed to choose another approach for this diff  I would spend more time trying to make this work.  ## Various Other "Plan Stats" Classes  I spent a while looking at a few promising looking classes  e.g. [TableStats](https://www.internalfb.com/code/fbsource/fbcode/github/presto-trunk/presto-spi/src/main/java/com/facebook/presto/spi/statistics/TableStatistics.java) before realizing that these are actually meant to be use by the Query Planner  and "Stats" specifically refers certain things used to estimate Costs for the Query Planner. This isn't exactly the same type of stat I needed.  However  this distinction becomes a bit blurred with HBO-stuff and when when we set the `confidence` parameter to `1.0`  indicating the stats are a "fact" pulled from actual Runtime Stats. It was tempting to pursue this because these classes already manage a) mapping from physical plan stats to logical plan nodes b) aggregating stats per-node.  Perhaps in an ideal world all of our logging and stats tracking would be coherently unified so that we only computing every stat exactly once  but for now it seemed best to avoid coupling to any of this Planner/HBO stuff.  # Justification for chosen approach  There's two main advantages to this approach:  - Simple (evidenced by the small number of lines changed) - Natural and extensible - While this requires making a change to OSS Presto  that change itself isn't coupled to our specific requirements. Wanting to look up the Plan Nodes in a Query Completed Event handler in order to enrich logs is a reasonable use case. We may even have future use cases which benefit from this.  Differential Revision: D69730285
prestodb,presto,2005b0b6e49f49cb6a85e24faef4f7eecd3503ea,https://github.com/prestodb/presto/commit/2005b0b6e49f49cb6a85e24faef4f7eecd3503ea,Add support for UPDATE in iceberg  This commit allows users to perform row-level updates when using the Iceberg connector with Java-based workers.  This is achieved by improving on the IcebergUpdatablePageSource to implement the updateRows method. The implementation passes a  generated row ID column as a field in the page required by updateRows. Then during updateRows  generated a positionDelete file entry for the row ID  and also writes the row's updated value to a new page sink for the newly updated data.  These new files are then commited in a rowDelta transaction within the Iceberg connector metadata after processing is complete.  Co-Authored-By: Nidhin Varghese <Nidhin.Varghese1@ibm.com> Co-Authored-By: Anoop V S <anoop.v.s@ibm.com>
prestodb,presto,41aba926697e8c14e8c860f18df874f04db351ca,https://github.com/prestodb/presto/commit/41aba926697e8c14e8c860f18df874f04db351ca,Add single node execution  To improve performance for small queries which can be executed within a single node  we introduce single worker execution mode: query will only use one node to execute and plan would be optimized accordingly.
prestodb,presto,f3b2f521a433f4b05334561b665d10981ff1af0f,https://github.com/prestodb/presto/commit/f3b2f521a433f4b05334561b665d10981ff1af0f,Add native plan checker to sidecar plugin  This adds a provider for a native plan checker that will send a plan fragment to the native sidecar where it is validated by performing a conversion to a Velox plan. If the conversion succeeds the query will continue  if it fails then the query will fail with an error from the native sidecar. The provider is added to the native sidecar plugin and is enabled with the config `native-plan-checker.plan-validation-enabled=true` from filename `etc/plan-checker-providers/native-plan-checker.properties`.  See also: prestodb#23649 RFC: https://github.com/prestodb/rfcs/blob/main/RFC-0008-plan-checker.md
prestodb,presto,07bf13ae09618ea51e34f74d8678d7aa19e2d2bc,https://github.com/prestodb/presto/commit/07bf13ae09618ea51e34f74d8678d7aa19e2d2bc,[Iceberg] Add statistics file caching  Adds a new connector-wide cache for statistics files. This prevents additional memory consumption and improves query planning performance by avoiding hits to the file system when generating table statistics.
prestodb,presto,1137a1f005e602054437cb68d7f96047b1e61111,https://github.com/prestodb/presto/commit/1137a1f005e602054437cb68d7f96047b1e61111,Change native spill compression kind to zstd  This helps reduce the spill bytes to speedup spill performance. For exmaple 20240608_210530_00001_iyjad execution time reduced from 12mins to 9 mins. The spilled bytes has been reduced from 2.62TB to 960GB. The total disk write time has been reduced from 12hrs to 4hrs. The max time of a single aggregation operator has been reduced from 2.4mins to 40s. And there is not much change to the compression time. And the disk read time has been reduced from 1.29 days to 14.46 hours. The max time of a single aggregation operator has been reduced from 5 mins to 2 mins. This explains the overall e2e time reduction
prestodb,presto,f87af00a462bbe4cce5e72af9af980732c69ba43,https://github.com/prestodb/presto/commit/f87af00a462bbe4cce5e72af9af980732c69ba43,Fix optimizer performance regression for large IN  This bug occurs due to large IN lists. When computing statistics for queries with this clause  we generate an instance of VariableStatisticEstimate for each expression in the list.  For example  a query with  SELECT ... IN (1  2   ... 10_000);  generates 10k VariableStatisticEstimate instances. Then  for each instance  we sum the statistics and distinct values to get a final result to determine the probability of a value occurring. Creating these VariableStatisticEstimate instances were not the root cause of the issue.  The main problem is that when the DIsjointRangeDomainHistogram was introduced it was designed as immutable. When folding all the instances together through the `reduce()` call at FilterStatsCalculator#L755  it re-creates the internal TreeRangeSet. The set of disjoint ranges is the size of the IN list  so we were re-creating the set 10k times. The first with range a set size of 1  then 2  ... etc up to 10k. So the running time of this ended up being polynomial.  The following change updates the DisjointRangeDomain histogram to use a lazily initialized RangeSet. This prevents incurring the high cost of re-creating the range set for every  new addition to the IN list.  Additionally  the StatisticRange class now serializes to a byte encoded format to decrease the amount of bytes required to serialize plans with many filters. This is mainly useful for when a query has thousands of filters in a complex plan and the filters are applied to the histogram.
prestodb,presto,1e0648e4638775fb53b83c3e7782b4ac2d18a1f4,https://github.com/prestodb/presto/commit/1e0648e4638775fb53b83c3e7782b4ac2d18a1f4,Add histograms for optimizer cost calculation  This commits provides two critical changes:  1. Adds a new enum value to ColumnStatisticType: Histogram. 2. Utilizes the new histograms in optimizer's cost calculations  Implementation details below:  With this change  a new column statistic type for histograms is introduced. In addition  a new SPI class `ConnectorHistogram` is also introduced. This interface is designed to be able to be implemented by either the connectors or in the main presto codebase. This should allow connectors to return utilize histogram statistics in any format regardless of the source.  The API is straightforward and includes 2 methods.  - cumulativeProbability(double value  boolean inclusive): -> CDF function - inverseCumulativeProbability(double probability) -> inverse CDF function  A reference implementation is provided inside of UniformDistributionHistogram. This implementation results in the same logic and same plans as the previous cost-based calculations. The math ends up being the same  but just utilizing the histogram API.  Additionally  to propagate histogram information further up into a plan  another implementation of histograms is provided inside of DisjointRangeDomainHistogram. This class is used to bound a source histogram with a given domain as additional filters may be applied further up in the plan.  Previously all cost calculations were performed inside of the ComparisonStatsCalculator using logic from the StatisticRange class to calculate the filter factors of overlapping and intersecting ranges. This change introduces cost calculation using the new histogram model and API. The core of the filter proportion calculation logic exists in the new HistogramCalculator utility class. If the underlying Histogram implementation is swapped from the UniformDistributionHistogram  then the stats calculator will calculate the costs using the histogram information.
questdb,questdb,d470b048c34b8020d3d2cad9d427882e15989f76,https://github.com/questdb/questdb/commit/d470b048c34b8020d3d2cad9d427882e15989f76,perf(sql): support for Oracle-style SQL hints (#5620)
questdb,questdb,61c4735edc716bdf2f2340e9475c7362a18b78fe,https://github.com/questdb/questdb/commit/61c4735edc716bdf2f2340e9475c7362a18b78fe,perf(sql): expanded fast-path support for non-keyed ASOF JOINs (#5553)
questdb,questdb,fe501c849c39eac2cce23083f56105e35f5d20dd,https://github.com/questdb/questdb/commit/fe501c849c39eac2cce23083f56105e35f5d20dd,perf(pgwire): improve performance of batch inserts via PostgreSQL driver (#5564)
questdb,questdb,ae02555ff0bbd40735764f2c8ce23dc03e04773e,https://github.com/questdb/questdb/commit/ae02555ff0bbd40735764f2c8ce23dc03e04773e,perf(sql): breaking change 💥 - parallel SAMPLE BY with time zone execution (#5493)
questdb,questdb,eda7de2438ef2a5e0e2ae557d5e7a047c5559c20,https://github.com/questdb/questdb/commit/eda7de2438ef2a5e0e2ae557d5e7a047c5559c20,perf(sql): speed-up of simple projections (#5529)
questdb,questdb,5fa15b60a0a87d661b34f2c451689e25ba741e1a,https://github.com/questdb/questdb/commit/5fa15b60a0a87d661b34f2c451689e25ba741e1a,perf(sql): stricter column pre-touch in parallel filter queries (#5481)
questdb,questdb,c83d63d57bf80e08aa4681cbcc1e462d12d5f570,https://github.com/questdb/questdb/commit/c83d63d57bf80e08aa4681cbcc1e462d12d5f570,perf(sql): speed up execution of window functions and joins on Windows platform (#5300)
questdb,questdb,7805cf5970ac578d5b2c654e6675e20df736d354,https://github.com/questdb/questdb/commit/7805cf5970ac578d5b2c654e6675e20df736d354,perf(sql): performance improvement in length(varchar) function (#5188)
questdb,questdb,8b2df2ba31ff20d86c3e2b9d71fa874972028729,https://github.com/questdb/questdb/commit/8b2df2ba31ff20d86c3e2b9d71fa874972028729,perf(core): reduce CPU idle load (#5190)
questdb,questdb,d80c827df4fa61b5e469131f6a2acc17c31fa078,https://github.com/questdb/questdb/commit/d80c827df4fa61b5e469131f6a2acc17c31fa078,perf(sql): introduce fast path for ordered and limited queries over single long column (#5152)
questdb,questdb,7112d4d5c099b8f1e16adbdca9e2f7cb9ae97106,https://github.com/questdb/questdb/commit/7112d4d5c099b8f1e16adbdca9e2f7cb9ae97106,perf(sql): improve performance of queries with negative limits and existing order by clauses (#5148)
questdb,questdb,a0547a14aea79dfcb1dfa8fa7e2fbab41cb7617d,https://github.com/questdb/questdb/commit/a0547a14aea79dfcb1dfa8fa7e2fbab41cb7617d,perf(sql): speed up negative limit queries with a wildcard (#5139)
questdb,questdb,c26f1b6c6f92fea2a7b2f4ab9aa0a69f50fbe97b,https://github.com/questdb/questdb/commit/c26f1b6c6f92fea2a7b2f4ab9aa0a69f50fbe97b,perf(sql): use faster hash table in window functions (#5135)
questdb,questdb,14b81ebb8a1fb12124567d9927452dbbcee42c2d,https://github.com/questdb/questdb/commit/14b81ebb8a1fb12124567d9927452dbbcee42c2d,perf(core): optimize StringSink buffer reuse - garbage elimination (#5127)
questdb,questdb,0068007fe068ce0ee38dbf84eac7aba6b5fd0d17,https://github.com/questdb/questdb/commit/0068007fe068ce0ee38dbf84eac7aba6b5fd0d17,perf(core): replace unconditional txn sync with a conditional one. 10x improvement of ingress perf for small transactions (#5115)
questdb,questdb,93d80819210bde411b4888b14ced4da6268d6ed9,https://github.com/questdb/questdb/commit/93d80819210bde411b4888b14ced4da6268d6ed9,perf(sql): parallelise correlation (#5079)
questdb,questdb,2899a028b444ffd11a6795f0f36f2e4635f1828b,https://github.com/questdb/questdb/commit/2899a028b444ffd11a6795f0f36f2e4635f1828b,perf(sql): parallelise sample and population covariance (#5074)
questdb,questdb,d18d3bb18eea62af1081e5d93080dc6e6aaeed1f,https://github.com/questdb/questdb/commit/d18d3bb18eea62af1081e5d93080dc6e6aaeed1f,perf(sql): parallelise sample and population standard deviation (#5075)
questdb,questdb,6d7488ccde2a2b26cc93aee82019adbf9032fe7f,https://github.com/questdb/questdb/commit/6d7488ccde2a2b26cc93aee82019adbf9032fe7f,perf(sql): parallelise sample and population variance  (#5072)
questdb,questdb,a198edee7cc9814f78ccca25a4c829c82eca929a,https://github.com/questdb/questdb/commit/a198edee7cc9814f78ccca25a4c829c82eca929a,perf(sql): reduce memory footprint of parallel aggregate functions (#5046)
questdb,questdb,cc018317021bf81a28c1c59c99bc16a183e32a77,https://github.com/questdb/questdb/commit/cc018317021bf81a28c1c59c99bc16a183e32a77,perf(sql): avoid redundant bytecode generation in parallel GROUP BY (#5023)
questdb,questdb,54bb689448cf185cfedb5cf178f6348fb9aaafa1,https://github.com/questdb/questdb/commit/54bb689448cf185cfedb5cf178f6348fb9aaafa1,perf(sql): reduce unnecessary TableReader partition reloads (#5010)
questdb,questdb,c08f276f23d9992aed569b2f1c604f6863a94722,https://github.com/questdb/questdb/commit/c08f276f23d9992aed569b2f1c604f6863a94722,perf(core): introduce static metadata cache (#4848)
questdb,questdb,0b151e69815abafb18d17a4b40240ccbd4b7bd56,https://github.com/questdb/questdb/commit/0b151e69815abafb18d17a4b40240ccbd4b7bd56,perf(sql): keyed ASOF JOIN optimizations (#4916)
questdb,questdb,4d83e76996289dfedd6126a52899b06c52494359,https://github.com/questdb/questdb/commit/4d83e76996289dfedd6126a52899b06c52494359,perf(sql): optimize memory usage in order by (#4888)
questdb,questdb,8df068d2e5c1a716b1aaa1e63ae32683749e0e42,https://github.com/questdb/questdb/commit/8df068d2e5c1a716b1aaa1e63ae32683749e0e42,perf(sql): optimize memory usage in hash joins (#4881)
questdb,questdb,710496e5d95865ed9e19e75bea1caa81441d86c0,https://github.com/questdb/questdb/commit/710496e5d95865ed9e19e75bea1caa81441d86c0,perf(sql): JIT-optimise IN timestamp_literal operator (#4750)
questdb,questdb,e56004ff121d73f7a47ea90a4b4f26dc9346a244,https://github.com/questdb/questdb/commit/e56004ff121d73f7a47ea90a4b4f26dc9346a244,perf(sql): use radix sort to speed up single integer column order by (#4891)
questdb,questdb,635d9bb0d5277fd099bfc62778c033a4be88b28e,https://github.com/questdb/questdb/commit/635d9bb0d5277fd099bfc62778c033a4be88b28e,perf(sql): optimize like/ilike/regexp functions on symbol column (#4871)
questdb,questdb,0e3b6ebfbca45f8db48af4d30eccaea507a8aca8,https://github.com/questdb/questdb/commit/0e3b6ebfbca45f8db48af4d30eccaea507a8aca8,perf(sql): optimize queries with virtual columns  filter  and limit (#4858)
questdb,questdb,3ec640a52dc51f745af8767b084dc6fadeb6deb9,https://github.com/questdb/questdb/commit/3ec640a52dc51f745af8767b084dc6fadeb6deb9,perf(sql): speed up regexp_replace(varchar) for simple patterns (#4668)
questdb,questdb,9ee0380e07d91e669093ba6de371be4f3ccdd64f,https://github.com/questdb/questdb/commit/9ee0380e07d91e669093ba6de371be4f3ccdd64f,perf(sql): speed up like/ilike operator on symbol column (#4794)
questdb,questdb,26b844d1b1b6925a0e83ef5f7b23d291bbc701bf,https://github.com/questdb/questdb/commit/26b844d1b1b6925a0e83ef5f7b23d291bbc701bf,perf(core): speedup small transaction writing 50-100% (#4793)
questdb,questdb,20ad756e96d21e22c26104cee9eaa25b4b16747c,https://github.com/questdb/questdb/commit/20ad756e96d21e22c26104cee9eaa25b4b16747c,perf(sql): more efficient pattern lookup for short ASCII patterns (#4706)
questdb,questdb,7feee4a82461d746f002207fe51cebb212ec24dc,https://github.com/questdb/questdb/commit/7feee4a82461d746f002207fe51cebb212ec24dc,perf(sql): speed up self-join queries on symbol columns (#4720)
questdb,questdb,84a4854357183498e36aabfd68f9d82086520862,https://github.com/questdb/questdb/commit/84a4854357183498e36aabfd68f9d82086520862,perf(sql): optimize l2price with up to 5 argument pairs (#4728)
questdb,questdb,2246e559966f5e94e2d9bea2aec0e46c2b91adbf,https://github.com/questdb/questdb/commit/2246e559966f5e94e2d9bea2aec0e46c2b91adbf,perf(sql): optimise aggregate query with LAST function on designated timestamp (#4676)
questdb,questdb,753f2045b0a2f6d34b7f35cd38b7ce18a2b0f9bc,https://github.com/questdb/questdb/commit/753f2045b0a2f6d34b7f35cd38b7ce18a2b0f9bc,perf(sql):  JIT-optimise `IN(numeric values)` operator (#4661)
questdb,questdb,ca2369cefa972bd85f6d5916a47d7826c7e3e98d,https://github.com/questdb/questdb/commit/ca2369cefa972bd85f6d5916a47d7826c7e3e98d,perf(sql): support ksum and nsum functions in parallel GROUP BY (#4682)
questdb,questdb,b227c4446a6e319f9969a44a12f43d17fd7f82d6,https://github.com/questdb/questdb/commit/b227c4446a6e319f9969a44a12f43d17fd7f82d6,perf(sql): support timestamp literals in JIT compiler (#4658)
questdb,questdb,f7c8afda6e525fff627526755eddebb87a45e507,https://github.com/questdb/questdb/commit/f7c8afda6e525fff627526755eddebb87a45e507,perf(sql): adaptive work stealing for parallel query execution on multicore machines (#4542)
apache,hadoop,949292eac6d26ddf5713bf2783d09c3317ffe695,https://github.com/apache/hadoop/commit/949292eac6d26ddf5713bf2783d09c3317ffe695,HADOOP-19571. Improve PrometheusMetricsSink#normalizeName performance (#7692) Contributed by Ivan Andika.  * HADOOP-19571. Improve PrometheusMetricsSink#normalizeName performance  Reviewed-by: Akira Ajisaka <aajisaka@apache.org> Signed-off-by: Shilun Fan <slfan1989@apache.org>
apache,hadoop,0e208c8abd982a658ecace110038a00c11ee41dc,https://github.com/apache/hadoop/commit/0e208c8abd982a658ecace110038a00c11ee41dc,HADOOP-19256. S3A: Support Conditional Overwrites  Amazon S3 now supports conditional overwrites  which can be be used when creating files through the createFile() API with two new builder options:  fs.option.create.conditional.overwrite:  Write if and only if there is no object at the target path. This is an atomic PUT-no-overwrite  checked in close()  not create().  fs.option.create.conditional.overwrite.etag  Write a file if and only if it is overwriting a file with a specific etag.  If the "fs.s3a.performance.flags" enumeration includes the flag "create" then file creation will use conditional creation to detect and reject overwrites.  The configuration option "fs.s3a.create.conditional.enabled" can be set to false to disable these features on third-party stores.  Contributed by Diljot Grewal  Saikat Roy and Steve Loughran
apache,hadoop,a314a1d71488f782749638ba27098515bbb3a8a4,https://github.com/apache/hadoop/commit/a314a1d71488f782749638ba27098515bbb3a8a4,YARN-11798. Precheck request separately to avoid redundant node checks and optimize performance for global scheduler. (#7516) Contributed by Tao Yang.  * YARN-11798. Precheck request separately to avoid redundant node checks and optimize performance for global scheduler.  * Add node for recording scheduler activities in RegularContainerAllocator#preCheckRequest to fix UT.  Signed-off-by: Shilun Fan <slfan1989@apache.org>
apache,hadoop,f0368bb2372f0fd2d63295eed5eec43c2afd3747,https://github.com/apache/hadoop/commit/f0368bb2372f0fd2d63295eed5eec43c2afd3747,HDFS-17405. [FGL] Using different metric name to trace performance for FGL and Global lock (#6600)
apache,hadoop,ea6e0f7cd58d0129897dfc7870aee188be80a904,https://github.com/apache/hadoop/commit/ea6e0f7cd58d0129897dfc7870aee188be80a904,HADOOP-19221. S3A: Unable to recover from failure of multipart block upload attempt (#6938)   This is a major change which handles 400 error responses when uploading large files from memory heap/buffer (or staging committer) and the remote S3 store returns a 500 response from a upload of a block in a multipart upload.  The SDK's own streaming code seems unable to fully replay the upload; at attempts to but then blocks and the S3 store returns a 400 response  "Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (Service: S3  Status Code: 400...)"  There is an option to control whether or not the S3A client itself attempts to retry on a 50x error other than 503 throttling events (which are independently processed as before)  Option:  fs.s3a.retry.http.5xx.errors Default: true  500 errors are very rare from standard AWS S3  which has a five nines SLA. It may be more common against S3 Express which has lower guarantees.  Third party stores have unknown guarantees  and the exception may indicate a bad server configuration. Consider setting fs.s3a.retry.http.5xx.errors to false when working with such stores.  Signification Code changes:  There is now a custom set of implementations of software.amazon.awssdk.http.ContentStreamProvidercontent in the class org.apache.hadoop.fs.s3a.impl.UploadContentProviders.  These:  * Restart on failures * Do not copy buffers/byte buffers into new private byte arrays  so avoid exacerbating memory problems..  There new IOStatistics for specific http error codes -these are collected even when all recovery is performed within the SDK.  S3ABlockOutputStream has major changes  including handling of Thread.interrupt() on the main thread  which now triggers and briefly awaits cancellation of any ongoing uploads.  If the writing thread is interrupted in close()  it is mapped to an InterruptedIOException. Applications like Hive and Spark must catch these after cancelling a worker thread.  Contributed by Steve Loughran
apache,hadoop,33c9ecb6521ca98b76123c91669be6a6c7833060,https://github.com/apache/hadoop/commit/33c9ecb6521ca98b76123c91669be6a6c7833060,HADOOP-19249. KMSClientProvider raises NPE with unauthed user (#6984)   KMSClientProvider raises a NullPointerException when an unauthorised user tries to perform the key operation  Contributed by Dhaval Shah
apache,hadoop,321a6cc55ed2df5222bde7b5c801322e8cf68203,https://github.com/apache/hadoop/commit/321a6cc55ed2df5222bde7b5c801322e8cf68203,HADOOP-19072. S3A: expand optimisations on stores with "fs.s3a.performance.flags" for mkdir (#6543)   If the flag list in fs.s3a.performance.flags includes "mkdir" then the safety check of a walk up the tree to look for a parent directory  -done to verify a directory isn't being created under a file- are skipped.  This saves the cost of multiple list operations.  Contributed by Viraj Jasani
apache,hadoop,a5806a9e7bc6d018de84e6511f10c359f110f78c,https://github.com/apache/hadoop/commit/a5806a9e7bc6d018de84e6511f10c359f110f78c,HADOOP-19161. S3A: option "fs.s3a.performance.flags" to take list of performance flags (#6789)    1. Configuration adds new method `getEnumSet()` to get a set of enum values from a configuration string. <E extends Enum<E>> EnumSet<E> getEnumSet(String key  Class<E> enumClass  boolean ignoreUnknown)  Whitespace is ignored  case is ignored and the value "*" is mapped to "all values of the enum". If "ignoreUnknown" is true then when parsing  unknown values are ignored. This is recommended for forward compatiblity with later versions.  2. This support is implemented in org.apache.hadoop.fs.s3a.impl.ConfigurationHelper -it can be used elsewhere in the hadoop codebase.  3. A new private FlagSet class in hadoop common manages a set of enum flags.  It implements StreamCapabilities and can be probed for a specific option being set (with a prefix)   S3A adds an option fs.s3a.performance.flags which builds a FlagSet with enum type PerformanceFlagEnum  * which initially contains {Create  Delete  Mkdir  Open} * the existing fs.s3a.create.performance option sets the flag "Create". * tests which configure fs.s3a.create.performance MUST clear fs.s3a.performance.flags in test setup.  Future performance flags are planned  with different levels of safety and/or backwards compatibility.  Contributed by Steve Loughran
apache,pulsar,1220951ac74fb4742abbbd331d6e751234c47015,https://github.com/apache/pulsar/commit/1220951ac74fb4742abbbd331d6e751234c47015,[improve][client][PIP-389] Add a producer config to improve compression performance (#23525)  PIP: https://github.com/apache/pulsar/pull/23526 ### Motivation  The motivation of this PIP is to provide a way to improve the compression performance by skipping the compression of small messages. We want to add a new configuration compressMinMsgBodySize to the producer configuration. This configuration will allow the user to set the minimum size of the message body that will be compressed. If the message body size is less than the compressMinMsgBodySize  the message will not be compressed.
apache,pulsar,3c2ec2bf8bfd94eded46b42c5089dd8321afd096,https://github.com/apache/pulsar/commit/3c2ec2bf8bfd94eded46b42c5089dd8321afd096,[improve][broker] Improve Consumer.equals performance (#23864)
apache,pulsar,d377bc9d7321a66201a301b6887fb1fea3ef8820,https://github.com/apache/pulsar/commit/d377bc9d7321a66201a301b6887fb1fea3ef8820,[improve][client] PIP-393: Improve performance of Negative Acknowledgement (#23600)  Co-authored-by: Lari Hotari <lhotari@apache.org>
apache,pulsar,73433cd06e65ce5e194372a657c5a414e820138b,https://github.com/apache/pulsar/commit/73433cd06e65ce5e194372a657c5a414e820138b,[improve] [broker] Optimize performance for checking max topics when the topic is a system topic (#23185)
apache,pulsar,1db3c5fddce45919c6cac3b5a10030183eed3d5c,https://github.com/apache/pulsar/commit/1db3c5fddce45919c6cac3b5a10030183eed3d5c,[improve][misc] Optimize TLS performance by omitting extra buffer copies (#23115)
apache,pulsar,e9deb408eaed2c04e30a27be5fba130f5d4e94b7,https://github.com/apache/pulsar/commit/e9deb408eaed2c04e30a27be5fba130f5d4e94b7,[improve][misc] Improve AES-GCM cipher performance (#23122)
apache,pulsar,77b6378ae8b9ac83962f71063ad44d6ac57f8e32,https://github.com/apache/pulsar/commit/77b6378ae8b9ac83962f71063ad44d6ac57f8e32,[improve][broker] Optimize the performance of individual acknowledgments (#23072)
quarkusio,quarkus,2a10bb3cfed0db3e1d9d2da7038efdc8bc9e1672,https://github.com/quarkusio/quarkus/commit/2a10bb3cfed0db3e1d9d2da7038efdc8bc9e1672,Make Stork optional for REST Client  If you want to use Stork  you now have to add the quarkus-smallrye-stork extension  which IMHO makes perfect sense.  This avoids initializing Stork for every application using the REST Client.  Fixes #47337
quarkusio,quarkus,751e7ee760505d5e5182c26602669c02240f3f4b,https://github.com/quarkusio/quarkus/commit/751e7ee760505d5e5182c26602669c02240f3f4b,Disable checks related to unsupported bytecode enhancement  And switch back to the legacy behavior of "hoping for the best".  In Quarkus  we expect model classes to be enhanced. Lack of enhancement could lead to many problems  from bad performance  to Quarkus-specific optimizations causing errors/data loss  to incorrect generated bytecode (references to non-existing methods).  So  in ORM 6.6 we've introduced checks to detect cases where bytecode enhancement is not possible  so that developers can fix their apps.  We had a rough start  the checks were under/over-reporting  leading to bug reports  so we fixed the checks.  We're now doing the best we can  and it turns out the checks still have have many false positives -- in particular they assume property access for mapped-superclasses and embeddables  because the access type cannot be determined locally for those  which leads to reporting some very valid setups as invalid.  We've tried our best  and it turns out the remedy is worse than the disease. Let's disable these checks.  Longer-term  the solution is just to make bytecode enhancement work even in these cases: https://hibernate.atlassian.net/browse/HHH-18825
quarkusio,quarkus,bd1edf9eecbdfc8bdf0045d255cfd147fb555a51,https://github.com/quarkusio/quarkus/commit/bd1edf9eecbdfc8bdf0045d255cfd147fb555a51,Merge pull request #47146 from alesj/rci1  Remove RoutingContextInterceptor from superfluousInterceptors
quarkusio,quarkus,9397b645888d7d53db281a5a688c7ee8936af131,https://github.com/quarkusio/quarkus/commit/9397b645888d7d53db281a5a688c7ee8936af131,Remove RoutingContextInterceptor from superfluousInterceptors  so we don't produce a warning
quarkusio,quarkus,387bec39702adb1dd0354706012c505d6542dd65,https://github.com/quarkusio/quarkus/commit/387bec39702adb1dd0354706012c505d6542dd65,Remove Unnecessary Gradle Test Task Dependencies  The `quarkusIntTest` and `quarkusNativeTest` tasks do not use the output of the test task  so these tasks should not specify the `test` task as a dependency.  Instead  these tasks now have a `shouldRunAfter` relationship to the test task  because ideally unit tests should run before performing heavier testing.  Fixes #44383
quarkusio,quarkus,064f4c80536fe98202a6976b6d1319355db6e198,https://github.com/quarkusio/quarkus/commit/064f4c80536fe98202a6976b6d1319355db6e198,Avoid repeatedly creating same DotNames in ResteasyReactiveProcessor  This is a follow-up of https://github.com/quarkusio/quarkus/pull/46464.  Given the purpose of the PR was to improve dev mode reload performance  I think it's worth avoid the extra allocations.
quarkusio,quarkus,b1da8300c1349dff310c439087199e90b0ae5b5b,https://github.com/quarkusio/quarkus/commit/b1da8300c1349dff310c439087199e90b0ae5b5b,Merge pull request #46429 from michalvavrik/feature/fix-form-auth-logout-logic  Provide a reliable way to perform form-based authentication logout
quarkusio,quarkus,d8daa7163caab19e7e4680a44fd06cc61c7a4e71,https://github.com/quarkusio/quarkus/commit/d8daa7163caab19e7e4680a44fd06cc61c7a4e71,Simplify and fix building the list of framework endpoints  We already have the full path at some point so there's really no need to rebuild it once again  especially since the code was incorrect as it was adding the /q/ prefix always if it wasn't there in the path.  It's perfectly valid to not have /q in the path  for instance if you set quarkus.smallrye-health.root-path to /something/else.  I added a new test based on the report in addition to the ones we added recently for related fixes.  Fixes #46040
quarkusio,quarkus,456043a6136f258830b61881aaef8e4c812cb18b,https://github.com/quarkusio/quarkus/commit/456043a6136f258830b61881aaef8e4c812cb18b,Merge pull request #45861 from michalvavrik/feature/fix-grpc-security-with-diff-root-path  gRPC: Perform authentication when gRPC server runs on the same server and root path is different than '/'
quarkusio,quarkus,4ba3a469181b17f9dcf3f8f39a2ae0df06b58e81,https://github.com/quarkusio/quarkus/commit/4ba3a469181b17f9dcf3f8f39a2ae0df06b58e81,Don't allow requests into restarting application  If the app is restarting then we should not short-circuit the hot reload handler/scan lock logic.  This is not perfect  as there will always be a possible race  but makes it much less likely a request will hit a torn down app.
quarkusio,quarkus,f4a406adb96920b135b3a8a4eb4bd83e05980fbc,https://github.com/quarkusio/quarkus/commit/f4a406adb96920b135b3a8a4eb4bd83e05980fbc,Merge pull request #45311 from geoand/RequestMapper-improvement  Slightly improve performance of RequestMapper construction
quarkusio,quarkus,9849effa0ea48d711a803dc6b993d23de80ae476,https://github.com/quarkusio/quarkus/commit/9849effa0ea48d711a803dc6b993d23de80ae476,Slightly improve performance of RequestMapper construction
quarkusio,quarkus,a799237de90be77ecfdb6fa9b5cac5b740e4e308,https://github.com/quarkusio/quarkus/commit/a799237de90be77ecfdb6fa9b5cac5b740e4e308,Replace reflection with MethodHandle in  DefaultInstanceFactory  This shouldn't really change anything in practice  but MethodHandles are supposed to perform slightly better than old school reflection
quarkusio,quarkus,0aff1b5ffc4d0d2a75a1b1e53ad699bd6d40bf2d,https://github.com/quarkusio/quarkus/commit/0aff1b5ffc4d0d2a75a1b1e53ad699bd6d40bf2d,Merge pull request #44817 from mkouba/issue-44797  Qute: fix regression in perf optimization of ReflectionValueResolver
quarkusio,quarkus,06f4006ebdb0f4f38aeeb423950810badc50af66,https://github.com/quarkusio/quarkus/commit/06f4006ebdb0f4f38aeeb423950810badc50af66,Qute: fix perf optimization in ReflectionValueResolver  - fixes #44797
quarkusio,quarkus,6a4fbd63b9a939b8b8fad4035ddca96c8da7c307,https://github.com/quarkusio/quarkus/commit/6a4fbd63b9a939b8b8fad4035ddca96c8da7c307,Cache: fix CacheInterceptor in case of non-ArC interceptor bindings  RESTEasy Classic's MP RestClient implementation produces annotations at runtime  so they are not created by ArC and therefore don't extend `AbstractAnnotationLiteral`. At the same time  that implementation produces an `ArcInvocationContext` and puts interceptor bindings into its context map under the ArC key.  Some places may expect that an `ArcInvocationContext` would always contain ArC-created `AbstractAnnotationLiteral` instances  but alas  per the description above  that is not the case.  There are multiple options for fixing that collision. My preferred one would be to get rid of `AbstractAnnotationLiteral` and treat all annotations uniformly. That unfortunately has negative performance implications on the `CacheInterceptor`  so is not an option yet [1].  This commit chooses another path: it modifies the only place in Quarkus that actually depends on `AbstractAnnotationLiteral` to check whether the `Set<AbstractAnnotationLiteral>` actually contains instances of `AbstractAnnotationLiteral`. I hope that before more places in Quarkus start depending on `AbstractAnnotationLiteral`  we can get rid of it.  This commit only checks the first annotation in the set  because if the bindings come from RESTEasy Classic  then none of them are instances of `AbstractAnnotationLiteral`  and if they come from ArC  then all of them are instances of `AbstractAnnotationLiteral`.  [1] The performance issue (JDK-8180450) is fixed in JDK 23 and has not been backported to any LTS release as of this writing.
quarkusio,quarkus,d211835606fea19e6ec510a4191604e1d3569dcd,https://github.com/quarkusio/quarkus/commit/d211835606fea19e6ec510a4191604e1d3569dcd,Add VertxContextSupport#executeBlocking()  - to improve WS next UX; i.e to run blocking code in HttpUpgradeCheck#perform() easily
quarkusio,quarkus,d3d92be049ba5ed06b6ae35fca0161fe32bbca68,https://github.com/quarkusio/quarkus/commit/d3d92be049ba5ed06b6ae35fca0161fe32bbca68,Qute: IfSectionHelper - improve performance for common use case
quarkusio,quarkus,c96ef2a73dd34e858b8966a5674af4ed8de9ae50,https://github.com/quarkusio/quarkus/commit/c96ef2a73dd34e858b8966a5674af4ed8de9ae50,Merge pull request #42773 from peuBouzon/fix-2161  Remove jakarta.json.Json usage for performance reasons
quarkusio,quarkus,35eddccd7eb095cb0e52a3b88055756b4f3e17c1,https://github.com/quarkusio/quarkus/commit/35eddccd7eb095cb0e52a3b88055756b4f3e17c1,remove jakarta.json.Json usage for performance reasons
quarkusio,quarkus,65712cea1913eae0bdf9fdca7381f9f083755d91,https://github.com/quarkusio/quarkus/commit/65712cea1913eae0bdf9fdca7381f9f083755d91,generate jackson serializers  fix object mapper generation for all primitive types  implement nested types in generated object mapper  implement SecureField annotation support  avoid generating mapper for pojo with unknown jackson annotations  implement collections serialization  fix all tests in rest-jackson module  wip  wip  refactor and simplification  performance tuning  make reflection-free serializers generation opt-in  add @Produces annotation to SimpleJsonResource rest endpoints where appropriate  generate serializers for returned types from rest methods regardless if the @Produces annotation is present or not  wip  add javadoc
quarkusio,quarkus,44e1cfeee55271fe6ddbd8f022c2c66f89ef9eb1,https://github.com/quarkusio/quarkus/commit/44e1cfeee55271fe6ddbd8f022c2c66f89ef9eb1,Close the deployment CL in the creator when augmenting  Currently  we pass the CL to something that will close it  rather than controlling the lifecycle in the creator method.  This is an issue in native ITs:  java.lang.RuntimeException: java.lang.IllegalStateException: This class loader has been closed at io.quarkus.test.junit.QuarkusIntegrationTestExtension.throwBootFailureException(QuarkusIntegrationTestExtension.java:373) at io.quarkus.test.junit.QuarkusIntegrationTestExtension.beforeEach(QuarkusIntegrationTestExtension.java:117) at java.base/java.util.ArrayList.forEach(ArrayList.java:1511) at java.base/java.util.ArrayList.forEach(ArrayList.java:1511) Caused by: java.lang.IllegalStateException: This class loader has been closed at io.quarkus.bootstrap.classloading.QuarkusClassLoader.ensureOpen(QuarkusClassLoader.java:716) at io.quarkus.bootstrap.classloading.QuarkusClassLoader.loadClass(QuarkusClassLoader.java:495) at java.base/java.lang.Class.forName0(Native Method) at java.base/java.lang.Class.forName(Class.java:467) at io.quarkus.runner.bootstrap.AugmentActionImpl.performCustomBuild(AugmentActionImpl.java:158) at io.quarkus.test.junit.IntegrationTestUtil.handleDevServices(IntegrationTestUtil.java:297) at io.quarkus.test.junit.QuarkusIntegrationTestExtension.doProcessStart(QuarkusIntegrationTestExtension.java:199) at io.quarkus.test.junit.QuarkusIntegrationTestExtension.ensureStarted(QuarkusIntegrationTestExtension.java:169) at io.quarkus.test.junit.QuarkusIntegrationTestExtension.beforeAll(QuarkusIntegrationTestExtension.java:130)  Related to #41233
quarkusio,quarkus,0f64a36669caca4793af5a441925da6356b5a014,https://github.com/quarkusio/quarkus/commit/0f64a36669caca4793af5a441925da6356b5a014,WebSockets Next: fix the default unhandled-failure-strategy  - do not attempt to close a closed connection and get rid of superfluous WARN message
quarkusio,quarkus,9f9b4546b3cd1054d18adf6b367606914807c2c1,https://github.com/quarkusio/quarkus/commit/9f9b4546b3cd1054d18adf6b367606914807c2c1,Merge pull request #41295 from mcruzdev/feature/micrometer-perf  Micrometer performance - use Meter.MeterProvider
quarkusio,quarkus,11643b97ac00abd4f034fb9dfbe6bb26fd2caee6,https://github.com/quarkusio/quarkus/commit/11643b97ac00abd4f034fb9dfbe6bb26fd2caee6,Increase Micrometer performance using Meter.MeterProvider
elastic,logstash,7f7af057f06651799068803fa65fb89172a25d72,https://github.com/elastic/logstash/commit/7f7af057f06651799068803fa65fb89172a25d72,Feature: health report api (#16520) (#16523)  * [health] bootstrap HealthObserver from agent to API (#16141)  * [health] bootstrap HealthObserver from agent to API  * specs: mocked agent needs health observer  * add license headers  * Merge `main` into `feature/health-report-api` (#16397)  * Add GH vault plugin bot to allowed list (#16301)  * regenerate webserver test certificates (#16331)  * correctly handle stack overflow errors during pipeline compilation (#16323)  This commit improves error handling when pipelines that are too big hit the Xss limit and throw a StackOverflowError. Currently the exception is printed outside of the logger  and doesn’t even show if log.format is json  leaving the user to wonder what happened.  A couple of thoughts on the way this is implemented:  * There should be a first barrier to handle pipelines that are too large based on the PipelineIR compilation. The barrier would use the detection of Xss to determine how big a pipeline could be. This however doesn't reduce the need to still handle a StackOverflow if it happens. * The catching of StackOverflowError could also be done on the WorkerLoop. However I'd suggest that this is unrelated to the Worker initialization itself  it just so happens that compiledPipeline.buildExecution is computed inside the WorkerLoop class for performance reasons. So I'd prefer logging to not come from the existing catch  but from a dedicated catch clause.  Solves #16320  * Doc: Reposition worker-utilization in doc (#16335)  * settings: add support for observing settings after post-process hooks (#16339)  Because logging configuration occurs after loading the `logstash.yml` settings  deprecation logs from `LogStash::Settings::DeprecatedAlias#set` are effectively emitted to a null logger and lost.  By re-emitting after the post-process hooks  we can ensure that they make their way to the deprecation log. This change adds support for any setting that responds to `Object#observe_post_process` to receive it after all post-processing hooks have been executed.  Resolves: elastic/logstash#16332  * fix line used to determine ES is up (#16349)  * add retries to snyk buildkite job (#16343)  * Fix 8.13.1 release notes (#16363)  make a note of the fix that went to 8.13.1: #16026  Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com>  * Update logstash_releases.json (#16347)  * [Bugfix] Resolve the array and char (single | double quote) escaped values of ${ENV} (#16365)  * Properly resolve the values from ENV vars if literal array string provided with ENV var.  * Docker acceptance test for persisting  keys and use actual values in docker container.  * Review suggestion.  Simplify the code by stripping whitespace before `gsub`  no need to check comma and split.  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  ---------  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  * Doc: Add SNMP integration to breaking changes (#16374)  * deprecate java less-than 17 (#16370)  * Exclude substitution refinement on pipelines.yml (#16375)  * Exclude substitution refinement on pipelines.yml (applies on ENV vars and logstash.yml where env2yaml saves vars)  * Safety integration test for pipeline config.string contains ENV .  * Doc: Forwardport 8.15.0 release notes to main (#16388)  * Removing 8.14 from ci/branches.json as we have 8.15. (#16390)  ---------  Co-authored-by: ev1yehor <146825775+ev1yehor@users.noreply.github.com> Co-authored-by: João Duarte <jsvd@users.noreply.github.com> Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com> Co-authored-by: Andrea Selva <selva.andre@gmail.com> Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * Squashed merge from 8.x  * Failure injector plugin implementation. (#16466)  * Test purpose only failure injector integration (filter and output) plugins implementation. Add unit tests and include license notes.  * Fix the degrate method name typo.  Co-authored-by: Andrea Selva <selva.andre@gmail.com>  * Add explanation to the config params and rebuild plugin gem.  ---------  Co-authored-by: Andrea Selva <selva.andre@gmail.com>  * Health report integration tests bootstrapper and initial tests implementation (#16467)  * Health Report integration tests bootstrapper and initial slow start scenario implementation.  * Apply suggestions from code review  Renaming expectation check method name.  Co-authored-by: kaisecheng <69120390+kaisecheng@users.noreply.github.com>  * Changed to branch concept  YAML structure simplified as changed to Dict.  * Apply suggestions from code review  Reflect `help_url` to the integration test.  ---------  Co-authored-by: kaisecheng <69120390+kaisecheng@users.noreply.github.com>  * health api: expose `GET /_health_report` with pipelines/*/status probe (#16398)  Adds a `GET /_health_report` endpoint with per-pipeline status probes  and wires the resulting report status into the other API responses  replacing their hard-coded `green` with a meaningful status indication.  ---------  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * docs: health report API  and diagnosis links (feature-targeted) (#16518)  * docs: health report API  and diagnosis links  * Remove plus-for-passthrough markers  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  ---------  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * merge 8.x into feature branch... (#16519)  * Add GH vault plugin bot to allowed list (#16301)  * regenerate webserver test certificates (#16331)  * correctly handle stack overflow errors during pipeline compilation (#16323)  This commit improves error handling when pipelines that are too big hit the Xss limit and throw a StackOverflowError. Currently the exception is printed outside of the logger  and doesn’t even show if log.format is json  leaving the user to wonder what happened.  A couple of thoughts on the way this is implemented:  * There should be a first barrier to handle pipelines that are too large based on the PipelineIR compilation. The barrier would use the detection of Xss to determine how big a pipeline could be. This however doesn't reduce the need to still handle a StackOverflow if it happens. * The catching of StackOverflowError could also be done on the WorkerLoop. However I'd suggest that this is unrelated to the Worker initialization itself  it just so happens that compiledPipeline.buildExecution is computed inside the WorkerLoop class for performance reasons. So I'd prefer logging to not come from the existing catch  but from a dedicated catch clause.  Solves #16320  * Doc: Reposition worker-utilization in doc (#16335)  * settings: add support for observing settings after post-process hooks (#16339)  Because logging configuration occurs after loading the `logstash.yml` settings  deprecation logs from `LogStash::Settings::DeprecatedAlias#set` are effectively emitted to a null logger and lost.  By re-emitting after the post-process hooks  we can ensure that they make their way to the deprecation log. This change adds support for any setting that responds to `Object#observe_post_process` to receive it after all post-processing hooks have been executed.  Resolves: elastic/logstash#16332  * fix line used to determine ES is up (#16349)  * add retries to snyk buildkite job (#16343)  * Fix 8.13.1 release notes (#16363)  make a note of the fix that went to 8.13.1: #16026  Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com>  * Update logstash_releases.json (#16347)  * [Bugfix] Resolve the array and char (single | double quote) escaped values of ${ENV} (#16365)  * Properly resolve the values from ENV vars if literal array string provided with ENV var.  * Docker acceptance test for persisting  keys and use actual values in docker container.  * Review suggestion.  Simplify the code by stripping whitespace before `gsub`  no need to check comma and split.  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  ---------  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  * Doc: Add SNMP integration to breaking changes (#16374)  * deprecate java less-than 17 (#16370)  * Exclude substitution refinement on pipelines.yml (#16375)  * Exclude substitution refinement on pipelines.yml (applies on ENV vars and logstash.yml where env2yaml saves vars)  * Safety integration test for pipeline config.string contains ENV .  * Doc: Forwardport 8.15.0 release notes to main (#16388)  * Removing 8.14 from ci/branches.json as we have 8.15. (#16390)  * Increase Jruby -Xmx to avoid OOM during zip task in DRA (#16408)  Fix: #16406  * Generate Dataset code with meaningful fields names (#16386)  This PR is intended to help Logstash developers or users that want to better understand the code that's autogenerated to model a pipeline  assigning more meaningful names to the Datasets subclasses' fields.  Updates `FieldDefinition` to receive the name of the field from construction methods  so that it can be used during the code generation phase  instead of the existing incremental `field%n`. Updates `ClassFields` to propagate the explicit field name down to the `FieldDefinitions`. Update the `DatasetCompiler` that add fields to `ClassFields` to assign a proper name to generated Dataset's fields.  * Implements safe evaluation of conditional expressions  logging the error without killing the pipeline (#16322)  This PR protects the if statements against expression evaluation errors  cancel the event under processing and log it. This avoids to crash the pipeline which encounter a runtime error during event condition evaluation  permitting to debug the root cause reporting the offending event and removing from the current processing batch.  Translates the `org.jruby.exceptions.TypeError`  `IllegalArgumentException`  `org.jruby.exceptions.ArgumentError` that could happen during `EventCodition` evaluation into a custom `ConditionalEvaluationError` which bubbles up on AST tree nodes. It's catched in the `SplitDataset` node. Updates the generation of the `SplitDataset `so that the execution of `filterEvents` method inside the compute body is try-catch guarded and defer the execution to an instance of `AbstractPipelineExt.ConditionalEvaluationListener` to handle such error. In this particular case the error management consist in just logging the offending Event.  ---------  Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com>  * Update logstash_releases.json (#16426)  * Release notes for 8.15.1 (#16405) (#16427)  * Update release notes for 8.15.1  * update release note  ---------  Co-authored-by: logstashmachine <43502315+logstashmachine@users.noreply.github.com> Co-authored-by: Kaise Cheng <kaise.cheng@elastic.co> (cherry picked from commit 2fca7e39e87c20fcfcd934e984720173ce3417e8)  Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>  * Fix ConditionalEvaluationError to do not include the event that errored in its serialiaxed form  because it's not expected that this class is ever serialized. (#16429) (#16430)  Make inner field of ConditionalEvaluationError transient to be avoided during serialization.  (cherry picked from commit bb7ecc203f698a56f341fa538bdc1cd4da15b28c)  Co-authored-by: Andrea Selva <selva.andre@gmail.com>  * use gnu tar compatible minitar to generate tar artifact (#16432) (#16434)  Using VERSION_QUALIFIER when building the tarball distribution will fail since Ruby's TarWriter implements the older POSIX88 version of tar and paths will be longer than 100 characters.  For the long paths being used in Logstash's plugins  mainly due to nested folders from jar-dependencies  we need the tarball to follow either the 2001 ustar format or gnu tar  which is implemented by the minitar gem.  (cherry picked from commit 69f0fa54ca07cb3f822846745fdbdd1504175cfb)  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  * account for the 8.x in DRA publishing task (#16436) (#16440)  the current DRA publishing task computes the branch from the version contained in the version.yml  This is done by taking the major.minor and confirming that a branch exists with that name.  However this pattern won't be applicable for 8.x  as that branch currently points to 8.16.0 and there is no 8.16 branch.  This commit falls back to reading the buildkite injected BUILDKITE_BRANCH variable.  (cherry picked from commit 17dba9f829a2514aba295ed7a8fa21655b55c86b)  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  * Fixes the issue where LS wipes out all quotes from docker env variables. (#16456) (#16459)  * Fixes the issue where LS wipes out all quotes from docker env variables. This is an issue when running LS on docker with CONFIG_STRING  needs to keep quotes with env variable.  * Add a docker acceptance integration test.  (cherry picked from commit 7c64c7394bf47e8b5316710876ed55350df46d61)  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * Known issue for 8.15.1 related to env vars references (#16455) (#16469)  (cherry picked from commit b54caf3fd8e907c526ab2b8897ce4de4656c2fd5)  Co-authored-by: Luca Belluccini <luca.belluccini@elastic.co>  * bump .ruby_version to jruby-9.4.8.0 (#16477) (#16480)  (cherry picked from commit 51cca7320e5c54865ab3fe2d4101496bd69cacca)  Co-authored-by: João Duarte <jsvd@users.noreply.github.com>  * Release notes for 8.15.2 (#16471) (#16478)  Co-authored-by: andsel <selva.andre@gmail.com> Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com> (cherry picked from commit 01dc76f3b55333f0c49d7190c0cd4ca14b74a7c0)  * Change LogStash::Util::SubstitutionVariables#replace_placeholders refine argument to optional (#16485) (#16488)  (cherry picked from commit 8368c00367cac0c5f5e0090c26be8795b2e8c7d2)  Co-authored-by: Edmo Vamerlatti Costa <11836452+edmocosta@users.noreply.github.com>  * Use jruby-9.4.8.0 in exhaustive CIs. (#16489) (#16491)  (cherry picked from commit fd1de39005cf4646d8faa3f89b1963c716ec6088)  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * Don't use an older JRuby with oraclelinux-7 (#16499) (#16501)  A recent PR (elastic/ci-agent-images/pull/932) modernized the VM images and removed JRuby 9.4.5.0 and some older versions.  This ended up breaking exhaustive test on Oracle Linux 7 that hard coded JRuby 9.4.5.0.  PR https://github.com/elastic/logstash/pull/16489 worked around the problem by pinning to the new JRuby  but actually we don't need the conditional anymore since the original issue https://github.com/jruby/jruby/issues/7579#issuecomment-1425885324 has been resolved and none of our releasable branches (apart from 7.17 which uses `9.2.20.1`) specify `9.3.x.y` in `/.ruby-version`.  Therefore  this commit removes conditional setting of JRuby for OracleLinux 7 agents in exhaustive tests (and relies on whatever `/.ruby-version` defines).  (cherry picked from commit 07c01f8231daf14113b2ce57791712ec74365799)  Co-authored-by: Dimitrios Liappis <dimitrios.liappis@gmail.com>  * Improve pipeline bootstrap error logs (#16495) (#16504)  This PR adds the cause errors details on the pipeline converge state error logs  (cherry picked from commit e84fb458ce2f092e065c63df649222f8cbda8c44)  Co-authored-by: Edmo Vamerlatti Costa <11836452+edmocosta@users.noreply.github.com>  * Logstash Health Report Tests Buildkite pipeline setup. (#16416) (#16511)  (cherry picked from commit 5195332bc6a758198cae70fea7d88dfddf0fa15a)  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * Make health report test runner script executable. (#16446) (#16512)  (cherry picked from commit 2ebf2658ff86678125b04c8826958b468ee0da1f)  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com>  * Backport PR #16423 to 8.x: DLQ-ing events that trigger an conditional evaluation error. (#16493)  * DLQ-ing events that trigger an conditional evaluation error. (#16423)  When a conditional evaluation encounter an error in the expression the event that triggered the issue is sent to pipeline's DLQ  if enabled for the executing pipeline.  This PR engage with the work done in #16322  the `ConditionalEvaluationListener` that is receives notifications about if-statements evaluation failure  is improved to also send the event to DLQ (if enabled in the pipeline) and not just logging it.  (cherry picked from commit b69d993d718dfd639603cdb5d340947b09a6687a)  * Fixed warning about non serializable field DeadLetterQueueWriter in serializable AbstractPipelineExt  ---------  Co-authored-by: Andrea Selva <selva.andre@gmail.com>  * add deprecation log for `--event_api.tags.illegal` (#16507) (#16515)  - move `--event_api.tags.illegal` from option to deprecated_option - add deprecation log when the flag is explicitly used relates: #16356  Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com> (cherry picked from commit a4eddb8a2a79c7e1eb7696140795580427792cb1)  Co-authored-by: kaisecheng <69120390+kaisecheng@users.noreply.github.com>  ---------  Co-authored-by: ev1yehor <146825775+ev1yehor@users.noreply.github.com> Co-authored-by: João Duarte <jsvd@users.noreply.github.com> Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com> Co-authored-by: Andrea Selva <selva.andre@gmail.com> Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com> Co-authored-by: kaisecheng <69120390+kaisecheng@users.noreply.github.com> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: Luca Belluccini <luca.belluccini@elastic.co> Co-authored-by: Edmo Vamerlatti Costa <11836452+edmocosta@users.noreply.github.com> Co-authored-by: Dimitrios Liappis <dimitrios.liappis@gmail.com>  ---------  Co-authored-by: ev1yehor <146825775+ev1yehor@users.noreply.github.com> Co-authored-by: João Duarte <jsvd@users.noreply.github.com> Co-authored-by: Karen Metts <35154725+karenzone@users.noreply.github.com> Co-authored-by: Andrea Selva <selva.andre@gmail.com> Co-authored-by: Mashhur <99575341+mashhurs@users.noreply.github.com> Co-authored-by: kaisecheng <69120390+kaisecheng@users.noreply.github.com> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: Luca Belluccini <luca.belluccini@elastic.co> Co-authored-by: Edmo Vamerlatti Costa <11836452+edmocosta@users.noreply.github.com> Co-authored-by: Dimitrios Liappis <dimitrios.liappis@gmail.com> (cherry picked from commit 7eb5185b4e75061cfa0a091a8bafb622eab5a2f2)  Co-authored-by: Ry Biesemeyer <yaauie@users.noreply.github.com>
eclipse-vertx,vert.x,e45336ed772e4e6f0af68bc73f6610a985c1a776,https://github.com/eclipse-vertx/vert.x/commit/e45336ed772e4e6f0af68bc73f6610a985c1a776,Rewrite the node selector implementation.  Motivation:  The node selector implementation can be simplified to use an drain/loop pattern approach which simplifies its implementation. The current implementation already has a queue approach with a linked list of waiters but it mandates to perform CAS like operations in the entries map.  Instead we can use a wip/mpsc queue node entry which is equivalent but simpler to understand.
eclipse-vertx,vert.x,1089a66cb0e45877202120f65e4ddfc591dd871c,https://github.com/eclipse-vertx/vert.x/commit/1089a66cb0e45877202120f65e4ddfc591dd871c,Motivation:  The HeadersMultiMap implementation of MultiMap performs a full copy of the same type on the setAll operation. A copy-on-write can be used instead if the copied MultiMap is read-only  this can be used to set all headers more efficiently on an existing MultiMap.  Changes:  Update the implementation of HeadersMultiMap with a read-only flag that can set when make an immutable copy of a MultiMap.  The dummy node creation of HeadersMultiMap has been removed in favor of head/tail fields which avoids to create a dummy node object.  The entries field is not anymore final and is now lazily created: upon creation of a new HeadersMultiMap no extra allocation is performed.  When copying or setting a read-only HeadersMultiMap  its head/tail/entries fields are used and a reference to the original is maintain to distinguish between rea-write and copy-on-write state.  When a copy-on-write HeadersMultiMap is modified  its state is copied before mutation.  Concurrent modifications are now detected when iterating over entries.
eclipse-vertx,vert.x,856913dad10a007fd29bc8c7898e63fd82b9ea12,https://github.com/eclipse-vertx/vert.x/commit/856913dad10a007fd29bc8c7898e63fd82b9ea12,Provide a coherent implementation of hashCode for JsonObject/JsonArray with respect to equals.  Motivation:  JsonArray/JsonObject hashCode implementations rely on the wrapped data structure hashCode implementation and on the generic Object hashCode function.  1. The hash value might differ from data structure implementations.  2. Number coercion is performed in equals which allows case were we do have equality (after coercion) but hash differs (e.g. 4L and 4D)  Changes:  JsonObject/JsonArray implement hashCode methods.  Number's hashCode is computed against the hashCode of their double value.
eclipse-vertx,vert.x,4a42084a43fef84f88d4242c7cd7b32c63d6b718,https://github.com/eclipse-vertx/vert.x/commit/4a42084a43fef84f88d4242c7cd7b32c63d6b718,VertxConnection enhancements.  Motivation:  1. the implementation of VertxConnection performs un-necessary flushes when a resume operation happens during a outbound drain that will flush the connection 2. resuming the connection while during a read in progress should be a no-op and handled by the channelRead/channelReadComplete operations
eclipse-vertx,vert.x,3eb4248c88f501facd555f2cf3e7bc59a737b5b9,https://github.com/eclipse-vertx/vert.x/commit/3eb4248c88f501facd555f2cf3e7bc59a737b5b9,Use Files.readString instead of Files.readAllBytes (#5428)  The former avoids creating a copy of byte array  so it's slightly more performant
eclipse-vertx,vert.x,6f750d74841a501f010da6c7cd79ee6ac74b6dfd,https://github.com/eclipse-vertx/vert.x/commit/6f750d74841a501f010da6c7cd79ee6ac74b6dfd,Complete H2 reset promise as soon as the reset frame has been sent.  Motivation:  The recent changes in HTTP reset API introduced a returned future to signal when the stream reset is performed. The implementation completes the future when the stream reset has been written  instead we should complete it as soon as the frame is written.  Changes:  Complete the H2 reset future after the reset frame has been written.
eclipse-vertx,vert.x,223e39647f563c0f0ff12d378f280c13aace8f20,https://github.com/eclipse-vertx/vert.x/commit/223e39647f563c0f0ff12d378f280c13aace8f20,Future.await should interrupt the current thread when the worker executor is closed.  Motivation:  Future.await incorrectly performs a no-op when the worker executor is closed (returns a null latch)  which reports a failure that might not exist.  Changes:  When the worker executor returns null  throw an interrupted exception.
eclipse-vertx,vert.x,46e8167b0f810140530f4c2ab726c3e0650e4842,https://github.com/eclipse-vertx/vert.x/commit/46e8167b0f810140530f4c2ab726c3e0650e4842,Merge pull request #5372 from bjornhusberg/master  ### Motivation  The Http2 server-implementation handles content encoding differently compared to the Http1.x server-implementation  which leads to issues.  Http1.x:  The HttpContentCompressor determines the encoding based on the Accept-Encoding header right before writing the headers. If the request handler has already set a Content-Encoding header there will be no further encoding and if the Content-Encoding header is set to "identity" the header is removed by HttpChunkContentCompressor and the response is returned as-is.  Http2:  The encoding is determined already when setting up the stream in Http2ServerConnection and is then passed down to Http2ServerRequest and Http2ServerResponse where it is set using the Content-Encoding-header. The header content is later picked up by the CompressorHttp2ConnectionEncoder which compresses the response accordingly.  While the Http1.x-implementation works fine the Http2-implementation leads to two problems:  1. If the response is already encoded by the request handler and the Content-Encoding header has been set accordingly  the CompressorHttp2ConnectionEncoder picks up the encoding and encodes the response again  resulting in garbage returned to the client.  2. If the Content-Encoding is explicitly set to "identity"  the response is not compressed by the CompressorHttp2ConnectionEncoder but the Content-Encoding holding "identity" leaks out to the client. This is unnecessary and leads to confusion for some clients.  ### Changes  Removes the setting of the Content-Encoding header in Http2ServerResponse.  - _Change motivation:_ Setting the header in one place and performing the compression in another makes it difficult to handle scenarios where the header is set by other code. For instance a request handler. Setting the encoding in the HttpServerResponse is also not the way it's done in the Http1-implementation. - _Actual changes:_ Removed the setting of the header from Http2ServerResponse. After that  the contentEncoding variable could be removed from the Http2ServerResponse  the Http2ServerRequest and the Push class and no longer needed to be determined in the Http2ServerConnection.initStream or Http2ServerConnection.doSendPush.  Extended the CompressorHttp2ConnectionEncoder with a new VertxCompressorHttp2ConnectionEncoder.  - _Change motivation:_ The new encoder determines the content-encoding right before writing the headers  similar to how it's done in HttpContentCompressor. This makes it possible to handle the case where the payload has already been encoded by a request handler or explicitly disabled using Content-Encoding: identity. - _Actual changes:_ Added a VertxCompressorHttp2ConnectionEncoder that determines if the payload should be compressed just before writing the headers. If content-encoding has already been set or could not be determined the compressor encoder is skipped. If the Content-Encoding header is set to "identity" it is removed.  Adds two new tests in Http2ServerTest that verifies the issues described in the GitHub issue.  ### Outcome  Fixes #5371
eclipse-vertx,vert.x,f37800c8af6bd4bd3f5d853fbd5a90346390e59f,https://github.com/eclipse-vertx/vert.x/commit/f37800c8af6bd4bd3f5d853fbd5a90346390e59f,Unregistration of an event-bus consumer is performed on the consumer context  instead it should be on the caller context.
eclipse-vertx,vert.x,c87a088bff9a70f906c584059b3b42d2990694e3,https://github.com/eclipse-vertx/vert.x/commit/c87a088bff9a70f906c584059b3b42d2990694e3,The OutboundWriteQueue returns the queue is unwritabile when the first submitted element is refused and concurrent/reentrant submissions made the queue unwritable. We end-up in a case were the unwritable flag is returned twice and it might corrupt the unwritable count performed by the OutboudMessageQueue.  The queue should return an unwritable flag when the addition/submission triggered it. Remove the unwritable check that when the first submitted element is refused.
eclipse-vertx,vert.x,77a0d21dabc39b05c2f2c57848432c4f2ca747de,https://github.com/eclipse-vertx/vert.x/commit/77a0d21dabc39b05c2f2c57848432c4f2ca747de,Have HttpServerRequest expose canUpgradeToWebSocket as this sounds legitimate for most users to check this before perfoming a WebSocket upgrade on the server
neo4j,neo4j,88d7cdcf639757a39ac5e41752c4dfe91e08a8d7,https://github.com/neo4j/neo4j/commit/88d7cdcf639757a39ac5e41752c4dfe91e08a8d7,Cleanup how PrivilegeDatabaseReference lookup performed for new transaction ()
neo4j,neo4j,86018238e74b98ff0364f1c2c48fc4d222de2b8b,https://github.com/neo4j/neo4j/commit/86018238e74b98ff0364f1c2c48fc4d222de2b8b,Performance improvements to ID updates in incremental import
neo4j,neo4j,80914d4a9ecab6055d0873e5701509214689c4a5,https://github.com/neo4j/neo4j/commit/80914d4a9ecab6055d0873e5701509214689c4a5,Expose a hidden admin import option to specify the number of ranges  This can be considered a temporary option for certain edge cases to try and improve the import performance  until the range calculation covers all cases.
neo4j,neo4j,3f944c536f5b3f987f5996bdc5ab54a2eb02a0bc,https://github.com/neo4j/neo4j/commit/3f944c536f5b3f987f5996bdc5ab54a2eb02a0bc,Fix bug and performance regression in error handling
neo4j,neo4j,6b2b60e68efdcbfa6435e831d555e02d5f5cae47,https://github.com/neo4j/neo4j/commit/6b2b60e68efdcbfa6435e831d555e02d5f5cae47,SPD: disable parallel access check when beginning a diver transaction  The driver might go to System database when figuring out the cluster topology If this happens it must be performed with the parallel access check disabled in case the operation is performed from a parallel runtime worked thread. Parallel access check checks that Cypher worker threads don't wonder off and touch something they should not. This case is a false positive.
neo4j,neo4j,89eb86e380ae62978dc3d065ec81a04930c85353,https://github.com/neo4j/neo4j/commit/89eb86e380ae62978dc3d065ec81a04930c85353,Refactor scoutNextSibling()  This is a pure refactor commit  where we perform the checks and bailout from the method earlier instead of keep accumulating variables and re-checking everything in the end.  Also add some extra comments to make it more clear why we're doing each check.
neo4j,neo4j,1525b18f6f14b7e37e7fb22c4937c927c524a71d,https://github.com/neo4j/neo4j/commit/1525b18f6f14b7e37e7fb22c4937c927c524a71d,Remove IndexUsageTracker  IndexUsageTracker existed so we could locally accumulate queried signals  before passing them to their main IndexUsageTracking. This was done so we would avoid high contention on the AtomicLongs used internally by DefaultIndexUsageTracking.  Since PR   we changed DefaultIndexUsageTracking to use LongAdder and LongAccumulator to perform the same operations. These implementations can trade a bit more of memory usage for much better access contention  since they will only be fully processed when we call `getAndReset()`.  So now we can ditch the whole IndexUsageTracker because it is error prone  since it requires properly handling to invoke `close()` at some point  otherwise we'll have missed query metrics. By bringing back the functionality into the IndexUsageTracking and having only it now  we can avoid those missed query counts.
neo4j,neo4j,58c1ae80136353006cc1d189dac73c177289d9ac,https://github.com/neo4j/neo4j/commit/58c1ae80136353006cc1d189dac73c177289d9ac,SPD: Query router is bypassed on SPD shards  When Cypher is used as RPC for internal in-cluster communication  the entire query routing logic can be bypassed for performance reasons. Such queries are not created by users  but generated by another DBMS instance  they are client-side routed and a session database is always the target one.
neo4j,neo4j,04f5766c2459e56eb4244a65bdf9061943e78aa8,https://github.com/neo4j/neo4j/commit/04f5766c2459e56eb4244a65bdf9061943e78aa8,Removes the generational approach in importer  Now overtaken by the node ID range approach. Doing the node ID ranges w/o the additional generations within each range performs much better and also removes the problem of bloated relationships XD store since relationships for each node is written only one or two times instead of potentially many times.
neo4j,neo4j,c1c077821fb4ac6bff715d835479ee89ec052df4,https://github.com/neo4j/neo4j/commit/c1c077821fb4ac6bff715d835479ee89ec052df4,In QueryCache: Block Thread if currently another Thread performs the same computation.
supertokens,supertokens-core,177ee2f49a02761b7ab6a0777349794e5e08f5db,https://github.com/supertokens/supertokens-core/commit/177ee2f49a02761b7ab6a0777349794e5e08f5db,feat: webauthn base (#1115)  * feat: new dependency: webauthn4j  * feat: add tables for webauthn  * fix: typo fixes  * feat: webauthn options  * feat: registercredentials wip  * feat: passkeys register credentials wip  * feat: recipe user sign up  * recipe user creation wip  * sign up recipe user  * feat: register credentials  * fix: temp  * feat: webauthn support wip  * feat: webauthn support wip  * merging  * feat: webauthn support wip  * feat: getuserinfolist draft  * feat: get user by account info - webauthn support  * fix: generate account recovery token api  * feat: get user by account info - webauthn support  * feat: signup with credentialsregister  * fix: fixes for tests  * fix: fixes for tests  * feat: get generated options api  * feat: webauthn sign in  * fix: account recovery  * fix: fixes for tests  * fix: fixing id name in response  * fix: fixing id encoding in response  * fix: base64 url encode the challenge insted of base64 encode  * fix: account recovery impl  * fix: base64 encoding changes  * fix: fixes for tests  * fix: fixing sql issues and encoding issues  * fix: fixes for tests  * fix: integration fix for signup  * fix: webauthn flow test stub  * fix: fixes for sdk tests  * feat: add webauthn recover account apis to webserver  * feat: crud apis addition  * feat: remove options api  * fix: additional field in the sign in options response  * fix: reworked error handling  * fix: fixing GET api not to expect json body  * fix: sign in + options check  * fix: more descriptive error messages for credentialsRegister  * fix: typo fixes  * fix: fixes for tests  * fix: not letting dependencies exception to leak out  * feat: clean up expired data cron  * fix: changing recovery token consume  * fix: fixing loginmethod collection  * fix: signin fixes  * fix: webauthn sign in fixes  * fix: add recipeUserId in signIn response  * feat: enable credentials listing api  * feat: extending user listing with webauthn  * fix: don't use the counter at signin check  * fix: small fixes  * fix: setting UV and RK to false  * feat: saving userVerification and userPresence values  * fix: change a bunch of error messages for sdk integration  * fix: change a bunch of error messages for sdk integration  * fix: include userVerification and userPresence in options response  * fix: error messages changes  * fix: refactor exceptions  * feat: get credential api  * fix: options generation no longer throws invalid options error as per reference impl  * fix: more error handling for sign in  * fix: rename methods for better readability  * fix: throw the right exception  * ci: experiment with a GHA to publish test/dev images  * ci: experiment with publishing dev docker images  * ci: experiment with publishing dev docker images  * ci: experiment with a GHA to publish test/dev images  * fix: options validation  * fix: options validation rpId doesn't have to be an url  * fix: additional validation  * fix: fixes for various sdk tests  * fix: Dockerfile setupTestEnv --local  * ci: remove arm64 build from dev-docker  * fix: add webauth4jn-test dependency  * fix: fixing email verification query for webauthn  * fix: authenticator mocking and example usage  * fix: sem ver and few test fixes  * fix: test fixes  * fix: cdi version increment in webauthn test  * fix: webauthn signIn should load all loginmethods of the user  * fix: fixing table locked issue with in memory db  * fix: remove unnecessary logging  * fix: add tests and fixes  * fix: add tests and fixes for email update  * fix: additional tests and fixes related to useridmapping  * fix: add null check  * fix: add test  * fix: additional indexes for performance optimization  * ci: fix dev-docker build  * fix: self-review fixes  * fix: update pluginInterfaceSupported to the right branch  * chore: changelog  version number  * fix: review fixes  * fix: review fixes  * fix: fixing email verified flag after email change  * fix: review fixes  * fix: handling potential error while saving options  * fix: review fixes  * chore: updating supported pluginInterface  * chore: updating supported pluginInterface  * test: API tests (#1118)  * fix: API tests template  * fix: options register APIs  * test: register credential  * test: fix  * fix: test get credential  * test: list credential  * test: remove credential  * test: remove credential  * test: sign in options  * test: sign-in  * test: sign-in  * test: update email  * fix: delete  * fix: tests for mongodb  * fix: tests  * fix: tests  * fix: review fixes  * fix: review fix: token generation changes  ---------  Co-authored-by: Sattvik Chakravarthy <sattvik@gmail.com> Co-authored-by: Mihaly Lengyel <mihaly@lengyel.tech> Co-authored-by: Sattvik Chakravarthy <sattvik@supertokens.com>
theonedev,onedev,e7dec6644b6929afb650d0e288b396476e00674b,https://github.com/theonedev/onedev/commit/e7dec6644b6929afb650d0e288b396476e00674b,feat: Buffer file writing to improve performance on slow storage devices (OD-2206)
theonedev,onedev,d35cf26d803b4ac751147e5e91d6f82bf96bf1dd,https://github.com/theonedev/onedev/commit/d35cf26d803b4ac751147e5e91d6f82bf96bf1dd,Some improvements over performance and usability  feat: Issue list performance improvements (OD-2042) feat: Add criteria to query project by id (OD-2041)
theonedev,onedev,12ff4107163abc55ea1cfb1a20386236506964af,https://github.com/theonedev/onedev/commit/12ff4107163abc55ea1cfb1a20386236506964af,chore: Improve label loading performance
deeplearning4j,deeplearning4j,722592d8a9efaefcdcd9ed9bb5ee144bca25a13f,https://github.com/deeplearning4j/deeplearning4j/commit/722592d8a9efaefcdcd9ed9bb5ee144bca25a13f,Update for publishing to snapshots (#10211)  * fix extra quote inserted by intellij  * Fix path  * fix executable permissions usage  * improve directory resolution and restore some files from the previous PR  * remove old unused dep versions Update to new march 2024 central release plugin  * update central publishing version  * temp disable mirror  * fix hallucinated version  * update maven invoker version for jetspeed  * remove old build properties add better directory resolution for copy flatc  * fix helpers sources for cpu (api updates) perform maven antrun upgrade  * more fixes for onednn helpers  * update more antrun usage  * fix pooling param usage  * remove findbugs on the 2 files it was used on  * debug odd build path issues  * try again  * remove bad ls  * try again  * another attempt  * Add troubleshooting logs again  * Remove specific gcc versions  * update build  * cd to original directory instead  * Remove enforcer check  * remove debug steps  * remove extra steps  * update linux  android and cuda versions  * update arm usage defaults  * remove 32 bit builds  * alternative path  * alternative path get rid of old tools  * get rid of over simplified bootstrap libnd4j download usage  * remove extra cmake  * remove manual path setting  * arm compilation fix  * debug armcompute  * update android-x86_64 openblas  * upgrade gcc  * minor armcompute const changes  * update openblas version  * fix armcompute paths  * remove ls  * update cuda versions and envrionment variables  * fix syntax errors  * fix syntax errors  * fix syntax errors  * fix syntax errors  * another rev  * update msys command  * rev  * fix classifier  * rev android-x86_64  * rev android-x86_64  * update cuda env passing  * fix linux-arm64 syntax error  * cuda rev  * cuda rev  * update arm else if branches for matching  * remove external PS script?  * remove external PS script?  * remove external PS script?  * improve architecture detection  * update list operation conv2d armcompute  * add flatbuffer generated code  * remove old maven auth  * remove old legacny average and accumulate  * remove cache due to 422 error  * ensure armcompute is optional  * ensure armcompute is optional  * disk space clean up on all workflows  * deal with windows service issue  * ensure nvcc is on path  * Add arm64 protoc Update cuda paths  * convert to use numeric types only mitigating lld errors found by clang  * Add arm64 protoc Update cuda paths  * fix syntax error  * fix overriding properties causing libnd4j not to be built with cuda  * update windows version  * try to update cl.exe paths  * update debugging for nvcc  * update cuda flags to work with linux/windows  * remove extra cuda install  * fix duplicate sources  * more unsupported compiler changes  * remove unneeded source and javadoc  * better cudnn detection  * update where we put unsupported compiler  * Collapse cmake logic  * Collapse cmake logic  * clean up consolidated file  * flatbuffers fix  * fix hallucinated paths  * generate flatbuffers by default  * fix hallucinated paths  * ensure imports present  * fix flatc target order  * fix elif syntax  * more rearrange  * fix git tag  * update template to allow proper configuration generation  * remove guard  * change quotes  * change quotes  * test default  * fix missing functions  * rearrange dependencies  * set the engine  * reintegrate some old cuda logic  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * Add back in function defs  * update api to use proper cuda versions  * update api to use proper cuda versions  * update api to use proper cuda versions  * Add back in function defs  * delete old paths  * aDd back include_ops.h generation  * delete old paths  * reinroduce old comand  * delete old paths  * delete old paths  * better tar unpacking  * fix for suffixes  * Add missing cpu sources  * fix duplicate profile  * change mac image  * change mac image  * get rid of old gpg key step  * get rid of javadoc on mac  * remove unneeded deps for mac  * update arm compute path  * remove old command checks  * remove gpg key step  * clang specific type erro  * clang specific type erro  * add another missing cpu path  * add another missing cpu path  * fix helpers sources ordering  * fix helpers sources ordering  * Add back createFromDescriptor  * fix helpers sources ordering  * fix helpers sources ordering  * fix helpers sources ordering  * fix helpers sources ordering  * Add back createFromDescriptor  * add debug for onednn  * add debug for onednn  * SFINAE for type aliases  * SFINAE for type aliases  * SFINAE for type aliases  * SFINAE for type aliases  * ensure we tell compiler we're ok with certain apple version minimums  * ensure we tell compiler we're ok with certain apple version minimums  * ensure we tell compiler we're ok with certain apple version minimums  * ensure we tell compiler we're ok with certain apple version minimums  * fix imports  * update linker path  * fix lock type usage  * change order of sources  * share mutex types  * share mutex types  * share mutex types  * update linker path  * share mutex types  * share mutex types  * share mutex types  * decrease type pairs for sort  * decrease type pairs for sort  * decrease type pairs for sort  * decrease type pairs for sort  * update the special methods to use combinations  * update the special methods to use combinations  * update the special methods to use combinations  * update the special methods to use combinations  * update the special methods to use combinations  * update the special methods to use combinations  * update the special methods to use combinations  * update the special methods to use combinations  * update the special methods to use combinations  * update the special methods to use combinations  * update the special methods to use combinations  * standardize output paths  * fix template paths  * refactor compiler flags  * update onednn to use similar approach to armcompute  * refactor compiler flags  * fix paths  * fix paths  * fix paths  * fix paths  * fix paths  * fix paths  * fix paths  * fix paths  * change target expand types  * change target expand types  * add new pairwise types  * add new pairwise types  * add new pairwise types  * update linker paths  * add new pairwise types  * add new pairwise types  * add new pairwise types  * fix strings with transform  * fix strings with transform  * update linker paths  * update default values for libnd4j.outputPath  * update default values for libnd4j.outputPath  * update default values for libnd4j.outputPath  * update pom.xml namespaces  * Update .github/workflows/build-deploy-linux-cuda-12.6.yml  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>  * Update .github/workflows/build-deploy-linux-x86_64.yml  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>  ---------  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>
deeplearning4j,deeplearning4j,0ab6236295ca75736f591c2ea805e438fc580dbd,https://github.com/deeplearning4j/deeplearning4j/commit/0ab6236295ca75736f591c2ea805e438fc580dbd,Remove more elementwise stride  introduce better TAD caching (#10165)  * minor fix to modular hasher compilation/usage fix performance issues with pairwise usage (shape::rank was dominating cpu usage caching values removes this)  * Refactor index2coords/coords2index inlineiing  * cuda updates for corresponding caching of shape information  * fix performance regression with the naive non view case with reduce  * fix reduce extra params typing  * remove print statement  * fix reduce extra params typing  * remove cpp markdown initialize directshapetrie  * fix license headers  * Add new TAD caching using tries similar to shape buffers Remove unused methods from shape.h Remove helpers/TAD.h Remove pairwise_util.h Remove commented code/imports Fix indexreduce linker errors  * restore benchmarks  * fix merging artifacts  * remove old impl comments more section removals  * fix cuda chunking issues  * restore benchmarks copyright  * more reversions  * more reversions  * fix cuda chunking issues
deeplearning4j,deeplearning4j,194956504ab3401cf1ca7fc9ecec61ccaf532d2d,https://github.com/deeplearning4j/deeplearning4j/commit/194956504ab3401cf1ca7fc9ecec61ccaf532d2d,Refactoring caching with index calculations (#10164)  * minor fix to modular hasher compilation/usage fix performance issues with pairwise usage (shape::rank was dominating cpu usage caching values removes this)  * Refactor index2coords/coords2index inlineiing  * cuda updates for corresponding caching of shape information  * fix performance regression with the naive non view case with reduce  * fix reduce extra params typing  * remove print statement  * fix reduce extra params typing  * remove cpp markdown initialize directshapetrie  * fix license headers
deeplearning4j,deeplearning4j,ddab396b8df01ecaa015aea0bdd626c5120fb66d,https://github.com/deeplearning4j/deeplearning4j/commit/ddab396b8df01ecaa015aea0bdd626c5120fb66d,Remove shape descriptor creation when caching shape buffers (#10162)  * add new smoke tests fix scalar index when using the indexing api fix usage of index2coords by migrating shape information to shape::shapeOf fix usage of coords2index by migrating accidental shapeOf invocations to strides  * Fix reduce3 buffer() calls in op execution  * Add opaquendarray cachcing/deletion  on close Add new memory pressure test Update nd4j benchmarking with jemalloc docs Add licenses Remove more old aurora code Add basic reduce smoke tests  * remove print statements  * add more debugging documentaiton under troubleshooting/ rewrite benchmarks Add new memory profiler Fix deallocation crash Add environment sourcing in the custom java executable to allow for more customization  * add more debugging documentaiton under troubleshooting/ rewrite benchmarks Add new memory profiler Fix deallocation crash Add environment sourcing in the custom java executable to allow for more customization  * performance optimizations  * update benchmarks remove shape descriptor usage introduce new trie based shape buffer cache fix test java bash script to use relative path for env.sh sourcing  * fix opaquendarray caching fix javacpp compilation remove print statements  * Add ADR Fix nits  * Collapse all the descriptor hashes in to a modular hasher  * Add ADR Fix nits  * clean up ADR  * remove commented code
apache,druid,8be787a51bbd695dbe83127002fba32e2ed4025f,https://github.com/apache/druid/commit/8be787a51bbd695dbe83127002fba32e2ed4025f,Allow use of centralized datasource schema and segment metadata cache together (#17996)  Description ----------- #17935 enables use of `HeapMemorySegmentMetadataCache` on the Coordinator. But it cannot be used in conjunction with centralized datasource schema (i.e. `SegmentSchemaCache`) This patch supports usage of both features on the Coordinator together.  Main Changes -------------- - Make `SegmentSchemaCache` a dependency of `HeapMemorySegmentMetadataCache` - Bind `SegmentMetadataCache` and `SegmentSchemaCache` in `MetadataManagerModule` - Add `NoopSegmentSchemaCache` to be used on the Overlord - Poll schemas in `HeapMemorySegmentMetadataCache` and update `SegmentSchemaCache` - Update the `used_status_last_updated` column of a segment record when its schema fingerprint is updated  Fix a race condition ------------------- Add a sync buffer duration of 10 seconds to `HeapMemorySegmentMetadataCache`  - Handles a race condition between sync and insert to cache (caught in `CompactionTaskRunTest`) - Prevents removal of entries from cache if they have a last updated time just before sync start and were added to the cache just after sync start - This means that non-leader nodes will continue to consider a segment as used if it was marked unused within 10 seconds of any other update done it (created  marked used  schema info added) - 10s is more than enough for this  since the cache is already performing as expected in several prod clusters.  Guice changes --------------- - Add `MetadataManagerModule` used only by Coordinator and Overlord to bind metadata managers - Restrict `SQLMetadataStorageDruidModule` to bind only SQL connector related stuff
apache,druid,e2fbb47d0d951e20d8945ee7f5db935dcec1716d,https://github.com/apache/druid/commit/e2fbb47d0d951e20d8945ee7f5db935dcec1716d,Improve concurrency in TaskQueue (#17828)  Description  The giant lock in TaskQueue is acquired when performing any update or read operation  such as:  add a task remove a task read the list of tasks sync from storage  On large clusters with several concurrent tasks  this can become a bottleneck.  This patch attempts to improve the concurrency in TaskQueue by using a concurrent hash map instead. Changes  Use a ConcurrentHashMap to track active tasks. The giant lock was needed only to handle competing updates made to the same task ID. This can be handled by a ConcurrentHashMap too. Perform any update on the entry for a task ID within ConcurrentHashMap.compute to ensure atomicity Convert the giant lock from a ReentrantLock to a ReentrantReadWriteLock Repurpose giant lock to ensure that the TaskQueue start and stop is mutually exclusive from any other normal operation on the TaskQueue Keep a lastUpdatedTime in every TaskEntry to handle race conditions when syncing from metadata store  Handling race conditions  The only possible race conditions are with syncFromStorage()  and are handled by maintaining a lastUpdatedTime for every task entry.  All updates to a task entry happen inside a ConcurrentHashMap.compute(taskId  entry -> update(entry)) and are thus thread-safe. [A] Sync should not add a task to queue if it has just been removed  This is handled by ensuring that only the syncFromStorage() method can remove tasks from the queue. A task that completed after the poll started would have a lastUpdatedTime after syncStartTime. In this case  since an entry already exists in the queue  a new one will not be added. The next invocation of syncFromStorage() will add the task  if necessary. [B] Sync should not remove a task from queue if it has just been added  A task added after the poll started would have a lastUpdatedTime after syncStartTime. In this case  the entry will not be removed. The next invocation of syncFromStorage() will clean up the task  if necessary.
apache,druid,3ef2e5e504f21e6b57366a9fae92f7d5a638350e,https://github.com/apache/druid/commit/3ef2e5e504f21e6b57366a9fae92f7d5a638350e,Add policy enforcer to sanity check on policy in query execution (#17774)  * Some debug configs  * use postgresql as the default metadata store and set a few debug log  * Add s3 extension  update local storage directory  use emoji in website title  * Update favicon  easier to find the console tab  * Add indexer server  add some basic security config  updated historical and broker to use the common druid root directory  * Some policy config  * add checks for SegmentMetadataQuery  * Add thread.sleep for flaky.  * auth config  * format  and remove temp folder rules  * added NoopPolicyEnforcer and RestrictAllTablesPolicyEnforcer class  * Support pushing and streaming task payload for HDFS (#17742)  Implement pushTaskPayload/streamTaskPayload as introduced in #14887 for HDFS storage to allow larger mm-less ingestion payloads when using HDFS as the deep storage location.  * Remove usages of deprecated API Files.write() (#17761)  * Add deprecated com.google.common.io.Files#write to forbiddenApis  * Replace deprecated Files.write()  * Doc: Fix description typo for sqlserver metadata store (#17771)  Mistakenly categories under deep storage instead of metadata store.  * Fix binding of segment metadata cache on CliOverlord (#17772)  Changes --------- - Bind `SegmentMetadataCache` only once to `HeapMemorySegmentMetadataCache` in `SQLMetadataStorageDruidModule` - Invoke start and stop of the cache from `DruidOverlord` rather than on lifecycle start/stop - Do not override the binding in `CliOverlord`  * Docs: Remove semicolon from example (#17759)  * Restrict segment metadata kill query till maxInterval from last kill task time (#17770)  Changes --------- - Use `maxIntervalToKill` to determine search interval for killing unused segments. - If no segment has been killed for the datasource yet  use durationToRetain  * Update the Supervisor endpoint to not restart the Supervisor if the spec was unmodified (#17707)  Add an optional query parameter called skipRestartIfUnmodified to the /druid/indexer/v1/supervisor endpoint. Callers can set skipRestartIfUnmodified=true to not restart the supervisor if the spec is unchanged.  Example:  curl -X POST --header "Content-Type: application/json" -d @supervisor.json localhost:8888/druid/indexer/v1/supervisor?skipRestartIfUnmodified=true  * Reduce noisy coordinator logs (#17779)  * Emit time lag from Kafka supervisor (#17735)  Changes --------- - Emit time lag from Kafka similar to Kinesis as metrics `ingest/kafka/lag/time`  `ingest/kafka/maxLag/time`  `ingest/kafka/avgLag/time` - Add new method in `KafkaSupervisor` to fetch timestamps of latest records in stream to compute time lag - Add new field `emitTimeLagMetrics` in `KafkaSupervisorIOConfig` to toggle emission of new metrics  * fix processed row formatting (#17756)  * Web console: add suggestions for table status filtering. (#17765)  * suggest filter values when known  * update snapshots  * add more d  * fix load rule clamp  * better segment timeline init  * Remove all usages of skife config (#17776)   Changes --------- - Usages of skife config had been deprecated in #14695 and `LegacyBrokerParallelMergeConfig` is the last config class that still uses it. - Remove `org.skife.config` from pom  licenses  log4j2.xml  etc. - Add validation for deleted property paths in `StartupInjectorBuilder.PropertiesValidator` - Use the replacement flattened configs (which remove the `.task` and `.pool` substring)  * Add field `taskLimits` to worker select strategies (#16889)  Changes --------- - Add field `taskLimits` to the following worker select strategies `equalDistribution`  `equalDistributionWithCategorySpec`  `fillCapacityWithCategorySpec`  `fillCapacity` - Add sub-fields `maxSlotCountByType` and `maxSlotRatioByType` to `taskLimits` - Apply these limits per worker when assigning new tasks  --------- Co-authored-by: sviatahorau <mikhail.sviatahorau@deep.bi> Co-authored-by: Benedict Jin <asdf2014@apache.org> Co-authored-by: Kashif Faraz <kashif.faraz@gmail.com>  * remove NullValueHandlingConfig  NullHandlingModule  NullHandling (#17778)  * Docs: Add SQL query example (#17593)  * Docs: Add query example  * Update after review  * Update query  * Update docs/api-reference/sql-api.md  ---------  Co-authored-by: Victoria Lim <vtlim@users.noreply.github.com>  * More logging cleanup on Overlord (#17780)  * Remove maven.twttr repo from pom (#17797)  remove usage of dependency:go-offline from build scripts - as it tries to download excluded artifacts  ---------  Co-authored-by: Zoltan Haindrich <kirk@rxd.hu>  * fix bug (#17791)  * Log query stack traces for DEVELOPER and OPERATOR personas. (#17790)  Currently  query stack traces are logged only when "debug: true" is set in the query context. This patch additionally logs stack traces targeted at the DEVELOPER or OPERATOR personas  because for these personas  stack traces are useful more often than not.  We continue to omit stack traces by default for USER and ADMIN  because these personas are meant to interact with the API  not with code or logs. Skipping stack traces minimizes clutter in the logs.  * Set useMaxMemoryEstimates=false for MSQ tasks (#17792)  * Web console: fix go to task selecting correct task type (#17788)  * fix go to task selecting correct task type  * support autocompact also  * support scheduled_batch  refactor  * one more state and update tests  * Enable ComponentSuppliers to run queries using Dart (#17787)    Enables Calcite*Test-s and quidem tests to run queries with Dart.  needed some minor tweaks:  changed to use interfaces at some places renamed DartWorkerClient to DartWorkerClientImpl and made DartWorkerClient an interface reused existing parts of the MSQ test system to run the query  * Fix single container config creates failing peon tasks (#17794)  * Fix single container config creates failing peon tasks  * More obvious array error output  * Update `k8s-jobs.md` reference (#17805)  Signed-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>  * Footer Copyright Year Update (#17751)  * Update docusaurus.config.js  * Update docusaurus.config.js  * [Revert] Reduce number of metadata transaction retries (#17808)  * Revert "Run JDK 21 workflows with latest JDK. (#17694)" (#17806)  * Revert "Run JDK 21 workflows with latest JDK. (#17694)"  This reverts commit 31ede5cb  * Review comments.  * Review comments.  * Revert "reject publishing actions with a retriable error code if a earlier task is still publishing (#17509)"  This reverts commit aca56d6bb842231853d624e7da07748ba002ac4f.  * Fix unstable tests after #17787 and dart usage in quidem-ut (#17814)  * fixes  * fix cleanup  * Use "mix" shuffle spec for target size with nil clusterBy. (#17810)  When a nil clusterBy is used  we have no way of achieving a particular target size  so we need to fall back to a "mix" spec (unsorted single partition).  This comes up for queries like "SELECT COUNT(*) FROM FOO LIMIT 1" when results use a target size  such as when we are inserting into another table or when we are writing to durable storage.  * Docs: Recommend using runtime property javaOptsArray instead of javaOpts  * Add minor checks in jetty utils (#17817)  Add minor checks in jetty utils class  * CI improvement: Leverage cancelled() instead of always() for CI jobs (#17819)  * Make MSQ tests use the same datasets as other similar tests (#17818)  MSQ tests had their own way of creating the segments/etc - this have lead to that custom datasets didn't worked with them. This patch alters a few things to make it possible to access CompleteSegment for the active segments - which fixed the issue and also enabled the removal of the extra loading codes.  * Add unnest tests to quidem (#17825)  This PR adds the sql-native unnest tests to quidem. This set of tests has 6392 queries in total  with 5247 positive tests and 1145 negative tests.  * Web console: show loader on aux queries (#17804)  * show loader on aux queries  * show supervisors if not on page 0  * refactor  * fix bug fetching data when columns are added or removed  * update test  * Use compaction dynamic config to enable compaction supervisors (#17782)  Changes --------- - Remove runtime property object `CompactionSupervisorConfig` - Add fields `useSupervisors` and `engine` to cluster-level compaction dynamic config - Remove unused field `useAutoScaleSlots`  * Retry segment publish task actions without holding locks (#17816)  #17802 reverted a retry of failed segment publish actions.  This patch attempts to address the original issue by retrying the segment publish task actions on the client (i.e. task) side without holding any locks so that other transactions are not blocked. Changes  Add retries to TransactionalSegmentPublisher Add field retryable to SegmentPublishResult Remove class DataStoreMetadataUpdateResult and use SegmentPublishResult instead  * Add the capability to turboload segments onto historicals (#17775)  Add the capability to set Historicals into a turbo loading mode  to focus on loading segments at the cost of query performance.  Context -------- Currently  when a new Historical is started  it initially starts out using a bootstrap thread pool. It uses this thread pool to load any existing cached segments and broadcast segments. Once it loads any segments from both these sources  the historical switches to a smaller thread-pool and begins to serve queries.  In certain cases  it would be useful to have the historical switch back to this mode  and focus on loading segments  either to continue loading the initial non-bootstrap segments  or to catch up with assigned segments.  This PR adds a coordinator dynamic config that allows servers to be configured to use the larger bootstrap threadpool to load segments faster.  Changes --------- - Added a new dynamic coordinator configuration  `turboLoadingNodes`. - Ignore  `druid.coordinator.loadqueuepeon.http.batchSize` for servers in `turboLoadingNodes` - Add API on historical to return loading capabilities i.e. num loading threads in normal and turbo mode  * Fix resource leak for GroupBy query merge buffer when query matched result cache (#17823)  * Fix resource leak for GroupBy query merge buffer when match result cache  * Fix resource leak for GroupBy query merge buffer when match result cache  * Add test  * Add test  * Add comment  * Add test  * Add metric and simulation test for turbo loading mode (#17830)  Changes --------- - Add field `loadingMode` to `SegmentChangeStatus` - Including loading mode in `DataSegmentChangeResponse` - Include loading mode in the `description` of metrics emitted from `HttpLoadQueuePeon` - Add simulation test to verify loading mode metrics  * Update query example (#17811)  * String util upgrade for jdk9+ (#17795)  * Update StringUtils.replace() after fix in JDK9  * Upgrade optimized string replace algorithm  * Update methods by re-using declared StringUtils#replace method  * Replace hard-coded UTF-8 encodings with StandardCharsets  * Documentation Fix (#17826)  * Enable to run quidem tests against multiple configurations; add conditionals; cleanup framework init (#17829)  * cleans up `SqlTestFramework` initialization to leave the `OverrideModule` empty - so that tests could more easily take over parts * remove the `QueryComponentSupplier#createEngine`  factory method - instead uses a `Class<SqlEngine>` and use the `injector` to initialize it * enables the usage of `!disabled <supplier> <message>` - to mark cases which are not yet supported with a specific configuration for some reason * fixes that `datasets` was not respecting the `rollup` specification of the ingest * enables to use `MultiComponentSupplier` backed tests - these will turn into matrix tests over multiple componentsuppliers - enabling running the same testcase in different scenarios  * Fix failing test in DimensionSchemaUtilsTest (#17832)  * Improve performance of segment metadata cache on Overlord (#17785)  Description ----------- #17653 introduces a cache for segment metadata on the Overlord. This patch is a follow up to that to make the cache more robust  performant and debug-friendly.  Changes --------- - Do not cache unused segments This significantly reduces sync time in cases where the cluster has a lot of unused segments. Unused segments are needed only during segment allocation to ensure that a duplicate ID is not allocated. This is a rare DB query which is supported by sufficient indexes and thus need not be cached at the moment. - Update cache directly when segments are marked as unused to avoid race conditions with DB sync. - Fix NPE when using segment metadata cache with concurrent locks. - Atomically update segment IDs and pending segments in a `HeapMemoryDatasourceSegmentCache` using methods `syncSegmentIds()` and `syncPendingSegments()` rather than updating one by one. This ensures that the locks are held for a shorter period and the update made to the cache is atomic.  Main updated classes ---------------------- - `IndexerMetadataStorageCoordinator` - `OverlordDataSourcesResource` - `HeapMemorySegmentMetadataCache` - `HeapMemoryDatasourceSegmentCache`  Cleaner cache sync -------------------- In every sync  the following steps are performed for each datasource:  - Retrieve ALL used segment IDs from metadata store - Atomically update segment IDs in cache and determine list of segment IDs which need to be refreshed. - Fetch payloads of segments that need to be refreshed - Atomically update fetched payloads into the cache - Fetch ALL pending segments - Atomically update pending segments into the cache - Clean up empty intervals from datasource caches  * GroupBy: Fix offsets on outer queries. (#17837)  Prior to this patch  an offset specified on a groupBy that itself has an inner groupBy would lead to an error like "Cannot push down offsets". This happened because of a violated assumption: the processing logic assumes that offsets have been pushed into limits (so limit pushdown optimizations can safely be used).  This patch adjusts processing to incorporate offsets into limits during processing of subqueries. Later on  in post-processing  offsets are applied as written.  * Enable build cache for web-console (#17831)  * run audit fix (#17836)  * Do not block task actions on Overlord if segment metadata cache is syncing (#17824)  * Do not use segment metadata cache until leader has synced  * Read from cache only when synced  but write even if sync is pending  * Fix compilation  * Fix checkstyle  test  * Revert some extra changes  * Add 3 modes of cache usage  * Move enum to SegmentMetadataCache  * Run tests in all 3 cache modes  * Fix docs and IT configs  * Fix config binding  * Remove forbidden api  * Fix typos  docs and enum casing  * Fix doc  * Add json  array  aggregation function tests to quidem (#17842)  This PR adds the sql-native portion of the json  array  and aggregation function tests to quidem.  It adds a total of 9965 queries  with 6752 positive tests and 3213 negative tests.  * Optionally include Content-Disposition header in statement results API response (#17840)  Adds support for an optional filename query parameter to the /druid/v2/sql/statements/{queryId}/results API. When provided  the response will include a header Content-Disposition: attachment; filename="{filename}"  which will instruct a web browser to save the response as a file rather than displaying it inline.  This save-as-attachment behavior could be achieved by adding a "download" attribute to the results link  but this only works for same-origin URLs (as in the Web Console). If the UI origin is different from the Druid API origin  browsers will ignore the attribute and serve the results inline  which is poor UX for files that are potentially very large.  For the sake of consistency  all successful responses in SqlStatementResource.doGetResults may include this header  even if there are no results. Release note  Improved: The "Get query results" statements API supports an optional filename query parameter. When provided  the response will instruct web browsers to save the results as a file instead of showing them inline (via the Content-Disposition header).  * Web console: download follow up (#17845)  * set filename  * update download button  * added markdown support  * add test  * better download  * fix TSV  * better download behaviour and tests  * always show download all button  * Fix flaky unit tests in SegmentBootstrapperTest and KinesisIndexTaskTest (#17841)  Changes: - Fix flakiness in SegmentBootstrapperTest - Make TestSegmentCacheManager thread safe by moving from ArrayList to CopyOnWriteArrayList - Modify assertions to disregard list ordering since order of list modifications is not always deterministic - Fix flaky KinesisIndexTask tests.  * Web console: responding to user feedback about the explore view and fixing bugs (#17844)  * better debounce  * better cumpose filter  * hook up preview filters  * better stack handling  * fix some props  * refactor stack to facet  * fix hover part 1  * line hover part 2  * start adding moduleWhere  * info popover  * add filter icon  * toggle button  * module filter bar  * update TestSegmentCacheManager  * revert some style changes  * validate datasource in CachingClusteredClient as well  * fix build failure and update style  * changes  * add inlineds test  * add sanity check on segment  * inject policy enforcer  * add PolicyEnforcer binding in MSQTestBase  * add check in SinkQuerySegmentWalker  * more tests in realtime server  * revert config change in examples  * revert config change in integration test config  * more tests in msq  * another test for unnest in msq  * add support for policy from extension  * more test  * refactor MSQTaskQueryMakerTest to use an instance of MSQTaskQueryMaker  * Add test for JoinDataSource  * add policyEnforcer to withPolicies  and validate segment after segment mapping  * fix binding and test  * add policy module  * mock planner toolbox  * revert some injection  * add test for stream appenderator  * update PolicyEnforcer to take ReferenceCountingSegment as param  * update to QueryLifecycleTest  * update to SqlTestFramework  * pass enforcer to BroadcastJoinSegmentMapFnProcessor and add test. PolicyEnforcer should also deal with multiple layer wrapped segments/  * ReferenceCountingSegment is not allowed to wrap with a SegmentReference  and PolicyEnforcer now validates all segments  remove test cases for inline/lookup.  * moving ReferenceCountingSegment to another pr  * Revert "Merge remote-tracking branch 'cecemei/debug' into policy"  This reverts commit 25ffb7ca8e5228786c1da65bc000b7de596dcd95  reversing changes made to 1e6632fcb779919f3fc54050ab9b44aba19d3265.  ---------  Signed-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com> Co-authored-by: Virushade <70288012+GWphua@users.noreply.github.com> Co-authored-by: Eyal Yurman <eyal.yurman@gmail.com> Co-authored-by: Kashif Faraz <kashif.faraz@gmail.com> Co-authored-by: Frank Chen <frank.chen021@outlook.com> Co-authored-by: Chetan Patidar <122344823+chetanpatidar26@users.noreply.github.com> Co-authored-by: aho135 <ash023@ucsd.edu> Co-authored-by: Adithya Chakilam <35785271+adithyachakilam@users.noreply.github.com> Co-authored-by: Vadim Ogievetsky <vadim@ogievetsky.com> Co-authored-by: Misha <mikhailsviatohorof@gmail.com> Co-authored-by: sviatahorau <mikhail.sviatahorau@deep.bi> Co-authored-by: Benedict Jin <asdf2014@apache.org> Co-authored-by: Clint Wylie <cwylie@apache.org> Co-authored-by: Katya Macedo <38017980+ektravel@users.noreply.github.com> Co-authored-by: Victoria Lim <vtlim@users.noreply.github.com> Co-authored-by: Zoltan Haindrich <kirk@rxd.hu> Co-authored-by: Gian Merlino <gianmerlino@gmail.com> Co-authored-by: Emmanuel Ferdman <emmanuelferdman@gmail.com> Co-authored-by: Om Kenge <88768848+omkenge@users.noreply.github.com> Co-authored-by: Karan Kumar <karankumar1100@gmail.com> Co-authored-by: Lars Francke <lars.francke@stackable.tech> Co-authored-by: Adarsh Sanjeev <adarshsanjeev@gmail.com> Co-authored-by: Akshat Jain <akjn11@gmail.com> Co-authored-by: Andy Tsai <61856143+weishiuntsai@users.noreply.github.com> Co-authored-by: Maytas Monsereenusorn <maytasm@apache.org> Co-authored-by: jtuglu-netflix <jtuglu@netflix.com> Co-authored-by: Lucas Capistrant <capistrant@users.noreply.github.com>
apache,druid,5e7d353eabfabee1e8095e5b789e3b42ac5b1bdc,https://github.com/apache/druid/commit/5e7d353eabfabee1e8095e5b789e3b42ac5b1bdc,Add audit logging to BasicAuthorizerResource update methods (#17916)  * Add audit logging to BasicAuthorizerResource update methods  This commit adds missing audit logging to three update methods in the BasicAuthorizerResource class:  - authorizerUserUpdateListener - authorizerGroupMappingUpdateListener - authorizerUpdateListener (deprecated)  Each method now calls performAuditIfSuccess() after processing but before returning the response  ensuring that successful update operations are properly recorded in the audit log. The audit messages include the authorizer name and payload size to provide context about the operations being performed.  This change improves security monitoring and compliance by ensuring all authorization-related changes are properly tracked in audit logs.  * Update BasicAuthorizerResource.java
apache,druid,c0cc27ce87ba4aa38cf3073f8f8a7c9d8ec43822,https://github.com/apache/druid/commit/c0cc27ce87ba4aa38cf3073f8f8a7c9d8ec43822,Improve performance of segment metadata cache on Overlord (#17785)  Description ----------- #17653 introduces a cache for segment metadata on the Overlord. This patch is a follow up to that to make the cache more robust  performant and debug-friendly.  Changes --------- - Do not cache unused segments This significantly reduces sync time in cases where the cluster has a lot of unused segments. Unused segments are needed only during segment allocation to ensure that a duplicate ID is not allocated. This is a rare DB query which is supported by sufficient indexes and thus need not be cached at the moment. - Update cache directly when segments are marked as unused to avoid race conditions with DB sync. - Fix NPE when using segment metadata cache with concurrent locks. - Atomically update segment IDs and pending segments in a `HeapMemoryDatasourceSegmentCache` using methods `syncSegmentIds()` and `syncPendingSegments()` rather than updating one by one. This ensures that the locks are held for a shorter period and the update made to the cache is atomic.  Main updated classes ---------------------- - `IndexerMetadataStorageCoordinator` - `OverlordDataSourcesResource` - `HeapMemorySegmentMetadataCache` - `HeapMemoryDatasourceSegmentCache`  Cleaner cache sync -------------------- In every sync  the following steps are performed for each datasource:  - Retrieve ALL used segment IDs from metadata store - Atomically update segment IDs in cache and determine list of segment IDs which need to be refreshed. - Fetch payloads of segments that need to be refreshed - Atomically update fetched payloads into the cache - Fetch ALL pending segments - Atomically update pending segments into the cache - Clean up empty intervals from datasource caches
apache,druid,08af98c73bf7f35e4c7e2b136a63158d11b02348,https://github.com/apache/druid/commit/08af98c73bf7f35e4c7e2b136a63158d11b02348,Add the capability to turboload segments onto historicals (#17775)  Add the capability to set Historicals into a turbo loading mode  to focus on loading segments at the cost of query performance.  Context -------- Currently  when a new Historical is started  it initially starts out using a bootstrap thread pool. It uses this thread pool to load any existing cached segments and broadcast segments. Once it loads any segments from both these sources  the historical switches to a smaller thread-pool and begins to serve queries.  In certain cases  it would be useful to have the historical switch back to this mode  and focus on loading segments  either to continue loading the initial non-bootstrap segments  or to catch up with assigned segments.  This PR adds a coordinator dynamic config that allows servers to be configured to use the larger bootstrap threadpool to load segments faster.  Changes --------- - Added a new dynamic coordinator configuration  `turboLoadingNodes`. - Ignore  `druid.coordinator.loadqueuepeon.http.batchSize` for servers in `turboLoadingNodes` - Add API on historical to return loading capabilities i.e. num loading threads in normal and turbo mode
apache,druid,d9a58a7bbd8846b7010d7379efbae366499ec5d4,https://github.com/apache/druid/commit/d9a58a7bbd8846b7010d7379efbae366499ec5d4,Move segment update APIs from Coordinator to Overlord (#17545)  Summary of changes --------------------- - Add `OverlordDataSourcesResource` with APIs to mark segments used/unused - Add corresponding methods to `OverlordClient` - Deprecate Coordinator APIs to update segments - Use `OverlordClient` in `DataSourcesResource` so that Coordinator APIs internally call the corresponding Overlord APIs - If the API call fails  fall back to updating the metadata store directly - Audit these actions only on the Overlord  Other minor changes --------------------- - Do not perform null check on `OverlordClient` on the coordinator side `DataSourcesResource`. `OverlordClient` is always non-null in production. - Add new tests  fix existing ones - Complete the implementation of `TestSegmentsMetadataManager`  New Overlord APIs ------------------ - Mark all segments of a datasource as unused: `POST /druid/indexer/v1/datasources/{dataSourceName}` - Mark all (non-overshadowed) segments of a datasource as used: `DELETE /druid/indexer/v1/datasources/{dataSourceName}` - Mark multiple segments as used `POST /druid/indexer/v1/datasources/{dataSourceName}/markUsed` - Mark multiple (non-overshadowed) segments as unused `POST /druid/indexer/v1/datasources/{dataSourceName}/markUnused` - Mark a single segment as used: `POST /druid/indexer/v1/datasources/{dataSourceName}/segments/{segmentId}` - Mark a single segment as unused: `DELETE /druid/indexer/v1/datasources/{dataSourceName}/segments/{segmentId}`
apache,druid,9c25226e06da1eb7d3a47742dd4e65337585142c,https://github.com/apache/druid/commit/9c25226e06da1eb7d3a47742dd4e65337585142c,QueryableIndexSegment: Re-use time boundary inspector. (#17397)  This patch re-uses timeBoundaryInspector for each cursor holder  which enables caching of minDataTimestamp and maxDataTimestamp.  Fixes a performance regression introduced in #16533  where these fields stopped being cached across cursors. Prior to that patch  they were cached in the QueryableIndexStorageAdapter.
apache,druid,60daddedf8b0a9faf1edeb4603eb4b8e86fd3dc2,https://github.com/apache/druid/commit/60daddedf8b0a9faf1edeb4603eb4b8e86fd3dc2,SeekableStreamSupervisor: Use workerExec as the client connectExec. (#17394)  * SeekableStreamSupervisor: Use workerExec as the client connectExec.  This patch uses the already-existing per-supervisor workerExec as the connectExec for task clients  rather than using the process-wide default ServiceClientFactory pool.  This helps prevent callbacks from backlogging on the process-wide pool. It's especially useful for retries  where callbacks may need to establish new TCP connections or perform TLS handshakes.  * Fix compilation  tests.  * Fix style.
apache,druid,db7cc4634c5df247376e251fb1f502cfc7ae934d,https://github.com/apache/druid/commit/db7cc4634c5df247376e251fb1f502cfc7ae934d,Dart: Smoother handling of stage early-exit. (#17228)  Stages can be instructed to exit before they finish  especially when a downstream stage includes a "LIMIT". This patch has improvements related to early-exiting stages.  Bug fix:  - WorkerStageKernel: Don't allow fail() to set an exception if the stage is already in a terminal state (FINISHED or FAILED). If fail() is called while in a terminal state  log the exception  then throw it away. If it's a cancellation exception  don't even log it. This fixes a bug where a stage that exited early could transition to FINISHED and then to FAILED  causing the overall query to fail.  Performance:  - DartWorkerManager previously sent stopWorker commands to workers even when "interrupt" was false. Now it only sends those commands when "interrupt" is true. The method javadoc already claimed this is what the method did  but the implementation did not match the javadoc. This reduces the number of RPCs by 1 per worker per query.  Quieter logging:  - In ReadableByteChunksFrameChannel  skip logging exception from setError if the channel has been closed. Channels are closed when readers are done with them  so at that point  we wouldn't be interested in the errors.  - In RunWorkOrder  skip calling notifyListener on failure of the main work  in the case when stop() has already been called. The stop() method will set its own error using CanceledFault. This enables callers to detect when a stage was canceled vs. failed for some other reason.  - In WorkerStageKernel  skip logging cancellation errors in fail(). This is made possible by the previous change in RunWorkOrder.
apache,druid,878adff9aaa8d28ddbba0119c44dae4809dd5744,https://github.com/apache/druid/commit/878adff9aaa8d28ddbba0119c44dae4809dd5744,MSQ profile for Brokers and Historicals. (#17140)  This patch adds a profile of MSQ named "Dart" that runs on Brokers and Historicals  and which is compatible with the standard SQL query API. For more high-level description  and notes on future work  refer to #17139.  This patch contains the following changes  grouped into packages.  Controller (org.apache.druid.msq.dart.controller):  The controller runs on Brokers. Main classes are   - DartSqlResource  which serves /druid/v2/sql/dart/. - DartSqlEngine and DartQueryMaker  the entry points from SQL that actually run the MSQ controller code. - DartControllerContext  which configures the MSQ controller. - DartMessageRelays  which sets up relays (see "message relays" below) to read messages from workers' DartControllerClients. - DartTableInputSpecSlicer  which assigns work based on a TimelineServerView.  Worker (org.apache.druid.msq.dart.worker)  The worker runs on Historicals. Main classes are   - DartWorkerResource  which supplies the regular MSQ WorkerResource  plus Dart-specific APIs. - DartWorkerRunner  which runs MSQ worker code. - DartWorkerContext  which configures the MSQ worker. - DartProcessingBuffersProvider  which provides processing buffers from sliced-up merge buffers. - DartDataSegmentProvider  which provides segments from the Historical's local cache.  Message relays (org.apache.druid.messages):  To avoid the need for Historicals to contact Brokers during a query  which would create opportunities for queries to get stuck  all connections are opened from Broker to Historical. This is made possible by a message relay system  where the relay server (worker) has an outbox of messages.  The relay client (controller) connects to the outbox and retrieves messages. Code for this system lives in the "server" package to keep it separate from the MSQ extension and make it easier to maintain. The worker-to-controller ControllerClient is implemented using message relays.  Other changes:  - Controller: Added the method "hasWorker". Used by the ControllerMessageListener to notify the appropriate controllers when a worker fails. - WorkerResource: No longer tries to respond more than once in the "httpGetChannelData" API. This comes up when a response due to resolved future is ready at about the same time as a timeout occurs. - MSQTaskQueryMaker: Refactor to separate out some useful functions for reuse in DartQueryMaker. - SqlEngine: Add "queryContext" to "resultTypeForSelect" and "resultTypeForInsert". This allows the DartSqlEngine to modify result format based on whether a "fullReport" context parameter is set. - LimitedOutputStream: New utility class. Used when in "fullReport" mode. - TimelineServerView: Add getDruidServerMetadata as a performance optimization. - CliHistorical: Add SegmentWrangler  so it can query inline data  lookups  etc. - ServiceLocation: Add "fromUri" method  relocating some code from ServiceClientImpl. - FixedServiceLocator: New locator for a fixed set of service locations. Useful for URI locations.
apache,druid,b9a4c73e525d7addd9cde078e62490e2943da6e9,https://github.com/apache/druid/commit/b9a4c73e525d7addd9cde078e62490e2943da6e9,Window Functions : Improve performance by comparing Strings in frame bytes without converting them (#17091)
apache,druid,5b7fb5fbca870fa4724734ef77481aa96cbe7dad,https://github.com/apache/druid/commit/5b7fb5fbca870fa4724734ef77481aa96cbe7dad,Speed up FrameFileTest  SuperSorterTest. (#17068)  * Speed up FrameFileTest  SuperSorterTest.  These are two heavily parameterized tests that  together  account for about 60% of runtime in the test suite.  FrameFileTest changes:  1) Cache frame files in a static  rather than building the frame file for each parameterization of the test.  2) Adjust TestArrayCursorFactory to cache the signature  rather than re-creating it on each call to getColumnCapabilities.  SuperSorterTest changes:  1) Dramatically reduce the number of tests that run with "maxRowsPerFrame" = 1. These are particularly slow due to writing so many small files. Some still run  since it's useful to test edge cases  but much fewer than before.  2) Reduce the "maxActiveProcessors" axis of the test from [1  2  4] to [1  3]. The aim is to reduce the number of cases while still getting good coverage of the feature.  3) Reduce the "maxChannelsPerProcessor" axis of the test from [2  3  8] to [2  7]. The aim is to reduce the number of cases while still getting good coverage of the feature.  4) Use in-memory input channels rather than file channels.  5) Defer formatting of assertion failure messages until they are needed.  6) Cache the cursor factory and its signature in a static.  7) Cache sorted test rows (used for verification) in a static.  * It helps to include the file.  * Style.
apache,druid,175636b28ffa296eae36eea57b6a46f358fca25b,https://github.com/apache/druid/commit/175636b28ffa296eae36eea57b6a46f358fca25b,Frame writers: Coerce numeric and array types in certain cases. (#16994)  This patch adds "TypeCastSelectors"  which is used when writing frames to perform two coercions:  - When a numeric type is desired and the underlying type is non-numeric or unknown  the underlying selector is wrapped  "getObject" is called and the result is coerced using "ExprEval.ofType". This differs from the prior behavior where the primitive methods like "getLong"  "getDouble"  etc  would be called directly. This fixes an issue where a column would be read as all-zeroes when its SQL type is numeric and its physical type is string  which can happen when evolving a column's type from string to number.  -  When an array type is desired  the underlying selector is wrapped  "getObject" is called  and the result is coerced to Object[]. This coercion replaces some earlier logic from #15917.
apache,druid,ba6f804f4844a6cecaee0fef27577af7828f928e,https://github.com/apache/druid/commit/ba6f804f4844a6cecaee0fef27577af7828f928e,Fix compaction status API response (#17006)  Description: #16768 introduces new compaction APIs on the Overlord `/compact/status` and `/compact/progress`. But the corresponding `OverlordClient` methods do not return an object compatible with the actual endpoints defined in `OverlordCompactionResource`.  This patch ensures that the objects are compatible.  Changes: - Add `CompactionStatusResponse` and `CompactionProgressResponse` - Use these as the return type in `OverlordClient` methods and as the response entity in `OverlordCompactionResource` - Add `SupervisorCleanupModule` bound on the Coordinator to perform cleanup of supervisors. Without this module  Coordinator cannot deserialize compaction supervisors.
apache,druid,e28424ea25871d59d3c8e96cc8a5ea5d666690b3,https://github.com/apache/druid/commit/e28424ea25871d59d3c8e96cc8a5ea5d666690b3,Enable rollup on multi-value dimensions for compaction with MSQ engine (#16937)  Currently compaction with MSQ engine doesn't work for rollup on multi-value dimensions (MVDs)  the reason being the default behaviour of grouping on MVD dimensions to unnest the dimension values; for instance grouping on `[s1 s2]` with aggregate `a` will result in two rows: `<s1 a>` and `<s2 a>`.  This change enables rollup on MVDs (without unnest) by converting MVDs to Arrays before rollup using virtual columns  and then converting them back to MVDs using post aggregators. If segment schema is available to the compaction task (when it ends up downloading segments to get existing dimensions/metrics/granularity)  it selectively does the MVD-Array conversion only for known multi-valued columns; else it conservatively performs this conversion for all `string` columns.
apache,druid,0603d5153d8177856687c957ae0061492c81e1d4,https://github.com/apache/druid/commit/0603d5153d8177856687c957ae0061492c81e1d4,Segments sorted by non-time columns. (#16849)  * Segments primarily sorted by non-time columns.  Currently  segments are always sorted by __time  followed by the sort order provided by the user via dimensionsSpec or CLUSTERED BY. Sorting by __time enables efficient execution of queries involving time-ordering or granularity. Time-ordering is a simple matter of reading the rows in stored order  and granular cursors can be generated in streaming fashion.  However  for various workloads  it's better for storage footprint and query performance to sort by arbitrary orders that do not start with __time. With this patch  users can sort segments by such orders.  For spec-based ingestion  users add "useExplicitSegmentSortOrder: true" to dimensionsSpec. The "dimensions" list determines the sort order. To define a sort order that includes "__time"  users explicitly include a dimension named "__time".  For SQL-based ingestion  users set the context parameter "useExplicitSegmentSortOrder: true". The CLUSTERED BY clause is then used as the explicit segment sort order.  In both cases  when the new "useExplicitSegmentSortOrder" parameter is false (the default)  __time is implicitly prepended to the sort order  as it always was prior to this patch.  The new parameter is experimental for two main reasons. First  such segments can cause errors when loaded by older servers  due to violating their expectations that timestamps are always monotonically increasing. Second  even on newer servers  not all queries can run on non-time-sorted segments. Scan queries involving time-ordering and any query involving granularity will not run. (To partially mitigate this  a currently-undocumented SQL feature "sqlUseGranularity" is provided. When set to false the SQL planner avoids using "granularity".)  Changes on the write path:  1) DimensionsSpec can now optionally contain a __time dimension  which controls the placement of __time in the sort order. If not present  __time is considered to be first in the sort order  as it has always been.  2) IncrementalIndex and IndexMerger are updated to sort facts more flexibly; not always by time first.  3) Metadata (stored in metadata.drd) gains a "sortOrder" field.  4) MSQ can generate range-based shard specs even when not all columns are singly-valued strings. It merely stops accepting new clustering key fields when it encounters the first one that isn't a singly-valued string. This is useful because it enables range shard specs on "someDim" to be created for clauses like "CLUSTERED BY someDim  __time".  Changes on the read path:  1) Add StorageAdapter#getSortOrder so query engines can tell how a segment is sorted.  2) Update QueryableIndexStorageAdapter  IncrementalIndexStorageAdapter  and VectorCursorGranularizer to throw errors when using granularities on non-time-ordered segments.  3) Update ScanQueryEngine to throw an error when using the time-ordering "order" parameter on non-time-ordered segments.  4) Update TimeBoundaryQueryRunnerFactory to perform a segment scan when running on a non-time-ordered segment.  5) Add "sqlUseGranularity" context parameter that causes the SQL planner to avoid using granularities other than ALL.  Other changes:  1) Rename DimensionsSpec "hasCustomDimensions" to "hasFixedDimensions" and change the meaning subtly: it now returns true if the DimensionsSpec represents an unchanging list of dimensions  or false if there is some discovery happening. This is what call sites had expected anyway.  * Fixups from CI.  * Fixes.  * Fix missing arg.  * Additional changes.  * Fix logic.  * Fixes.  * Fix test.  * Adjust test.  * Remove throws.  * Fix styles.  * Fix javadocs.  * Cleanup.  * Smoother handling of null ordering.  * Fix tests.  * Missed a spot on the merge.  * Fixups.  * Avoid needless Filters.and.  * Add timeBoundaryInspector to test.  * Fix tests.  * Fix FrameStorageAdapterTest.  * Fix various tests.  * Use forceSegmentSortByTime instead of useExplicitSegmentSortOrder.  * Pom fix.  * Fix doc.
apache,druid,e2516d9a674ce49342ac80e7509f1ccf5375a1c0,https://github.com/apache/druid/commit/e2516d9a674ce49342ac80e7509f1ccf5375a1c0,WriteOutBytes improvements  This PR generally improves the working of WriteOutBytes and WriteOutMedium. Some analysis of usage of TmpFileSegmentWriteOutMedium shows that they periodically get used for very small things. The overhead of creating a tmp file is actually very large. To improve the performance in these cases  this PR modifies TmpFileSegmentWriteOutMedium to return a heap-based WriteOutBytes that falls back to making a tmp file when it actually fills up. --------- Co-authored-by: imply-cheddar <eric.tschetter@imply.io>
apache,druid,806649f8af6110c4a60b3014c9014ef4e012008c,https://github.com/apache/druid/commit/806649f8af6110c4a60b3014c9014ef4e012008c,SQL: Fix nullable DATE  TIMESTAMP reduction. (#16915)  Reduction of nullable DATE and TIMESTAMP expressions did not perform a necessary null check  so would in some cases reduce to 1970-01-01 00:00:00 (epoch) rather than NULL.
apache,druid,eaa09937bc6081a8401e40bd0be49fbd5ce8a65c,https://github.com/apache/druid/commit/eaa09937bc6081a8401e40bd0be49fbd5ce8a65c,SuperSorter: direct merging  increased parallelism. (#16775)  Two performance enhancements:  1) Direct merging of input frames to output channels  without any temporary files  if all input frames fit in memory.  2) When doing multi-level merging (now called "external mode")  improve parallelism by boosting up the number of mergers in the penultimate level.  To support direct merging  FrameChannelMerger is enhanced such that the output partition min/max values are used to filter input frames. This is necessary because all direct mergers read all input frames  but only rows corresponding to a single output partition.
apache,druid,dca31d466c90f9bf2faa1fa0a0f6e7aab62c8fb5,https://github.com/apache/druid/commit/dca31d466c90f9bf2faa1fa0a0f6e7aab62c8fb5,minor adjustments for performance (#16714)  changes: * switch to stop using some string.format * switch some streams to classic loops
apache,doris,4da1c8ab1942108811007ccf69879929fce38c6d,https://github.com/apache/doris/commit/4da1c8ab1942108811007ccf69879929fce38c6d,[opt](mtmv) optimize mtmv rewrite performance (#49514)  ### What problem does this PR solve?  - Obtaining available materialized view adjustments after partition pruning RBO rewrite rules. This reduces partition version comparison overhead for partitioned materializations  thereby improving performance.  - Set maximum time threshold for transparent query rewriting with default value of 1000ms. Rewrite attempts will be terminated if exceeding this threshold. set materialized_view_rewrite_duration_threshold_ms = 1000  - Optimize code structure in transparent rewriting framework: 1. Replace Lambda expressions with conventional for-loops 2. Remove redundant member variables  - Cache available partitions of materialized views. Since calculating available partitions for materialized views is time-consuming  caching can significantly enhance performance.   Co-authored-by: zhangdong <zhangdong@selectdb.com>
apache,doris,5f0b89f06ac12b46a37b032c553c5b11471a26c1,https://github.com/apache/doris/commit/5f0b89f06ac12b46a37b032c553c5b11471a26c1,[improvement](statistics)Agg table set preagg on when doing sample analyzing. (#49918)  ### What problem does this PR solve?  This pr includes 3 changes. 1. Nereids support set ScanNode preagg on by hint  like this: select * from table1 /*+PREAGGOPEN*/ 2. When sample analyze agg table and mor unique table  set preagg on to improve performance. 3. Skip sample analyzing agg table and mor unique table's value columns.
apache,doris,3aa18fc50d926b374438bcb0965d4c0d30421222,https://github.com/apache/doris/commit/3aa18fc50d926b374438bcb0965d4c0d30421222,[feature](agg function) support corr_welford agg function (#49712)  In the past  due to performance considerations  the implementation of corr was relatively simple  avoiding division operations except in the final step. However  this approach resulted in larger errors  so corr_welford was introduced to improve the accuracy of the calculations. ``` 0.7605339114107809 corr_result 0.7605339114107733 corr_welford_result ```
apache,doris,0d8d4827e75e3cbc54ad601b19488ec0b3c7c944,https://github.com/apache/doris/commit/0d8d4827e75e3cbc54ad601b19488ec0b3c7c944,[Chore](runtime-filter) enlarge default value of runtimeBloomFilterMaxSize  runtimeFilterMaxInNum (#49689)  ### What problem does this PR solve? enlarge default value of runtimeBloomFilterMaxSize  runtimeFilterMaxInNum In some scenarios with small data volume  using in filter performance is better than bloom filter. In some scenarios with large data volume  we need a larger runtimeBloomFilterMaxSize so that the bloom filter can filter data normally.  ### Check List (For Author)  - Test <!-- At least one of them must be included. --> - [ ] Regression test - [ ] Unit Test - [ ] Manual test (add detailed scripts or steps below) - [x] No need to test or manual test. Explain why: - [ ] This is a refactor/code format and no logic has been changed. - [x] Previous test can cover this change. - [ ] No code files have been changed. - [ ] Other reason <!-- Add your reason?  -->  - Behavior changed: - [x] No. - [ ] Yes. <!-- Explain the behavior change -->  - Does this need documentation? - [x] No. - [ ] Yes. <!-- Add document PR link here. eg: https://github.com/apache/doris-website/pull/1214 -->  ### Check List (For Reviewer who merge this PR)  - [ ] Confirm the release note - [ ] Confirm test cases - [ ] Confirm document - [ ] Add branch pick label <!-- Add branch pick label that this PR should merge into -->
apache,doris,a65e0806ce55224b3b10be5d896f33ce7f30a2cd,https://github.com/apache/doris/commit/a65e0806ce55224b3b10be5d896f33ce7f30a2cd,[opt](metrics) optimize performance of metrics endpoint (#49380)  the `http://<fe_ip>:<fe_http_port>/metrics` maybe made the frontends hung  when the threads num too large  this pr replace `ThreadMXBean.getThreadInfos` to `ThreadMXBean.dumpAllThreads` to optimize performance  see also: 1. https://issues.apache.org/jira/browse/HADOOP-16850 2. https://bugs.openjdk.org/browse/JDK-8185005 3. https://github.com/apache/skywalking/discussions/9190
apache,doris,7a78b9d67198624f7ddcbe3b7f1a8ee6d88e1e70,https://github.com/apache/doris/commit/7a78b9d67198624f7ddcbe3b7f1a8ee6d88e1e70,[feature](inverted index) Add a basic tokenizer (#48716)  Problem Summary:  1. Implement a basic tokenizer capable of efficiently performing basic segmentation on both Chinese and English text.
apache,doris,0c958d21467311a728e61bdb0da64c745b1fd317,https://github.com/apache/doris/commit/0c958d21467311a728e61bdb0da64c745b1fd317,[opt](mtmv) Opt materialized view rewrite performance when the num of struct infos are huge (#48782)  ### What problem does this PR solve?  Opt materialized view rewrite performance when the num of struct infos are huge  Optimize the recursive algorithm to reduce the number of recursive calls. If a group has already been refreshed  skip subsequent refreshes.
apache,doris,22cfc09ed9ec824b0ba50bcb5812bc979b4fa1cc,https://github.com/apache/doris/commit/22cfc09ed9ec824b0ba50bcb5812bc979b4fa1cc,[opt](vault) Check hdfs connectivity when creating hdfs storage vault (#48369)  When creating an HDFS storage vault  perform connection and permission checks to avoid outputting error messages that could cause the storage vault to throw errors during usage after creation.
apache,doris,ccbc3286a92d881a93537bb83b8e26e61b08970b,https://github.com/apache/doris/commit/ccbc3286a92d881a93537bb83b8e26e61b08970b,[enhance](mtmv) insert overwrite of mtmv force drop partition (#48074)  insert overwrite of mtmv force drop partition  because recyclebin process partition lead to performance issue.
apache,doris,3a723b05a4fb4794b5698b06f966887d760487bd,https://github.com/apache/doris/commit/3a723b05a4fb4794b5698b06f966887d760487bd,[fix](Outfile) add two fields to `SELECT INTO OUTFILE` (#48144)  Problem Summary:  To better monitor the performance of Outfile  we've added two new fields to the Outfile return results: WriteTime and WriteSpeed. WriteTime is the time each writer takes to write data  measured in seconds. WriteSpeed is the average data write speed for each writer  measured in KB/s.
apache,doris,235ffb81c320cabdd48781644409e90855f59712,https://github.com/apache/doris/commit/235ffb81c320cabdd48781644409e90855f59712,[fix](Export) change the export logical (#48022)  Problem Summary:  Previously  Export would split the task into multiple threads  and each thread would further split the task into multiple outfiles  which were executed sequentially. Each thread will decide how many outfiles to split into according to `maximum_tablets_of_outfile_in_export` and the actual number of partitions/buckets of the data. Each thread executes multiple outfiles sequentially  with each Outfile task waiting for the previous one to complete before starting. This not only reduces export performance but also worsens the user experience  as users cannot clearly determine the actual concurrency performance of an Export Job.  Now  this logic has been simplified  and each thread will only handle one outfile without splitting it into multiple Outfile tasks. The parallelism value specified by the user in the `EXPORT` statement corresponds to the number of threads  with each thread handling one Outfile statement. This not only improves the user experience but also increases CPU utilization.
apache,doris,48ea35b464feff1f8f38d5c5326db95fc5de311b,https://github.com/apache/doris/commit/48ea35b464feff1f8f38d5c5326db95fc5de311b,[opt](paimon)Use the API instead of reading from the meta table (#47544)  ### What problem does this PR solve?  1. Use `latestSnapshotId` to get the latest snapshot id. 2. If there are no additional options  no copy operation is performed on the table. 3. Use `DataTable.schemaManager().schema()` to get schema. 4. Use `Catalog.listPartitions` to get partitions.
apache,doris,2945de9e2359aac1be78061c10b63db0bb5043b4,https://github.com/apache/doris/commit/2945de9e2359aac1be78061c10b63db0bb5043b4,[opt](iceberg)Improve performance by not retrieving table objects for hms (#47782)  ### What problem does this PR solve?  When Iceberg retrieves all tables in a database  it first fetches all table names  then retrieves the corresponding table objects based on the table names  and finally determines whether a table is an Iceberg table by checking if the type attribute of the table is "iceberg".  However  for lower - version HMS (Hive Metastore Service)  it doesn't have the `getTableObjectsByName` method and can only use the `get_table` method. When there are 1000 tables in a database  the `get_table` method will be called 1000 times  which is extremely time consuming.  Therefore  by default  the table type is not checked  and all tables are displayed.  If you still want to perform this check  you can add a configuration to the catalog:`"list-all-tables"="false"`
apache,doris,9e0c754ae56bbfecde24a7014a3f7ca179a72ea2,https://github.com/apache/doris/commit/9e0c754ae56bbfecde24a7014a3f7ca179a72ea2,[fix](bdb) reset interrupted flag before calling bdbje (#47874)  ### What problem does this PR solve?  This pull request includes changes to ensure that the interrupted flag of the current thread is reset before performing certain operations to prevent failures in acquiring locks. The changes primarily affect the `BDBEnvironment` and `BDBJEJournal` classes.  ``` Caused by: java.lang.InterruptedException at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326) ~[?:1.8.0_352-352] at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.tryLock(ReentrantReadWriteLock.java:871) ~[?:1.8.0_352-352] at com.sleepycat.je.latch.SharedLatchImpl.acquireShared(SharedLatchImpl.java:103) ~[je-18.3.14-doris-SNAPSHOT.jar:18.3.14-doris-SNAPSHOT] at com.sleepycat.je.tree.Tree.getRootINInternal(Tree.java:447) ~[je-18.3.14-doris-SNAPSHOT.jar:18.3.14-doris-SNAPSHOT] at com.sleepycat.je.tree.Tree.getRootIN(Tree.java:431) ~[je-18.3.14-doris-SNAPSHOT.jar:18.3.14-doris-SNAPSHOT] at com.sleepycat.je.tree.Tree.search(Tree.java:2185) ~[je-18.3.14-doris-SNAPSHOT.jar:18.3.14-doris-SNAPSHOT] at com.sleepycat.je.tree.Tree.getFirstNode(Tree.java:812) ~[je-18.3.14-doris-SNAPSHOT.jar:18.3.14-doris-SNAPSHOT] at com.sleepycat.je.dbi.CursorImpl.positionFirstOrLast(CursorImpl.java:1776) ~[je-18.3.14-doris-SNAPSHOT.jar:18.3.14-doris-SNAPSHOT] at com.sleepycat.je.dbi.CursorImpl.traverseDbWithCursor(CursorImpl.java:3953) ~[je-18.3.14-doris-SNAPSHOT.jar:18.3.14-doris-SNAPSHOT] at com.sleepycat.je.dbi.DbTree.getDbNames(DbTree.java:1808) ~[je-18.3.14-doris-SNAPSHOT.jar:18.3.14-doris-SNAPSHOT] at com.sleepycat.je.Environment.getDatabaseNames(Environment.java:2458) ~[je-18.3.14-doris-SNAPSHOT.jar:18.3.14-doris-SNAPSHOT] ... 21 more ```  This a self-defend logic. Because we found that some times other logic may set the thread as interrupted and does not handle it.
apache,doris,1ff07a391a540882b40f7a3f5ab693dbee69ffbf,https://github.com/apache/doris/commit/1ff07a391a540882b40f7a3f5ab693dbee69ffbf,[Opt](multi-catalog)Improve performance by introducing cache of list directory files when getting split for each query. (#43913)  ### What problem does this PR solve?  Refer to trino to implement the cache mechanism of multiple hive tables at the query level to obtain the file split list of each partition. Because files within a query should have the same visibility  the split list of partitions that see the same table should be consistent across the query scope. So this cache is reasonable and should be enabled by default. The mechanism in Trino is transactional level. A transaction can see the same table  so the command is `TransactionScopeCachingDirectoryLister`. This name is retained for Doris to expand to the transaction concept in the future. In addition  for this scenario  because the caffeine cache currently used by doris has an elimination phase strategy  the existing cache items in the window area may be eliminated immediately after the weight is updated. Therefore  `EvictableCache` which based on guava was introduced and eliminated based on segment LRU.
apache,doris,e5f4deab77049fc88cc5f86d7002ac68ffbd0a09,https://github.com/apache/doris/commit/e5f4deab77049fc88cc5f86d7002ac68ffbd0a09,[Fix](catalog)Fixes query failures for Paimon tables stored in Kerberized HDFS (#47192)  ### What problem does this PR solve? Using JNI to directly read Paimon tables can lead to query failures when the Paimon storage is on HDFS with Kerberos authentication enabled.  #### Reproduction Steps: - Create a Paimon catalog stored on an HDFS cluster with Kerberos authentication enabled. - Execute the command: SET force_jni_scanner=true;. - To ensure a clean environment  restart the BE (Backend) service. - Perform any query on a table within the catalog.  ``` 2025-01-18 09:25:06  WARN Thread-13 org.apache.doris.paimon.PaimonJniScanner.open(PaimonJniScanner.java:126) - Failed to open paimon_scanner: java.io.UncheckedIOException: org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN  KERBEROS] com.google.common.util.concurrent.UncheckedExecutionException: java.io.UncheckedIOException: org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN  KERBEROS] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2085) at com.google.common.cache.LocalCache.get(LocalCache.java:4017) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4040) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4989) at org.apache.doris.paimon.PaimonTableCache.getTable(PaimonTableCache.java:84) at org.apache.doris.paimon.PaimonJniScanner.initTable(PaimonJniScanner.java:237) at org.apache.doris.paimon.PaimonJniScanner.open(PaimonJniScanner.java:122) Caused by: java.io.UncheckedIOException: org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN  KERBEROS] at org.apache.paimon.hive.HiveCatalog.createHiveCatalog(HiveCatalog.java:708) at org.apache.paimon.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:48) at org.apache.paimon.catalog.CatalogFactory.createCatalog(CatalogFactory.java:76) at org.apache.paimon.catalog.CatalogFactory.createCatalog(CatalogFactory.java:66) at org.apache.doris.paimon.PaimonTableCache.createCatalog(PaimonTableCache.java:75) at org.apache.doris.paimon.PaimonTableCache.loadTable(PaimonTableCache.java:58) at org.apache.doris.paimon.PaimonTableCache.access$000(PaimonTableCache.java:38) at org.apache.doris.paimon.PaimonTableCache$1.load(PaimonTableCache.java:51) at org.apache.doris.paimon.PaimonTableCache$1.load(PaimonTableCache.java:48) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3574) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2189) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2079) ... 6 more ``` #### changes  This PR addresses an issue where queries fail when attempting to directly read Paimon tables using JNI  specifically in environments where HDFS is used as the storage backend and Kerberos authentication is enabled. The failure is caused by the lack of proper Kerberos authentication handling in the JNI implementation.
apache,doris,e91468b20b77482cb353c3760abf51c78e9cfbe2,https://github.com/apache/doris/commit/e91468b20b77482cb353c3760abf51c78e9cfbe2,[enhance](runtime filter) impl partition pruning in runtime filer (#47025)  This PR implements partition pruning through runtime filters. When executing a SQL query like: ```sql SELECT count(*) FROM int_partition_table WHERE partition_col = ( SELECT partition_col FROM int_partition_table GROUP BY partition_col HAVING count(*) > 0 ORDER BY partition_col DESC LIMIT 1 ) ``` During execution  the backend (BE) will receive a dynamic runtime filter condition `partition_col = xxx`. Since partition_col is a partitioning column  we can use its value to determine if the partition can be pruned.  Additionally  this mechanism also supports filtering queries like: ```sql SELECT count(*) FROM int_partition_table WHERE func(partition_col) = xxx ``` If func cannot be evaluated at the frontend (FE)  the frontend will not perform partition pruning. However  since the backend can compute func  this mechanism allows us to handle pruning scenarios that are not possible at the frontend  providing a more efficient pruning process on the backend side.
apache,doris,6cbde0c4fdbcabca947a82245486268c236ced22,https://github.com/apache/doris/commit/6cbde0c4fdbcabca947a82245486268c236ced22,[opt](paimon)Upgrade the Paimon version to 1.0.0 and Iceberg to 1.6.1 (#46990)  ### What problem does this PR solve?  Problem Summary:  Upgrade the Paimon version to 1.0.0  By default  paimon uses a caching catalog to cache some data to improve read performance. FYI: https://paimon.apache.org/docs/1.0/maintenance/configurations/#catalogoptions  If you do not want to use this catalog  you can add a configuration `paimon.cache-enabled ` to turn it off: ``` CREATE CATALOG `c1` PROPERTIES ( "type" = "paimon"  "paimon.catalog.type" = "xxx"  "paimon.cache-enabled" = "false"  "warehouse" = "xxx" ); ``` If you want to modify cache-related parameters  you can add the `paimon.` prefix to the parameters supported by paimon  such as: ``` CREATE CATALOG `c1` PROPERTIES ( "type" = "paimon"  "paimon.catalog.type" = "xxx"  "warehouse" = "xxx"  "paimon.cache.expiration-interval" = "20 min"  "paimon.cache.manifest.small-file-memory"="10 mb" ); ```  Note: During the doris upgrade process  this error may occur:  ![image](https://github.com/user-attachments/assets/47ec8216-9e3f-4d8e-95ef-17cce6b7c486)  This is because doris will upgrade be first  and then upgrade fe. During this process  the version of paimon on be may be higher than that on fe. This is normal. Because bucketkey judgment is newly added in the higher version of paimon  which is not available in the lower version. After the fe upgrade is completed normally  there will be no more errors.
apache,doris,b4dcf89aa93e0692a73c6cf4321a623792f2dd3a,https://github.com/apache/doris/commit/b4dcf89aa93e0692a73c6cf4321a623792f2dd3a,[opt](nereids) optimize one bucket tpcds performance (#47371)  ### What problem does this PR solve?  use one bucket shuffle hash join is slow than shuffle hash join  so we should downgrade to shuffle hash join when the table only contains one bucket.  this pr is optimized for nereids distribute planner in master branch
apache,doris,e35e01920d5d895b9333ac89e2278ed426a51f7b,https://github.com/apache/doris/commit/e35e01920d5d895b9333ac89e2278ed426a51f7b,[opt](load)  Add config to control commit lock scope for tables (#46996)   Problem Summary: Previously  all tables required commit locks during transaction commit  which helped reduce conflicts at the MetaService level. However  this approach may not be optimal for all scenarios since only MOW (Merge-on-Write) tables truly need strict concurrency control.  This PR adds a new config `enable_commit_lock_for_all_tables` (default: true) to control the commit lock strategy:  - When enabled (default): All tables will acquire commit locks during transaction commit  which helps reduce conflicts at MetaService level by queueing transactions at FE level - When disabled: Only MOW tables will acquire commit locks  which may improve concurrency for non-MOW tables but could increase conflicts at MetaService level  The default setting maintains the original behavior to avoid potential performance impact from increased MetaService conflicts  while providing flexibility to optimize for different deployment scenarios.
apache,doris,565edd9d13353f481093731d08915e9ef77e803b,https://github.com/apache/doris/commit/565edd9d13353f481093731d08915e9ef77e803b,[feature](nereids) in predicate extract non constant expressions (#46794)  Problem Summary: if an in predicate contains non-literal  backend process it will reduce performance. so we need to extract the non constant from the in predicate.  this pr add an expression rewrite rule InPredicateExtractNonConstant  it will extract all the non-constant out of the in predicate. for example:  ``` k1  in (k2   k3 + 3    1  2  3 + 3)  => k1 in (1  2  3 + 3) or k1 = k2 or k1 = k3 + 1 ```
apache,doris,584a256a25274722d22c36eafe57ba3cb8e0dc88,https://github.com/apache/doris/commit/584a256a25274722d22c36eafe57ba3cb8e0dc88,[opt](coordinator) optimize parallel degree of shuffle when use nereids (#44754)  optimize parallel degree of shuffle when use nereids   this pr can fix some performance rollback when upgrade doris from 1.2 to 2.x/3.x
apache,doris,c28c00aa1e77e1404c8e669ad4c76e3087a4e222,https://github.com/apache/doris/commit/c28c00aa1e77e1404c8e669ad4c76e3087a4e222,[enhance](nereids) add rule MultiDistinctSplit (#45209)  ### What problem does this PR solve?  Problem Summary:  This pr add a rewrite rule  which can do this 2 type of rewrite: 1. This rewrite can greatly improve the execution speed of multiple count(distinct) operations. When 3be  ndv=10000000  the performance can be improved by three to four times.  select count(distinct a) count(distinct b) count(distinct c) from t; -> with tmp as (select * from t) select * from (select count(distinct a) from tmp) t1 cross join  (select count(distinct b) from tmp) t2 cross join  (select count(distinct c) from tmp) t3   2.Before this PR  the following SQL statement would fail to execute due to an error: "The query contains multi count distinct or sum distinct  each can't have multi columns". This PR rewrites this type of SQL statement as follows  making it executable without an error.  select count(distinct a d) count(distinct b c) count(distinct c) from t; -> with tmp as (select * from t) select * from (select count(distinct a d) from tmp) t1 cross join  (select count(distinct b c) from tmp) t2 cross join  (select count(distinct c) from tmp) t3  ### Release note  Support multi count distinct with different parameters
apache,doris,44cc25494b21aa46c677633498ed2af3703d607a,https://github.com/apache/doris/commit/44cc25494b21aa46c677633498ed2af3703d607a,[Improve](nereids) use hash set replace three set in DiscreteValue  to improve in predicate performance (#45181)
apache,doris,17667aeb92efc5580b8615e53a3ef4f514a27204,https://github.com/apache/doris/commit/17667aeb92efc5580b8615e53a3ef4f514a27204,[Fix](catalog)Remove the fs.disable.cache parameter to prevent excessive FS-associated objects and memory leaks (#46184)  ### Background In the current file system implementation  the fs.disable.cache parameter allows disabling FS caching. While this provides flexibility  it introduces several critical issues: ```  1:      22537201      721190432  java.util.HashMap$Node 2:      21559238      689895616  javax.management.MBeanAttributeInfo 3:      21559098      517418352  javax.management.Attribute 4:      19380247      465125928  org.apache.hadoop.metrics2.impl.MetricCounterLong 5:        122603      461180096  [J 6:        294309      255533536  [B 7:        724598      252264048  [Ljava.lang.Object; 8:       2012368      189047432  [C 9:        159442      131064400  [Ljava.util.HashMap$Node; 10:        114752       88075072  [Ljavax.management.MBeanAttributeInfo; 11:       1899581       45589944  java.lang.String 12:       1720140       41283360  org.apache.hadoop.metrics2.impl.MetricGaugeLong ```  #### Unbounded FS Instance Creation When fs.disable.cache=true  a new FS instance is created for every access  preventing instance reuse. ```  String disableCacheName = String.format("fs.%s.impl.disable.cache"  scheme); if (conf.getBoolean(disableCacheName  false)) { LOGGER.debug("Bypassing cache to create filesystem {}"  uri); return createFileSystem(uri  conf); } ```  #### Resource Leakage Associated objects  such as thread metrics and connection pools  are not properly released due to excessive FS instance creation  leading to memory leaks.  #### Performance Degradation Frequent creation and destruction of FS instances impose significant overhead  especially in high-concurrency scenarios.    ### Release note  None  ### Check List (For Author)  - Test <!-- At least one of them must be included. --> - [ ] Regression test - [ ] Unit Test - [x] Manual test (add detailed scripts or steps below) ``` CREATE CATALOG `iceberg_cos` PROPERTIES ( "warehouse" = "cosn://ha/ha/ha/stress/multi_fs"  "type" = "iceberg"  "iceberg.catalog.type" = "hadoop"  "cos.secret_key" = "*XXX"  "cos.region" = "ap-beijing"  "cos.endpoint" = "cos.ap-beijing.myqcloud.com"  "cos.access_key" = "**************" );  Create a catalog using object storage  then write a scheduled script to continuously refresh the catalog. Query the catalog periodically and monitor whether the thread memory behaves as expected. ``` <img width="1131" alt="image" src="https://github.com/user-attachments/assets/c7b04a5a-449f-432c-975b-524fdb81247a" />  At 22:30  I replaced it with the fixed version.
apache,doris,cd9e5bb895ac02e901f0fb05e5abd806a04031d0,https://github.com/apache/doris/commit/cd9e5bb895ac02e901f0fb05e5abd806a04031d0,[improvement](statistics)Async drop table stats while doing truncate and schema change. (#45923)  ### What problem does this PR solve?  Async drop table stats while doing truncate and schema change. Truncate can schema change operation may hold table's write lock. And these two operations will trigger drop old stats info. Drop stats with write lock holding may bring performance issue.  Issue Number: close #xxx  Related PR: #xxx  Problem Summary:  ### Release note  None
apache,doris,2a1209d3cc77dac4f3ee7073240cd354bd6575c8,https://github.com/apache/doris/commit/2a1209d3cc77dac4f3ee7073240cd354bd6575c8,[opt](catalog) cache the Configuration object (#45433)  ### What problem does this PR solve?  Problem Summary: Creating Configuration object is very costly  so we cache it for better performance
apache,doris,f85b40877c7ee20489d51fb287579f6ac92935f0,https://github.com/apache/doris/commit/f85b40877c7ee20489d51fb287579f6ac92935f0,[chore](arrow-flight-sql) Add Arrow Flight Sql demo for Java (#45306)  ### What problem does this PR solve?  # How to use:  1. mvn clean install -U 2. mvn package 3. java --add-opens=java.base/java.nio=org.apache.arrow.memory.core ALL-UNNAMED -cp java-0.1.jar doris.arrowflight.demo.Main "sql" "fe_ip" "fe_arrow_flight_port" "fe_query_port"  # What can this demo do:  This is a java demo for doris arrow flight sql  you can use this to test various connection methods for sending queries to the doris arrow flight server  help you understand how to use arrow flight sql and test performance. You should install maven prior to run this demo.  # Performance test  Section 6.2 of https://github.com/apache/doris/issues/25514 is the performance test results of the Doris Arrow Flight SQL using java.  # Output  ``` WARNING: Unknown module: org.apache.arrow.memory.core specified to --add-opens ************************************* |          FlightAdbcDriver         | ************************************* FlightAdbcDriver > loadArrowBatch SLF4J(W): No SLF4J providers were found. SLF4J(W): Defaulting to no-operation (NOP) logger implementation SLF4J(W): See https://www.slf4j.org/codes.html#noProviders for further details. > Schema<l_shipdate: Date(DAY) not null> > 1994-08-17  > batchCount: 25  rowCount: 100000 > cost: 1704 ms.  FlightAdbcDriver > loadArrowBatchToString > Schema<l_shipdate: Date(DAY) not null> > 1992-01-02  > batchCount: 25  rowCount: 100000 > cost: 1692 ms.  ************************************* |          FlightJdbcDriver         | ************************************* FlightJdbcDriver > loadArrowBatch > Schema<l_shipdate: Date(DAY) not null> > 1997-01-30  > batchCount: 98  rowCount: 100000 > cost: 1840 ms.  FlightJdbcDriver > loadArrowBatchToString > Schema<l_shipdate: Date(DAY) not null> > 1992-01-02  > batchCount: 98  rowCount: 100000 > cost: 1712 ms.  ************************************* |          JdbcDriverManager        | ************************************* JdbcDriverManager > jdbc:mysql > loadJdbcResult > rowCount: 100000  columnCount: 1 > cost: 11431 ms.  JdbcDriverManager > jdbc:mysql > loadJdbcResultToString > 1992-01-02  > rowCount: 100000  columnCount: 1 resultSize: 100000 > cost: 5164 ms.  JdbcDriverManager > jdbc:arrow-flight-sql > loadJdbcResultToString > rowCount: 100000  columnCount: 1 > cost: 1736 ms.  JdbcDriverManager > jdbc:arrow-flight-sql > loadJdbcResultToString > 1997-01-29  > rowCount: 100000  columnCount: 1 resultSize: 100000 > cost: 2442 ms.   ************************************* |           FlightSqlClient         | ************************************* FlightSqlClient > getFlightInfoFromDorisFe > Schema<l_shipdate: Date(DAY) not null> > 1994-08-15  > batchCount: 25  rowCount: 100000  FlightSqlClient > constructDummyFlightInfo  don't be afraid! expected to get error `INVALID_ARGUMENT: Malformed ticket` org.apache.arrow.flight.FlightRuntimeException: INVALID_ARGUMENT: Malformed ticket  size: 1 at org.apache.arrow.flight.CallStatus.toRuntimeException(CallStatus.java:121) at org.apache.arrow.flight.grpc.StatusUtils.fromGrpcRuntimeException(StatusUtils.java:161) at org.apache.arrow.flight.grpc.StatusUtils.fromThrowable(StatusUtils.java:182) at org.apache.arrow.flight.FlightStream$Observer.onError(FlightStream.java:489) at org.apache.arrow.flight.FlightClient$1.onError(FlightClient.java:371) at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481) at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40) at org.apache.arrow.flight.grpc.ClientInterceptorAdapter$FlightClientCallListener.onClose(ClientInterceptorAdapter.java:118) at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564) at io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72) at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729) at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710) at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base/java.lang.Thread.run(Thread.java:842)  Process finished with exit code 0 ```
apache,doris,cc51e99dae339381c22a494460914e8a0431e6fb,https://github.com/apache/doris/commit/cc51e99dae339381c22a494460914e8a0431e6fb,[feat](nereids) expression add min max scope for slot reference (#45081)  expression add min max scope for slot reference  so that olap scan can reduce field's searching scope.  detail: in simplify range rule  record min and max range of the slot reference  then add 'e > min and e < max' to the expression.  example:  ``` TA >= 10 and TA <= 20 or TA >= 50 and TA <= 60 or TA >= 100 and TA <= 120 => (TA <= 20 or TA >= 50 and TA <= 60 or TA >= 100) AND TA >= 10 and TA <= 120  TA in (10  50  100) or TA >= 70 and TA <= 90 => (TA in (10  50  100) or TA >= 70 AND TA <= 90) AND TA >= 10 AND TA <= 100  TA between 10 and 20 and TB between 10 and 20 or TA between 100 and 120 and TB between 100 and 120 => (TA <= 20 and TB <= 20 or TA >= 100 and TB >= 100) AND TA >= 10 AND TA <= 120 AND TB >= 10 AND TB <= 120  ISNULL (TA > 10) and TA > 10 and TA < 20 or TA > 50 and TA < 60 or TA > 100 and TA < 120 => (ISNULL(TA > 10) and TA < 20 or TA > 50 and TA < 60 or TA > 100) AND TA > 10 and TA < 120 ```  benchmark on tpch tpcds 1T   performance no change.
apache,doris,df90de94618865e4832648e967b279c68e1c8703,https://github.com/apache/doris/commit/df90de94618865e4832648e967b279c68e1c8703,[performance](load) increase max_broker_concurrency to 100 (#44929)  Increase default `max_broker_concurrency` to 100 to improve broker load performance. This option will affect the max number of scan / sink instances allowed in a broker load.  ``` parallel instance = min(max_broker_concurrency  source file size / min_bytes_per_broker_scanner  num backends * load_parallelism) ```  S3 load time of tpcds_1000g catalog_sales: * before: 438s * after: 225s
apache,doris,18dc92a40d33b7fe19e0c04dcfc675f4da0bcfcf,https://github.com/apache/doris/commit/18dc92a40d33b7fe19e0c04dcfc675f4da0bcfcf,[fix](catalog) opt the count pushdown rule for iceberg/paimon/hive scan node (#44038)  ### What problem does this PR solve?  1. Opt the parallelism when doing count push down optimization  Count push down optimization is used to optimize queries such as `select count(*) from table`. In this scenario  we can directly obtain the number of rows through the row count statistics of the external table  or the metadata of the Parquet/ORC file  without reading the actual file content  thereby speeding up such queries.  Currently  we support count push down optimization for Hive  Iceberg  and Paimon tables. There are two ways to obtain the number of rows:  1. Obtain directly from statistics  For Iceberg tables  we can obtain the number of rows directly from statistics. However  due to the historical issues of Iceberg  if there is position/equality delete in the table  this method cannot be used to prevent incorrect row count. In this case  it will degenerate to obtaining from the metadata of the file.  2. Obtain from the metadata of the file  For Hive  Paimon  and some of Iceberg tables  the number of rows can be obtained directly from the metadata of the Parquet/ORC file. For Text format tables  efficiency can also be improved by only performing row separation  without column separation.  In the task splitting logic  for Count push-down optimization  the number of split tasks should comprehensively consider the file format  number of files  parallelism  number of BE nodes  and the Local Shuffle:  1. Count push-down optimization should avoid Local Shuffle  so the number of split tasks should be greater than or equal to `parallelism * number of BE nodes`.  2. Fix the incorrect logic of Count push-down optimization  In the previous code  for Iceberg and Paimon tables  Count push-down optimization did not take effect because we did not push CountPushDown information to FileFormatReader inside TableForamtReader. This PR fixes this problem.  3. Store SessionVaraible variables in FileQueryScanNode.  SessionVaraible is a variable in ConnectionContext. And ConnectionContext is a ThreadLocal variable. In FileQueryScanNode  SessionVaraible may be accessed in other threads in some cases  so ThreadLocal variables may not be obtained. Therefore  the SessionVaraible reference is stored in FileQueryScanNode to prevent illegal access.  4. Independent FileSplitter class.  The FileSplitter class is a tool class that allows users to split `Split` according to different strategies. This PR does not modify the splitting strategy  but only extracts this part of the logic separately  to be able to perform logic optimization later.
apache,doris,5aee0cc8d122f30499f39c060ff20747ce362cbf,https://github.com/apache/doris/commit/5aee0cc8d122f30499f39c060ff20747ce362cbf,[fix](planner) query should be cancelled if limit reached (#44338)  ### What problem does this PR solve?  Problem Summary: When there is a `limit` cluse in SQL  if FE has obtained data with more than the `limit` number of rows  it should send a cancel command to BE to cancel the query to prevent BE from reading more data. However  this function has problems in the current code and does not work. Especially in external table query  this may result in lots of unnecessary network io read.  1. `isBlockQuery`  In the old optimizer  if a query statement contains a `sort` or `agg` node  `isBlockQuery` will be marked as true  otherwise it will be false. In the new optimizer  this value is always true.  Regardless of the old or new optimizer  this logic is wrong. But only when `isBlockQuery = false` will the reach limit logic be triggered.  2. Calling problem of reach limit logic  The reach limit logic judgment will only be performed when `eos = true` in the rowBatch returned by BE. This is wrong. Because for `limit N` queries  each BE's own `limit` is N. But for FE  as long as the total number of rows returned by all BEs exceeds N  the reach limit logic can be triggered. So it should not be processed only when `eos = true`.  The PR mainly changes:  1. Remove `isBlockQuery`  `isBlockQuery` is only used in the reach limit logic. And it is not needed. Remove it completely.  2. Modify the judgment position of reach limit.  When the number of rows obtained by FE is greater than the limit  it will check the reach limit logic.  3. fix wrong `limitRows` in `QueryProcessor`  the limitRows should be got from the first fragment  not last.  4. In scanner scheduler on BE side  if scanner has limit  ignore the scan bytes threshold per round.  ### Release note  [fix](planner) query should be cancelled if limit reached
apache,doris,0671f57221378144e7369490a28c2292b940f90a,https://github.com/apache/doris/commit/0671f57221378144e7369490a28c2292b940f90a,[feat](catalog)Replace HadoopUGI with HadoopKerberosAuthenticator to Support Kerberos Ticket Auto-Renewal (#44916)  ### Background The current implementation uses the HadoopUGI method  which invokes the ugiDoAs function for each operation to log in and execute actions based on the configuration. However  this approach has the following issues:  - Lack of Auto-Renewal: If the Kerberos TGT (Ticket Granting Ticket) expires  manual re-login is required as there is no support for automatic ticket renewal. - Redundant Login Overhead: Each operation requires reinitializing or checking UserGroupInformation  potentially causing performance bottlenecks. - Complex Management: The HadoopUGI design does not unify the lifecycle management of UGI instances  leading to duplicated logic across the codebase. ### Objective  - Auto-Renewal: Automatically renew Kerberos credentials when the TGT is expired or near expiry. - UGI Caching: Maintain reusable UserGroupInformation instances during their lifecycle to avoid repetitive logins. - Unified Management: Simplify the management of UGI instances and Kerberos credentials.
apache,doris,6a8ae7705a9641ce31aa1d9693044c8ddd54e394,https://github.com/apache/doris/commit/6a8ae7705a9641ce31aa1d9693044c8ddd54e394,[Enchancement](runtime-filter) improvement for datetimev2 bloom filter hash method (#44924)  ### What problem does this PR solve? improvement for datetimev2 bloom filter hash method In the past  datetimev2 use to_int64 to get a 64bit data and truncate to 32bit data  then use this 32bit data to build the bloom filter. this can lead to poor performance and bad filterability.
apache,doris,d77bfa09d85d9759965c109cc50ee89e8fbde499,https://github.com/apache/doris/commit/d77bfa09d85d9759965c109cc50ee89e8fbde499,[Improvement](shuffle) Use a knob to decide whether a serial exchange… (#44676)  … should be used  This improvement was completed in #43199 and reverted by #44075 due to performance fallback. After fixing it  this improvement is re-submited.  A new knob to control a exchange node should be serial or not. For example  a partitioned hash join should be executed like below: ``` ┌────────────────────────────┐                  ┌────────────────────────────┐ │                            │                  │                            │ │Exchange(HASH PARTITIONED N)│                  │Exchange(HASH PARTITIONED N)│ │                            │                  │                            │ └────────────────────────────┴─────────┬────────┴────────────────────────────┘ │ │ │ │ │ │ ┌──────▼──────┐ │             │ │ HASH  JOIN  │ │             │ └─────────────┘ ```  After turning on this knob  the real plan should be: ``` ┌──────────────────────────────┐                        ┌──────────────────────────────┐ │                              │                        │                              │ │ Exchange (HASH PARTITIONED 1)│                        │ Exchange (HASH PARTITIONED 1)│ │                              │                        │                              │ └────────────┬─────────────────┘                        └────────────┬─────────────────┘ │                                                       │ │                                                       │ │                                                       │ │                                                       │ │                                                       │ ┌──────────────▼─────────────────────┐                  ┌──────────────▼─────────────────────┐ │                                    │                  │                                    │ │ Local  Exchange(HASH PARTITIONED N)│                  │ Local  Exchange(HASH PARTITIONED N)│ │              1 -> N                │                  │              1 -> N                │ └────────────────────────────────────┴─────────┬────────┴────────────────────────────────────┴ │ │ │ │ │ │ ┌──────▼──────┐ │             │ │ HASH  JOIN  │ │             │ └─────────────┘ ```  For large cluster  X (mappers) * Y (reducers) rpc channels can be reduced to X (mappers) * Z (BEs).
apache,doris,faa8a6425e692f9604e342800c6143b79ca9008e,https://github.com/apache/doris/commit/faa8a6425e692f9604e342800c6143b79ca9008e,[chore](sink) `enable_parallel_result_sink` default value is changed to false (#43933)  ### What problem does this PR solve?  Problem Summary:  For most queries  result sink will not become a performance bottleneck  but the parallel result sink will increase the pressure of RPC between fe and be.
apache,doris,05b6f5aca1ea609abbdd447c64d4d57fd474b295,https://github.com/apache/doris/commit/05b6f5aca1ea609abbdd447c64d4d57fd474b295,[fix](external) fix count(*) performance fallback in external table query (#44172)  ### What problem does this PR solve?  Related PR: #41789  Problem Summary:  This PR #41789 change the local shuffle logic  but forget to implement `numScanBackends()` in FileQueryScanNode  which causing `select count(*) from hive_table` query performance fallback.  This PR implement this method in `FileQueryScanNode()`
apache,doris,07ec65712d67efd072667247704b40adaf3e35a7,https://github.com/apache/doris/commit/07ec65712d67efd072667247704b40adaf3e35a7,[opt](Nereids) optimize performance in new distribute planner (#44048)  optimize new distribute planner performance in tpc-h  because #41730 made some performance rollback has occurred  1. fix the wrong runtime filter thrift parameters 2. not default to print distribute plan in profile  you should config `set profile_level=3` to see it 3. for shuffle join which two sides distribution of natural + execution_bucketed  support compare cost between plans of shuffle to left/right
apache,doris,9b983ca1d0eb550f60b80db2ead11073849a4b8d,https://github.com/apache/doris/commit/9b983ca1d0eb550f60b80db2ead11073849a4b8d,[feat](catalog)Support Pre-Execution Authentication for HMS Type Iceberg Catalog Operations.  (#43445)  ### What problem does this PR solve?  Support Pre-Execution Authentication for HMS Type Iceberg Catalog Operations Summary This PR introduces a new utility class  PreExecutionAuthenticator  which is designed to ensure pre-execution authentication for HMS (Hive Metastore) type operations on Iceberg catalogs. This is especially useful in environments where secure access is required  such as Kerberos-based Hadoop ecosystems. By integrating PreExecutionAuthenticator  each relevant operation will undergo an authentication step prior to execution  maintaining security compliance.  ### Motivation In environments utilizing an Iceberg catalog with an HMS backend  many operations may require authentication to access secure data or perform privileged tasks. Given that operations on HMS-type catalogs typically run within a Hadoop environment secured by Kerberos  ensuring each operation is executed within an authenticated context is essential. Previously  there was no standardized mechanism to enforce pre-execution authentication  which led to potential security gaps. This PR aims to address this issue by introducing an extensible authentication utility.  ### Key Changes Addition of PreExecutionAuthenticator Utility Class  Provides a standard way to perform pre-execution authentication for tasks. Leverages HadoopAuthenticator (when available) to execute tasks within a privileged context using doAs. Supports execution with or without authentication  enabling flexibility for both secure and non-secure environments. Integration with Iceberg Catalog Operations  All relevant HMS-type catalog operations will now use PreExecutionAuthenticator to perform pre-execution authentication. Ensures that operations like createDb  dropDb  and other privileged tasks are executed only after authentication. Extensible Design  PreExecutionAuthenticator is adaptable to other future authentication methods  if needed  beyond Hadoop and Kerberos. CallableToPrivilegedExceptionActionAdapter class allows any Callable task to be executed within a PrivilegedExceptionAction  making it versatile for various task types.   ### Check List (For Author)  - Test <!-- At least one of them must be included. -->  - [x] Manual test (add detailed scripts or steps below) ``` mysql> CREATE TABLE ha ->        ( ->            vendor_id BIGINT  ->            trip_id BIGINT  ->            trip_distance FLOAT  ->            fare_amount DOUBLE  ->            store_and_fwd_flag STRING  ->            ts DATETIME ->        ); Query OK  0 rows affected (2.08 sec)  mysql> show create table ha; +-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Table | Create Table                                                                                                                                                                                                                                                                                                                                                                                                              | +-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | ha    | CREATE TABLE `ha` ( `vendor_id` bigint NULL  `trip_id` bigint NULL  `trip_distance` float NULL  `fare_amount` double NULL  `store_and_fwd_flag` text NULL  `ts` datetimev2(6) NULL ) ENGINE=ICEBERG_EXTERNAL_TABLE LOCATION 'xxxxx' PROPERTIES ( "doris.version" = "doris-2.1.6-rc04-67ee7f53e6"  "write.parquet.compression-codec" = "zstd" );  mysql>        INSERT INTO iceberg.ck_iceberg.ha ->        VALUES ->         (1  1000371  1.8  15.32  'N'  '2024-01-01 9:15:23')  ->         (2  1000372  2.5  22.15  'N'  '2024-01-02 12:10:11')  ->         (2  1000373  0.9  9.01  'N'  '2024-01-01 3:25:15')  ->         (1  1000374  8.4  42.13  'Y'  '2024-01-03 7:12:33'); Query OK  4 rows affected (5.10 sec) {'status':'COMMITTED'  'txnId':'35030'}  mysql> select * from ha; +-----------+---------+---------------+-------------+--------------------+----------------------------+ | vendor_id | trip_id | trip_distance | fare_amount | store_and_fwd_flag | ts                         | +-----------+---------+---------------+-------------+--------------------+----------------------------+ |         1 | 1000371 |           1.8 |       15.32 | N                  | 2024-01-01 09:15:23.000000 | |         2 | 1000372 |           2.5 |       22.15 | N                  | 2024-01-02 12:10:11.000000 | |         2 | 1000373 |           0.9 |        9.01 | N                  | 2024-01-01 03:25:15.000000 | |         1 | 1000374 |           8.4 |       42.13 | Y                  | 2024-01-03 07:12:33.000000 | +-----------+---------+---------------+-------------+--------------------+----------------------------+ 4 rows in set (1.20 sec) ```
apache,doris,d593ffa23e6959ac1b90c875d1727fc65bcdd860,https://github.com/apache/doris/commit/d593ffa23e6959ac1b90c875d1727fc65bcdd860,[fix](iceberg)Fix count(*) error with dangling delete problem (#44039)  ### What problem does this PR solve?  Prevent 'dangling delete' problem after `rewrite_data_files` action. ref: https://iceberg.apache.org/docs/nightly/spark-procedures/#rewrite_position_delete_files.  Because we don’t know whether the user has performed a rewrite operation  `total-records` will only be used directly when equalitydelete and positiondelete are both 0.  Issue Number: close #42240
apache,doris,aab5ba3a7d4d9ccd26c2bf800e798004885769be,https://github.com/apache/doris/commit/aab5ba3a7d4d9ccd26c2bf800e798004885769be,[performance](load) fix broker load scan ranges for unsplittable files (#43161)
apache,doris,cd514b6a3dbd06aabefe67600ef1d1c262fa1aa2,https://github.com/apache/doris/commit/cd514b6a3dbd06aabefe67600ef1d1c262fa1aa2,[feature](function) add approx_top_k aggregation function (#40813)  ## Proposed changes  1. select approx_top_k(clientip  status  size  10  300) from tbl;  This code implements an approximate Top-N query function based on the SpaceSaving algorithm. SpaceSaving is an efficient streaming algorithm commonly used to handle frequent element query problems in large datasets. Below is a description of the main functionalities:  (1) Data Structures and Memory Management: The SpaceSavingArena class provides a memory pool to manage memory allocation and deallocation. For keys of type StringRef  it handles the memory by copying the string into the memory pool. The Counter struct stores the key  count  and error for each element  and provides serialization and deserialization functions.  (2) Insertion and Updates: The insert method is used to insert new elements or update the count of existing elements. If the current capacity is not full  it inserts the new element; if it is full  it replaces the element with the smallest count based on the element's count and error.  (3) Merge Operation: The merge method allows merging two SpaceSaving objects. During the merge  it adjusts the counts and errors of the existing elements  ensuring that the result maintains the correct order.  (4) Top-K Query: The top_k method returns the current Top-K most frequent elements  sorted by their count and error.  (5) Capacity Expansion and Shrinking: The resize method allows adjusting the storage capacity  and it recalculates the size of the alpha_map accordingly.  (6) Serialization and Deserialization: The write and read methods are provided for serializing the SpaceSaving structure to disk or reading data from disk.  (7) Optimization and Performance: The code uses a hash table-based approach for lookup and storage  and dynamically adjusts the alpha_map size to optimize performance and reduce memory waste.  In summary  the SpaceSaving class efficiently implements Top-N queries for large data streams within limited memory  with efficient insertion  updating  and merging mechanisms.  Co-authored-by: zzzxl1993 <yangsiyu@selectdb.com>
apache,doris,50fd9971b0380f5a4f360067b60c24a381e224f3,https://github.com/apache/doris/commit/50fd9971b0380f5a4f360067b60c24a381e224f3,[Opt](orc)Optimize the merge io when orc reader read multiple tiny stripes. (#42004)  ### What problem does this PR solve?  When reading orc files  we may encounter a scenario where the stripe byte size is very small but the number of stripes is very large.  This pr introduces three session variables `orc_tiny_stripe_threshold_bytes`  `orc_once_max_read_bytes`  and `orc_max_merge_distance_bytes` to optimize io reading for the above scenarios.  If a stripe byte size is less than `orc_tiny_stripe_threshold_bytes`  we will consider it as a tiny stripe. For multiple tiny stripes  we will perform IO merge reading according to the `orc_once_max_read_bytes` and `orc_max_merge_distance_bytes` parameters. Among them  `orc_once_max_read_bytes` indicates the maximum size of the merged IO. You should not set `orc_once_max_read_bytes` less than `orc_tiny_stripe_threshold_bytes`  although we will not force an error. When using tiny stripe reading optimization  since tiny stripes are not necessarily continuous  when the distance between two tiny stripes is greater than `orc_max_merge_distance_bytes`  we will not merge them into one IO.  If you don't want to use this optimization  you can `set orc_tiny_stripe_threshold_bytes = 0`.   Default parameters: ```mysql orc_tiny_stripe_threshold_bytes = 8388608 (8M) orc_once_max_read_bytes = 8388608 (8M) orc_max_merge_distance_bytes = 1048576 (1M) ```  We also add relevant profiles for this purpose so that parameters can be adjusted to optimize reading. `RangeCacheFileReader`: 1. `CacheRefreshCount`: how many IOs are merged 2. `ReadToCacheBytes`: how much data is actually read after merging 3. `ReadToCacheTime`: how long it takes to read data after merging 4. `RequestBytes`: how many bytes does the apache-orc library actually need to read the orc file 5. `RequestIO`: how many times the apache-orc library calls this read interface 6. `RequestTime`: how long it takes the apache-orc library to call this read interface  It should be noted that `RangeCacheFileReader` is a wrapper of the reader that actually reads data  such as the hdfs reader  so strictly speaking  `CacheRefreshCount` is not equal to how many IOs are initiated to hdfs  because each time the hdfs reader is requested  the hdfs reader may not be able to read all the data at once.  This pr also involves changes to the apache-orc third-party library: https://github.com/apache/doris-thirdparty/pull/244. Reference implementation: https://github.com/trinodb/trino/blob/master/lib/trino-orc/src/main/java/io/trino/orc/OrcDataSourceUtils.java#L36  #### Summary: ```mysql set orc_tiny_stripe_threshold_bytes = xxx; set orc_once_max_read_bytes = xxx; set orc_max_merge_distance_bytes = xxx;  # xxx is the size in bytes ```  ### Release note Introduces three session variables `orc_tiny_stripe_threshold_bytes`  `orc_once_max_read_bytes`  and `orc_max_merge_distance_bytes` to optimize io reading of scenarios where the orc stripe byte size is very small but the number of stripes is very large.   Co-authored-by: kaka11chen <kaka11.chen@gmail.com> Co-authored-by: daidai <changyuwei@selectdb.com>
apache,doris,ab27598511ce5c65e337a64b8b16d2abf4fa30cd,https://github.com/apache/doris/commit/ab27598511ce5c65e337a64b8b16d2abf4fa30cd,[fix](auth)Fix concurrency issue during role manager upgrade  (#42419)  Change the map to a concurrency safe map，because there is a possibility of concurrent read and write operations  For example  when a node performs the rectifyPrivs operation  it needs to traverse 'roles' and may be adding or deleting roles at the same time
apache,doris,5f07b884819a0187862d0005ec60cae96edcafea,https://github.com/apache/doris/commit/5f07b884819a0187862d0005ec60cae96edcafea,[improvement](external)add some improvements for external scan (#38946)  ## Proposed changes  1. add session variable: `use_consistent_hash_for_external_scan`  which can specify consistent hash for external scan. 2. add session variable: `ignore_split_type`  which can ignore splits of the specified type  use for performance tuning. 3. add split weight for paimon split with consistent hash. 4. add `executeFilter` for paimon jni split.
apache,doris,19016b147731ec825845686dc61bbf6e3d1116b9,https://github.com/apache/doris/commit/19016b147731ec825845686dc61bbf6e3d1116b9,[improve](routine load) adjust default values to make routine load more convenient to use (#42491)  For a routine load job  it will be divided into many tasks  each of which is a transaction. Currently  the default time consumed(max_batch_interval) is 10 seconds. The benefits of increasing this value are: 1. Larger batch consumption can lead to better performance. 2. Reducing the number of transactions can alleviate the pressure of compaction and the conflicts of concurrent transaction submissions.  related doc: https://github.com/apache/doris-website/pull/1236/files
apache,doris,bf737b12530301cd3e2dd6d50918cc3051e76a7c,https://github.com/apache/doris/commit/bf737b12530301cd3e2dd6d50918cc3051e76a7c,[Improvement](local shuffle) Improve local shuffle strategy (#41789)  Add local shuffle to unpartitioned fragment to add parallel for perfomance ```sql SELECT h1.UserID  h2.URL  COUNT(*) AS visit_count FROM ( SELECT * FROM hits_10m LIMIT 5000 ) AS h1 CROSS JOIN ( SELECT * FROM hits_10m LIMIT 5000 ) AS h2 GROUP BY h1.UserID  h2.URL ORDER BY visit_count DESC LIMIT 1000 ```  Add a rule to apply local exchanger:  ``` ┌───────────────────────┐               ┌───────────────────────┐ │                       │               │                       │ │Exchange(UNPARTITIONED)│               │Exchange(UNPARTITIONED)│ │                       │               │                       │ └───────────────────────┴──────┬────────┴───────────────────────┘ │ │ │ │ │ │ ┌──────▼──────┐ │             │ │ CROSS JOIN  │ │             │ └──────┬──────┘ │ │ │ ┌──────────────────▼─────────────────────┐ │                                        │ │ LOCAL EXCHANGE (HASH PARTITION) 1 -> n │ │                                        │ └──────────────────┬─────────────────────┘ │ │ │ │ ▼        ┌──▼────┐ │       │ │  AGG  │ │       │ └───────┘ ```   before: 1 min 17.79 sec after: 16.73 sec
apache,doris,5ede16abe01d7bd9ef3a4d85bc481c83025aeb71,https://github.com/apache/doris/commit/5ede16abe01d7bd9ef3a4d85bc481c83025aeb71,[improvement](jdbc catalog) Add catalog property to enable jdbc connection pool (#41992)  We initially introduced jdbc connection pool to improve the connection performance of jdbc catalog  but we always found that connection pool would bring some unexpected errors  so we chose to add a catalog property: `enable_connection_pool` to choose whether to enable the jdbc connection pool of jdbc catalog  and the default false.However  the created catalog will still open the connection pool when it is upgraded  and only the newly created catalog will be false  And we conducted performance tests on this  the performance loss is within the expected range.  - Enable connection pool: mysqlslap -uroot -h127.0.0.1 -P9030 --concurrency=1 --iterations=100 --query='SELECT * FROM mysql.test.test limit 1;' --create-schema=mysql --delimiter=";" --verbose Benchmark Average number of seconds to run all queries: 0.008 seconds Minimum number of seconds to run all queries: 0.004 seconds Maximum number of seconds to run all queries: 0.133 seconds Number of clients running queries: 1 Average number of queries per client: 1  - Disable connection pool: mysqlslap -uroot -h127.0.0.1 -P9030 --concurrency=1 --iterations=100 --query='SELECT * FROM mysql_no_pool.test.test limit 1;' --create-schema=mysql --delimiter=";" --verbose Benchmark Average number of seconds to run all queries: 0.054 seconds Minimum number of seconds to run all queries: 0.047 seconds Maximum number of seconds to run all queries: 0.184 seconds Number of clients running queries: 1 Average number of queries per client: 1
apache,doris,b7faf57163a6825d19782ede7b0a6a0c77315690,https://github.com/apache/doris/commit/b7faf57163a6825d19782ede7b0a6a0c77315690,[improvement](jdbc catalog) Disallow non-constant type conversion pushdown and implicit conversion pushdown (#42102)  Add a variable `enable_jdbc_cast_predicate_push_down`  the default value is false  which prohibits the pushdown of non-constant predicates with type conversion and all predicates with implicit conversion. This change can prevent the wrong predicates from being pushed down to the Jdbc data source  resulting in query data errors  because the predicates with cast were not correctly pushed down to the data source before. If you find that the data is read correctly and the performance is better before this change  you can manually set this variable to true  ``` | Expression                                          | Can Push Down | |-----------------------------------------------------|---------------| | column type equals const type                       | Yes           | | column type equals cast const type                  | Yes           | | cast column type equals const type                  | No            | | cast column type equals cast const type             | No            | | column type not equals column type                  | No            | | column type not equals cast const type              | No            | | cast column type not equals const type              | No            | | cast column type not equals cast const type         | No            |  ```
apache,doris,22aabb56fdffe5f41e4fde9c1a6e4c5b47d078de,https://github.com/apache/doris/commit/22aabb56fdffe5f41e4fde9c1a6e4c5b47d078de,[opt](Catalog) Remove unnecessary conjuncts handling on External Scan (#41218)  In the previous FileScanNode  some parts that used conjuncts for predicate conversion were placed in the init phase. However  for the Nereids planner  pushing the filter down to the scan happens in the Translator  which means that the ScanNode can only get the complete conjuncts in the finalized phase. Therefore  in this PR  I have removed all conjuncts variables in External for the Nereids planner. They no longer need to store conjuncts themselves or add them to the ScanNode. Instead  all places in the ScanNode that use conjuncts should be moved to the finalized phase.  This refactor also fix a performance issue introduced from #40176 After introducing the change of generating SelectNode for consecutive projects or filters  FileScan still adds conjuncts too early in the init phase  resulting in the discovery of consecutive filters when the upper layer continues to translate  a selectnode was unexpectedly generated on the scannode  causing the project to be unable to prune the scannode columns. However  the Project node trims columns of SelectNode and ScanNode differently  which causes ScanNode to scan unnecessary columns.  My modification removes the addition of conjuncts in the scannode step  so that we can keep the structure from ScanNode to Project and achieve correct column trimming.
apache,doris,c9886c5c4c607cd45c9df2a83e48be9f288b8112,https://github.com/apache/doris/commit/c9886c5c4c607cd45c9df2a83e48be9f288b8112,[improvement](statistics)Use min row count of all replicas as tablet/table row count. (#41894)  Use min row count of all replicas with same version as tablet/table row count. Because replica with the least row count means it perform more compaction operation than the others. Use it as tablet row count is more accurate. Meanwhile  use min row count as tablet row count while choosing tablets during sample analyze.
apache,doris,a74ee9ad8da37a27ff760236a2a9cf9949e0eb51,https://github.com/apache/doris/commit/a74ee9ad8da37a27ff760236a2a9cf9949e0eb51,[fix](arrow-flight-sql) Fix Arrow Flight bearer token cache evict after expired (#41754)  Guava LoadingCache expiration check is performed when the cache is accessed  not immediately when the expiration time is reached.  So manually call the cleanUp method regularly to clean up expired cache items.
apache,doris,127ac8d93ece5310afe20eb5521f6ec883d7282a,https://github.com/apache/doris/commit/127ac8d93ece5310afe20eb5521f6ec883d7282a,[Enhancement](ExternalTable)Optimize the performance of getCachedRowCount when reading ExternalTable (#41659)  ## Proposed changes Because ExternalTable will initialize the previously uninitialized table when `getCachedRowCount()`  which is unnecessary. So for the uninitialized table  we directly return -1. This will increase the speed of our query `information_schema.tables`.
apache,doris,6ed0bc813c484777a93605ada8fb629c627d8fd7,https://github.com/apache/doris/commit/6ed0bc813c484777a93605ada8fb629c627d8fd7,[enhance](auth) Optimize the authentication logic of Ranger Doris (#41207)  - Set the authentication type for row policy and datamask as `select` to avoid traversing all permission items within the ranger - The permission items on the ranger page are consistent with the grant syntax permission items on Doris - Add global permissions on the ranger side  and the ranger page needs to be specified in the input box* - No longer using cache to cache datamask and row policy  as changing the log level and specifying permission items has already made the speed fast enough - When using a ranger  there is no need to create a role with the same name within Doris - If you have sub level permissions  you can see the current level when showing. For example  if you have query permissions for table1 under db1  db1 will be displayed when showing databases  ranger ui: morningman/ranger#1  performance testing:  Each time the ranger authentication method is called  it takes less than 1ms   If the user has global/db/table permissions  querying a large wide table takes approximately 1ms.  If the user only has partial column query permission  it will take approximately 80ms to query 50 columns
apache,doris,3b18b1f000427e8d3b7d9d06ac57c08ec0959703,https://github.com/apache/doris/commit/3b18b1f000427e8d3b7d9d06ac57c08ec0959703,[fix](scanner) Fix incorrect _max_thread_num in scanner context when many queries are running. (#41273)  1. Minor refactor for scanner constructor  calculation of _max_thread_num is moved to init method 2. The expected value of _max_thread_num is changed. There is no need to submit too many scan task to scan scheduler  since thread num is limited. 3. Calculation of _max_bytes_in_queue is changed. _max_bytes_in_queue for each scan instance is limited to 100MB by default.  ``` mysql [tpch]>select count(*) from supplier; -------------- select count(*) from supplier --------------  +----------+ | count(*) | +----------+ |  1000000 | +----------+ 1 row in set (0.04 sec)  mysql [tpch]>select count(*) from revenue0; -------------- select count(*) from revenue0 --------------  +----------+ | count(*) | +----------+ |  1000000 | +----------+ 1 row in set (0.19 sec) ``` To illustrate the effect  we need to create much scanners  so ``` set global experimental_parallel_scan_min_rows_per_scanner=29715 ``` default value is `2097152`  we can make scanner num almost equal to `experimental_parallel_scan_max_scanners_count` which is 48.  Lets use mysqlslap to do concurrent test.  Current master: ```text [hezhiqiang@VM-10-8-centos be_1]$ mysqlslap -hxxxx -uroot -Pyyyy  --create-schema=tpch -c 20 -i 5 -q "select     s_suppkey      s_name      s_address      s_phone      total_revenue from     supplier      revenue0 where     s_suppkey = supplier_no     and total_revenue = (         select             max(total_revenue)         from             revenue0     ) order by     s_suppkey;" Benchmark Average number of seconds to run all queries: 12.480 seconds Minimum number of seconds to run all queries: 12.159 seconds Maximum number of seconds to run all queries: 12.843 seconds Number of clients running queries: 20 Average number of queries per client: 1  [hezhiqiang@VM-10-8-centos be_1]$ mysqlslap -hyyyy -uroot -Pyyyy  --create-schema=tpch -c 25 -i 5 -q "select     s_suppkey      s_name      s_address      s_phone      total_revenue from     supplier      revenue0 where     s_suppkey = supplier_no     and total_revenue = (         select             max(total_revenue)         from             revenue0     ) order by     s_suppkey;" mysqlslap: Cannot run query select     s_suppkey      s_name      s_address      s_phone      total_revenue from     supplier      revenue0 where     s_suppkey = supplier_no     and total_revenue = (         select             max(total_revenue)         from             revenue0     ) order by     s_suppkey; ERROR : errCode = 2  detailMessage = (10.16.10.8)[TOO_MANY_TASKS]Failed to submit scanner to scanner pool reason:Thread pool Scan_normal is at capacity (192/192 tasks running  102400/102400 tasks queued)|type:0 ```  After this pr ``` [hezhiqiang@VM-10-8-centos lib]$ mysqlslap -hxxx -uroot -Pxxx  --create-schema=tpch -c 50 -i 5 -q "select     s_suppkey      s_name      s_address      s_phone      total_revenue from     supplier      revenue0 where     s_suppkey = supplier_no     and total_revenue = (         select             max(total_revenue)         from             revenue0     ) order by     s_suppkey;" Benchmark Average number of seconds to run all queries: 31.520 seconds Minimum number of seconds to run all queries: 30.164 seconds Maximum number of seconds to run all queries: 34.131 seconds Number of clients running queries: 50 Average number of queries per client: 1 ```  The max concurrency increased from 25 to 50.  Actually  for sequential query test  the performance does not decrease  `submit_many_scan_tasks_for_potential_performance_issue` can be remove in the future.
apache,doris,f9bd4efb9e57f5c64ed4d3610b6dff9dedca8878,https://github.com/apache/doris/commit/f9bd4efb9e57f5c64ed4d3610b6dff9dedca8878,[fix](oracle scan) Fix performance issues caused by version judgment (#41407)
apache,doris,d4b9c5edba238bda21135de9d93c7ca8df4e7ea0,https://github.com/apache/doris/commit/d4b9c5edba238bda21135de9d93c7ca8df4e7ea0,[improvement](jdbc catalog) Optimize JdbcCatalog case mapping stability (#40891)  This PR makes the following changes to the uppercase and lowercase mapping of JdbcCatalog 1. The identifierMapping is managed by JdbcExternalCatalog instead of JdbcClient to better control its lifecycle 2. The identifierMapping no longer loads remoteName alone  but Catalog controls the loading uniformly 3. The identifierMapping will be loaded when each FE performs makeSureInitialized() to ensure that each FE has a mapping 4. The initialization of mapping will only be performed once in makeSureInitialized()  which means that even if you use metaCache  if your source data is updated when identifierMapping is enabled  you must refresh the catalog to query normally. 5. The identifierMapping is only responsible for the properties of the Catalog and is no longer affected by the fe config  simplifying the processing logic 6. If lower_case_mete_names is false and meta_names_mapping is empty in the catalog properties  the identifierMapping will no longer take effect  further enhancing the stability of the default settings 7. The JdbcClient is no longer closed during onRefreshCache  reducing the repeated creation of resources  improving reuse  and reducing the leakage of some global shared threads
apache,doris,8e33cda7ac237d273a6bb3413d6436d833c4a021,https://github.com/apache/doris/commit/8e33cda7ac237d273a6bb3413d6436d833c4a021,[improvement](statistics)Reduce partition column sample BE memory consumption. (#41203)  For string type columns  use xxhash_64 to transfer column value to an integer  and then calculate the NDV based on the integer hash value. In this case  we can reduce the memory cost of sample analyze and improve the performance. For example  l_comment column of TPCH 100G lineitem table. The memory cost to calculate its NDV is reduced to 8GB from 22GB
apache,doris,5868e10fb48da5c5454c477f656d1307403313be,https://github.com/apache/doris/commit/5868e10fb48da5c5454c477f656d1307403313be,[opt](nereids) refine operator estimation (#40762)  Stats deriving refinement step 3: refine operator estimation  a. refine filter estimation - refine column-column/column-constant stats estimation  refine in predicate estimation for future extension - unify original "enforceValid" and "normalizeByRatio" function => normalizeColumnStatistics  which can be able to update column statistics  such as ndv  numNulls  based on self's stats and current row count info. - unify original "cover" function => intersect in StatisticsRange - add normalizeColumnStatistics at each partial step of estimation  such as middle stage of and/or estimation  to ensure the returning stats is valid and avoid serious stats deviation from unhandled column statistics. - standardize the comparison function name  by using more meaningful name  such as column/constant. - to avoid serious estimation problem from inaccurate minmax  the range related estimation path has been splitted by judging instance of RangeScalable  and the non RangeScalable type  such as string type  will use alternative way more conservatively to do the estimation. - the notNullSelectivity's computing has been refined at former pr  in this pr  the refined getNotNullSelectivity will use original numNulls  ndv  rowCount  to computing a more accurate notNullSelectivity. however  current column2column notNullSelectivity has not been activated and will do it in the future. - not/is null estimation will use "normalizeColumnStatistics" to do the normalization.  b. refine join  estimation - unify join's estimation function. - standardize the naming  such as removing the physical concepts  such as hash/nestloop  to logical concepts. - add other join type's updateJoinConditionColumnStatistics action  besides inner join  during join stats' estimation  the updateJoinConditionColumnStatistics will have different behavior for different join types. - semi/anti's estimation method has been updated but currently commented  will be activated in the future.  c. row count and ndv value normalization ([0 1] => 1) - not activated in this pr  will be opened in the future.  Benchmark performance impact: - tpcds 1t query58: 0.60s -> 0.66s - tpcds 1t query64: no impact  Co-authored-by: zhongjian.xzj <zhongjian.xzj@zhongjianxzjdeMacBook-Pro.local>
apache,doris,51ba957fd6b274886d89ad28b6c8c4899bd5bdba,https://github.com/apache/doris/commit/51ba957fd6b274886d89ad28b6c8c4899bd5bdba,[improve](partition_topn) Add partition threshold check in hash table to control partition nums (#39057)  ## Proposed changes 1.  Add a session variable to control partition_topn partition threshold 2. move the partition threshold check at emplace data to hash table to control partition nums  so get check every rows. this could improve some bad case about 50%+ performance improvement， and some better case before  after move the check in hash table  maybe have performance degradation almost 10%  I think this is within the acceptable result。  <!--Describe your changes.-->
apache,doris,80482c5fc4f0d4098fc8a7c06d1c1f5c07b73833,https://github.com/apache/doris/commit/80482c5fc4f0d4098fc8a7c06d1c1f5c07b73833,[opt](nereids) clean count usage in ColumnStatistic during stats deriving (#40654)  ## Proposed changes  Stats deriving refinement step 1: clean up count usage in ColumnStatistic during stats deriving(mainly for stats-available)  to avoid serious stats deriving problem. a. use Statistics rowCount instead of count in ColumnStatistic in stats deriving  since these two infos may be inconsistent and lead to stats deriving problem. b. remove setCount interface to avoid using this count field during deriving unexpectedly in the future. c. refine notNullSelectivity computing and corresponding estimation.  Benchmark plan shape change: - tpcds query74: no performance impact.  ---------  Co-authored-by: zhongjian.xzj <zhongjian.xzj@zhongjianxzjdeMacBook-Pro.local>
apache,doris,549196049a65dc0638f093cf44318097ce287ec5,https://github.com/apache/doris/commit/549196049a65dc0638f093cf44318097ce287ec5,[bugfix](hive)Fix cache inconsistency issue (#40729)  ## Proposed changes   1. Concurrency issues: a. One thread is performing a refresh catalog action. b. One thread is performing an insert table action  and after completion  it executes a refresh table action.  The `partitionCache` will be refreshed only if the corresponding table exists in the `partitionValuesCache`. However  the `partitionValuesCache` may have been refreshed by the `refresh catalog`  resulting in the inability to find the corresponding table through the `partitionValuesCache`  resulting in the `partitionCache` not being refreshed. Similarly  the `fileCacheRef` may not be refreshed either. Therefore  directly search for all keys to match to prevent them from being refreshed.  2. No need to perform refreshAfterWriteSec operation on partitionCache. 3. Increase the thread pool size.
apache,doris,b92b63e153789646a96a520456f549202b5eef06,https://github.com/apache/doris/commit/b92b63e153789646a96a520456f549202b5eef06,Revert "[fix](scanner) Fix incorrect _max_thread_num in scanner context" (#40804)  Reverts apache/doris#40569  We need more test to avoid performance issue
apache,doris,efd1cd0c692b2f67c0b423acd41784c5919e799a,https://github.com/apache/doris/commit/efd1cd0c692b2f67c0b423acd41784c5919e799a,[improve](routine load) delay schedule EOF tasks to avoid too many small transactions (#39975)  We encountered a scenario where a large number of small transactions were generated  resulting in an impact on query performance: Kafka's data comes in batches of very small data every very short time  which leads to tasks being frequently scheduled and ending very quickly  resulting in a large number of small transactions.  To solve this problem  we delay the scheduling of tasks that perceive EOF  which would not delay data consumption  for perceiving EOF indicates that the consumption speed is greater than the production speed.
apache,doris,53dcf497958237634266636d9626b530cc8de881,https://github.com/apache/doris/commit/53dcf497958237634266636d9626b530cc8de881,[Feature](Variant) Implement inner nested data type for variant type (#39022)  # Background Currently  importing nested data formats  such as:  ``` json { "a": [{"nested1": 1}  {"nested2": "123"}] } ``` This results in the a column type becoming JSON  which has worse compression and query performance compared to native arrays  mainly due to the inability to leverage low cardinality optimizations and the overhead of parsing JSON during queries.  A common example:  ``` json { "eventId": 1  "firstName": "Name1"  "lastName": "Surname1"  "body": { "phoneNumbers": [ { "number": "5550219210"  "type": "GSM"  "callLimit": 5 }  { "number": "02124713252"  "type": "HOME"  "callLimit": 3 }  { "number": "05550219211"  "type": "WORK"  "callLimit": 2 } ] } } ```   # Design Consider storing the expanded nested structure so that the schema merge logic can be utilized directly  and querying becomes easier  for example: ``` json { "n": [{"a": 1  "b": 2}  {"a": 10  "b": 11  "c": 12}  {"a": 1001  "d": "12"}] }  { "n": [{"x": 1  "y": 2}] } ``` Data would be stored as follows  with following storage format Column | Row 0 | Row 1 -- | -- | -- n.a (array<int>) | [1  10  1001] | [null] n.b (int) | [2  11  null] | [null] n.c (int) | [null  12  null] | [null] n.d (text) | [null  null  "12"] | [null] n.x | [null  null  null] | [1] n.y | [null  null  null] | [1]  Data offsets are aligned (equal size).  # Compaction To maintain the relationship between nested nodes  such as n.a  n.b  n.c  and n.d  during compaction  if any of these columns are missing  their offsets are filled using any sibling column's offset.  # Queries ```sql SELECT v['n']['a'] FROM tbl; --- This outputs [1  10  1001]. ```  ```  sql SELECT v['n'] FROM tbl; --- This outputs [{"a" : 1  "b" : 2}  {"a" : 10  "b" : 11  "c" : 12}  {"a":1001  "d" : "12"}]. ```  During queries  the path's nested information is not perceived because this information is ignored during path evaluation (not stored in the subcolumn tree).
apache,doris,45ddb8ce9c82b250d9d4c7ce3157615dda5cf692,https://github.com/apache/doris/commit/45ddb8ce9c82b250d9d4c7ce3157615dda5cf692,[fix](mtmv) Add debug log decide for performance when query rewrite by materialized view (#39914)  ## Proposed changes In method `AbstractMaterializedViewRule#isMaterializationValid` Should add `LOG.isDebugEnabled()` before print debug log. Because `Plan#treeString` in debug log is performance consume.
apache,doris,2c154c6d9710c16d6057e0d08ba866521a12bbb0,https://github.com/apache/doris/commit/2c154c6d9710c16d6057e0d08ba866521a12bbb0,[fix](jdbc catalog) Fix Memory Leak by Enabling Weak References in HikariCP (#39582)  This PR addresses a memory leak issue caused by FastList objects in HikariCP being retained by ThreadLocal variables  which are not easily garbage collected in long-running JNI threads. To mitigate this  a system property com.zaxxer.hikari.useWeakReferences is set to true  ensuring that WeakReference is used for ThreadLocal objects  allowing the garbage collector to reclaim memory more effectively. Even though setting this will affect some performance  solving resource leaks is relatively more important Performance difference before and after setting Before setting: 10 concurrency 0.02-0.05 100 concurrency 0.18-0.4 After setting: 10 concurrency 0.02-0.07 100 concurrency 0.18-0.7
apache,doris,3e8c19f6977494968f701f04985be43d5b0c4f85,https://github.com/apache/doris/commit/3e8c19f6977494968f701f04985be43d5b0c4f85, [improvement](mtmv) Only Generate rewritten plan when generate mv plan for performance (#39541)  Before query rewrite by materialized view  we collecet the table which query used by method org.apache.doris.mtmv.MTMVCache#from.  In MTMVCache#from we calcute the cost of plan which is useless for collecting table. So add boolean needCost param in method MTMVCache#from to identify that if need cost of plan or not for performance.
apache,doris,109dba687f596df30268601fc00fffb0ebd5bc58,https://github.com/apache/doris/commit/109dba687f596df30268601fc00fffb0ebd5bc58,[enhance](mtmv)Improve the performance of obtaining partition/table versions (#39301)  Batch retrieve version information of all tables and partitions used by MTMV and store it in MTMVRefreshContext
apache,doris,f3dd685645bb5f6ea6ae54b8ae15c6d6497154ef,https://github.com/apache/doris/commit/f3dd685645bb5f6ea6ae54b8ae15c6d6497154ef,[Improvement](runtime-filter) do not use bloom to replace in_or_bloom when rf need merge (#39147)  ## Proposed changes do not use bloom to replace in_or_bloom when rf need merge Because in some cases  this will lead to poor performance  <img width="298" alt="图片" src="https://github.com/user-attachments/assets/bcee330f-bb38-4e51-af76-1a181bd205f9"> <img width="298" alt="图片" src="https://github.com/user-attachments/assets/481a4b06-929d-4f4a-8d10-bf2901e68fdf">
apache,doris,81438d561bee8d07f7a35fbcab5e727adda39458,https://github.com/apache/doris/commit/81438d561bee8d07f7a35fbcab5e727adda39458,[fix](jdbc scan) Remove the `conjuncts.remove` call in JdbcScan (#39180)  In #37565  due to the change in the calling order of finalize  the final generated Plan will be missing the PREDICATES that have been pushed down in Jdbc. Although this behavior is correct  before perfectly handling the push down of various PREDICATES  we need to keep all conjuncts to ensure that we can still filter data normally when the data returned by Jdbc is a superset.
apache,doris,2fb98f5558e79ae64429ecccd87397008e8c0f29,https://github.com/apache/doris/commit/2fb98f5558e79ae64429ecccd87397008e8c0f29,[Feature](Cloud) Support session variable disable_file_cache and enable_segment_cache in query (#37141)  Currently  whether to read from file cache or remote storage is controlled by the BE config `enable_file_cache` in cloud mode. This PR proposed to control the file cache behavior via session variables when executing queries in cloud mode. It's more convenient when have such a session variable  cache behavior could be controlled per query/session without changing BE configs  such as: 1. **Performance test**. Test the query performance when read from local file cache or remote storage for queries. 2. **Data correctness**. Check if it's file cache issue for certain tables or queries.  The read path has three kinds of caches: segment cache  page cache and file cache.  | module       | cache| BE config    | session variable| |------------|------|----------| ---- | | Segment | segment cache | disable_segment_cache | **enable_segment_cache** (supportted by this PR) | | PageIO | page cache | disable_storage_page_cache | enable_page_cache | | FileReader | file cache | enable_file_cache | **disable_file_cache** (supportted by this PR) |  The modification of the PR:  - **enable_segment_cache**: add a new session variable enable_segment_cache to control use segment cache or not. - **disable_file_cache**: disable_file_cache was for write path in cloud mode. It's supported for read path when executing queries in the PR.  With this PR   data is read from remote storage without cache: ```sql set enable_segment_cache=false; set enable_page_cache=false; set disable_file_cache=true; ```  Co-authored-by: Gavin Chou <gavineaglechou@gmail.com>
apache,doris,f18d7b6c72866b29b9639bcfccf279a8742f5304,https://github.com/apache/doris/commit/f18d7b6c72866b29b9639bcfccf279a8742f5304,[fix](group commit) Fix group commit debug log and improve performance (#38754)  ## Proposed changes  1. show `query_id` in the debug log of when group commit insert does not work 2. remove string.format to improve performance
apache,doris,faaa24cbed541748f9f56aa9ae524d357c242717,https://github.com/apache/doris/commit/faaa24cbed541748f9f56aa9ae524d357c242717,[Enhancement](audit log) Add print audit log sesssion variable (#38419)  ## Proposed changes  For the `insert into` statements during group commit load via JDBC. Printing audit logs can severely impact performance. Therefore  we have introduced a session variable to control whether to print audit logs. It is recommended to turn off audit logs only during group commit load via JDBC.  <!--Describe your changes.-->
apache,doris,d6dd62e368e977fcc8eebb951854c2e10d77f0c2,https://github.com/apache/doris/commit/d6dd62e368e977fcc8eebb951854c2e10d77f0c2,[enhencement](trino-connector) trino-connector supports push down projection to connectors (#37874)  Invoke the `applyProjection` method of connectorMetadata` to push the projection down to the connector. This reduces the amount of data retrieved by the connector and enhances query performance.  Projection pushdown is particularly important for the BigQuery connector.
apache,doris,ba5aa3112dd08077089bb15fc24e569b7b47b4ce,https://github.com/apache/doris/commit/ba5aa3112dd08077089bb15fc24e569b7b47b4ce,[Fix](JobManager)Release the lock immediately after modifying job metadata to avoid holding the lock for an extended period. (#38162)  ## Proposed changes  When deleting a job in MV  a DB lock is held. However  due to the larger lock granularity of the Job Manager  this may prevent obtaining the lock. This issue arises because during the creation of a job  immediately scheduled tasks need to perform their initial scheduling and compensate for tasks within the time window. To alleviate this issue  we are attempting to reduce the lock granularity. <!--Describe your changes.-->
apache,doris,d9e363aadc795c59acaa556f285b441805d3bf8f,https://github.com/apache/doris/commit/d9e363aadc795c59acaa556f285b441805d3bf8f,[Fix](ScanNode) Move the finalize phase of ScanNode to after the end of the Physical Translate phase. (#37565)  ## Proposed changes  Currently  Doris first obtains splits and then performs projection. After column pruning  it calls `updateRequiredSlots` to update the scanRange information. However  the Trino connector's column pruning pushdown needs to be completed before obtaining splits.  Therefore  we move the finalize phase of `ScanNode` to after the end of the `Physical Translate` phase  so that `createScanRangeLocations` can use the final columns which have been pruning.
apache,doris,7211c2d6594821a78a00423754d9f0e608e45192,https://github.com/apache/doris/commit/7211c2d6594821a78a00423754d9f0e608e45192,[conf](parallel) Reduce parallel tasks for large cluster (#38196)  For large cluster  too many parallel tasks will cause performance issue. So this PR limit the max parallel tasks in Doris.
apache,doris,47dfd41de58ec4d7cae2b2969ff4cc57eb90dfed,https://github.com/apache/doris/commit/47dfd41de58ec4d7cae2b2969ff4cc57eb90dfed,[feature](inverted index) Add multi_match function (#37722)  ## Proposed changes  1. select * from tbl where multi_match(c1  'c2  c3  c4'  'phrase_prefix'  'xxx'); 2. multi_match performs a match_phrase_prefix query on the columns specified in the first and second parameters. The value to be searched is specified in the fourth parameter.
apache,doris,e2bdb95abb83823b7e443ade2af059cdad6458fc,https://github.com/apache/doris/commit/e2bdb95abb83823b7e443ade2af059cdad6458fc,[opt](nereids) refine left semi/anti cost under short-cut opt (#37951)  Refine left semi/anti cost computing under short-cut opt  for the case whose semi/anti join has the small left side and big right side  which original solution can't support. This pr reduce the left style cost by reduce the right side cost and improve the possibility of choosing left style joins.  Pass the performance test on tpch/tpcds/usercase.  previous work: #37060
apache,doris,5c3d39cd05fdcbfe0eaf53d1f2cef948d567eb40,https://github.com/apache/doris/commit/5c3d39cd05fdcbfe0eaf53d1f2cef948d567eb40,[feature](csv)Supports reading CSV data using LF and CRLF as line separators. (#37687)  ## Proposed changes  Supports reading CSV data using LF and CRLF as line separators.  csv file: ``` 1 abc 2 def\r 3 qwe 4 hello\r ``` if you  `set keep_carriage_return = false` you will get : ```mysql 1   abc 2   def 3   qwe 4   hello ``` Here  both \r\n and \n are used as delimiters.  if you  `set keep_carriage_return = true` you will get : ```mysql 1   abc 2   def\r 3   qwe 4   hello\r ``` Here only \n is used as a delimiter.  ## warning It should be noted that `set keep_carriage_return = true` is valid for tvf  but not for stream load/mysql load. This means that when you perform stream load/mysql load  crlf and lf will be automatically used as delimiters  even if you `set keep_carriage_return = true`.
apache,doris,a1b0264544ce14245c4198f2e84399d400dfdf9a,https://github.com/apache/doris/commit/a1b0264544ce14245c4198f2e84399d400dfdf9a,[improvement](statistics)Async drop stats while truncating table. (#37715)  Drop stats for table with many partitions may slow  because to invalidate partition stats cache is time consuming. Truncate table operation do the drop stats synchronously  so the truncate table may be very slow for partition tables. This pr is to improvement the performance of truncate table. Do the drop stats asynchronously.  Time consumed for truncate a table with 10000 partitions and 10 columns reduced to 2.5s from 10s.
apache,doris,756b9c26540375847fdb5b79baffd201a247caee,https://github.com/apache/doris/commit/756b9c26540375847fdb5b79baffd201a247caee,[bugfix](external)Modify the default value of `pushdowncount` (#37754)  ## Proposed changes  1. Do not use `0`: If the number of entries in the table is 0  it is unclear whether optimization has been performed. 2. Do not use `null` or `-`: This makes it easier for the program to parse the `explain` data.
apache,doris,2f0e57455475c0a1037b98874eb84e947925ccc4,https://github.com/apache/doris/commit/2f0e57455475c0a1037b98874eb84e947925ccc4,[enhancement](log)Updated the default setting of sys_log_mode in fe.conf for better performance (#37793)  ## Proposed changes  <!--Describe your changes.--> Updated the default setting of `sys_log_mode` in `fe.conf` from `NORMAL` to `ASYNC` for better performance.
apache,doris,5367f7ed8b15f91e63ee5c2ff98de82ec6450742,https://github.com/apache/doris/commit/5367f7ed8b15f91e63ee5c2ff98de82ec6450742,[performance](broker-load) increase default broker load batch size (#36477)
apache,doris,701c7db45a9debc9c5d8b374b889c2dd0a7419e2,https://github.com/apache/doris/commit/701c7db45a9debc9c5d8b374b889c2dd0a7419e2,[improvement](mtmv) improve mv rewrite performance by reuse the shuttled expression (#37197)  Optimizations: 1. Expression shuttle is expensive in materialized view rewritting  So reuse the shuttled expression. 2. Generate shuttledExpressions by planOutput is also expensive  so generate and store in struct info once and used later
apache,doris,94ce626b49b72bdc4bdbc78bc7c9669c2296c9a3,https://github.com/apache/doris/commit/94ce626b49b72bdc4bdbc78bc7c9669c2296c9a3,[improvement](statistics)Enable estimate hive table row count using file size. (#37218)  Enable fetch hive table row count through file list by default. Change the sample partition count to 30 to reduce resource consumption. Using a table with 100000 partition to test the performance. Row count could be fetch within 0.2 second and the cpu used is less than 1 core.
apache,doris,40dbb5998df09bc002c8bd8f4c56c86d8c2c90ae,https://github.com/apache/doris/commit/40dbb5998df09bc002c8bd8f4c56c86d8c2c90ae,[feat](Nereids) Add support for slot pruning in functional dependencies (#37045)  Implement slot pruning functionality for functional dependencies to optimize performance and resource utilization. This enhancement allows for more efficient handling of dependencies by removing unnecessary slots.
apache,doris,6889225b19e5826d74582c518f3d38982a1e3886,https://github.com/apache/doris/commit/6889225b19e5826d74582c518f3d38982a1e3886,[feat](Nereids) Optimize query by pushing down aggregation through join on foreign key (#36035)  ## Proposed changes  This PR optimizes query performance by pushing down aggregations through joins when grouped by a foreign key. This adjustment reduces data processing overhead above the join  improving both speed and resource efficiency.  Transformation Example:  Before Optimization: ``` Aggregation(group by fk) | Join(pk = fk) /  \ pk  fk ``` After Optimization: ``` Join(pk = fk) /     \ pk  Aggregation(group by fk) | fk ```
apache,doris,c8f1b9f4ae0075d051a2714dceae2a8b208f71fd,https://github.com/apache/doris/commit/c8f1b9f4ae0075d051a2714dceae2a8b208f71fd,[opt](hive) save hive table schema in transaction (#37008)  Save the table schema  reduce the number of HMS calls  and improve write performance.
apache,doris,bafbb944a9afa714958297dbd24440ccfdf7dfc0,https://github.com/apache/doris/commit/bafbb944a9afa714958297dbd24440ccfdf7dfc0,[improvement](jdbc catalog) Modify the maximum number of connections in the connection pool to 30 by default (#36720)  In many cases  we found that users would use JDBC Catalog to perform a large number of queries  which resulted in the maximum of 10 connections being insufficient  so I adjusted it to 30  which covered most needs.
apache,doris,89bbad9db7964cc8a62d6d8f12c9756407c656d6,https://github.com/apache/doris/commit/89bbad9db7964cc8a62d6d8f12c9756407c656d6,[improvement](statistics)Disable fetch stats for iceberg table through Iceberg api by default. (#36931)  Disable fetch iceberg table stats through iceberg api by default. Because it could be very heavy and external table performs fine without column stats. Could enable it by : set global enable_fetch_iceberg_stats=true
apache,doris,3da83514cb5baa85dbd914a8dd086156bb4fc282,https://github.com/apache/doris/commit/3da83514cb5baa85dbd914a8dd086156bb4fc282,[fix](mtmv) Fix high nest level materialized view can not be rewritten  because low level mv aggregate roll up (#36567)  Query is aggregate  the query group by expression is less than materialzied view group by expression. when the more dimensions than queries in materialzied view can be eliminated with functional dependencies. it can be rewritten with out roll up aggregate. For example as following: mv def is  CREATE MATERIALIZED VIEW mv BUILD IMMEDIATE REFRESH AUTO ON MANUAL DISTRIBUTED BY RANDOM BUCKETS 2 PROPERTIES ('replication_num' = '1') AS select l_orderkey  l_partkey  l_suppkey  o_orderkey  o_custkey  ps_partkey  cast( sum( IFNULL(ps_suppkey  0) * IFNULL(ps_partkey  0) ) as decimal(28  8) ) as agg2 from lineitem_1 inner join orders_1 on lineitem_1.l_orderkey = orders_1.o_orderkey inner join partsupp_1 on l_partkey = partsupp_1.ps_partkey and l_suppkey = partsupp_1.ps_suppkey where partsupp_1.ps_suppkey > 1 group by l_orderkey  l_partkey  l_suppkey  o_orderkey  o_custkey  ps_partkey;  query is as following:  select l_orderkey  l_partkey  l_suppkey  o_orderkey  o_custkey  cast( sum( IFNULL(ps_suppkey  0) * IFNULL(ps_partkey  0) ) as decimal(28  8) ) as agg2 from lineitem_1 inner join orders_1 on lineitem_1.l_orderkey = orders_1.o_orderkey inner join partsupp_1 on l_partkey = partsupp_1.ps_partkey and l_suppkey = partsupp_1.ps_suppkey where partsupp_1.ps_suppkey > 1 group by l_orderkey  l_partkey  l_suppkey  o_orderkey  o_custkey;  we can see that query doesn't use `ps_partkey` which is in mv group by expression. Normally will add roll up aggragate on materialized view if the gorup by dimension in mv is mucher than query group by dimension. And  in this scane we can get the function dependency on `l_suppkey = ps_suppkey `. and we doesn't need to add roll up aggregate on materialized view in rewritten plan. this improve performance and is beneficial for nest materialized view rewrite.
apache,doris,9b5a764623873f3ec3165e9d8eca3980cb67fcd7,https://github.com/apache/doris/commit/9b5a764623873f3ec3165e9d8eca3980cb67fcd7,[feat](Nereids) Optimize Sum Literal Rewriting by Excluding Single Instances (#35559)  ## Proposed changes  This PR introduces a change in the method removeOneSumLiteral to enhance the performance of sum literal rewriting in SQL queries. The modification ensures that sum literals appearing only once  such as in expressions like select count(id1 + 1)  count(id2 + 1) from t  are not rewritten.
apache,doris,41d4618f7cf599df35c9b9401d7c760a23f12dc0,https://github.com/apache/doris/commit/41d4618f7cf599df35c9b9401d7c760a23f12dc0,[opt](Nereids) Optimize Join Penalty Calculation Based on Build Side Data Volume (#35773)  This PR introduces an optimization that adjusts the penalty applied during join operations based on the volume of data on the build side. Specifically  when the number of rows and width of the tables being joined are equal  the materialization costs are now considered more accurately. The update ensures that joins with a larger dataset on the build side incur a higher penalty  improving overall query performance and resource allocation.
apache,doris,b2f04fa9a7bf9eacbdf2b9c4af2a37536cd82556,https://github.com/apache/doris/commit/b2f04fa9a7bf9eacbdf2b9c4af2a37536cd82556,[Feat](nereids) add transform rule MergePercentileToArray (#34313)  MergePercentileToArray is to perform a transformation in this case:  select ss_item_sk  percentile(ss_quantity 0.9)  percentile(ss_quantity 0.6)  percentile(ss_quantity 0.3) from store_sales group by ss_item_sk;  ===>  select ss_item_sk  percentile_array(ss_quantity [0.3 0.6 0.9]) from store_sales group by ss_item_sk;
apache,doris,a939b59dd4cb291a74ff694a1662bf2228230cea,https://github.com/apache/doris/commit/a939b59dd4cb291a74ff694a1662bf2228230cea, [opt](mtmv) Improve the mv rewrite performance by optimize code usage (#35674)  Improve the performance from two points  one is optimize decide model method and another is to reuse the mv struc info:  1. Instead of use java.util.List#containsAll by java.util.Set#containsAll in method AbstractMaterializedViewRule#decideMatchMode  2. Reuse the mv struct info in different query  because mv struct info is immutable.  Notes: tableBitSet in struct info is relevant to the statementContext in cascadesContext  if reuse the mv struct info for different query  we should re generate table bitset and construct new struct info with method StructInfo#withTableBitSet
apache,doris,4665fa162afbbf10aa4a1714be5a69b3cfe9b5a8,https://github.com/apache/doris/commit/4665fa162afbbf10aa4a1714be5a69b3cfe9b5a8,[fix](Nereids) Optimize BFS Memory Usage to Mitigate Exponential Data Growth (#35440)  The origin pr is #34948 and the temporary solution is #35408.  In our effort to streamline and optimize dependency handling  we implement the following steps:  - Detect Circular Dependencies: Identify any circular references within functional dependencies. If any are found  we remove the specific dependencies responsible for creating these cycles. - Clean Up Group By Dependencies: Remove all dependencies listed in the 'group by' clauses to simplify and enhance query performance.
pinpoint-apm,pinpoint,710a2c8bec968a8fc42b75d4032631b93d0bef64,https://github.com/pinpoint-apm/pinpoint/commit/710a2c8bec968a8fc42b75d4032631b93d0bef64,[#12159] Change sort key combination to improve query performance
pinpoint-apm,pinpoint,98cc821a34c0e0f436937e0b3de4836170f0f2e6,https://github.com/pinpoint-apm/pinpoint/commit/98cc821a34c0e0f436937e0b3de4836170f0f2e6,[#10318] Improved serialization performance of primitive values
pinpoint-apm,pinpoint,06584b08cf2437654c0ca2bc6620c16592158878,https://github.com/pinpoint-apm/pinpoint/commit/06584b08cf2437654c0ca2bc6620c16592158878,[#11497]  Improve atomicity and performance of Redis call in ActiveThread
pinpoint-apm,pinpoint,19250305230e94f3b6e5152f600a9d9f953dd2e3,https://github.com/pinpoint-apm/pinpoint/commit/19250305230e94f3b6e5152f600a9d9f953dd2e3,[#11411] Improve write performance of ServerMap Link
pinpoint-apm,pinpoint,ccd7e28f4074477b0a78cae5966bad898cb7d07a,https://github.com/pinpoint-apm/pinpoint/commit/ccd7e28f4074477b0a78cae5966bad898cb7d07a,[#11337] Improve performance for ActiveAgent
languagetool-org,languagetool,5f79678349473d697fa8a047da3e9c9fc1a841dd,https://github.com/languagetool-org/languagetool/commit/5f79678349473d697fa8a047da3e9c9fc1a841dd,Revert "Memory and performance optimizations (#11027)" (#11043)  This reverts commit 6f823cfdc5f7d70eaea63cc9b49cfa16a94b2a86.
languagetool-org,languagetool,6f823cfdc5f7d70eaea63cc9b49cfa16a94b2a86,https://github.com/languagetool-org/languagetool/commit/6f823cfdc5f7d70eaea63cc9b49cfa16a94b2a86,Memory and performance optimizations (#11027)  * Do not instantiate French language manually multiple times  * Do not instantiate GermanyGerman language multiple times  Do not preserve language in static fields.  Such approach causes multiple instantiation of other classes when initialization of classA loading classB and classB has a static field with instance of classA (this is a simplified example)  * Do not instantiate languages multiple times on `Languages` class loading  * Added multiple instantiation protection for the French languages
languagetool-org,languagetool,edac13dd6f0e9f80b10bd6c1b49f5a53af6d815f,https://github.com/languagetool-org/languagetool/commit/edac13dd6f0e9f80b10bd6c1b49f5a53af6d815f,perf: various optimizations for hot spots in profiling data (#10704)  * perf: various optimizations for hot spots in profiling data  avoid creating regexes via Pattern.split and String.replace remove unneccessary lock in wordsToAdd cache data in HunspellRule initialization  * perf: remove other synchronized blocks in wordsToAdd
google,guice,17aa84c3ce2bdb7cb81acd7d9cde92e4f53359cb,https://github.com/google/guice/commit/17aa84c3ce2bdb7cb81acd7d9cde92e4f53359cb,Simplify the 'factory' MethodHandle signatures to drop 'dependency' types from them  I thought modeling native types would be better but on reflection it just adds complexity for little gain.  So turn all generated factories into `(InternalContext)->Object` types.  This eliminates a bunch of `asType` adapters and should improve linkage performance a bit (not that this is a huge concern)  but the major benefit is just simplicity.  Performance is an interesting question here.  In principle we are just adding 'no op' cast to `Object` on constructor and provides method invocations  and then `checkCast` instructions at constructor and method parameter injection points.  So the waste is those extra `cast` operations.  However  in the JVM those are nearly free (simple pointer check  can often be elided as part of normal invocation type checking). So my guess is that this is a no-op performance wise.  Some limited benchmarking of `@Provides` and `@Inject` constructors bears this out.  PiperOrigin-RevId: 740791709
google,guice,34c84e975d118c0c2ae717f6d0e008146144e348,https://github.com/google/guice/commit/34c84e975d118c0c2ae717f6d0e008146144e348,Make `InternalContext.setDependency` a no-op when cyclic proxies are disabled  This will save a tiny bit of memory and time and should simplify the `MethodHandle` implementations when we get there.  Behavior is nearly perfectly concerned  but as the `CircularDependencyTest` points out  error messages will change when failing with cycles that go through scoped bindings.  PiperOrigin-RevId: 738385449
google,guice,ab1ef98c117d3d62b786a78f71ae64cd3f48d7c5,https://github.com/google/guice/commit/ab1ef98c117d3d62b786a78f71ae64cd3f48d7c5,Introduce `LinkageContext` to resolve cycles that occur during `getHandle` calls.  If 2 `InternalFactory` objects get linked to each other recursively we need to handle and detect cycles to prevent `StackOverflow`  For `get` calls this is managed by `InternalContext` and it is extremely performance sensitive.  For `getHandle` calls  this is currently unhandled which can lead to StackOverflow as more InternalFactory types are migrated.  This change addresses that via a new class `LinkageContext` which can detect and automatically resolve a cycle.  The specific strategy is to use a `MutableCallSite` as a layer of indirection to link back to the original method handle once it is done being constructed.  This constructs a recursive `MethodHandle`  much in the same way the normal cycle resolution uses a `Proxy` for that same layer of indirection.  See also [The farnsworth parabox](https://en.wikipedia.org/wiki/The_Farnsworth_Parabox).  Testing this scenario right now is difficult since we don't support many kinds of factories  but the next change introducing provider methods requires this.  PiperOrigin-RevId: 737708332
google,guice,41e8c06f6ff8d349ba89aaaa5b91f04c25868d27,https://github.com/google/guice/commit/41e8c06f6ff8d349ba89aaaa5b91f04c25868d27,Improve the performance of `isCircularProxy`  The map implementation used by `MapMaker` is fairly old and requires 2 levels of indirection (segments->table).  Switching to `ConcurrentHashMap` would be better but this will still be redundant with logic inside of `Proxy` so instead we just use that.  Based on some simple benchmarks this is about 5X as fast  and should save a bit memory.  A number of alternatives were considered  * adding a marker interface to the list of proxy interfaces This is a completely ideal solution but fails due to our need to support proxying interfaces from all `ClassLoaders` and it isn't possible to proxy interfaces that cannot mutually 'see' each other.  So we would need to 'inject' our interface into the Bootstrap classloader which seems impossible and risky  or create special 'child' classloaders that can bridge it  but this triggers issues with proxying package-private interfaces... sigh. * using a `ClassValue`  bootstrapping the value is tricky and while this is faster than the status quo  it would allocate an entry for every class we queried which could add up in a large application  and the approach in this change is faster. * using `Proxy.isProxyInstance` this works but is slower than an `instanceof` query.  PiperOrigin-RevId: 721984180
google,guice,38005e00693c35bc21fdc8c4340723d3d45004f7,https://github.com/google/guice/commit/38005e00693c35bc21fdc8c4340723d3d45004f7,Simplify the API between factories and construction contexts by moving more logic into InternalContext.  Instead of having factories manage a `ConstructionContext` object and call some subset of methods on it  we simplify things into a number of abstract operations on `InternalContext` - `tryStartConstruction`: called when we are starting construction - This is amortized O(1) work and only allocates when we are resizing the internal hash tables. - `finishConstruction`: called when construction is complete  clears the table - This is amortized O(1) work due to the hash search and some datastructure maintenance - `finishConstructionAndSetReference`: called by `ConstructorInjector` to support member injectors calling back into it - `clearCurrentReference`: called by `ConstructorInjector` after member injection is complete.  This enables the following optimizations  * When `disableCircularProxies` is set can get away with a single `int[]` to track the set of currently constructing factories and turn `finishConstructionAndSetReference` into a no-op * When circular proxies are enabled we can eliminate the `ConstructionContext` object and only allocate when we actually construct a proxy. * We can 'statically' allocate each factory a unique non-zero integer (a `circularFactoryId`). This allows us to create a open-addressed hashtable with integer keys that reduces allocation  hashing and GC overhead. - this does limit us to 2^32-1 circular factories before we get collisions  but that should be enough! pretty sure that other things will fail first. * The new `InternalContext` protocol enables some simplifications in the factories which allows to delete an implementation  Previously we would insert `ConstructionContext` objects into the hash table and reuse them if the same factory was used multiple times.  This was generally rare  and now we instead remove keys from the table after construction.  In theory this means we always perform 2 hash table lookups per construction where the prior approach might only do one.  This is true but in general all lookups missed so the prior approach also required 2 lookups (`get` and `put`).  This is an optimization all on its own  but the new simpler protocol will enable more optimizations in a followup.  PiperOrigin-RevId: 713781971
apache,zookeeper,3e02328048a9c753624d44aba51ac70cab60619a,https://github.com/apache/zookeeper/commit/3e02328048a9c753624d44aba51ac70cab60619a,ZOOKEEPER-4872: SnapshotCommand should not perform fastForwardFromEdits  Reviewers: kezhuw Author: li4wang Closes #2210 from li4wang/ZOOKEEPER-4872
redis,jedis,eb34e054763fdc3b50284ece1b5c0dd08615c037,https://github.com/redis/jedis/commit/eb34e054763fdc3b50284ece1b5c0dd08615c037,Run pipeline in current thread if all the keys on same node (#4149)  * perf:last node run in current thread directly  * fix: connection leak  we should return it to connection pool  * noop sync when pipelinedResponses.isEmpty()  * revert rename  * clean  * remove last node run in current thread when multi node  * add test for pipeline all keys at same node  * fix: make all keys on same node  * formatting  ---------  Co-authored-by: ggivo <ivo.gaydazhiev@redis.com>
metersphere,metersphere,13ed85e91eeba0d5210e3937563bdfa16cf07d6f,https://github.com/metersphere/metersphere/commit/13ed85e91eeba0d5210e3937563bdfa16cf07d6f,perf(系统设置): 删除空代码类
grpc,grpc-java,f866c805c2f78271de9f2b61254363d009cee8c6,https://github.com/grpc/grpc-java/commit/f866c805c2f78271de9f2b61254363d009cee8c6,util: SocketAddress.toString() cannot be used for equality  Some addresses are equal even though their toString is different (InetSocketAddress ignores the hostname when it has an address). And some addresses are not equal even though their toString might be the same (AnonymousInProcessSocketAddress doesn't override toString()).  InetSocketAddress/InetAddress do not cache the toString() result. Thus  even in the worst case that uses a HashSet  this should use less memory than the earlier approach  as no strings are formatted. It probably also significantly improves performance in the reasonably common case when an Endpoint is created just for looking up a key  because the string creation in the constructor isn't then amorized. updateChildrenWithResolvedAddresses()  for example  creates n^2 Endpoint objects for lookups.
plantuml,plantuml,dac564a687e9ff851803abb580ce9a057d24fe30,https://github.com/plantuml/plantuml/commit/dac564a687e9ff851803abb580ce9a057d24fe30,feat: add builtin function `%mod` (modulo operator) (#1865)  This change adds the `%mod` function to perform modulo division between two integers.
plantuml,plantuml,47df55c69cc0db30d41cdc4f0b1728343c83883f,https://github.com/plantuml/plantuml/commit/47df55c69cc0db30d41cdc4f0b1728343c83883f,fix: performance issue on substring https://github.com/plantuml/plantuml/issues/1819
debezium,debezium,6a01cf5a6e3521c3a57f3a6373ece6607cb3cfb4,https://github.com/debezium/debezium/commit/6a01cf5a6e3521c3a57f3a6373ece6607cb3cfb4,DBZ-9030 Fix performance regression with transaction inserts
debezium,debezium,e5236e6a5896dcd222468f0edd42ad409c0e3c5a,https://github.com/debezium/debezium/commit/e5236e6a5896dcd222468f0edd42ad409c0e3c5a,DBZ-8986 Remove superfluous argument on LogMinerEventRow
debezium,debezium,1d587fb6ff0a200b58d22c5e6e6a18059e782c98,https://github.com/debezium/debezium/commit/1d587fb6ff0a200b58d22c5e6e6a18059e782c98,DBZ-8879: Fix performance regression in debezium-core
debezium,debezium,16654038c8df55ef8cbacd6f06e6d2136713d6ba,https://github.com/debezium/debezium/commit/16654038c8df55ef8cbacd6f06e6d2136713d6ba,DBZ-8879: Fix performance regression in debezium-core
debezium,debezium,695e56cb22804471fc69443e0bf1558b8fc1a350,https://github.com/debezium/debezium/commit/695e56cb22804471fc69443e0bf1558b8fc1a350,DBZ-8879: Fix performance regression in debezium-core
debezium,debezium,d0676405ebd835fba7ec878fbd9af8f7acafb841,https://github.com/debezium/debezium/commit/d0676405ebd835fba7ec878fbd9af8f7acafb841,DBZ-8925 Improve Hybrid strategy performance with the Oracle ObjectId cache
debezium,debezium,f2109c1b46ec419eff33471d5584707f4daaaf17,https://github.com/debezium/debezium/commit/f2109c1b46ec419eff33471d5584707f4daaaf17,DBZ-8860 Improve transaction/event cache performance
debezium,debezium,56a2746e2d5b443f51db56756242c9881b52198b,https://github.com/debezium/debezium/commit/56a2746e2d5b443f51db56756242c9881b52198b,DBZ-8665 Improve performance for handling constraint violations
debezium,debezium,141aae0b917809009b0e6ab9b7a0457f6bbea0f0,https://github.com/debezium/debezium/commit/141aae0b917809009b0e6ab9b7a0457f6bbea0f0,DBZ-8493 Explicitly pin the bean registry JDBC connection to PDB  By pinning the bean registry JDBC connection to the PDB  this avoids the need to execute "ALTER SESSION"  which cannot be performed when an exception is raised. This allows the reselect post processor to safely execute the fallback query without any ORA-01003 errors.
debezium,debezium,2299e2356f42aec39a4ebddd3a3b64014b9ba12b,https://github.com/debezium/debezium/commit/2299e2356f42aec39a4ebddd3a3b64014b9ba12b,DBZ-8423 Resolved the issue where the debezium_offset_storage table remained locked after offset storage initialization.  During the load function  a SELECT operation is performed on the debezium_offset_storage table. This operation locks the table  and the lock is not released after the offset storage initialization. We added a commit of the connection to the load function to ensure that the lock on the debezium_offset_storage table is released when no longer needed.
debezium,debezium,f4a2854c856cebeb8fda9c1af04d86ad769e4956,https://github.com/debezium/debezium/commit/f4a2854c856cebeb8fda9c1af04d86ad769e4956,DBZ-8071 Improve hybrid mining performance with object-id-to-table-id cache
OpenRefine,OpenRefine,f7e50cc0efa147462c323a96fcec6baa060b2e7a,https://github.com/OpenRefine/OpenRefine/commit/f7e50cc0efa147462c323a96fcec6baa060b2e7a,Enable custom clustering for non-GREL expressions (#7243)  * Enable user defined clustering with non-GREL  Removes a hard coded value that allowed user defined clustering only for GREL expressions.  Adds some tests with a mock parser to show that we now support arbitrary expressions  as long as they have a registered parser to evaluate them.  * Fix requests in clustering dialogs  - Add missing and/or fix wrong csrf tokens to post requests. - Add missing language prefix to expressions.  * Add value1 and value2 to Jython and Clojure  In UserDefinedDistance.java (#6612) we introduced two new variables `value1` and `value2`. In GREL these are accessible as "globals".  But in Jython and Clojure we encapsulate the expressions in functions and only pass certain variables as parameters (value  cell  cells  row  rowIndex).  This means that in Jython and Clojure expressions we can not access variables like `columnName`.  This also means that we somehow have to make the new variables `value1` and `value2` accessible to Jython and Clojure expressions.  Without rewriting the whole Jython and Clojure integrations it felt most idiomatic to also introduce these variables as parameters to the Jython and Clojure functions that encapsulate the expressions.  But it is still somewhat unsatisfying that we have a different pool of available variables in GREL and (possibly) each other language integration.  * Perform automatic linting
trinodb,trino,73015615900443f2921c71f3cc71bb12b8a47a29,https://github.com/trinodb/trino/commit/73015615900443f2921c71f3cc71bb12b8a47a29,Parallelize materialized view base table freshness retrieval in Iceberg  Previously  base table freshness was retrieved sequentially  which could make materialized view refreshes inefficient  especially when base tables changed frequently or loaded slowly or the materialized view involves other materialized views.  This change parallelizes the retrieval process  improving refresh performance  particularly for workloads with frequently changing or slow-loading base tables.  Benchmark Results  with 20 base tables in a materialized view using `REFRESH MATERIALIZED VIEW`: * Avg table load time: 10ms → Refresh time reduced from 560ms to 310ms. * Avg table load time: 100ms → Refresh time reduced by more than 1s.
trinodb,trino,1dbabd8ba7bad0e6b4fc2aa874bbf2f5c5ae55b3,https://github.com/trinodb/trino/commit/1dbabd8ba7bad0e6b4fc2aa874bbf2f5c5ae55b3,Allow BlockBuilder resetTo() to work with partially filled complex objs  BlockBuilder resetTo() is designed to allow rolling back state on errors. However  if any of the complex structures (MAP  ARRAY  ROW) have an exception thrown while filling in an entry  they will be in an unclosed state that  prevents resetTo() from being performed. This change fixes that behavior to allow resetTo() to be called even for partially constructed complex structures.
trinodb,trino,819b1935110eff239b710ae540da5c0285e3c02f,https://github.com/trinodb/trino/commit/819b1935110eff239b710ae540da5c0285e3c02f,Print splits count and distribution time in EXPLAIN ANALYZE  Split distribution time can be a bottleneck in some scenarios. Having it in the EXPLAIN output makes it easier to diagnose without accessing query json. Split count is helpful to analyze query performance when there is either a big number of small splits or there is just one split like with basic JDBC connectors. The split count is visible also in the "Input rows distribution" metric but it is only available in the VERBOSE mode.
trinodb,trino,4f480877d01f28fa68665af19f41f217c4156201,https://github.com/trinodb/trino/commit/4f480877d01f28fa68665af19f41f217c4156201,Improve performance when listing columns in Iceberg
trinodb,trino,61a79dd875fa48e2e0c2b3230051a526ecd51ad7,https://github.com/trinodb/trino/commit/61a79dd875fa48e2e0c2b3230051a526ecd51ad7,Make IcebergSplitSource async  Previously  all IcebergSplitSource scan planning activies happened on the calling scheduler thread  which meant that all table scan planning would happen sequentially across table scans. This hurts performance when queries contain multiple iceberg (or other connector) table scans that could proceed with split generation in parallel to one another.
trinodb,trino,d8001277bb7adbf815b007ac1ac1e55e3adf8de7,https://github.com/trinodb/trino/commit/d8001277bb7adbf815b007ac1ac1e55e3adf8de7,Fix redirecting from some Delta Lake tables for Glue V1  Don't attempt to read Delta Lake table columns when performing table redirect to a different catalog. This allows redirecting from incomplete or invalid Delta Lake tables.
trinodb,trino,eb46a88a04916f96135defb809836632c122d22c,https://github.com/trinodb/trino/commit/eb46a88a04916f96135defb809836632c122d22c,Fix redirecting from some Delta Lake tables  Don't attempt to read Delta Lake table columns when performing table redirect to a different catalog. This allows redirecting from incomplete or invalid Delta Lake tables.
trinodb,trino,dce5a39f697fee3bdbf4029618526c6745f6ce47,https://github.com/trinodb/trino/commit/dce5a39f697fee3bdbf4029618526c6745f6ce47,Extract IANA timezone ID from timestamp(p) and time(p) with tz  This commit adds functions to extract the IANA timezone ID from timestamp(p) with time zone and time(p) with time zone. Previously  only the timezone offsets could be extracted (TIMEZONE_HOUR and TIMEZONE_MINUTE)  which loses information about the actual timezone.  Resolves: #20893  Refactor timezone extraction logic and update documentation  - Removed unnecessary line-breaks to improve code style - Replaced String.format with optimized string operations for better performance - Updated documentation to clarify method functionality and usage  Resolves: #20893  Resolve function name conflict  timezone(timestamp(p) with time zone) and timezone(time(p) with time zone) in conflict which lead to build failure in trino-docs. :no-index: prevents this conflict.  Resolves: #20893
trinodb,trino,7ec87bcee3b38f1e5f1bec1df7638fdb83cf7887,https://github.com/trinodb/trino/commit/7ec87bcee3b38f1e5f1bec1df7638fdb83cf7887,Use dynamic filters to reduce hive partitions metadata listing  Improves performance of splits generation on large partitioned hive tables by using dynamic filters to reduce the partitions metadata fetched by io.trino.metastore.HiveMetastore#getPartitionsByNames
trinodb,trino,a524522a56f21623ebe6298c30f50f2bb2cc698d,https://github.com/trinodb/trino/commit/a524522a56f21623ebe6298c30f50f2bb2cc698d,Do not materialize stream early  It doesn't improve performance much
trinodb,trino,42c13dfe008c2579dee4df1f641fd00a8b3f041e,https://github.com/trinodb/trino/commit/42c13dfe008c2579dee4df1f641fd00a8b3f041e,Materialize input stream early  According to a Jackson's performance guidelines  parsing from byte[] is faster than parsing from InputStream.
trinodb,trino,60c494c2fadd4b8bc5b1dcd73e62a99313a87af5,https://github.com/trinodb/trino/commit/60c494c2fadd4b8bc5b1dcd73e62a99313a87af5,Improve input validation for approx_percentile  Ensure that approx_percentile performs explicit input validation and throw TrinoException with proper error code.
trinodb,trino,185e071a379fc49e977a6740d1cfd81a4016ded0,https://github.com/trinodb/trino/commit/185e071a379fc49e977a6740d1cfd81a4016ded0,Add MultipleDistinctAggregationsToSubqueries  The rule splits distinct aggregations on different arguments to sub-queries and joins the grouped results using grouping keys if any. This allows SingleDistinctAggregationToGroupBy to kick in and improve parallelism and performance significantly when the grouped query is cheap to duplicate.
trinodb,trino,401669e72faf819f583c7d99b748901d719ccc63,https://github.com/trinodb/trino/commit/401669e72faf819f583c7d99b748901d719ccc63,Skip PreAggregateCaseAggregations it aggregations are not reduced  If the number of pre-aggregations is equal to number of case aggregations  then there is no performance gain in rule execution. Additionally  firing rule in such case could lead to infinite rule execution loop  since new pre-aggregations could be eligable for further optimization.
trinodb,trino,5ff71dff926934a919d722691cb5892b47259392,https://github.com/trinodb/trino/commit/5ff71dff926934a919d722691cb5892b47259392,Extract join sort expressions for both sides of BETWEEN predicates  For expressions like `x BETWEEN y AND z` two sort expressions will be extracted `x >= y` and `x <= z` which then will be used to perform fast join lookups.
trinodb,trino,cae2e9b70671e8dfefd690d2d2d8f0b464ededb5,https://github.com/trinodb/trino/commit/cae2e9b70671e8dfefd690d2d2d8f0b464ededb5,Reduce thread congestion in HashBuilderOperator  Memory management in HashBuilderOperator.addInput caused significant thread congestion. When memory tracking accuracy is traded for calling delegate.setBytes() less frequently  we are getting better performance for queries which tend to process tiny pages in HashBuilder operator.
trinodb,trino,bd289dc4773baca88c94670f625538a4b98f8398,https://github.com/trinodb/trino/commit/bd289dc4773baca88c94670f625538a4b98f8398,Increase the max number of http connections in Azure native FS  By default  OkHttp only allows 5 concurrent HTTP connections per host  and 64 total concurrent connections. Increase these values to avoid a performance degradation compared to the legacy FS  when executing queries with a large number of splits  on nodes with large number of split runner threads.
trinodb,trino,3803b411fc2ba92ed72232c1d531e11eab63b66b,https://github.com/trinodb/trino/commit/3803b411fc2ba92ed72232c1d531e11eab63b66b,Optimize ParquetTypeUtils#getDescriptors  Existing logic was complex due to performing case-insensitive matching. This was unnecesary because fileSchema and requestedSchema already contain lower cased names. Also  since requestedSchema is derived from fileSchema  we can build descriptors map directly from result of getColumns instead of repeating look-ups in fileSchema.
trinodb,trino,04227e35841191485b1ac5a6d7ae5c2c6f04f952,https://github.com/trinodb/trino/commit/04227e35841191485b1ac5a6d7ae5c2c6f04f952,Add JsonStringArrayExtract function  Optimzie common case of transform(cast(json_parse(varchar_col) as array<json>)  json -> json_extract_scalar(json  jsonPath)) case to be single and more performant call of JsonStringArrayExtract.
trinodb,trino,0eb5bfc06bdb15c7b525987e548438cc7149291c,https://github.com/trinodb/trino/commit/0eb5bfc06bdb15c7b525987e548438cc7149291c,Implement incremental refresh for single-table  predicate-only MVs (#20959)  Add incremental refresh support for simple Iceberg MVs  When MV refresh is executing  the planner suggests either incremental or full refresh to the connector. In this first phase  incremental refresh is suggested only when Scan/Filter/Project nodes are present in the plan tree - otherwise full refresh is performed. The Iceberg connector can act on this signal and use IncrementalAppendScan to scan the 'delta' records only and append them to the MV storage table (without truncation).  Co-authored-by: Karol Sobczak <napewnotrafi@gmail.com>
trinodb,trino,623bcc2fb328665467d12ae02cb8700d6ae0a389,https://github.com/trinodb/trino/commit/623bcc2fb328665467d12ae02cb8700d6ae0a389,Improve performance of json functions  Avoid allocating heap ByteBuffer used by InputStreamReader.
trinodb,trino,0c4e91b5fd0ad53cd874782d7c935ef743865de3,https://github.com/trinodb/trino/commit/0c4e91b5fd0ad53cd874782d7c935ef743865de3,Pass source table handles for statements using the merge mechanism  Add the possibility to perform analysis on the dependencies of the statement using the merge mechanism. Specifically one connector could potentially figure out whether concurrent UPDATE/DELETE/MERGE operations which add data into the same table as the one from which data is being selected collide with each other.
trinodb,trino,cd415149a1eca481c2b69cdde8c504ea70e597d3,https://github.com/trinodb/trino/commit/cd415149a1eca481c2b69cdde8c504ea70e597d3,Improve performance of SortedRangeSet discrete union
jhy,jsoup,001b4dda1f1728dcfc29c7af37b473c3f8c3d756,https://github.com/jhy/jsoup/commit/001b4dda1f1728dcfc29c7af37b473c3f8c3d756,Consume escaped subqueries correctly  Now that escaped sequences can have spaces  that needs to be handled in consumeSubQuery correctly.  The unescape -> escape step is a bit redundant. If perf critical we could have a method that consumes escape sequences without actually unescaping them. Or  we could refactor consumeSubQuery to parse into Evaluators directly.  But the rest is already pretty stringy  and users can keep parsed Evaluators for reuse  so am deeming OK for now.  Fixes issue noted in #2305
jhy,jsoup,1a031b6742464a6dbb5b42c5132a75e46a45c180,https://github.com/jhy/jsoup/commit/1a031b6742464a6dbb5b42c5132a75e46a45c180,Clone tags from source TagSet on demand  This allows the TagSet to vend mutable Tags on demand  without requiring the entire tagset to be cloned on initialization. Otherwise for the HTML tagset  it would create many unused duplicates (all the known tags) on every pass.  Gets the perf to effectively where we were in 1.19.1 with immutable flyweight tags  but with the ability to mutate as required.
jhy,jsoup,0679bef07f1e29ae72ae54102d5af9a1f80d45d4,https://github.com/jhy/jsoup/commit/0679bef07f1e29ae72ae54102d5af9a1f80d45d4,Perf: removed redundant lowercase normalization
jhy,jsoup,d80275e16ebd34bae5b48f29f3e4437e1b207955,https://github.com/jhy/jsoup/commit/d80275e16ebd34bae5b48f29f3e4437e1b207955,Performance tweak when appending tag names  For some crafted HTML  this path was accumulating an ultra-long tag name. Removed redundant
jhy,jsoup,36ba3edad1de83e61dff71ca929909587eebd834,https://github.com/jhy/jsoup/commit/36ba3edad1de83e61dff71ca929909587eebd834,Optimize `#id .class` selector performance  Fixes #2254
apereo,cas,6424562dda83e41dec8e798605e9833b46b80f10,https://github.com/apereo/cas/commit/6424562dda83e41dec8e798605e9833b46b80f10,Fix the OIDC logout support (back channel) in authentication delegation  After fixing the SAML back channel logout support (https://github.com/apereo/cas/pull/6213)  I'm now focusing on the OIDC back channel logout support.  I always perform the same test: one CAS server acting as the OIDC client and another CAS server acting as the OIDC server. After an authentication delegation  the logout is not transmitted from the OIDC server to the OIDC client because of two inconsistencies in the OIDC logout request: 1) the "exp" claim is missing (mandatory in the spec: https://openid.net/specs/openid-connect-backchannel-1_0.html#LogoutToken) 2) the "sid" claim is not properly computed for the logout: it should be a SHA of the "jwtId" which is itself a SHA512 of the TGT identifier (like in the `OidcIdTokenGeneratorService`).  This PR fixes these two issues in the `OidcSingleLogoutMessageCreator`. Unit tests have been updated accordingly.
apereo,cas,d36e395259c2e8c2ae93a8f904840de0692cdc61,https://github.com/apereo/cas/commit/d36e395259c2e8c2ae93a8f904840de0692cdc61,Fix the SAML logout support (back channel) in authentication delegation  After fixing the SAML (front channel) logout support (https://github.com/apereo/cas/pull/6205)  I'm focusing my efforts on the back channel way.  It partially works. I still have a CAS server delegating via SAML to another CAS server and after a successful login  I perform a logout on the second CAS server expecting it to send a logout call to the first CAS server. The logout propagation works if I log in during the authn delegation  but not if I'm already authenticated on the second CAS server before the authn delegation.  This comes from the way we compute the session index at the authentication stage. Getting the "ticket" parameter only works half of the time  it looks and is fragile.  This PR proposes to **explicitly** pass the service ticket as the session index in the `SamlProfileBuilderContext` and other components that need it.  Side note: I don't have a service ticket in the `ECPSamlIdPProfileHandlerController` so I pass `null` here.
apereo,cas,d1c66b1f52e951cde84e972da8dce9f2f82d8931,https://github.com/apereo/cas/commit/d1c66b1f52e951cde84e972da8dce9f2f82d8931,Fix double SSO session inconsistency  Currently  when a new SSO session is created while an existing one already exists  the CAS server only destroys the existing TGT  but it does not perform a SLO. This happens for: - a first login and then a second login with renew=true - a first login from one browser tab and a second login from another browser tab.  So  if I log in into app1 as user1 and then log in with renew=true as user2 in app2  I will remain user1 in app1: so I will be user1 in app1 and user2 in app2. More critically  I won't be able to logout from app1 even when performing a CAS SLO: only the user2 will be logged out from app2.  This seems inconsistent to me and this PR fixes the problem by performing a SLO.
apereo,cas,2445f633a7685adc31e663f6c4085fefc7b6e08e,https://github.com/apereo/cas/commit/2445f633a7685adc31e663f6c4085fefc7b6e08e,improve redis ticket registry performance - #2
apereo,cas,db973e2a019d7b972e84e7a5a862605267390bcb,https://github.com/apereo/cas/commit/db973e2a019d7b972e84e7a5a862605267390bcb,improve redis ticket registry performance - #1
apereo,cas,b1d7e7aae87ba9b26a8af30da4f86e795bb87736,https://github.com/apereo/cas/commit/b1d7e7aae87ba9b26a8af30da4f86e795bb87736,switch jackson to use objectwriters for slightly better perf
apereo,cas,e1c6d28fb40b591d962293ed7666644df3837cdc,https://github.com/apereo/cas/commit/e1c6d28fb40b591d962293ed7666644df3837cdc,improve performance of MongoDb/GoogleCloudFirestore ticket registry (#6129)
apereo,cas,6294bf6a7e9b579b985c6a97f9ccc8383e87e78a,https://github.com/apereo/cas/commit/6294bf6a7e9b579b985c6a97f9ccc8383e87e78a,minor hz performance improvements
apereo,cas,d49f0c1cf7a7a8fdc7d3125e435b897bc446ea12,https://github.com/apereo/cas/commit/d49f0c1cf7a7a8fdc7d3125e435b897bc446ea12,Improve sessions counting by using Jet for the Hazelcast ticket registry  The performance of the sessions counting can be greatly improved by using Jet for the Hazelcast ticket registry.  The `HazelcastTicketRegistryTests` already covered this use case.
apereo,cas,e07301b64209ba145501ec761659c37f66b99178,https://github.com/apereo/cas/commit/e07301b64209ba145501ec761659c37f66b99178,perf improvements for hz and redis ticket registry  support parallel streams and records for data classes
PaperMC,Paper,287eb52fa459d9b348b016f167a72d8fffa34574,https://github.com/PaperMC/Paper/commit/287eb52fa459d9b348b016f167a72d8fffa34574,Use hidden classes for event executors (#11848)  Static final MethodHandles perform similar to direct calls. Additionally  hidden classes simplify logic around ClassLoaders as they can be defined weakly coupled to their defining class loader. All variants of methods (static  private  non-void) can be covered by this mechanism.
xpipe-io,xpipe,b39c74b4ff5caefcf8078d8ebdc9346987330f3e,https://github.com/xpipe-io/xpipe/commit/b39c74b4ff5caefcf8078d8ebdc9346987330f3e,Improve list box performance by synchronizing less
xpipe-io,xpipe,e69da5d5b63e3433f51bede3716034d0d16daa08,https://github.com/xpipe-io/xpipe/commit/e69da5d5b63e3433f51bede3716034d0d16daa08,Don't animate transition in performance mode
xpipe-io,xpipe,39ee41e7b4871235062dcc29d67dcc1a38b94520,https://github.com/xpipe-io/xpipe/commit/39ee41e7b4871235062dcc29d67dcc1a38b94520,More performance adjustments
xpipe-io,xpipe,ee418ed0ffd9c25b309988af32070c2b0acaa294,https://github.com/xpipe-io/xpipe/commit/ee418ed0ffd9c25b309988af32070c2b0acaa294,Section performance fixes
xpipe-io,xpipe,a898341011e19cb99307eff7f67443bcc2d81edb,https://github.com/xpipe-io/xpipe/commit/a898341011e19cb99307eff7f67443bcc2d81edb,Various performance fixes
xpipe-io,xpipe,afaf206a4253f85558a4eb52e2b850a53d18a2ed,https://github.com/xpipe-io/xpipe/commit/afaf206a4253f85558a4eb52e2b850a53d18a2ed,More performance fixes
xpipe-io,xpipe,572a0f03418e99b589cb142f087574f54d21e8d4,https://github.com/xpipe-io/xpipe/commit/572a0f03418e99b589cb142f087574f54d21e8d4,Various performance fixes
opensearch-project,OpenSearch,560ac1030c941c8cb01c4fcd29a919bd45ccabee,https://github.com/opensearch-project/OpenSearch/commit/560ac1030c941c8cb01c4fcd29a919bd45ccabee,Improve sort-query performance by retaining the default totalHitsThreshold for approximated match_all queries (#18189)  Signed-off-by: Prudhvi Godithi <pgodithi@amazon.com>
opensearch-project,OpenSearch,58eb44e7ece913aca6de34d32f6b837a512541ae,https://github.com/opensearch-project/OpenSearch/commit/58eb44e7ece913aca6de34d32f6b837a512541ae,[Tiered Cache] Using a single cache manager for all ehcache disk caches (#17513)  * Using a single cache manager for all ehcache disk caches  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Added changelog  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Fixing cache manager UT  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Addressing comments  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Removing commented out code  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Adding changelog  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Changes to perform mutable changes for cache manager under a lock  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Changes to fix UT  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Addressing minor comments  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  ---------  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com> Signed-off-by: Sagar <99425694+sgup432@users.noreply.github.com>
opensearch-project,OpenSearch,47d3655d8e86e44ab8ea4b58079d09fa0439db8c,https://github.com/opensearch-project/OpenSearch/commit/47d3655d8e86e44ab8ea4b58079d09fa0439db8c,Add dfs transformation function in XContentMapValues (#17612)  * Add transformation function in XContentMapValues  Adds a transformation function for XContentMapValues that performs depth first traversal into a map  potentially applying transformations to different values along the way. Main application for the method will be to provide masks that change values in the map without compromising the structure.  Signed-off-by: John Mazanec <jmazane@amazon.com>  * Switch to stack based  Signed-off-by: John Mazanec <jmazane@amazon.com>  * Implement buildTransformerTrie without recursion  Signed-off-by: John Mazanec <jmazane@amazon.com>  * Add inplace transform  Signed-off-by: John Mazanec <jmazane@amazon.com>  * Fix changelog  Signed-off-by: John Mazanec <jmazane@amazon.com>  * Add test for shared path  Signed-off-by: John Mazanec <jmazane@amazon.com>  ---------  Signed-off-by: John Mazanec <jmazane@amazon.com>
opensearch-project,OpenSearch,27946550b686df41fd0ff53cb340ce52c73a7b45,https://github.com/opensearch-project/OpenSearch/commit/27946550b686df41fd0ff53cb340ce52c73a7b45,Improve flat_object field parsing performance by reducing two passes to a single pass (#16297)
opensearch-project,OpenSearch,92088be9e859d95422aa94f3e2db3619d1a90209,https://github.com/opensearch-project/OpenSearch/commit/92088be9e859d95422aa94f3e2db3619d1a90209,Optimize innerhits query performance (#16937)  Signed-off-by: kkewwei <kewei.11@bytedance.com> Signed-off-by: kkewwei <kkewwei@163.com>
opensearch-project,OpenSearch,ba0c4f3e0e2a34d85c55ccecd344df8db9d256cf,https://github.com/opensearch-project/OpenSearch/commit/ba0c4f3e0e2a34d85c55ccecd344df8db9d256cf,Improve performance of bitmap terms filtering (#16936)   ---------  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>
opensearch-project,OpenSearch,d2a1477ac452db8bbb31a8988c51dec62b8ea23f,https://github.com/opensearch-project/OpenSearch/commit/d2a1477ac452db8bbb31a8988c51dec62b8ea23f,Deprecate performing update operation with default pipeline or final pipeline (#16712)  * Deprecate performing update operation with default pipeline or final pipeline  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Modify the warning message  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Modify changelog  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Fix test issue  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  ---------  Signed-off-by: Gao Binlong <gbinlong@amazon.com>
opensearch-project,OpenSearch,e07499a771afbc335e1f7f08a82f8197e5826939,https://github.com/opensearch-project/OpenSearch/commit/e07499a771afbc335e1f7f08a82f8197e5826939,Improve performance for resolving derived fields (#16564)  Doing the type check before the string comparison makes it much faster to resolve derived fields.  Signed-off-by: Robson Araujo <robson.araujo@glean.com>
opensearch-project,OpenSearch,6f1b59e54bec41d40772f8571c7b65d4b523f8b1,https://github.com/opensearch-project/OpenSearch/commit/6f1b59e54bec41d40772f8571c7b65d4b523f8b1,Add logic in master service to optimize performance and retain detailed logging for critical cluster operations. (#16421)  Signed-off-by: Sumit Bansal <sumitsb@amazon.com> Signed-off-by: shwetathareja <shwetathareja@live.com> Co-authored-by: shwetathareja <shwetathareja@live.com>
opensearch-project,OpenSearch,dc8a435f9f14eb6eb679c63d06f4baca3def3215,https://github.com/opensearch-project/OpenSearch/commit/dc8a435f9f14eb6eb679c63d06f4baca3def3215,[Star tree] Performance optimizations during flush flow (#16037)  ---------  Signed-off-by: Bharathwaj G <bharath78910@gmail.com>
opensearch-project,OpenSearch,2e9db40a50735eacc95a4fc8926e8bb7042a696a,https://github.com/opensearch-project/OpenSearch/commit/2e9db40a50735eacc95a4fc8926e8bb7042a696a,Introduce ApproximateRangeQuery and ApproximateQuery (#13788)  This introduces a basic "approximation" framework that improves query performance by modifying the query in a way that should be functionally equivalent.  To start  we can reduce the bounds of a range query in order to satisfy the `track_total_hits` value (which defaults to 10 000).  ---------  Signed-off-by: Harsha Vamsi Kalluri <harshavamsi096@gmail.com> Signed-off-by: Michael Froh <froh@amazon.com> Co-authored-by: Michael Froh <froh@amazon.com>
opensearch-project,OpenSearch,597747dcbf7c14513dd07887048976620164f4e0,https://github.com/opensearch-project/OpenSearch/commit/597747dcbf7c14513dd07887048976620164f4e0,Add ThreadContextPermission for markAsSystemContext and allow core to perform the method (#15016)  * Add RuntimePermission for markAsSystemContext and allow core to perform the method  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * private  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Surround with doPrivileged  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Create ThreadContextAccess  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Create notion of ThreadContextPermission  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add to CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add javadoc  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add to test-framework.policy file  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Mark as internal  Signed-off-by: Craig Perkins <cwperx@amazon.com>  ---------  Signed-off-by: Craig Perkins <cwperx@amazon.com>
opensearch-project,OpenSearch,eb306d2bab43de789b59adc01265c683a8fb69fb,https://github.com/opensearch-project/OpenSearch/commit/eb306d2bab43de789b59adc01265c683a8fb69fb,Add queryGroupId to search workload tasks at co-ordinator and data node level (#14708)  * add logic to add headers to Task  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add logic to add queryGroupId to task headers  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * remove redundant code  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add changelog entry  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * address comments  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * fix precommit  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add UTs for RemoteIndexMetadataManager (#14660)  Signed-off-by: Shivansh Arora <hishiv@amazon.com> Co-authored-by: Arpit-Bandejiya <abandeji@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix match_phrase_prefix_query not working on text field with multiple values and index_prefixes (#10959)  * Fix match_phrase_prefix_query not working on text field with multiple values and index_prefixes Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Add more test  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * modify change log  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Fix test failure  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Change the indexAnalyzer used by prefix field  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Skip old version for yaml test  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Optimize some code  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Fix test failure  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Modify yaml test description  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Remove the name parameter for setAnalyzer()  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  ---------  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Offline calculation of total shard per node and caching it for weight calculation inside LocalShardBalancer (#14675)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [bug fix] validate lower bound for top n size (#14587)  Signed-off-by: Chenyang Ji <cyji@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Create SystemIndexRegistry with helper method matchesSystemIndex (#14415)  * Create new extension point in SystemIndexPlugin for a single plugin to get registered system indices  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add to CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * WIP on system indices from IndexNameExpressionResolver  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add test in IndexNameExpressionResolverTests  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Remove changes in SystemIndexPlugin  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add method in IndexNameExpressionResolver to get matching system indices  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Show how resolver can be chained to get system indices  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Fix forbiddenApis check  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Update CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Make SystemIndices internal  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Remove unneeded changes  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Fix CI failures  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Fix precommit errors  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Use Regex instead of WildcardMatcher  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Address code review feedback  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Allow caller to pass index expressions  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Create SystemIndexRegistry  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Update CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Remove singleton limitation  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add javadoc  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add @ExperimentalApi annotation  Signed-off-by: Craig Perkins <cwperx@amazon.com>  ---------  Signed-off-by: Craig Perkins <cwperx@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Refactor Grok validate pattern to iterative approach (#14206)  * grok validate patterns recusrion to iterative  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * Add max depth in resolving a pattern to avoid OOM  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * change path from deque to arraylist  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * rename queue to stack  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * Change max depth to 500  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * typo originPatternName fix  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  * spotless  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com>  ---------  Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump opentelemetry from 1.39.0 to 1.40.0 (#14674)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump jackson from 2.17.1 to 2.17.2 (#14687)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add release notes for release 1.3.18 (#14699)  Signed-off-by: Zelin Hao <zelinhao@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump reactor from 3.5.19 to 3.5.20 (#14697)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add unit tests for read flow of RemoteClusterStateService and bug fix for transient settings (#14476)  Signed-off-by: Shivansh Arora <hishiv@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Update version check for the bug fix of match_phrase_prefix_query not working on text field with multiple values and index_prefixes (#14703)  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Remove unnecessary cast to int from test (#14696)  Signed-off-by: Lukáš Vlček <lukas.vlcek@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * print reason why parent task was cancelled (#14604)  Signed-off-by: kkewwei <kkewwei@163.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Use set of shard routing for shard in unassigned shard batch check. (#14533)  Signed-off-by: Swetha Guptha <gupthasg@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add versioning for UploadedIndexMetadata (#14677)  * Add versioning for UploadedIndexMetadata * Handle componentPrefix for backward compatibility  Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix: update help output for _cat (#14722)  * fixed help output for _cat  Signed-off-by: ahmedsobeh <ahmed.sobeh@aiven.io>  * updated changelog  Signed-off-by: ahmedsobeh <ahmed.sobeh@aiven.io>  * updated changelog  Signed-off-by: ahmedsobeh <ahmed.sobeh@aiven.io>  ---------  Signed-off-by: ahmedsobeh <ahmed.sobeh@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix hdfs-fixture kerb-admin & hadoop-minicluster dependencies are not being updated / false positive reports on CVEs (#14729)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Update to Gradle 8.9 (#14574)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix hdfs-fixture hadoop-minicluster dependencies are not being updated / false positive reports on CVEs (#14732)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add `strict_allow_templates` dynamic mapping option (#14555)  * The dynamic mapping parameter supports strict_allow_templates  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Modify change log  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Modify skip version in yml test file  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Refactor some code  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Keep the old methods  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * change public to private  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Optimize some code  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Do not override toString method for Dynamic  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Optimize some code and modify the changelog  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  ---------  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump net.minidev:json-smart from 2.5.0 to 2.5.1 in /plugins/repository-azure (#14748)  * Bump net.minidev:json-smart in /plugins/repository-azure  Bumps [net.minidev:json-smart](https://github.com/netplex/json-smart-v2) from 2.5.0 to 2.5.1. - [Release notes](https://github.com/netplex/json-smart-v2/releases) - [Commits](https://github.com/netplex/json-smart-v2/compare/2.5.0...2.5.1)  --- updated-dependencies: - dependency-name: net.minidev:json-smart dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * remove query insights plugin from core (#14743)  Signed-off-by: Chenyang Ji <cyji@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add `strict_allow_templates` dynamic mapping option (#14555) (#14737) (#14742)  * The dynamic mapping parameter supports strict_allow_templates  * Modify change log  * Modify skip version in yml test file  * Refactor some code  * Keep the old methods  * change public to private  * Optimize some code  * Do not override toString method for Dynamic  * Optimize some code and modify the changelog  ---------  (cherry picked from commit 6b8b3efe01a62c221f308a2e3b019d75a7f5ad8a)  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Co-authored-by: opensearch-trigger-bot[bot] <98922864+opensearch-trigger-bot[bot]@users.noreply.github.com> Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix create or update alias API doesn't throw exception for unsupported parameters (#14719)  * Fix create or update alias API doesn't throw exception for unsupported parameters  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Update version check in yml test  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * modify change log  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  ---------  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Remove query categorization from core (#14759)  * Remove query categorization from core  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Add changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Trigger Build  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  ---------  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add changes to propagate queryGroupId across child requests and nodes (#14614)  * add query group header propagator  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * apply spotless check  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add new propagator in ThreadContext  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * spotlessApply  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * address comments  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.microsoft.azure:msal4j from 1.15.1 to 1.16.0 in /plugins/repository-azure (#14610)  * Bump com.microsoft.azure:msal4j in /plugins/repository-azure  Bumps [com.microsoft.azure:msal4j](https://github.com/AzureAD/microsoft-authentication-library-for-java) from 1.15.1 to 1.16.0. - [Release notes](https://github.com/AzureAD/microsoft-authentication-library-for-java/releases) - [Changelog](https://github.com/AzureAD/microsoft-authentication-library-for-java/blob/dev/changelog.txt) - [Commits](https://github.com/AzureAD/microsoft-authentication-library-for-java/compare/v1.15.1...v1.16.0)  --- updated-dependencies: - dependency-name: com.microsoft.azure:msal4j dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Bugfix] Fix ICacheKeySerializerTests flakiness (#14564)  * Fix testInvalidInput flakiness  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  * Addressed andrross's comment  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  * rerun security check  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  ---------  Signed-off-by: Peter Alfonsi <petealft@amazon.com> Co-authored-by: Peter Alfonsi <petealft@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Correct typo in method name (#14621)  Signed-off-by: vatsal <vatsal.v.anand@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Refactoring FilterPath.parse by using an iterative approach instead of recursion. (#14200)  * Refactor FilterPath parse function (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Implement unit tests for FilterPathTests (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Write warn log if Filter is empty; Add comments (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Add changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove unnecessary log statement  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove unused logger  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Spotless apply  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove incorrect changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  ---------  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Co-authored-by: Robin Friedmann <robinfriedmann.rf@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Removing String format in RemoteStoreMigrationAllocationDecider to optimise performance(#14612)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Clear templates before Adding; Use NamedWriteableAwareStreamInput for RemoteCustomMetadata; Correct the check for deciding upload of HashesOfConsistentSettings (#14513)  * Clear templates before Adding; Use NamedWriteableAwareStreamInput for RemoteCustomMetadata * Correct the check for deciding upload of hashes of consistent settings  Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add changelog  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add PR link changelog  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Improve reroute performance by optimising List.removeAll in LocalShardsBalancer to filter remote search shard from relocation decision (#14613)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix assertion failure while deleting remote backed index (#14601)  Signed-off-by: Sachin Kale <kalsac@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Allow system index warning in OpenSearchRestTestCase.refreshAllIndices (#14635)  * Allow system index warning  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add to CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Address code review comments  Signed-off-by: Craig Perkins <cwperx@amazon.com>  ---------  Signed-off-by: Craig Perkins <cwperx@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Star tree codec changes (#14514)  --------- Signed-off-by: Bharathwaj G <bharath78910@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.github.spullara.mustache.java:compiler from 0.9.13 to 0.9.14 in /modules/lang-mustache (#14672)  * Bump com.github.spullara.mustache.java:compiler  Bumps [com.github.spullara.mustache.java:compiler](https://github.com/spullara/mustache.java) from 0.9.13 to 0.9.14. - [Commits](https://github.com/spullara/mustache.java/compare/mustache.java-0.9.13...mustache.java-0.9.14)  --- updated-dependencies: - dependency-name: com.github.spullara.mustache.java:compiler dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump net.minidev:accessors-smart from 2.5.0 to 2.5.1 in /plugins/repository-azure (#14673)  * Bump net.minidev:accessors-smart in /plugins/repository-azure  Bumps [net.minidev:accessors-smart](https://github.com/netplex/json-smart-v2) from 2.5.0 to 2.5.1. - [Release notes](https://github.com/netplex/json-smart-v2/releases) - [Commits](https://github.com/netplex/json-smart-v2/compare/2.5.0...2.5.1)  --- updated-dependencies: - dependency-name: net.minidev:accessors-smart dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * move query group thread context propagator out of ThreadContext  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  ---------  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com> Signed-off-by: dependabot[bot] <support@github.com> Signed-off-by: Peter Alfonsi <petealft@amazon.com> Signed-off-by: vatsal <vatsal.v.anand@gmail.com> Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: Sachin Kale <kalsac@amazon.com> Signed-off-by: Craig Perkins <cwperx@amazon.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Co-authored-by: Peter Alfonsi <peter.alfonsi@gmail.com> Co-authored-by: Peter Alfonsi <petealft@amazon.com> Co-authored-by: Vatsal <36672090+imvtsl@users.noreply.github.com> Co-authored-by: Siddhant Deshmukh <deshsid@amazon.com> Co-authored-by: Robin Friedmann <robinfriedmann.rf@gmail.com> Co-authored-by: rishavz_sagar <rishavsagar4b1@gmail.com> Co-authored-by: Sooraj Sinha <81695996+soosinha@users.noreply.github.com> Co-authored-by: Sachin Kale <sachinpkale@gmail.com> Co-authored-by: Craig Perkins <cwperx@amazon.com> Co-authored-by: Bharathwaj G <bharath78910@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add consumers to remote store based index settings (#14764)  Signed-off-by: Shourya Dutta Biswas <114977491+shourya035@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add matchesPluginSystemIndexPattern to SystemIndexRegistry (#14750)  * Add matchesPluginSystemIndexPattern to SystemIndexRegistry  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add to CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Use single data structure to keep track of system indices  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Address code review comments  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add test for getAllDescriptors  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Update server/src/main/java/org/opensearch/indices/SystemIndexRegistry.java  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Craig Perkins <craig5008@gmail.com>  ---------  Signed-off-by: Craig Perkins <cwperx@amazon.com> Signed-off-by: Craig Perkins <craig5008@gmail.com> Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * SPI for loading ABC templates (#14659)  * SPI for loading ABC templates  Signed-off-by: mgodwan <mgodwan@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix bulk upsert ignores the default_pipeline and final_pipeline when the auto-created index matches the index template (#12891)  * Fix bulk upsert ignores the default_pipeline and final_pipeline when auto-created index matches with the index template  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Modify changelog & comment  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Use new approach  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  * Fix test failure  Signed-off-by: Gao Binlong <gbinlong@amazon.com>  ---------  Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix flaky test due to node being used across all tests (#14787)  Signed-off-by: Mohit Godwani <mgodwan@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Star Tree Implementation [OnHeap] (#14512)  --------- Signed-off-by: Sarthak Aggarwal <sarthagg@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add Gao Binlong as maintainer (#14796)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Clear ehcache disk cache files during initialization (#14738)  * Clear ehcache disk cache files during initialization  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Adding UT to fix line coverage  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Addressing comment  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Adding more Uts for better line coverage  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Throwing exception in case we fail to clear cache files during startup  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Adding more UTs  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Adding a UT for more coverage  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Fixing gradle build  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  * Update ehcache disk cache close() logic  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com>  ---------  Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Refactor remote-routing-table service inline with remote state interfaces (#14668)  --------- Signed-off-by: Arpit Bandejiya <abandeji@amazon.com> Signed-off-by: Arpit-Bandejiya <abandeji@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Set version to 2.15 for determining metadata during migration to remote store  Signed-off-by: Sandeep Kumawat <skumwt@amazon.com> Co-authored-by: Sandeep Kumawat <skumwt@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix bulk upsert ignores the default_pipeline and final_pipeline when the auto-created index matches the index template (#14793)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix create or update alias API doesn't throw exception for unsupported parameters (#14769)  Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Change RCSS info logs to debug (#14814)  Signed-off-by: Shivansh Arora <hishiv@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Bugfix] Fix NPE in ReplicaShardAllocator (#13993) (#14385)  * [Bugfix] Fix NPE in ReplicaShardAllocator (#13993)  Signed-off-by: Daniil Roman <daniilroman.cv@gmail.com>  * Add fix info to CHANGELOG.md  Signed-off-by: Daniil Roman <danroman17397@gmail.com>  ---------  Signed-off-by: Daniil Roman <daniilroman.cv@gmail.com> Signed-off-by: Daniil Roman <danroman17397@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Run performance benchmark on pull requests (#14760)  * add performance benchmark workflow for pull requests  Signed-off-by: Rishabh Singh <sngri@amazon.com>  * Update PERFORMANCE_BENCHMARKS.md  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  * Update PERFORMANCE_BENCHMARKS.md  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  * Update .github/workflows/benchmark-pull-request.yml  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  * Update .github/workflows/benchmark-pull-request.yml  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  * Update .github/workflows/benchmark-pull-request.yml  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  * Update .github/workflows/benchmark-pull-request.yml  Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com>  ---------  Signed-off-by: Rishabh Singh <sngri@amazon.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com> Co-authored-by: Andriy Redko <drreta@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * fix constant_keyword field type (#14807)  Signed-off-by: kkewwei <kkewwei@163.com>  test  Signed-off-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Co-authored-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Remote Store Migration] Reconcile remote store based index settings during STRICT mode switch (#14792)  Signed-off-by: Shourya Dutta Biswas <114977491+shourya035@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add prefix mode verification setting for repository verification (#14790)  * Add prefix mode verification setting for repository verification  Signed-off-by: Ashish Singh <ssashish@amazon.com>  * Add UTs and randomise prefix mode repository verification  Signed-off-by: Ashish Singh <ssashish@amazon.com>  * Incorporate PR review feedback  Signed-off-by: Ashish Singh <ssashish@amazon.com>  ---------  Signed-off-by: Ashish Singh <ssashish@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add length check on comment body for benchmark workflow (#14834)  Signed-off-by: Rishabh Singh <sngri@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add restore-from-snapshot test procedure for snapshot run benchmark config (#14842)  Signed-off-by: Rishabh Singh <sngri@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix env variable name typo (#14843)  Signed-off-by: Rishabh Singh <sngri@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Use circuit breaker in InternalHistogram when adding empty buckets (#14754)  * introduce circuit breaker in InternalHistogram  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * use circuit breaker from reduce context  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * add test  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * revert use_real_memory change in OpenSearchNode  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * add change log  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  ---------  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Remote State] Create interface RemoteEntitiesManager (#14671)  * Create interface RemoteEntitiesManager  Signed-off-by: Shivansh Arora <hishiv@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Optimise TransportNodesAction to not send DiscoveryNodes for NodeStat… (#14749)  * Optimize TransportNodesAction to not send DiscoveryNodes for NodeStats  NodesInfo and ClusterStats call  Signed-off-by: Pranshu Shukla <pranshushukla06@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Enabling term version check on local state for all ClusterManager Read Transport Actions (#14273)  * enabling term version check on local state for all admin read actions  Signed-off-by: Rajiv Kumar Vaidyanathan <rajivkv@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Reduce logging in DEBUG for MasterService:run (#14795)  * Reduce logging in DEBUG for MasteService:run by introducing short and long summary in Taskbatcher  Signed-off-by: Sumit Bansal <sumitsb@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add SplitResponseProcessor to Search Pipelines (#14800)  * Add SplitResponseProcessor for search pipelines  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Register the split processor factory  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Address code review comments  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Avoid list copy by casting array  Signed-off-by: Daniel Widdis <widdis@gmail.com>  ---------  Signed-off-by: Daniel Widdis <widdis@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add integration tests for RemoteRoutingTable Service. (#14631)  Signed-off-by: Shailendra Singh <singhlhs@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add SortResponseProcessor to Search Pipelines (#14785)  * Add SortResponseProcessor for search pipelines  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Add stupid and unnecessary javadocs to satisfy overly strict CI  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Split casting and sorting methods for readability  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Register the sort processor factory  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Address code review comments  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Cast individual list elements to avoid creating two lists  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Add yamlRestTests  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Clarify why there's unusual sorting  Signed-off-by: Daniel Widdis <widdis@gmail.com>  * Use instanceof instead of isAssignableFrom  Signed-off-by: Daniel Widdis <widdis@gmail.com>  ---------  Signed-off-by: Daniel Widdis <widdis@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix allowUnmappedFields  mapUnmappedFieldAsString settings to be applied when parsing query string query (#13957)  * Modify to invoke QueryShardContext.fieldMapper() method to apply allowUnmappedFields and mapUnmappedFieldAsString settings  Signed-off-by: imyp92 <pyw5420@gmail.com>  * Add test cases to verify returning 400 responses if unmapped fields are included for some types of query  Signed-off-by: imyp92 <pyw5420@gmail.com>  * Add changelog  Signed-off-by: imyp92 <pyw5420@gmail.com>  ---------  Signed-off-by: imyp92 <pyw5420@gmail.com> Signed-off-by: gaobinlong <gbinlong@amazon.com> Co-authored-by: gaobinlong <gbinlong@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.microsoft.azure:msal4j from 1.16.0 to 1.16.1 in /plugins/repository-azure (#14857)  * Bump com.microsoft.azure:msal4j in /plugins/repository-azure  Bumps [com.microsoft.azure:msal4j](https://github.com/AzureAD/microsoft-authentication-library-for-java) from 1.16.0 to 1.16.1. - [Release notes](https://github.com/AzureAD/microsoft-authentication-library-for-java/releases) - [Changelog](https://github.com/AzureAD/microsoft-authentication-library-for-java/blob/dev/changelog.txt) - [Commits](https://github.com/AzureAD/microsoft-authentication-library-for-java/compare/v1.16.0...v1.16.1)  --- updated-dependencies: - dependency-name: com.microsoft.azure:msal4j dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.gradle.develocity from 3.17.5 to 3.17.6 (#14856)  * Bump com.gradle.develocity from 3.17.5 to 3.17.6  Bumps com.gradle.develocity from 3.17.5 to 3.17.6.  --- updated-dependencies: - dependency-name: com.gradle.develocity dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump org.jline:jline in /test/fixtures/hdfs-fixture (#14859)  Bumps [org.jline:jline](https://github.com/jline/jline3) from 3.26.2 to 3.26.3. - [Release notes](https://github.com/jline/jline3/releases) - [Changelog](https://github.com/jline/jline3/blob/master/changelog.md) - [Commits](https://github.com/jline/jline3/compare/jline-parent-3.26.2...jline-parent-3.26.3)  --- updated-dependencies: - dependency-name: org.jline:jline dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Use Lucene provided Persian stem (#14847)  Lucene provided Persian stem apparently isn't hooked yet and this change is doing that based on what is done for Arabic stem support.  Signed-off-by: Ebrahim Byagowi <ebrahim@gnu.org> Signed-off-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Co-authored-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump actions/checkout from 2 to 4 (#14858)  * Bump actions/checkout from 2 to 4  Bumps [actions/checkout](https://github.com/actions/checkout) from 2 to 4. - [Release notes](https://github.com/actions/checkout/releases) - [Changelog](https://github.com/actions/checkout/blob/main/CHANGELOG.md) - [Commits](https://github.com/actions/checkout/compare/v2...v4)  --- updated-dependencies: - dependency-name: actions/checkout dependency-type: direct:production update-type: version-update:semver-major ...  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Deprecate batch_size parameter on bulk API (#14725)  By default the full _bulk payload will be passed to ingest processors as a batch  with any sub batching logic to be implemented by each processor if necessary.  Signed-off-by: Liyun Xiu <xiliyun@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add perms for remote snapshot cache eviction on scripted query (#14411)  Signed-off-by: Finn Carroll <carrofin@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add transport interceptor to populate queryGroupId in task headers  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Add rest  transport layer changes for Hot to warm tiering - dedicated setup (#13980)  Signed-off-by: Neetika Singhal <neetiks@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Create listener to refresh search thread resource usage (#14832)  * [bug fix] fix incorrect coordinator node search resource usages  Signed-off-by: Chenyang Ji <cyji@amazon.com>  * fix bug on serialization when passing task resource usage to coordinator  Signed-off-by: Chenyang Ji <cyji@amazon.com>  * add more unit tests  Signed-off-by: Chenyang Ji <cyji@amazon.com>  * remove query insights plugin related code  Signed-off-by: Chenyang Ji <cyji@amazon.com>  * create per request listener to refresh task resource usage  Signed-off-by: Chenyang Ji <cyji@amazon.com>  * Make new listener API public  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Add changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove wrong files added  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Address review comments  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Build fix  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Make singleton  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Address review comments  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Make sure listener runs before plugin listeners  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Spotless  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Minor fix  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  ---------  Signed-off-by: Chenyang Ji <cyji@amazon.com> Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Signed-off-by: Jay Deng <jayd0104@gmail.com> Co-authored-by: Chenyang Ji <cyji@amazon.com> Co-authored-by: Jay Deng <jayd0104@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Caching avg total bytes and avg free bytes inside ClusterInfo (#14851)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Use default value when index.number_of_replicas is null (#14812)  * Use default value when index.number_of_replicas is null  Signed-off-by: Liyun Xiu <xiliyun@amazon.com>  * Add integration test  Signed-off-by: Liyun Xiu <xiliyun@amazon.com>  * Add changelog  Signed-off-by: Liyun Xiu <xiliyun@amazon.com>  ---------  Signed-off-by: Liyun Xiu <xiliyun@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Remote Routing Table] Implement write and read flow for shard diff file. (#14684)  * Implement write and read flow to upload/download shard diff file.  Signed-off-by: Shailendra Singh <singhlhs@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Optimized ClusterStatsIndices to precomute shard stats (#14426)  * Optimize Cluster Stats Indices to precomute node level stats  Signed-off-by: Pranshu Shukla <pranshushukla06@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix constraint bug which allows more primary shards than average primary shards per index (#14908)  Signed-off-by: Gaurav Bafna <gbbafna@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Optmising AwarenessAllocationDecider for hashmap.get call (#14761)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * update comment  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix IngestServiceTests.testBulkRequestExecutionWithFailures (#14918)  The test would previously fail if the randomness led to only a single indexing request being included in the bulk payload. This change guarantees multiple indexing requests in order to ensure the batch logic kicks in.  Also replace some unneeded mocks with real classes.  Signed-off-by: Andrew Ross <andrross@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add queryGroupTask  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * remove unnecessary imports  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add QueryGroupTask tests  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * rename WLM transport request handler  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add CHANGELOG entry  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * fix ut  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * address comments  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * fix UT to remove the verify for final method  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * apply spotless  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  ---------  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com> Signed-off-by: Shivansh Arora <hishiv@amazon.com> Signed-off-by: Gao Binlong <gbinlong@amazon.com> Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Chenyang Ji <cyji@amazon.com> Signed-off-by: Craig Perkins <cwperx@amazon.com> Signed-off-by: Sandesh Kumar <sandeshkr419@gmail.com> Signed-off-by: Andriy Redko <andriy.redko@aiven.io> Signed-off-by: Zelin Hao <zelinhao@amazon.com> Signed-off-by: Lukáš Vlček <lukas.vlcek@aiven.io> Signed-off-by: kkewwei <kkewwei@163.com> Signed-off-by: Swetha Guptha <gupthasg@amazon.com> Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: ahmedsobeh <ahmed.sobeh@aiven.io> Signed-off-by: dependabot[bot] <support@github.com> Signed-off-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Signed-off-by: Peter Alfonsi <petealft@amazon.com> Signed-off-by: vatsal <vatsal.v.anand@gmail.com> Signed-off-by: Sachin Kale <kalsac@amazon.com> Signed-off-by: Shourya Dutta Biswas <114977491+shourya035@users.noreply.github.com> Signed-off-by: Craig Perkins <craig5008@gmail.com> Signed-off-by: mgodwan <mgodwan@amazon.com> Signed-off-by: Mohit Godwani <mgodwan@amazon.com> Signed-off-by: Sagar Upadhyaya <sagar.upadhyaya.121@gmail.com> Signed-off-by: Sandeep Kumawat <skumwt@amazon.com> Signed-off-by: Daniil Roman <daniilroman.cv@gmail.com> Signed-off-by: Daniil Roman <danroman17397@gmail.com> Signed-off-by: Rishabh Singh <sngri@amazon.com> Signed-off-by: Rishabh Singh <rishabhksingh@gmail.com> Signed-off-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Signed-off-by: Ashish Singh <ssashish@amazon.com> Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com> Signed-off-by: Pranshu Shukla <pranshushukla06@gmail.com> Signed-off-by: Rajiv Kumar Vaidyanathan <rajivkv@amazon.com> Signed-off-by: Sumit Bansal <sumitsb@amazon.com> Signed-off-by: Daniel Widdis <widdis@gmail.com> Signed-off-by: Shailendra Singh <singhlhs@amazon.com> Signed-off-by: imyp92 <pyw5420@gmail.com> Signed-off-by: gaobinlong <gbinlong@amazon.com> Signed-off-by: Ebrahim Byagowi <ebrahim@gnu.org> Signed-off-by: Liyun Xiu <xiliyun@amazon.com> Signed-off-by: Finn Carroll <carrofin@amazon.com> Signed-off-by: Neetika Singhal <neetiks@amazon.com> Signed-off-by: Jay Deng <jayd0104@gmail.com> Signed-off-by: Gaurav Bafna <gbbafna@amazon.com> Signed-off-by: Andrew Ross <andrross@amazon.com> Co-authored-by: Shivansh Arora <hishiv@amazon.com> Co-authored-by: Arpit-Bandejiya <abandeji@amazon.com> Co-authored-by: gaobinlong <gbinlong@amazon.com> Co-authored-by: rishavz_sagar <rishavsagar4b1@gmail.com> Co-authored-by: Chenyang Ji <cyji@amazon.com> Co-authored-by: Craig Perkins <cwperx@amazon.com> Co-authored-by: Sandesh Kumar <sandeshkr419@gmail.com> Co-authored-by: Andriy Redko <andriy.redko@aiven.io> Co-authored-by: Zelin Hao <zelinhao@amazon.com> Co-authored-by: Lukáš Vlček <lukas.vlcek@aiven.io> Co-authored-by: kkewwei <kkewwei@163.com> Co-authored-by: SwethaGuptha <156877431+SwethaGuptha@users.noreply.github.com> Co-authored-by: Sooraj Sinha <81695996+soosinha@users.noreply.github.com> Co-authored-by: Ahmed Sobeh <ahmed.sobeh@aiven.io> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Co-authored-by: opensearch-trigger-bot[bot] <98922864+opensearch-trigger-bot[bot]@users.noreply.github.com> Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com> Co-authored-by: Siddhant Deshmukh <deshsid@amazon.com> Co-authored-by: Peter Alfonsi <peter.alfonsi@gmail.com> Co-authored-by: Peter Alfonsi <petealft@amazon.com> Co-authored-by: Vatsal <36672090+imvtsl@users.noreply.github.com> Co-authored-by: Robin Friedmann <robinfriedmann.rf@gmail.com> Co-authored-by: Sachin Kale <sachinpkale@gmail.com> Co-authored-by: Bharathwaj G <bharath78910@gmail.com> Co-authored-by: Shourya Dutta Biswas <114977491+shourya035@users.noreply.github.com> Co-authored-by: Craig Perkins <craig5008@gmail.com> Co-authored-by: Andriy Redko <drreta@gmail.com> Co-authored-by: Mohit Godwani <81609427+mgodwan@users.noreply.github.com> Co-authored-by: Sarthak Aggarwal <sarthagg@amazon.com> Co-authored-by: Sagar <99425694+sgup432@users.noreply.github.com> Co-authored-by: Sandeep Kumawat <2025sandeepkumawat@gmail.com> Co-authored-by: Sandeep Kumawat <skumwt@amazon.com> Co-authored-by: Daniil Roman <danroman17397@gmail.com> Co-authored-by: Rishabh Singh <rishabhksingh@gmail.com> Co-authored-by: kkewwei <kewei.11@bytedance.com> Co-authored-by: Daniel (dB.) Doubrovkine <dblock@amazon.com> Co-authored-by: Ashish Singh <ssashish@amazon.com> Co-authored-by: bowenlan-amzn <bowenlan23@gmail.com> Co-authored-by: Pranshu Shukla <55992439+Pranshu-S@users.noreply.github.com> Co-authored-by: rajiv-kv <157019998+rajiv-kv@users.noreply.github.com> Co-authored-by: Sumit Bansal <sumit.asr@gmail.com> Co-authored-by: Daniel Widdis <widdis@gmail.com> Co-authored-by: shailendra0811 <167273922+shailendra0811@users.noreply.github.com> Co-authored-by: Park  Yeongwu <pyw5420@gmail.com> Co-authored-by: ebraminio <ebraminio@gmail.com> Co-authored-by: Liyun Xiu <xiliyun@amazon.com> Co-authored-by: Finn <carrofin@amazon.com> Co-authored-by: Neetika Singhal <neetiks@amazon.com> Co-authored-by: Jay Deng <jayd0104@gmail.com> Co-authored-by: Gaurav Bafna <85113518+gbbafna@users.noreply.github.com> Co-authored-by: Andrew Ross <andrross@amazon.com>
opensearch-project,OpenSearch,d33d24e9ff48bd20c12636349ec8c0eb67b38eb2,https://github.com/opensearch-project/OpenSearch/commit/d33d24e9ff48bd20c12636349ec8c0eb67b38eb2,Add changes to propagate queryGroupId across child requests and nodes (#14614)  * add query group header propagator  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * apply spotless check  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add new propagator in ThreadContext  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * spotlessApply  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * address comments  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.microsoft.azure:msal4j from 1.15.1 to 1.16.0 in /plugins/repository-azure (#14610)  * Bump com.microsoft.azure:msal4j in /plugins/repository-azure  Bumps [com.microsoft.azure:msal4j](https://github.com/AzureAD/microsoft-authentication-library-for-java) from 1.15.1 to 1.16.0. - [Release notes](https://github.com/AzureAD/microsoft-authentication-library-for-java/releases) - [Changelog](https://github.com/AzureAD/microsoft-authentication-library-for-java/blob/dev/changelog.txt) - [Commits](https://github.com/AzureAD/microsoft-authentication-library-for-java/compare/v1.15.1...v1.16.0)  --- updated-dependencies: - dependency-name: com.microsoft.azure:msal4j dependency-type: direct:production update-type: version-update:semver-minor ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * [Bugfix] Fix ICacheKeySerializerTests flakiness (#14564)  * Fix testInvalidInput flakiness  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  * Addressed andrross's comment  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  * rerun security check  Signed-off-by: Peter Alfonsi <petealft@amazon.com>  ---------  Signed-off-by: Peter Alfonsi <petealft@amazon.com> Co-authored-by: Peter Alfonsi <petealft@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Correct typo in method name (#14621)  Signed-off-by: vatsal <vatsal.v.anand@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Refactoring FilterPath.parse by using an iterative approach instead of recursion. (#14200)  * Refactor FilterPath parse function (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Implement unit tests for FilterPathTests (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Write warn log if Filter is empty; Add comments (#12067) Signed-off-by: Robin Friedmann <robinfriedmann.rf@gmail.com>  * Add changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove unnecessary log statement  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove unused logger  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Spotless apply  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  * Remove incorrect changelog  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com>  ---------  Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Co-authored-by: Robin Friedmann <robinfriedmann.rf@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Removing String format in RemoteStoreMigrationAllocationDecider to optimise performance(#14612)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Clear templates before Adding; Use NamedWriteableAwareStreamInput for RemoteCustomMetadata; Correct the check for deciding upload of HashesOfConsistentSettings (#14513)  * Clear templates before Adding; Use NamedWriteableAwareStreamInput for RemoteCustomMetadata * Correct the check for deciding upload of hashes of consistent settings  Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add changelog  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * add PR link changelog  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Improve reroute performance by optimising List.removeAll in LocalShardsBalancer to filter remote search shard from relocation decision (#14613)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Fix assertion failure while deleting remote backed index (#14601)  Signed-off-by: Sachin Kale <kalsac@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Allow system index warning in OpenSearchRestTestCase.refreshAllIndices (#14635)  * Allow system index warning  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Add to CHANGELOG  Signed-off-by: Craig Perkins <cwperx@amazon.com>  * Address code review comments  Signed-off-by: Craig Perkins <cwperx@amazon.com>  ---------  Signed-off-by: Craig Perkins <cwperx@amazon.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Star tree codec changes (#14514)  --------- Signed-off-by: Bharathwaj G <bharath78910@gmail.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump com.github.spullara.mustache.java:compiler from 0.9.13 to 0.9.14 in /modules/lang-mustache (#14672)  * Bump com.github.spullara.mustache.java:compiler  Bumps [com.github.spullara.mustache.java:compiler](https://github.com/spullara/mustache.java) from 0.9.13 to 0.9.14. - [Commits](https://github.com/spullara/mustache.java/compare/mustache.java-0.9.13...mustache.java-0.9.14)  --- updated-dependencies: - dependency-name: com.github.spullara.mustache.java:compiler dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * Bump net.minidev:accessors-smart from 2.5.0 to 2.5.1 in /plugins/repository-azure (#14673)  * Bump net.minidev:accessors-smart in /plugins/repository-azure  Bumps [net.minidev:accessors-smart](https://github.com/netplex/json-smart-v2) from 2.5.0 to 2.5.1. - [Release notes](https://github.com/netplex/json-smart-v2/releases) - [Commits](https://github.com/netplex/json-smart-v2/compare/2.5.0...2.5.1)  --- updated-dependencies: - dependency-name: net.minidev:accessors-smart dependency-type: direct:production update-type: version-update:semver-patch ...  Signed-off-by: dependabot[bot] <support@github.com>  * Updating SHAs  Signed-off-by: dependabot[bot] <support@github.com>  * Update changelog  Signed-off-by: dependabot[bot] <support@github.com>  ---------  Signed-off-by: dependabot[bot] <support@github.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  * move query group thread context propagator out of ThreadContext  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com>  ---------  Signed-off-by: Kaushal Kumar <ravi.kaushal97@gmail.com> Signed-off-by: dependabot[bot] <support@github.com> Signed-off-by: Peter Alfonsi <petealft@amazon.com> Signed-off-by: vatsal <vatsal.v.anand@gmail.com> Signed-off-by: Siddhant Deshmukh <deshsid@amazon.com> Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com> Signed-off-by: Sooraj Sinha <soosinha@amazon.com> Signed-off-by: Sachin Kale <kalsac@amazon.com> Signed-off-by: Craig Perkins <cwperx@amazon.com> Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> Co-authored-by: dependabot[bot] <dependabot[bot]@users.noreply.github.com> Co-authored-by: Peter Alfonsi <peter.alfonsi@gmail.com> Co-authored-by: Peter Alfonsi <petealft@amazon.com> Co-authored-by: Vatsal <36672090+imvtsl@users.noreply.github.com> Co-authored-by: Siddhant Deshmukh <deshsid@amazon.com> Co-authored-by: Robin Friedmann <robinfriedmann.rf@gmail.com> Co-authored-by: rishavz_sagar <rishavsagar4b1@gmail.com> Co-authored-by: Sooraj Sinha <81695996+soosinha@users.noreply.github.com> Co-authored-by: Sachin Kale <sachinpkale@gmail.com> Co-authored-by: Craig Perkins <cwperx@amazon.com> Co-authored-by: Bharathwaj G <bharath78910@gmail.com>
opensearch-project,OpenSearch,58d1164f74e921a26b7a73b6185b38cc87bbc7a9,https://github.com/opensearch-project/OpenSearch/commit/58d1164f74e921a26b7a73b6185b38cc87bbc7a9,Improve reroute performance by optimising List.removeAll in LocalShardsBalancer to filter remote search shard from relocation decision (#14613)  Signed-off-by: RS146BIJAY <rishavsagar4b1@gmail.com>
opensearch-project,OpenSearch,57fb50b22bf30148a632bd4c5e78bde53116f00f,https://github.com/opensearch-project/OpenSearch/commit/57fb50b22bf30148a632bd4c5e78bde53116f00f,Apply the date histogram rewrite optimization to range aggregation (#13865)  * Refactor the ranges representation  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Refactor try fast filter  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Main work finished; left the handling of different numeric data types  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * buildRanges accepts field type  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * first working draft probably  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * add change log  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * accommodate geo distance agg  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Fix test  support all numeric types minus one on the upper range  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * [Refactor] range is lower inclusive  right exclusive  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * adding test  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Adding test and refactor  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * refactor  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * add test  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * add test and update the compare logic in tree traversal  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * fix test  add random test  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * refactor to address comments  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * small potential performance update  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * fix precommit  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * refactor  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * refactor  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * set refresh_interval to -1  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * address comment  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * address comment  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * address comment  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Fix test  To understand fully about the double and bigdecimal usage in scaled float field will take more time.  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  ---------  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>
opensearch-project,OpenSearch,2f8cb079c2c220304b8e6965853a15732c3bf57c,https://github.com/opensearch-project/OpenSearch/commit/2f8cb079c2c220304b8e6965853a15732c3bf57c,Support Dynamic Pruning in Cardinality Aggregation (#13821)  * Cardinality aggregation dynamic pruning changes  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Reading  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * remaining disjunction scorer full understand  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * utilize competitive iterator api to perform pruning  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * handle missing input  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * add change log  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * clean up  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Clean up  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Test fix  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Do all the scoring within Cardinality  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * clean unnecessary  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * fix  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Add dynamic flag for this feature  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Add random test  small bug fix  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * address comment  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * Address comments  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  * address comments  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com>  ---------  Signed-off-by: bowenlan-amzn <bowenlan23@gmail.com> Co-authored-by: Rishabh Maurya <rishabhmaurya05@gmail.com>
apple,pkl,17f431370a9560776d7e23c5ebe3459d4273eb54,https://github.com/apple/pkl/commit/17f431370a9560776d7e23c5ebe3459d4273eb54,Fix exception handling of PklRootNode's (#837)  Motivation: - Perform same exception handling for every implementation of PklRootNode.execute(). - Avoid code duplication.  Changes: - Change PklRootNode.execute() to be a final method that performs exception handling and calls abstract method executeImpl()  which is implemented by subclasses. - Remove executeBody() methods  which served a similar purpose but were more limited. - Remove duplicate exception handling code.  Result: - More reliable exception handling. This should fix known problems such as misclassifying stack overflows as internal errors and displaying errors without a stack trace. - Less code duplication.
apple,pkl,1bc473ba545167af3a48f7f24048b3b1c42d139a,https://github.com/apple/pkl/commit/1bc473ba545167af3a48f7f24048b3b1c42d139a,Improve lazy type checking of listings and mappings (#789)  Motivation: - simplify implementation of lazy type checking - fix correctness issues of lazy type checking (#785)  Changes: - implement listing/mapping type cast via amendment (`parent`) instead of delegation (`delegate`) - handle type checking of *computed* elements/entries in the same way as type checking of computed properties - ElementOrEntryNode is the equivalent of TypeCheckedPropertyNode - remove fields VmListingOrMapping.delegate/typeNodeFrame/cachedMembers/checkedMembers - fix #785 by executing all type casts between a member's owner and receiver - fix #823 by storing owner and receiver directly instead of storing the mutable frame containing them (typeNodeFrame) - remove overrides of VmObject methods that are no longer required - good for Truffle partial evaluation and JVM inlining - revert a85a173faa785c2e8b14221d6c213c8837925d72 except for added tests - move `VmUtils.setOwner` and `VmUtils.setReceiver` and make them private - these methods aren't generally safe to use  Result: - simpler code with greater optimization potential - VmListingOrMapping can now have both a type node and new members - fewer changes to surrounding code - smaller memory footprint - better performance in some cases - fixes https://github.com/apple/pkl/issues/785 - fixes https://github.com/apple/pkl/issues/823  Potential future optimizations: - avoid lazy type checking overhead for untyped listings/mappings - improve efficiency of forcing a typed listing/mapping - currently  lazy type checking will traverse the parent chain once per member  reducing the performance benefit of shallow-forcing a listing/mapping over evaluating each member individually - avoid creating an intermediate untyped listing/mapping in the following cases: - `new Listing<X> {...}` - amendment of `property: Listing<X>`
apple,pkl,b93cb9b32237760e357d9229b87686b81390a519,https://github.com/apple/pkl/commit/b93cb9b32237760e357d9229b87686b81390a519,Exclude non file-based modules from synthesized *GatherImports task (#821)  This fixes an issue where certain modules tasks fail due to the plugin attempting to analyze their imports  but the arguments may not actually be Pkl modules.  For example  the pkldoc task accepts entire packages in its "sourceMoules" property.  This changes the gather imports logic to only analyze file-based modules. This is also a performance improvement; non file-based modules are unlikely to import files due to insufficient trust levels.  Also: fix a bug when generating pkldoc on Windows
apple,pkl,7868d9d9c81b5e87ef6615f89ce90775b2bc371e,https://github.com/apple/pkl/commit/7868d9d9c81b5e87ef6615f89ce90775b2bc371e,Typecheck Mapping/Listing members lazily (#628)  This changes how the language performs typechecks for mappings and listings.  Currently  Pkl will shallow-force any Mapping and Listing to check it the type parameter (e.g. Listing<Person> means each element is checked to be an instance of Person).  This changes the language to check each member's type when the member is accessed.  This also adjust test runner to handle thrown errors from within tests.  With the change to make mapping/listing typechecks lazy  we can now correctly handle thrown errors from within a single test case.  This adjusts the test runner to consider any thrown errors as a failure for that specific test case.
datahub-project,datahub,48ef215b21b0ff2d409aa792983f21600fbae53d,https://github.com/datahub-project/datahub/commit/48ef215b21b0ff2d409aa792983f21600fbae53d,perf(search): reduce highlight fragments (#11349)
datahub-project,datahub,36ae5afbb573f53b8078e060f24e142f847f5229,https://github.com/datahub-project/datahub/commit/36ae5afbb573f53b8078e060f24e142f847f5229,feat(neo4j): improve neo4j read query performance by specifying labels (#10593)
Activiti,Activiti,cbfdba0e28097475c08da9f7c961a02357500a08,https://github.com/Activiti/Activiti/commit/cbfdba0e28097475c08da9f7c961a02357500a08,MNT-24335: improve historical tasks performance (#4702) (#4716)  * MNT-24335: improve historical tasks performance (#4702)  (cherry picked from commit 84b6e253bc3631e45769c2750edbf939949def42)  * MNT-24335: removed support for mysql5 * MNT-24335: added missing Activiti 7.11.1 version on db schema
LawnchairLauncher,lawnchair,d401ba32fdb18a2338ee8aa95770142f83768c2c,https://github.com/LawnchairLauncher/lawnchair/commit/d401ba32fdb18a2338ee8aa95770142f83768c2c,Fix widget-related crash in LauncherWidgetHolder  - Adding null checks before calling methods on mWidgetHost - Ensuring the listening flag is always updated  even if an exception occurs  This should resolve the "java.lang.NullPointerException: Attempt to read from field 'com.android.server.appwidget.AppWidgetServiceImpl$ProviderId'" error and improve overall stability when dealing with widgets  error: Uncaught exception  java.lang.RuntimeException: Unable to stop activity {app.lawnchair.nightly/app.lawnchair.LawnchairLauncher}: java.lang.NullPointerException: Attempt to read from field 'com.android.server.appwidget.AppWidgetServiceImpl$ProviderId com.android.server.appwidget.AppWidgetServiceImpl$Provider.id' on a null object reference in method 'android.util.SparseArray com.android.server.appwidget.AppWidgetServiceImpl$Host.getWidgetUids()' at android.app.ActivityThread.callActivityOnStop(ActivityThread.java:5484) at android.app.ActivityThread.performStopActivityInner(ActivityThread.java:5456) at android.app.ActivityThread.handleStopActivity(ActivityThread.java:5525) at android.app.servertransaction.StopActivityItem.execute(StopActivityItem.java:43) at android.app.servertransaction.ActivityTransactionItem.execute(ActivityTransactionItem.java:45) at android.app.servertransaction.TransactionExecutor.executeLifecycleState(TransactionExecutor.java:176) at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:97) at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2423) at android.os.Handler.dispatchMessage(Handler.java:106) at android.os.Looper.loopOnce(Looper.java:233) at android.os.Looper.loop(Looper.java:334) at android.app.ActivityThread.main(ActivityThread.java:8348) at java.lang.reflect.Method.invoke(Native Method) at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:582) at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1065) Caused by: java.lang.NullPointerException: Attempt to read from field 'com.android.server.appwidget.AppWidgetServiceImpl$ProviderId com.android.server.appwidget.AppWidgetServiceImpl$Provider.id' on a null object reference in method 'android.util.SparseArray com.android.server.appwidget.AppWidgetServiceImpl$Host.getWidgetUids()' at android.os.Parcel.createExceptionOrNull(Parcel.java:2446) at android.os.Parcel.createException(Parcel.java:2424) at android.os.Parcel.readException(Parcel.java:2407) at android.os.Parcel.readException(Parcel.java:2349) at com.android.internal.appwidget.IAppWidgetService$Stub$Proxy.stopListening(IAppWidgetService.java:792) at android.appwidget.AppWidgetHost.stopListening(AppWidgetHost.java:274) at com.android.launcher3.widget.LauncherWidgetHolder.stopListening(Unknown Source:2) at com.android.launcher3.widget.LauncherWidgetHolder.setShouldListenFlag(Unknown Source:40) at com.android.launcher3.widget.LauncherWidgetHolder.setActivityStarted(Unknown Source:1) at com.android.launcher3.Launcher.onStop(Unknown Source:25) at android.app.Instrumentation.callActivityOnStop(Instrumentation.java:1496) at android.app.Activity.performStop(Activity.java:8599) at android.app.ActivityThread.callActivityOnStop(ActivityThread.java:5476) ... 14 more Caused by: android.os.RemoteException: Remote stack trace: at com.android.server.appwidget.AppWidgetServiceImpl$Host.getWidgetUids(AppWidgetServiceImpl.java:4444) at com.android.server.appwidget.AppWidgetServiceImpl.stopListening(AppWidgetServiceImpl.java:866) at com.android.internal.appwidget.IAppWidgetService$Stub.onTransact(IAppWidgetService.java:311) at android.os.Binder.execTransactInternal(Binder.java:1179) at android.os.Binder.execTransact(Binder.java:1143)
LawnchairLauncher,lawnchair,5fbc9a5d5def34d7b966b8dc6bf196d25b6928c3,https://github.com/LawnchairLauncher/lawnchair/commit/5fbc9a5d5def34d7b966b8dc6bf196d25b6928c3,Perform state switch to Overview from -1 screen  - This fixed a regression caused by ag/27904180  where -1 is no longer considered `visibleLauncher`. -1 still have Launcher being top activity  and recents animation will always fail - Added isInMinusOne in LauncherActivityInterface  and return Launcher in getVisibleLauncher for -1 case - -1 should be the only case where Launcher is top activity  but is not started  Fix: 352254279 Test: Recents button from the following scenarios: Home  -1  apps  transparent apps  video from -1 Flag: EXEMPT bugfix (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:1cd3fe27f229da4f7bb80a2e04d1416cc050fc94) Merged-In: I0898d202b9dd4742c0c7d2c0d6b340c748e8acf7 Change-Id: I0898d202b9dd4742c0c7d2c0d6b340c748e8acf7
LawnchairLauncher,lawnchair,7a183e40516abf58288b26ba79325695a4ee5ded,https://github.com/LawnchairLauncher/lawnchair/commit/7a183e40516abf58288b26ba79325695a4ee5ded,Adjust previews in a row to align with each other and remove whitespace.  - Introduces a WidgetTableRow that identifies preview height for all of its children while keeping them aligned with minimum whitespace - The fullSheet's move / change animations can conflict with the resize so  we wait for them before performing resize of the row.  Bug: 335715046 Test: Manual Flag: NONE BUGFIX Change-Id: Id843430c7adfc228c219ba54d504baddba792df0
StarRocks,starrocks,0c48197c56d31c7e1da5842261fee6f63b6471c6,https://github.com/StarRocks/starrocks/commit/0c48197c56d31c7e1da5842261fee6f63b6471c6,[Enhancement] Change proc_profile collect time to 2min (#59005)  ## Why I'm doing:  Some cluster memory usage spikes suddenly  and it may take only 1–2 minutes from the start of the memory spike to the process getting stuck. If the profile collection time is too long  it can cause the collected process to also get stuck  resulting in no output.  ## What I'm doing: Change the default collect time to 2min. Remove `proc_profile_collect_interval_s` param. In actual scenarios  profile collection must not have intervals. If users feel that collecting profiles affects performance  they can simply disable this function.  Signed-off-by: gengjun-git <gengjun@starrocks.com>
StarRocks,starrocks,4be26d2c0bd4379bc72e23634abc6c62f58b9bbb,https://github.com/StarRocks/starrocks/commit/4be26d2c0bd4379bc72e23634abc6c62f58b9bbb,[Enhancement] Add groovy unix socket debug server (#56788)  ## Why I'm doing:  Easy to debug FE in env that only has host shell access and sql client is not availble.  ## What I'm doing:  Add a unix-socket based groovy shell server  so FE jvm can be connectted using unix domain socket from FE host and user can perform some debug operations.  Some predefined binding var for easy of use: ``` binding.setVariable("LOG"  LOG);  // log to FE log binding.setVariable("out"  io.out); // printwriter to output to console  do not use System.out which will output to FE process's stdout binding.setVariable("globalState"  GlobalStateMgr.getCurrentState()); // GlobalStateMgr env root binding.setVariable("metastore"  GlobalStateMgr.getCurrentState().getLocalMetastore()); ```  ``` (py3) decster@decster-MS-7C94:~/projects/starrocks/localrun$ nc -U fe/groovy_debug.sock  or using rlwrap for better interaction  (py3) decster@decster-MS-7C94:~/projects/starrocks/localrun$ rlwrap nc -U fe/groovy_debug.sock Groovy Shell (4.0.9  JVM: 17.0.6) Type ':help' or ':h' for help. ------------------------------------------------------------------------------------------------------------------------------------------- groovy:000> :h :h  For information about Groovy  visit: http://groovy-lang.org  Available commands: :help      (:h ) Display this help message ?          (:? ) Alias to: :help :exit      (:x ) Exit the shell :quit      (:q ) Alias to: :exit import     (:i ) Import a class into the namespace :display   (:d ) Display the current buffer :clear     (:c ) Clear the buffer and reset the prompt counter :show      (:S ) Show variables  classes or imports :inspect   (:n ) Inspect a variable or the last result with the GUI object browser :purge     (:p ) Purge variables  classes  imports or preferences :edit      (:e ) Edit the current buffer :load      (:l ) Load a file or URL into the buffer .          (:. ) Alias to: :load :save      (:s ) Save the current buffer to a file :record    (:r ) Record the current session to a file :history   (:H ) Display  manage and recall edit-line history :alias     (:a ) Create an alias :set       (:= ) Set (or list) preferences :grab      (:g ) Add a dependency to the shell environment :register  (:rc) Register a new command with the shell :doc       (:D ) Open a browser window displaying the doc for the argument  For help on a specific command type: :help command  groovy:000> 1+1 1+1 ===> 2 groovy:000> def getAllFieldsAsString(obj) { def clazz = obj.getClass() def result = new StringBuilder("Class: ${clazz.name}\n") clazz.declaredFields.each { field -> field.setAccessible(true) result.append("${field.name} = ${field.get(obj)}\n") } return result.toString() } def getAllFieldsAsString(obj) { groovy:001> def clazz = obj.getClass() groovy:002> def result = new StringBuilder("Class: ${clazz.name}\n") groovy:003> clazz.declaredFields.each { field -> groovy:004> field.setAccessible(true) groovy:005> result.append("${field.name} = ${field.get(obj)}\n") groovy:006> } groovy:007> return result.toString() groovy:008> } ===> true groovy:000> out.println(getAllFieldsAsString(globalState)) out.println(getAllFieldsAsString(globalState)) Class: com.starrocks.server.GlobalStateMgr LOG = com.starrocks.server.GlobalStateMgr:INFO in 5cb0d902 NEXT_ID_INIT_VALUE = 10000 REPLAY_INTERVAL_MS = 1 IMAGE_DIR = /image REPLAYER_MAX_MS_PER_LOOP = 1000 REPLAYER_MAX_LOGS_PER_LOOP = 100000 imageDir = /home/decster/projects/starrocks/localrun/fe/meta/image epoch = 0 lock = com.starrocks.common.util.concurrent.QueryableReentrantLock@1920ea15[Unlocked] nodeMgr = com.starrocks.server.NodeMgr@2effe44c ... sqlPlanStorage = com.starrocks.sql.spm.SQLPlanGlobalStorage@7a938001 jwkMgr = com.starrocks.authentication.JwkMgr@6a01e407  ===> null groovy:000> :exit :exit   ```  Signed-off-by: Binglin Chang <decstery@gmail.com>
StarRocks,starrocks,c1bc8d31d114dba7232a683d4cb3581ace2132d7,https://github.com/StarRocks/starrocks/commit/c1bc8d31d114dba7232a683d4cb3581ace2132d7,[Enhancement] adjust translate function syntax structure to prevent parser performance rollback (#54830)  Signed-off-by: stephen <stephen5217@163.com>
StarRocks,starrocks,d928cac11183d39783f2b27fda9975f3a6c0f9ac,https://github.com/StarRocks/starrocks/commit/d928cac11183d39783f2b27fda9975f3a6c0f9ac,[Enhancement] Optimize partition retention condition compensation rewrite performance in force_mv mode (#54072)  Signed-off-by: shuming.li <ming.moriarty@gmail.com>
StarRocks,starrocks,c70eadf8ee80322562ddbaf500b98767ee1a2d05,https://github.com/StarRocks/starrocks/commit/c70eadf8ee80322562ddbaf500b98767ee1a2d05,[Enhancement] improve performance of show materialized views statement (#54374)  Signed-off-by: Murphy <mofei@starrocks.com>
StarRocks,starrocks,fb89e478fbe1ed60fd883cc056029b92dd56fd73,https://github.com/StarRocks/starrocks/commit/fb89e478fbe1ed60fd883cc056029b92dd56fd73,[Enhancement] Improve command SHOW PROC "/statistics" performance and some code refinements (#51718)  ## Why I'm doing: If there are many tablet in a single db  like hundreds of thousand  there would be millions replica. Then  command SHOW PROC '/statistic' may be very slow to finish  in the meanwhile  it will occupy the db lock during execution  which may slow down other query executions.  The performance bootle neck is that it calls method SystemInfoService.getBackendIds too many times (one call for each replica). If there are 200 backends  each call may take hundred milliseconds: ![image](https://github.com/user-attachments/assets/11c56743-8a13-4077-bb7e-fb10a374e76f)   To improve command `SHOW PROC "/statistics" `performance  and other general operation which may call `SystemInfoService.getBackendIds`. The main performance bottle neck of method `SystemInfoService.getBackendIds` is that it calls `this.getBackend(...)`  which is unnecessary. <img width="576" alt="image" src="https://github.com/user-attachments/assets/1851dff7-ac28-44ff-899b-78254e47e5e1">  I made a micro benchmark for this improvement  if there are 200 backends  and 1000000 replicas  we can gain 5X performance improvement. ![image](https://github.com/user-attachments/assets/b0468bc5-f5b7-41f1-a92d-e9a60dc40146) ![image](https://github.com/user-attachments/assets/ef207e86-e06a-4637-be44-cac9707fc1c9) ![image](https://github.com/user-attachments/assets/52b0382b-bd58-4881-a447-bb32c532acad)  Fixes #51715  Signed-off-by: ganggewang <ganggewang@tencent.com>
StarRocks,starrocks,e3c6b4ead3b2d1c9824e66b8cdf0d3aacec10ea2,https://github.com/StarRocks/starrocks/commit/e3c6b4ead3b2d1c9824e66b8cdf0d3aacec10ea2,[Enhancement] Optimize iceberg mor performance of iceberg equality delete (#51050)  Signed-off-by: stephen <stephen5217@163.com>
StarRocks,starrocks,45d72ace192a6dda42ca5eb2fad1d492dd55d197,https://github.com/StarRocks/starrocks/commit/45d72ace192a6dda42ca5eb2fad1d492dd55d197,[Enhancement][FlatJson] Improve flat json performace and extract strategy (#50696)  Signed-off-by: Seaven <seaven_7@qq.com>
StarRocks,starrocks,f0cb5e97c8fd7176d01d5e3b9b9677cf82f174b7,https://github.com/StarRocks/starrocks/commit/f0cb5e97c8fd7176d01d5e3b9b9677cf82f174b7,[Enhancement] Optimize memory tracker (#49841)  ## Why I'm doing: Currently  memory statistics consume a lot of performance because they are all calculated each time.  ## What I'm doing: Get some samples to estimate the manager's memory  even though it is inaccurate  but it's helpful to solve memory leak bugs.  Signed-off-by: gengjun-git <gengjun@starrocks.com>
StarRocks,starrocks,556928ff43157296907dc4229ec207b9eb3a6d09,https://github.com/StarRocks/starrocks/commit/556928ff43157296907dc4229ec207b9eb3a6d09,[Enhancement] Optimize view based mv rewrite performance (#50256)  Signed-off-by: shuming.li <ming.moriarty@gmail.com>
StarRocks,starrocks,89a6b7741abdfefe28d4347b4b3f987a5126c434,https://github.com/StarRocks/starrocks/commit/89a6b7741abdfefe28d4347b4b3f987a5126c434,[Enhancement] Support configurable hll_sketch and optimize hll_sketch performance (#48939)  Signed-off-by: shuming.li <ming.moriarty@gmail.com>
StarRocks,starrocks,888e20136b5d2ea776038542a2880aba05fed1f5,https://github.com/StarRocks/starrocks/commit/888e20136b5d2ea776038542a2880aba05fed1f5,[Enhancement] Improve cache select performance (#48262)  Signed-off-by: Smith Cruise <chendingchao1@126.com>
StarRocks,starrocks,70c621c0795edf99c2fe24a8b2e29eba3c14985d,https://github.com/StarRocks/starrocks/commit/70c621c0795edf99c2fe24a8b2e29eba3c14985d,[Enhancement] Optimize performance of get physical partition from olap table (#47198)  Signed-off-by: meegoo <meegoo.sr@gmail.com>
StarRocks,starrocks,a8e64b69a8874481b59062898f1230f28466af61,https://github.com/StarRocks/starrocks/commit/a8e64b69a8874481b59062898f1230f28466af61,[Enhancement] Optimize text based mv rewrite performance (#49330)  Signed-off-by: shuming.li <ming.moriarty@gmail.com>
StarRocks,starrocks,4d1ea245db8142970a113679b004ac02349e6533,https://github.com/StarRocks/starrocks/commit/4d1ea245db8142970a113679b004ac02349e6533,[BugFix] Fix incorrect connection state update when register/unregister conn (#48056)  Why I'm doing: Connection state update is incorrect.  What I'm doing: Currently we need to update a few maps and counters when register/unregister connection from FE  we don't have some sync mechanism to protect this update  which may cause incorrect connection state and prompt "connection reach limit" unexpectly to user login. Add lock protection when register/unregister connection  the performance should be ok  we have moved some heavy operations like cleaning tmp table outside of lock  and the rest are just some simple map get/put operations.
StarRocks,starrocks,8e88f8b8c82167988cfb6b770ba1c27b20b9276d,https://github.com/StarRocks/starrocks/commit/8e88f8b8c82167988cfb6b770ba1c27b20b9276d,[Enhancement] optimize the performance of refreshExternalTable stage of MV refresh (#47809)  Signed-off-by: Murphy <mofei@starrocks.com>
StarRocks,starrocks,a79e8eb38b66496a3865269fc5148920b8a5a88e,https://github.com/StarRocks/starrocks/commit/a79e8eb38b66496a3865269fc5148920b8a5a88e,[Enhancement] optimize performance and memeory use in FE (#47012)  Signed-off-by: Seaven <seaven_7@qq.com>
StarRocks,starrocks,2d074050257936f0da73a536e026a999771fdb31,https://github.com/StarRocks/starrocks/commit/2d074050257936f0da73a536e026a999771fdb31,[Enhancement] Performance optimization to Reduce SqlToScalarOperatorTranslator function calls in QueryTransformer the about window function. (#46527)  Signed-off-by: edwinhzhang <edwinhzhang@tencent.com>
StarRocks,starrocks,d4fbb1268243d5cdda7c40c75518e97322e96960,https://github.com/StarRocks/starrocks/commit/d4fbb1268243d5cdda7c40c75518e97322e96960,[Enhancement] optimize performance in cast large string to array (#46957)  Signed-off-by: packy92 <wangchao@starrocks.com>
StarRocks,starrocks,ecc1c148156b8abbbfe9614f8fa2eced34f6308d,https://github.com/StarRocks/starrocks/commit/ecc1c148156b8abbbfe9614f8fa2eced34f6308d,[Enhancement] Optimize performance of get physical partition from temp partition (#47148)  Signed-off-by: meegoo <meegoo.sr@gmail.com>
StarRocks,starrocks,455be1732b1afb7daf6092acb909f3d293f1fdd4,https://github.com/StarRocks/starrocks/commit/455be1732b1afb7daf6092acb909f3d293f1fdd4,[Enhancement] use hashset instead of list to fix the performance bottleneck (#46845)  Signed-off-by: Murphy <mofei@starrocks.com>
signalapp,Signal-Server,93ba6616d1624a96866b964f32e901ad0c92d50f,https://github.com/signalapp/Signal-Server/commit/93ba6616d1624a96866b964f32e901ad0c92d50f,Perform device list validations in the scope of a pessimistic account lock
signalapp,Signal-Server,a4b98f38a67ef39dbdb4d95338f87918bf5e35d3,https://github.com/signalapp/Signal-Server/commit/a4b98f38a67ef39dbdb4d95338f87918bf5e35d3,Use a `Callable` for tasks performed within the scope of a pessimistic lock
signalapp,Signal-Server,886984861f52ba7e722e95f94595815a798506eb,https://github.com/signalapp/Signal-Server/commit/886984861f52ba7e722e95f94595815a798506eb,remove performance based turn routing from CallRoutingControllerV2
SonarSource,sonarqube,9042dc991247cb82fe011994c8e0414658164e83,https://github.com/SonarSource/sonarqube/commit/9042dc991247cb82fe011994c8e0414658164e83,SCA-125 Perf: releases endpoint (#13178)
SonarSource,sonarqube,04b0453797b06c53fc9092c2961e5f24c10ebc28,https://github.com/SonarSource/sonarqube/commit/04b0453797b06c53fc9092c2961e5f24c10ebc28,SONAR-24317 Move ProcessWrapperFactory to sonar-core
SonarSource,sonarqube,24bd8856af66bf2e1148c42279739225b254e84e,https://github.com/SonarSource/sonarqube/commit/24bd8856af66bf2e1148c42279739225b254e84e,NO-JIRA improved performance of tests in sonar-scanner-engine module
spinnaker,spinnaker,93f27cca937ff4b4411ce046ee662dba615a47d1,https://github.com/spinnaker/spinnaker/commit/93f27cca937ff4b4411ce046ee662dba615a47d1,perf(pipeline): Improve execution times for dense pipeline graphs (#4824)  * test(pipeline): Define StartStageHandler performance  when given a complex pipeline with multiple layers of upstream stages  * fix(pipeline): Memoize anyUpstreamStagesFailed results  This turns the anyUpstreamStagesFailed calculation from one that scales exponentially based on the number of (branches+downstream stages) in a pipeline to one that scales linearly based on the total number of stages in a pipeline. This is a significant performance improvement  especially for very large and complicated pipelines  * refactor(stage): Move anyUpstreamStagesFailed to StartStageHandler  since that's the only place where it gets used  * fix(stage): Avoid using a ConcurrentHashMap  for memoization. The recursive anyUpstreamStagesFailed(StageExecution) function runs in a single thread  so ConcurrentHashMap is not necessary here  * docs(test): Use a more concise test name  * perf(stage): First check if a stage has been visited  before checking for parent stages. stage.getRequisiteStageRefIds is a more expensive call because the underlying implementation creates a copy of a List. Therefore  start with the cheaper operation first hoping to short-circuit and avoid the more expensive check  * perf(stage): Filter out non-synthetic stages  The javadocs state that the syntheticStageOwner property is null for non-synthetic stages. Use this information to filter out non-synthetic stages before attempting a potentially expensive operation to check for synthetic parents of previousStages  * perf(stage): Precompute requisiteStageRefIds  StageExecutionImpl.getRequisiteStageRefIds() returns a copy of a Set. This is a costly operation that has the potential to get repeated for every unvisited stage. To avoid this  compute the value before entering a loop  * perf(stage): Only use withAuth when needed  withAuth is only necessary when starting a stage. Since withAuth is very computationally expensive for complex pipelines  only call it when it is absolutely necessary.  * perf(stage): Remove duplicate call to withStage  StartStageHandler already makes a call to message.withStage at the beginning of the handle() method. Therefore  this call within the catch block is unnecessary  * chore(import): Clean up unused imports  ---------  Co-authored-by: Daniel Zheng <d.zheng@salesforce.com>
spinnaker,spinnaker,ea57f38127eb7ddef74c86c1f1e39039c8ad8687,https://github.com/spinnaker/spinnaker/commit/ea57f38127eb7ddef74c86c1f1e39039c8ad8687,perf(web): Query for individual pipelines (#1836)  * refactor(web): adjust error message in PipelineController.invokePipelineConfig  and use pipeline name instead of id in InvokePipelineConfigTest  to reduce diffs when ApplicationService.getPipelineConfigForApplication changes the queries it makes to front50  * perf(web): change ApplicationService.getPipelineConfigForApplication to query front50 for individual pipelines  instead of querying for all pipelines in an application and then filtering locally.  * refactor(web): teach ApplicationService.getPipelineConfigForApplication to handle errors with SpinnakerRetrofitErrorHandler  to centralize exception handling there / remove the need for callers to check for null  ---------  Co-authored-by: David Byron <dbyron@salesforce.com>
spinnaker,spinnaker,3cdf32e46b6bafc2647ff9f79eb74b23fb6c5af2,https://github.com/spinnaker/spinnaker/commit/3cdf32e46b6bafc2647ff9f79eb74b23fb6c5af2,perf(ecs): Narrowing the cache search for the ECS provider on views (#6256)  * perf(ecs): Narrowing the cache search for the ECS provider on views  * perf(ecs): ECS alarms to be cached/searched with EcsClusterName id
spinnaker,spinnaker,11707d495925eee4665e1e8faf09c0c61ca73a04,https://github.com/spinnaker/spinnaker/commit/11707d495925eee4665e1e8faf09c0c61ca73a04,perf(cache): Optimise heap usage in KubernetesCachingAgent (#6255)  This pull request addresses a performance issue discovered in the Clouddriver  specifically within the `KubernetesServiceHandler.addAllReplicaSetLabels` method.  With allocation profiling I found that 40% of allocations happen for converting `replicaSet` object to `KubernetesManifest` class. The thing is that this conversion is redundant because `replicaSet` is already `KubernetesManifest`.  The objects aren't changed later  so it should be safe to stop parsing and making a copy. After this optimization we reduced memory usage by half and also got rid of regular GC pauses.
spinnaker,spinnaker,f12a0439f2da58ab80fc037451b5011e516d513d,https://github.com/spinnaker/spinnaker/commit/f12a0439f2da58ab80fc037451b5011e516d513d,chore(build): enable cross compilation plugin for Java 17 (#6226)  * chore(build): enable cross compilation plugin for Java 17  * chore(build): upgrade ErrorProne to support JDK 17  * chore(build): run spotless  * chore(test): remove usage of deprecated AssertJ methods  * chore(cloudfoundry): remove org.junit.platform.gradle.plugin  since it's deprecated (see https://github.com/junit-team/junit5/issues/1317).  Once it's gone  the regular gradle test task properly reports failures.  * chore(cloudfoundry/test): replace deprecated isEqualToComparingFieldByFieldRecursively  with usingRecursiveComparison().isEqualTo  * fix(cloudfoundry/test): use ignoringCollectionOrder to fix test failures  like:  $ ./gradlew -PenableCrossCompilerPlugin=true :clouddriver-cloudfoundry:test --tests CloudFoundryLoadBalancerCachingAgentTest  > Task :clouddriver-cloudfoundry:test  CloudFoundryLoadBalancerCachingAgentTest > handleShouldReturnOnDemandResultsWithCacheTimeAndNoProcessedTime() FAILED java.lang.AssertionError: Expecting actual: OnDemandAgent.OnDemandResult(sourceAgentType=account/CloudFoundryLoadBalancerCachingAgent-OnDemand  authoritativeTypes=[]  cacheResult=com.netflix.spinnaker.cats.agent.DefaultCacheResult@8aeab9e  evictions={}) to be equal to: OnDemandAgent.OnDemandResult(sourceAgentType=account/CloudFoundryLoadBalancerCachingAgent-OnDemand  authoritativeTypes=[]  cacheResult=com.netflix.spinnaker.cats.agent.DefaultCacheResult@2a2dc0a  evictions={}) when recursively comparing field by field  but found the following difference:  field/property 'cacheResult.cacheResults.loadBalancers' differ: - actual value  : [com.netflix.spinnaker.clouddriver.cloudfoundry.cache.ResourceCacheData@bd31962c] - expected value: [com.netflix.spinnaker.clouddriver.cloudfoundry.cache.ResourceCacheData@bd3195f1] The following actual elements were not matched in the expected SingletonSet: [com.netflix.spinnaker.clouddriver.cloudfoundry.cache.ResourceCacheData@bd31962c]  The recursive comparison was performed with this configuration: - no overridden equals methods were used in the comparison (except for java types) - these types were compared with the following comparators: - java.lang.Double -> DoubleComparator[precision=1.0E-15] - java.lang.Float -> FloatComparator[precision=1.0E-6] - java.nio.file.Path -> lexicographic comparator (Path natural order) - actual and expected objects and their fields were compared field by field recursively even if they were not of the same type  this allows for example to compare a Person to a PersonDto (call strictTypeChecking(true) to change that behavior). at com.netflix.spinnaker.clouddriver.cloudfoundry.provider.agent.CloudFoundryLoadBalancerCachingAgentTest.handleShouldReturnOnDemandResultsWithCacheTimeAndNoProcessedTime(CloudFoundryLoadBalancerCachingAgentTest.java:191)  It's not clear why ignoring the order when comparing what looks like a singleton set is important  but I'd like to unblock the java 17 progress.  ---------  Co-authored-by: David Byron <dbyron@salesforce.com> Co-authored-by: mergify[bot] <37929162+mergify[bot]@users.noreply.github.com>
apache,cassandra,f327b63db09a907206749a3c88aba38a4554e548,https://github.com/apache/cassandra/commit/f327b63db09a907206749a3c88aba38a4554e548,Introduce SSTableSimpleScanner for compaction  This removes the usage of index files during compaction and simplifies and improves the performance of compaction.  patch by Branimir Lambov; reviewed by Sylvain Lebresne for CASSANDRA-20092
apache,cassandra,513509ee2cd46da4604a73c7873a9bb634ddad0b,https://github.com/apache/cassandra/commit/513509ee2cd46da4604a73c7873a9bb634ddad0b,Accord's ConfigService lock is held over large areas which cause deadlocks and performance issues  patch by David Capwell; reviewed by Benedict Elliott Smith for CASSANDRA-20065
apache,cassandra,91def312841123088562f5c3e59bb1b003772be3,https://github.com/apache/cassandra/commit/91def312841123088562f5c3e59bb1b003772be3,do not schedule additional durability attempts while some in flight; plus minor performance improvements
apache,cassandra,1ed52038ce350b37c563b9176fab995dcbef32b0,https://github.com/apache/cassandra/commit/1ed52038ce350b37c563b9176fab995dcbef32b0,This commits contains the following two patches in order to reduce the amount of conflicts resolution necessary for future rebasing:  (Accord): C* stores table in Range which will cause ranges to be removed from Accord when DROP TABLE is performed patch by David Capwell  Sam Tunnicliffe; reviewed by Sam Tunnicliffe for CASSANDRA-18675  CEP-15: (Accord) sequence EpochReady.coordinating to allow syncComplete to be learned from newer epochs patch by David Capwell; reviewed by Alex Petrov  Blake Eggleston for CASSANDRA-19769
apache,cassandra,fbfb633bda1211d8b64f46d4c03ce26e7526ce39,https://github.com/apache/cassandra/commit/fbfb633bda1211d8b64f46d4c03ce26e7526ce39,CEP-15 (C*) - misc accord perf improvements  Patch by Blake Eggleston; Reviewed by David Capwell for CASSANDRA-19940  Changes: Increase accord repair range splitting Streamline table metadata fetching - removes some unnecessary abstraction from the table metadata lookup path Remote unnecessary set building when building lists of overlapping keys Add separate recover delay for repair and increase default recover delay
apache,cassandra,7dadc080cb689b3b82c8472403785b2d7241fdc9,https://github.com/apache/cassandra/commit/7dadc080cb689b3b82c8472403785b2d7241fdc9,perf improvements
apache,cassandra,8404d2fd5cbda4ba5210522ee612ac2fd169278e,https://github.com/apache/cassandra/commit/8404d2fd5cbda4ba5210522ee612ac2fd169278e,Improve performance when getting writePlacementAllSettled from ClusterMetadata in large cluster with many range movements  Patch by marcuse; reviewed by Sam Tunnicliffe for CASSANDRA-20526
apache,cassandra,f1bec5d0c5dccce4128b3ff9bc087cdf0577f2b0,https://github.com/apache/cassandra/commit/f1bec5d0c5dccce4128b3ff9bc087cdf0577f2b0,Improve performance of DistributedSchema.validate for large schemas  patch by Abe Ratnofsky; reviewed by Caleb Rackliffe  Benedict Elliott Smith  Matt Byrd  Sam Tunnicliffe for CASSANDRA-20360
apache,cassandra,cc86fc8865e2794a83078db5629ba5096498045e,https://github.com/apache/cassandra/commit/cc86fc8865e2794a83078db5629ba5096498045e,Merge branch 'cassandra-5.0' into trunk  * cassandra-5.0: Add selected SAI index state and query performance metrics to nodetool tablestats
apache,cassandra,79630fb42ae9e42691f516590100c279f372ac89,https://github.com/apache/cassandra/commit/79630fb42ae9e42691f516590100c279f372ac89,Add selected SAI index state and query performance metrics to nodetool tablestats  patch by Sunil Ramchandra Pawar; reviewed by Caleb Rackliffe  Matt Byrd  and Maxim Muzafarov for CASSANDRA-20026
apache,cassandra,3078aea1cfc70092a185bab8ac5dc8a35627330f,https://github.com/apache/cassandra/commit/3078aea1cfc70092a185bab8ac5dc8a35627330f,Introduce SSTableSimpleScanner for compaction  This removes the usage of index files during compaction and simplifies and improves the performance of compaction.  patch by Branimir Lambov; reviewed by Sylvain Lebresne for CASSANDRA-20092
spring-projects,spring-security,ab629cc1cad533501c5c6ad3db5ae1128ef7dfc4,https://github.com/spring-projects/spring-security/commit/ab629cc1cad533501c5c6ad3db5ae1128ef7dfc4,Add AuthorizationGrantType.toString()  This adds AuthorizationGrantType.toString() which makes debuging easier. In particular  it will help when performing unit tests which validate the AuthorizationGrantType.  Issue gh-16382
spring-projects,spring-security,612b15abcc1a3a87c45db0b05fc4a33ad19fc458,https://github.com/spring-projects/spring-security/commit/612b15abcc1a3a87c45db0b05fc4a33ad19fc458,JdbcOneTimeTokenService.setCleanupCron  Spring Security uses setter methods for optional member variables. Allows for a null cleanupCron to disable the cleanup.  In a clustered environment it is likely that users do not want all nodes to be performing a cleanup because it will cause contention on the ott table.  Another example is if a user wants to invoke cleanUpExpiredTokens with a different strategy all together  they might want to disable the cron job.  Issue gh-15735
spring-projects,spring-security,8917cdb404979ec23694ffe081ac891aa2e82bbd,https://github.com/spring-projects/spring-security/commit/8917cdb404979ec23694ffe081ac891aa2e82bbd,Improve Performance of IPv4 Check  Closes gh-15324
flyway,flyway,2dbbab3e138519b8c658d80add3bd93427561403,https://github.com/flyway/flyway/commit/2dbbab3e138519b8c658d80add3bd93427561403,Bump version to flyway-10.21.0  Please see the GH release for the release notes  Update H2 2.3.224 to 2.3.232  Improve repair performance  Upgrade snowflake-jdbc 3.14.3 to 3.20.0 to fix CVE-2024-43382
java-native-access,jna,c9e389567554df573e4466b44d78e937812325c8,https://github.com/java-native-access/jna/commit/c9e389567554df573e4466b44d78e937812325c8,Merge pull request #1626 from brettwooldridge/master  Improve ``Structure`` (subclass) construction performance.
checkstyle,checkstyle,88101b21d575b66935df5e1f6599a49531f45287,https://github.com/checkstyle/checkstyle/commit/88101b21d575b66935df5e1f6599a49531f45287,Issue #6207: Xpath Regresssion test for Superfinalize
checkstyle,checkstyle,7b8037aa8320c2f67e61f136725a76b7eb92c6f8,https://github.com/checkstyle/checkstyle/commit/7b8037aa8320c2f67e61f136725a76b7eb92c6f8,Issue #13345: Enable examples tests for SuperFinalizeCheck
apache,shenyu,5bd78ff6bc303925a4e419153ca48b645ec92c4a,https://github.com/apache/shenyu/commit/5bd78ff6bc303925a4e419153ca48b645ec92c4a,perf(logging): Optimize the performance of log collection (#5931)  - Adjust the serialization timing of the request body and response body to avoid unnecessary string operations. - Perform string conversion only after confirming the need to record logs to reduce resource consumption.  Co-authored-by: 宗杰 <1491040549@qq.com>
apache,shenyu,8f3a1d07e2e29d68c7db26a44d4085bf085598cf,https://github.com/apache/shenyu/commit/8f3a1d07e2e29d68c7db26a44d4085bf085598cf,[type:optimize] Optimize BodyParamUtils with Caffeine cache (#5905)  * [type:optimize] Optimize BodyParamUtils with Caffeine cache  - Added Caffeine cache for base type checking - Improved isBaseType() performance with 5000-entry cache - Reduced class loading overhead  * [type:fix] Fix for style check.  ---------  Co-authored-by: aias00 <liuhongyu@apache.org>
apache,seatunnel,dc3c23981b3a78a3f1af69bcde57fbd5bf57d7b4,https://github.com/apache/seatunnel/commit/dc3c23981b3a78a3f1af69bcde57fbd5bf57d7b4,[Improve][Jdbc] Skip all index when auto create table to improve performance of write (#7288)
flowable,flowable-engine,fe4502c50f77249c8421502f862624d4b79f1958,https://github.com/flowable/flowable-engine/commit/fe4502c50f77249c8421502f862624d4b79f1958,Do not perform Cmmn Validation on already deployed models  This aligns the CmmnDeployer with the BpmnDeployer
flowable,flowable-engine,70a529076d0adc7af555c5e383d3f66a9cd7a57f,https://github.com/flowable/flowable-engine/commit/70a529076d0adc7af555c5e383d3f66a9cd7a57f,Add support for SQL Server NVarchar (#3909)  * Work in progress: to support differentiating between varchar/nvarchar  (which is needed to improve sql server performance)  all mappings need to get the correct type. For other databases  regular varchar is fine  hence why the typehandler is always the same for those databases  * Nvarchar mappings for TimerJob  * event subscription and cmmn engine updates  * Nvarchar mappings for Execution  * Nvarchar mappings for Batch  * Nvarchar mappings for ExternalWorkerJob  * Nvarchar mappings for ByteArray  * cmmn engine updates  * Nvarchar mappings for Job  * cmmn engine updates  * Nvarchar mappings for Property and Comment  * cmmn engine updates  * Nvarchar mappings for HistoricDetail and HistoricTaskInstance  * cmmn engine updates  * cmmn engine updates  * cmmn engine updates  * Nvarchar mappings for HistoricProcessInstance and Resource  * Nvarchar mappings for Model and ProcessDefinitionInfo  * Simplify nvarchar test  * Nvarchar mappings for Model and HistoricEntityLink  * cmmn engine updates  * Nvarchar mappings for VariableInstance  * cmmn engine updates  * Nvarchar mappings for HistoricActivityInstance  * Nvarchar mappings for DMN  * App engine nvarchar changes  * Comment out varchar to nvarchar changes  * IDM nvarchar changes  * event registry updates  * fixes  ---------  Co-authored-by: Joram Barrez <jbarrez@users.noreply.github.com>
apache,beam,16f7bb613a69d27ca7e0185b8a0efd07ead8a944,https://github.com/apache/beam/commit/16f7bb613a69d27ca7e0185b8a0efd07ead8a944,Change UnboundedSourceAsSdfWrapperFn to share the cache across instances. (#33901)  add a utility class to enable sharing across all deserialized instances of a DoFn and use it in UnboundedSourceAsSdfWrapperFn to cache Readers across dofn instances
apache,beam,83b373570fef4405482ebee4207b33d4b3212700,https://github.com/apache/beam/commit/83b373570fef4405482ebee4207b33d4b3212700,[flink-runner] Improve Datastream for batch performances (#32440)  * [Flink] Set return type of bounded sources * [Flink] Use a lazy split enumerator for bounded sources * [Flink] fix lazy enumerator package * [Flink] Default to maxParallelism = parallelism in batch * [Flink] Avoid re-serializing trigger on every element * [Flink] Avoid re-evaluating options every time a new state is stored * [Flink] Only serialize states namespace keys if necessary * [Flink] Make ToKeyedWorkItem part of the DoFnOperator * [Flink] Remove ToBinaryKV * [Flink] Refactor CombinePerKeyTranslator * [Flink] Combine before Reduce (no side-input only) * [Flink] Implement partial reduce * [Flink] dead code cleanup * [Flink] persistent PartialReduceBundleOperator operator state * [Flink] Combine before GBK * [Flink] Combine before reduce (with side input) * [Flink] Force slot sharing group in batch mode * [Flink] Disable bundling in batch mode * [Flink] Lower default max bundle size in batch mode * [Flink] Code cleanup * [Flink] fix WindowDoFnOperatorTest * [Flink] Remove 1.14 compat code * [Flink] Fix flaky test * [Flink] Use a custom key type to better distribute load * [Flink] fix GBK streaming with side input * [Flink] fix error management in lazy source * [Flink] disable operator chaining in validatesRunner * [Flink] fix lazy source enumerator behaviour on error * [Flink] set validates runner parallelism to 1 * [Flink] add org.apache.beam.sdk.transforms.ParDoTest to sickbay * [Flink] fix lazy source enumerator behaviour on error (again) * [Flink] Add org.apache.beam.sdk.transforms.ViewTest.testTriggeredLatestSingleton to sickbay
apache,beam,5e031abd2322ec973c73bdc6eb643eb6b3261362,https://github.com/apache/beam/commit/5e031abd2322ec973c73bdc6eb643eb6b3261362,Merge pull request #33385 Introduce a BoundedTrie metric.  Introduce a BoundedTrie metric which is used to efficiently store and aggregate a collection of string sequences (FQNs) with a limited size.  It is recommended to review this PR by commits.  BoundedTrie is a space-saving way to store many string sequences (like FQN/file paths). It acts like a tree with branches  holding sequences within a size limit. It can efficiently add  combine  and search and perform trimming of children when the size increases beyond defined max.  Let's say we want to store these sequences  with a size limit of 3:  "folder1/file1.txt" "folder1/file2.txt" "folder2/file3.txt" Here's how the BoundedTrie might look:  root - folder1 - file1.txt - file2.txt - folder2 - file3.txt If we try to add "folder1/file4.txt"  the trie might trim to "folder1"  dropping all children to stay within the size limit.  This will be used to replace the StringSet metric for lineage tracking for very large lineage graphs to overcome the size limits.
apache,beam,b47646643a3e0da2e17ec80698beea8f8ed47c90,https://github.com/apache/beam/commit/b47646643a3e0da2e17ec80698beea8f8ed47c90,Address comments  perform deep copies and support synchronization for mutable BoundedTrieData
apache,beam,2fc7646c9b828a8e7d4bd05f41f0dd8e70d93c02,https://github.com/apache/beam/commit/2fc7646c9b828a8e7d4bd05f41f0dd8e70d93c02,Merge pull request #33626 Expose Coder.getEncodedElementByteSize publicly  Make Coder.getEncodedElementByteSize publicly available to allow performance improvements in higher level coders.  Actually making this method public would be a backwards incompatible change.
apache,beam,bf7e317a7aedd104099c4c1d029bb9716016f617,https://github.com/apache/beam/commit/bf7e317a7aedd104099c4c1d029bb9716016f617,[Managed BigQuery] use file loads with Avro format for better performance (#33392)  * use avro file format  * add comment  * add unit test
apache,beam,f3e6c66c0a5d3a8638fd94978adf503be5081274,https://github.com/apache/beam/commit/f3e6c66c0a5d3a8638fd94978adf503be5081274,Improve performance of BigQueryIO connector when withPropagateSuccessfulStorageApiWrites(true) is used (#31840)  * Performance improvements related to conversion of BigQuery's Storage Write API proto's to TableRows when withPropagateSuccessfulStorageApiWrites(true) is used.  * Fix spotless findings.  * Update CHANGES.md  * Update CHANGES.md - moved the entry to 2.59.0 section.
pentaho,pentaho-kettle,db5f39cc7d9f6223ef20e6da5102d5910e91af27,https://github.com/pentaho/pentaho-kettle/commit/db5f39cc7d9f6223ef20e6da5102d5910e91af27,[PDI-20322] -Disable gather performance metrics when scheduling jobs/ktrs from Spoon's schedule perspective
pentaho,pentaho-kettle,7bed3d1ca615680cb85a2956debe55aad5f47726,https://github.com/pentaho/pentaho-kettle/commit/7bed3d1ca615680cb85a2956debe55aad5f47726,[PPP-5411] [PPP-5415] Update Pentaho Data Integration community release to display business source license on the splash page and perform limited smoke test - Remove lingering OSS License references - Keep one LICENSE.TXT file - Copy license file into each artifact: core  client  static  with and without osgi
apache,tomcat,82367b3891b9ffd501ee7046b71ee2461303562c,https://github.com/apache/tomcat/commit/82367b3891b9ffd501ee7046b71ee2461303562c,Remove the case sensitivity check  The performance impact is minimal and getting the check right in all cases is difficult due to various edge cases
apache,tomcat,e451872281c1d1e6d7f2b61d2f6f1382573e2166,https://github.com/apache/tomcat/commit/e451872281c1d1e6d7f2b61d2f6f1382573e2166,Refactor CORS validation order with explicit guard clauses  - Check for null originHeader first to prevent NPEs. - Validate empty or invalid originHeader before performing same-origin checks.
apache,tomcat,5144218a399027a59199cb5cf3aaafe9033f7ffb,https://github.com/apache/tomcat/commit/5144218a399027a59199cb5cf3aaafe9033f7ffb,Follow-up to BZ 69381. Additional location for performance improvement
apache,tomcat,de680af8843f1f61311d43f54ddee772bf81315b,https://github.com/apache/tomcat/commit/de680af8843f1f61311d43f54ddee772bf81315b,BZ 69419 - further performance improvements
apache,tomcat,3b30c3d6ef600b0820417c698236c91d5a82a5c8,https://github.com/apache/tomcat/commit/3b30c3d6ef600b0820417c698236c91d5a82a5c8,BZ 69419: getAttribute() performance for nested ApplicationHttpRequest  Avoid repeated getSpecial() calls with nested ApplicationHttpRequest https://bz.apache.org/bugzilla/show_bug.cgi?id=69419 Based on a patch by John Engebretson
apache,tomcat,5b30d52b5140a9472a19e8c214f98e27b30ad0f5,https://github.com/apache/tomcat/commit/5b30d52b5140a9472a19e8c214f98e27b30ad0f5,Fix BZ 69381 Improve method lookup performance  When the method has no arguments there is no requirement to consider casting or coercion. Shortcut the method lookup process in that case.  Based on PR #770 by John Engebretson.
apache,tomcat,033d30c717926216dffb2fd6a475781acd4c7101,https://github.com/apache/tomcat/commit/033d30c717926216dffb2fd6a475781acd4c7101,Switch lock tokens to UUID as recommended by RFC 4918  Remove secret init parameter. Remove some int constants  replaced with an enum for the type of PROPFIND performed. Process LOCK request body only if present  to avoid relying on an IOE (if a body is present and bad  return 400 instead).
apache,tomcat,9ab668b2e9b21d5259acca3221a4554a6d41f158,https://github.com/apache/tomcat/commit/9ab668b2e9b21d5259acca3221a4554a6d41f158,Partial fix for BZ 69338. Better performance for >2 operand And/Or  https://bz.apache.org/bugzilla/show_bug.cgi?id=69338
apache,tomcat,3a1659ebe6a9afc7aa771ee594680a4b57620812,https://github.com/apache/tomcat/commit/3a1659ebe6a9afc7aa771ee594680a4b57620812,Rename method that performs commit to commit()  This refactoring is in preparation for RFC 8293 (Early Hints) support. That will introduce a method that writes headers without a commit. This rename will reduce potential confusion between writeHeaders() and sendHeaders().
cabaletta,baritone,1e2ae34dbe452a1eebc12019ae632b2279210cad,https://github.com/cabaletta/baritone/commit/1e2ae34dbe452a1eebc12019ae632b2279210cad,crucial performance optimization
cabaletta,baritone,99f9dd1671b9b2adb216dcbbdf789c781ebe18ee,https://github.com/cabaletta/baritone/commit/99f9dd1671b9b2adb216dcbbdf789c781ebe18ee,Performance
cabaletta,baritone,b25a6305ce23d68ffb2c948418668599789bcee7,https://github.com/cabaletta/baritone/commit/b25a6305ce23d68ffb2c948418668599789bcee7,Don't bother testing reachability for far away blocks  This is a massive performance improvement for big farms.
aeron-io,aeron,c9477c49cf52c0c9630f8167a495c4645ec88cc0,https://github.com/aeron-io/aeron/commit/c9477c49cf52c0c9630f8167a495c4645ec88cc0,Publication revoke (#1781)  * [Java] initial revoke POC  * [Java] network/ipc revoke basics are in  * [Java] revoke works against spies  * [Java] tighten up system tests  cleanup code a bit  * [Java] add agent logging for revoke events  add log buffer metadata isPublicationRevoked flag  * [Java] split out pub/sub side agent events  add system counters  * [Java] remove extra IpcPublication transitionToLinger  * [Java] refactor processPendingLoss()  * [Java] fix IpcPublication  update system test to support C media driver  * [C] initial C driver support for pub revoke  * [Java] concurrent publications revoked by a separate publication are not automatically closed  * [C] add client APIs for revoke()  * [Java] add revokeTestExclusive  * [C CppW] add revoke methods to C++ wrapper objects  add system tests for revoke  * [Java] checkstyle  * [C] remove case statement fallthrough  * [C] add agent logging for revoke events  * [C] add fall through comment  * [C] cleanup a few TODOs  * [Java] update onStatusMessage to mirror logic in the C driver  * [Java] remove commented out code  * [Java] major simplification - no more client to driver revoke  * [Java] remove REVOKED state for publication image  * [C] port of java refactor into C  * [Java] remove superfluous set on metadata buffer  * [Java] remove some comments  * [C] remove comments  * [Java] push setting of revoke flag down into the media driver  * [C] push setting of revoke flag down into the media driver  * [Java] put constants on the left  * [C] use offsetof  * [Java] add pub revoke system test that sends lots of messages  * [Java] cleanup a few things based on comments  * [C] cleanup a few things based on comments  * [Java] add multicast and mdc revoke test  * [Java] update javadoc
aeron-io,aeron,5d0fb96dd79bf767c8c4b865303280556b9eb4f4,https://github.com/aeron-io/aeron/commit/5d0fb96dd79bf767c8c4b865303280556b9eb4f4,[Java] Do not check consensus module state when performing `is-leader` check.
software-mansion,react-native-svg,7b5d4daaed9fd62d1b6a07ffb3742f56c49eda20,https://github.com/software-mansion/react-native-svg/commit/7b5d4daaed9fd62d1b6a07ffb3742f56c49eda20,fix: scaling when mask is set (#2299)  # Summary  This PR resolves an issue raised in #1451. Currently  when a mask is used  we render the element as a bitmap (or platform equivalent)  but the bitmap's size does not update accordingly with transformations. With these changes  the problem is addressed as follows: * **Android**: We utilize the original canvas layers to render the mask and element with the appropriate blending mode. * **iOS**: We create an offscreen context with the size multiplied by the screen scale and apply the original UIGraphics CTM (current transformation matrix) to the offscreen context. This ensures that the same transformations are applied as on the original context.  Additionally  there is a significant performance improvement on Android as we are not creating three new Bitmaps and three new Canvases.  ## Test Plan  There are many ways for testing these changes  but the required ones are: * `TestsExample` app -> `Test1451.tsx` * `Example` app -> Mask section * `FabricExample` app -> Mask section  ## Compatibility  | OS      | Implemented | | ------- | :---------: | | Android |    ✅     | | iOS     |    ✅     |  ## Preview  <img width="337" alt="image" src="https://github.com/software-mansion/react-native-svg/assets/39670088/93dbae85-edbd-452a-84b0-9a50107b1361"> <img width="337" alt="image" src="https://github.com/software-mansion/react-native-svg/assets/39670088/07838dff-cb2d-4072-a2fc-5c16a76f6c33">
Graylog2,graylog2-server,7845ab99117250085170ffede89de0e8ae84de7f,https://github.com/Graylog2/graylog2-server/commit/7845ab99117250085170ffede89de0e8ae84de7f,Improve performance for working with indices (#21195)  * Issue #18563: Don't iterate over all IndexSets for a given indexName  instead use `getForIndex(String)` directly. Also converted some loops to streaming.  * Add changelog  * Review comments  * Review comments part two
Graylog2,graylog2-server,91857703a15a7037af0a473e6365613a8f6df903,https://github.com/Graylog2/graylog2-server/commit/91857703a15a7037af0a473e6365613a8f6df903,Supporting backend API types generation for plugins. (#14202)  * Pass cloud flag to generator.  * Using name instead of path for API filenames.  * Allow toggling prefixing name of APIs for plugins.  * Adding new inline type for additional properties.  * Running frontend build in right phase.  * Making sure that build is performed after `compile` is completed.  * Escaping more characters  banning more function names.  * Try to parse default value as JSON.  * Moving task to build apidefs to parent POM.  * Reverting phase change of `yarn build`.  * Reverting phase change of `yarn build`.  * Reverting phase change of `yarn build`.  * Copying web UI resources at `process-classes` stage.
Graylog2,graylog2-server,70888ea4dffeeef42b4667fd78be365300d0cad1,https://github.com/Graylog2/graylog2-server/commit/70888ea4dffeeef42b4667fd78be365300d0cad1,Refresh migrated indices in data node migration (#20157)  * add index refresh and range recalculation after index reindexing finished  * add index range cleanup after migration  * perform maintenance before and after reindexing
langchain4j,langchain4j,770c77038ad00b3512e6997f043ea4c31003fe6b,https://github.com/langchain4j/langchain4j/commit/770c77038ad00b3512e6997f043ea4c31003fe6b,Rewrite the PgVectorEmbeddingStore search query to enable index (#2485)  Closes #2484  ## Change Now the query looks like this and it corresponds the pgvector documentation so it uses the index (not just seq scan) which will improve the performance for large tables.  ``` SELECT (embedding <=> '[...]') AS score  embedding_id  embedding  text FROM document_embeddings WHERE (embedding <=> '[...]') <= 0.5 ORDER BY embedding <=> '[...]' LIMIT 10; ```  And here is the execution plan: ``` Limit  (cost=411.07..412.29 rows=10 width=47) (actual time=5.074..5.183 rows=10 loops=1) ->  Index Scan using document_embeddings_ivfflat_index on document_embeddings  (cost=411.07..2014.16 rows=13180 width=47) (actual time=5.073..5.180 rows=10 loops=1) "        Order By: (embedding <=> '[...]'::vector)" "        Filter: ((embedding <=> '[...]'::vector) <= '0.5'::double precision)" Planning Time: 0.095 ms Execution Time: 5.225 ms ```  Note: I haven't added new tests since I didn't add or remove any functionality. This PR is just a search performance fix.  ## General checklist  - [x] There are no breaking changes - [ ] I have added unit and/or integration tests for my change - [ ] The tests cover both positive and negative cases - [x] I have manually run all the unit and integration tests in the module I have added/changed  and they are all green - [x] I have manually run all the unit and integration tests in the [core](https://github.com/langchain4j/langchain4j/tree/main/langchain4j-core) and [main](https://github.com/langchain4j/langchain4j/tree/main/langchain4j) modules  and they are all green
langchain4j,langchain4j,f3312919fdc8514a633293b16cfe7788c9cf4e8d,https://github.com/langchain4j/langchain4j/commit/f3312919fdc8514a633293b16cfe7788c9cf4e8d,Optimization of UUID Generation Using HexFormat (#2024)  ## Issue  This pull request introduces the HexFormat class (Java 17+) to optimize the hexadecimal encoding of SHA-256 hashes used for UUID generation in the generateUUIDFrom method. By replacing the manual byte-to-hex conversion with HexFormat  this update aims to improve code readability and potentially enhance performance.   I created JMH test to ensure that HexFormat is more efficient than format of String.  ```java package dev.langchain4j;     import org.openjdk.jmh.annotations.*; import org.openjdk.jmh.annotations.Benchmark; import org.openjdk.jmh.runner.Runner; import org.openjdk.jmh.runner.RunnerException; import org.openjdk.jmh.runner.options.CommandLineOptionException; import org.openjdk.jmh.runner.options.CommandLineOptions; import org.openjdk.jmh.runner.options.Options; import org.openjdk.jmh.runner.options.OptionsBuilder;  import java.security.MessageDigest; import java.security.NoSuchAlgorithmException; import java.util.HexFormat; import java.util.UUID; import java.util.concurrent.TimeUnit;  import static java.nio.charset.StandardCharsets.UTF_8;  @State(Scope.Benchmark) @BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.NANOSECONDS) @Fork(1) @Warmup(iterations = 5) @Measurement(iterations = 2) public class BenchmarkHex {  private static String largeString;  @Setup(Level.Trial) public void setup() { largeString = generateLargeString(100000); }  @Benchmark public void benchmarkOldBehavior(){ boolean optimized = false; generateUUIDFrom(largeString  optimized); }   @Benchmark public void benchmarkOptimizedBehavior(){ boolean optimized = true; generateUUIDFrom(largeString  optimized); }  private String generateUUIDFrom(String input  boolean optimized) { byte[] hashBytes = getSha256Instance().digest(input.getBytes(UTF_8)); if (optimized) { return UUID.nameUUIDFromBytes(HexFormat.of().formatHex(hashBytes).getBytes(UTF_8)).toString(); } StringBuilder sb = new StringBuilder(); for (byte b : hashBytes) { sb.append("%02x".formatted(b)); } return UUID.nameUUIDFromBytes(sb.toString().getBytes(UTF_8)).toString(); }  private MessageDigest getSha256Instance() { try { return MessageDigest.getInstance("SHA-256"); } catch (NoSuchAlgorithmException e) { throw new IllegalArgumentException(e); } }  private String generateLargeString(int length) { StringBuilder sb = new StringBuilder(); for (int i = 0; i < length; i++) { sb.append(UUID.randomUUID()); } return sb.toString(); }  public static void main(String[] args) throws RunnerException  CommandLineOptionException { Options opt = new OptionsBuilder() .parent(new CommandLineOptions(args)) .timeUnit(TimeUnit.NANOSECONDS) .include(BenchmarkHex.class.getSimpleName()) .build();  new Runner(opt).run(); }    }  ```  Results: ```java Benchmark                                Mode  Cnt  Score   Error  Units BenchmarkHex.benchmarkOldBehavior        avgt       1.811          ms/op BenchmarkHex.benchmarkOptimizedBehavior  avgt       1.625          ms/op ```   It's not a really big enhancement  but it's can be here for improving the code    ## Change <!-- Please describe the changes you made. -->   ## General checklist - [x] There are no breaking changes - [ ] I have added unit and integration tests for my change - [x] I have manually run all the unit and integration tests in the module I have added/changed  and they are all green - [x] I have manually run all the unit and integration tests in the [core](https://github.com/langchain4j/langchain4j/tree/main/langchain4j-core) and [main](https://github.com/langchain4j/langchain4j/tree/main/langchain4j) modules  and they are all green <!-- Before adding documentation and example(s) (below)  please wait until the PR is reviewed and approved. --> - [ ] I have added/updated the [documentation](https://github.com/langchain4j/langchain4j/tree/main/docs/docs) - [ ] I have added an example in the [examples repo](https://github.com/langchain4j/langchain4j-examples) (only for "big" features) - [ ] I have added/updated [Spring Boot starter(s)](https://github.com/langchain4j/langchain4j-spring) (if applicable)
langchain4j,langchain4j,650a0a6293f9caebb9b1fe2449f9872deeb89400,https://github.com/langchain4j/langchain4j/commit/650a0a6293f9caebb9b1fe2449f9872deeb89400,#725: PR 2 / 4: Addresses `EnumOutputParser` not working properly in some r… (#1392)  ## Issue  #725  PR 2 / 4: Addresses `EnumOutputParser` not working properly in some rare cases due to chat model returnig enum name in square brackets.  ## Change  Made ˙EnumOutputParser˙ imune to square brackets (effectivelyl removing them if presnt before enum match is perfomed). Also introduced dedicated unit test (`EnumOutputParserTest`).  ## General checklist  - [X] There are no breaking changes - [X] I have added unit and integration tests for my change - [X] I have manually run all the unit and integration tests in the module I have added/changed  and they are all green - [X] I have manually run all the unit and integration tests in the [core](https://github.com/langchain4j/langchain4j/tree/main/langchain4j-core) and [main](https://github.com/langchain4j/langchain4j/tree/main/langchain4j) modules  and they are all green
langchain4j,langchain4j,2b2c09f05d5123edbd3b80b659eef88e0a6989a7,https://github.com/langchain4j/langchain4j/commit/2b2c09f05d5123edbd3b80b659eef88e0a6989a7,Implement remove methods for InMemoryEmbeddingStore (#1220)  ## Issue [https://github.com/langchain4j/langchain4j/issues/301](https://github.com/langchain4j/langchain4j/issues/301)  ## Change I've implemented remove methods for InMemoryEmbeddingStore.  Current problems: 1. I'm not completely sure how to test all of this. And  as I've seen  you don't fully test it too) 2. There is a very nasty problem with `removeAll(Filter filter)` . Let me try to break it in small pieces: 1. `EmbeddingStore`'s are parametrized with `Embedded` type parameter. 2. `removeAll(Filter filter)` method in `EmbeddingStore` accepts `Filter`. 3. `Filter` accepts `Metadata` object. 4. `Metadata` object is stored in `TextSegment`. 5. `TextSegment` is usually what is `Embedded`. But it's not guaranteed.  How my pull request deals with this problem: 1) for every entry in `InMemoryEmbeddingStore` check the type of `Embedded`. 2) If it's a `TextSegment`  then proceed. 3) If it's not  then raise `UnsupportedOperationException`. The issue with this strategy is that we perform the check for every entry. I haven't found a way in Java to check the type of type parameter (because it's erasured in runtime). (*In C++  for example  there are `constexpr if`  `<type_traits>`  `std::is_same<U  V>`  explicit specialization  etc.*)  We could solve this problem if `InMemoryEmbeddingStore` would support only `TextSegment` as `Embedded`  but that's a breaking change.  I could also not implement this method  but  I really really need it in my project (if we chose to stick with `InMemoryEmbeddingStore`).   ## General checklist <!-- Please double-check the following points and mark them like this: [X] --> - [x] There are no breaking changes - [ ] I have added unit and integration tests for my change - [ ] I have manually run all the unit and integration tests in the module I have added/changed  and they are all green - [ ] I have manually run all the unit and integration tests in the [core](https://github.com/langchain4j/langchain4j/tree/main/langchain4j-core) and [main](https://github.com/langchain4j/langchain4j/tree/main/langchain4j) modules  and they are all green <!-- Before adding documentation and example(s) (below)  please wait until the PR is reviewed and approved. --> - [ ] I have added/updated the [documentation](https://github.com/langchain4j/langchain4j/tree/main/docs/docs) - [ ] I have added an example in the [examples repo](https://github.com/langchain4j/langchain4j-examples) (only for "big" features)
langchain4j,langchain4j,2c8ff58c028c6d1927d20ec7ad392864114eef6a,https://github.com/langchain4j/langchain4j/commit/2c8ff58c028c6d1927d20ec7ad392864114eef6a,Milvus: improve insert performance
google,closure-compiler,a8712c62b910791c6c1498ea19e11122d4b01fb8,https://github.com/google/closure-compiler/commit/a8712c62b910791c6c1498ea19e11122d4b01fb8,Add --compilation_level=TRANSPILE_ONLY flag.  Fixes https://github.com/google/closure-compiler/issues/4231  Add a new `--compilation_level=TRANSPILE_ONLY` option that transpiles input code to the specified output language level and injects necessary polyfills  without performing any optimizations.  PiperOrigin-RevId: 762150572
google,closure-compiler,ea3ac7718c665b8a2fe70ac4d93447a975e1b4b2,https://github.com/google/closure-compiler/commit/ea3ac7718c665b8a2fe70ac4d93447a975e1b4b2,Implement marker passes for JSCompiler stage 2 splitting  This CL implements the proposal to use marker passes for splitting JSCompiler's Stage 2 optimization phase.  Previously  users adding custom passes could encounter errors if the target pass for 'addBefore' was only present in one segment of the split stage 2. This change resolves the issue by:  -   Always returning the complete list of stage 2 passes in `DefaultPassConfig.getOptimizations()`  including an empty marker pass (`PassNames.OPTIMIZATIONS_HALFWAY_POINT`) to indicate the split. -   Modifying `Compiler.performTranspilationAndOptimizations()` to construct the actual pass list for each stage 2 segment based on the marker pass  and then providing this list to `PhaseOptimizer`.  PiperOrigin-RevId: 734309816
google,closure-compiler,fbd722a65c2dc619925f3ba65c5c6fb082306dc5,https://github.com/google/closure-compiler/commit/fbd722a65c2dc619925f3ba65c5c6fb082306dc5,Factor out common logic in AbstractCommandLineRunner  This is mostly a no-op - the only change should be that now compiler.performPostCompilationTasks(); is consistently called even if compiler.hasErrors() is true; previously  different methods would sometimes call it & sometimes not.  PiperOrigin-RevId: 721016838
google,closure-compiler,c464aa543174f1d807056af33edd8d62bbb44a75,https://github.com/google/closure-compiler/commit/c464aa543174f1d807056af33edd8d62bbb44a75,Rewrite all calls to performTranspilationAndOptimizations() to specify what list of stage 2 passes to run.  This CL just specifies that every call should request to run `ALL` stage 2 passes.  PiperOrigin-RevId: 716429985
google,closure-compiler,7a2a710232dd39a2b7a4822a428df363dfaca6f4,https://github.com/google/closure-compiler/commit/7a2a710232dd39a2b7a4822a428df363dfaca6f4,Use a RangeSet to improve the performance of checking whether a given line and column is within a closure-unaware source range.  PiperOrigin-RevId: 712966714
google,closure-compiler,8eda0eac26a389e45cd71b4a2f7d508f8be53b2d,https://github.com/google/closure-compiler/commit/8eda0eac26a389e45cd71b4a2f7d508f8be53b2d,Remove unnecessary String.split calls in StripCode  This is a slight performance improvement.  PiperOrigin-RevId: 704434673
google,closure-compiler,84866704f0857985019755099993f78e134a6a89,https://github.com/google/closure-compiler/commit/84866704f0857985019755099993f78e134a6a89,Avoiding rerunning `ProcessDefines` in `J2clUtilGetDefineRewriterPass`  The way `J2clUtilGetDefineRewriterPass` currently works is pretty messy as the rerun of `ProcessDefines` does not work as one would expect. When we actually rerun it all of the `goog.define` calls have already been removed  so the define name is actually based on the name of the node annotated with `@define`.  Instead of doing this we can have `ProcessDefines` stash the name of all the defines that it found. That's the only information that `J2clUtilGetDefineRewriterPass` needs to operate  and no new defines are going to be added along the way.  This does not change the requirement that the J2CL requires the define name to match a `goog.provide` of the same name. Resolving that problem is quite a bit more involved. For now this change just improves the improves the performance and reduces the brittleness of the existing solution.  PiperOrigin-RevId: 700492292
google,closure-compiler,0238a87b57031fe900362a2fd28151245448f522,https://github.com/google/closure-compiler/commit/0238a87b57031fe900362a2fd28151245448f522,Merge transpiler passes that do small  independent  local rewritings into a PeepholeTranspilationsPass.java doing a single traversal.  Most notably  for build performance  this means that the single peephole pass traversal performs all the small rewritings instead of a doing a separate AST traversal for each rewriting.  PiperOrigin-RevId: 657759081
apache,iceberg,c1d4182b3fb9fcb162a0233b8aa304a65ffa56f1,https://github.com/apache/iceberg/commit/c1d4182b3fb9fcb162a0233b8aa304a65ffa56f1,Parquet: Fix performance regression in reader init (#12305)
apache,iceberg,67e084c8d47db68c6135f9837b9cff6f88da0309,https://github.com/apache/iceberg/commit/67e084c8d47db68c6135f9837b9cff6f88da0309,API: Support removeUnusedSpecs in ExpireSnapshots (#10755)  Implement an API in ExpireSnapshots to remove partition specs no longer in use by performing a reachability analysis  so that metadata sizes can be maintained.  Co-authored-by: Russell_Spitzer <rspitzer@apple.com> Co-authored-by: Amogh Jahagirdar <amoghj@apache.org>
apache,iceberg,ea5da1789f3eb80fc3196bcdd787a95ac0c493ba,https://github.com/apache/iceberg/commit/ea5da1789f3eb80fc3196bcdd787a95ac0c493ba,AWS: Switch to base2 entropy in ObjectStoreLocationProvider for optimized S3 performance (#11112)  Co-authored-by: Drew Schleit <aschleit@amazon.com>
apache,iceberg,c67c9124d324207d742307982de31e5ff2ebcd01,https://github.com/apache/iceberg/commit/c67c9124d324207d742307982de31e5ff2ebcd01,Core  Spark: Spark writes/actions should only perform cleanup if failure is cleanable (#10373)
gocd,gocd,02d4acbb976d58586ce58a9cc64dc444714643c7,https://github.com/gocd/gocd/commit/02d4acbb976d58586ce58a9cc64dc444714643c7,Remove need for Guava within agents  - Still need to review performance and synchronize the PluginRoleUsersStore
google,error-prone,ccc9a8ab65aa7930373761e9f691b6249e5b69fd,https://github.com/google/error-prone/commit/ccc9a8ab65aa7930373761e9f691b6249e5b69fd,Inliner: parse the expression and use the AST to construct a replacement by performing careful  AST-backed surgery on the replacement expression tree.  This fixes several bugs we have tests before even getting to the one I want to fix.  PiperOrigin-RevId: 733406180
google,error-prone,ce0f3445fbf469512957bbcb8f1ea9fa5a8ad78d,https://github.com/google/error-prone/commit/ce0f3445fbf469512957bbcb8f1ea9fa5a8ad78d,replaceIncludingComments: skip synthetic variable declarations in records.  This isn't perfect because it fails to find the canonical constructor as a member  but it'll prevent howling great bugs.  PiperOrigin-RevId: 714993354
google,error-prone,da7be27b3c4a9f7d232072fdf3dc95e553156b6d partially fixed it.,This seems to have gotten left here by accident and then https://github.com/google/error-prone/commit/da7be27b3c4a9f7d232072fdf3dc95e553156b6d partially fixed it.,Remove superfluous `super.visitVariable`.  `super.visitVariable` is already called in the `visitVariable` method. The VariableFinder adds stuff to maps&sets so extra visiting is a no-op  but just confusing. 
google,error-prone,207e5df917a60a50ac291368d384d686d3365b88,https://github.com/google/error-prone/commit/207e5df917a60a50ac291368d384d686d3365b88,
google,error-prone,d70a18cce3fa5bd3daeb057c4b8e4ad2fe0e3d5c.),- Complain about recursive calls even if they come after `catch (SomeException e) { return; }`, and attempt to clarify how this fits into how we otherwise treat exceptions. (This follows through on my post-submit comment from https://github.com/google/error-prone/commit/d70a18cce3fa5bd3daeb057c4b8e4ad2fe0e3d5c.),Assorted InfiniteRecursion perfectionism: 
google,error-prone,405b5c2b88d01e7309db5ae552415d4fa74a3d2b,https://github.com/google/error-prone/commit/405b5c2b88d01e7309db5ae552415d4fa74a3d2b,Test that handling of switch expressions falls naturally out of the existing code  since it looks for `CaseTree` and not `SwitchTree` (and thus doesn't need an update for `SwitchExpressionTree`). - Leave a comment about how we _could_ get _even better_ handling of both kinds of `switch`  plus `if` and `?:` and maybe more  if we wanted to check whether all branches make recursive calls. (Such an approach likely _would_ lead us to look for `SwitchTree` and `SwitchExpressionTree`  rather than just `CaseTree`.)  PiperOrigin-RevId: 699467832
google,error-prone,81e3cfa0113f2c9d19ae8cfd4f2d33b52cffa7bd,https://github.com/google/error-prone/commit/81e3cfa0113f2c9d19ae8cfd4f2d33b52cffa7bd,Introduce `IdentifierName:AllowInitialismsInTypeName` flag  While default `IdentifierName` behavior is unchanged  this flag allows users to slightly relax the type name validation performed by this check.  Fixes #4646  COPYBARA_INTEGRATE_REVIEW=https://github.com/google/error-prone/pull/4646 from PicnicSupermarket:sschroevers/allow-initialisms-in-type-names 82abeaf229a11e1f6211c3fac170c811359e8210 PiperOrigin-RevId: 691425146
google,error-prone,494e2c604df4c4ab84428896f53a15c9d9de7d7b,https://github.com/google/error-prone/commit/494e2c604df4c4ab84428896f53a15c9d9de7d7b,Don't perform the inlining if parameter names are stripped.  #inlineme  PiperOrigin-RevId: 672578943
google,error-prone,a706e8d800abcafd6bfeb48c21bfd953261b0519.,The check is already performed in the caller of matchClass(). See discussion in https://github.com/google/error-prone/commit/a706e8d800abcafd6bfeb48c21bfd953261b0519.,Remove unnecessary check for suppressed condition. 
google,error-prone,b5e604106d2d4a8eca5083bc75be5c4063a1929e,https://github.com/google/error-prone/commit/b5e604106d2d4a8eca5083bc75be5c4063a1929e,
google,error-prone,2656f48902f6723f3147caa117372309dbc6c15f,https://github.com/google/error-prone/commit/2656f48902f6723f3147caa117372309dbc6c15f,Create `ThreadSafeTypeParameter` to replace `ThreadSafe.TypeParameter`.  This is the first step in open sourcing the threadsafe equivalent of `ImmutableTypeParameter`. After this an LSC will be performed to replace all usages of `ThreadSafe.TypeParameter` with `ThreadSafeTypeParameter` after which and remaining references to the former (e.g. in messages) will be removed.  ``` Startblock: has LGTM b/352709884 is fixed ``` PiperOrigin-RevId: 653005735
google,error-prone,008cfb02434f698b636cb850c283c0e73121afe0,https://github.com/google/error-prone/commit/008cfb02434f698b636cb850c283c0e73121afe0,Remove redundant `complete()` call and `CompletionFailure` check.  Those operations are already [performed during our call to `getSymbolFromString`](https://github.com/google/error-prone/blob/03d15b56478a40534b2b104e21d070d0eebc0701/check_api/src/main/java/com/google/errorprone/VisitorState.java#L432)  which returns `null` in case of failure.  (I've found the comment that I'm removing a little confusing  too: I think it might be making the point that we don't want to let the `CompletionFailure` propagate because that would fail Error Prone entirely when in fact the check that called `annotationsAmong` might find one of the _other_ annotations it was looking for among the annotations inherited by the class? (We would already have handled any _directly_ present annotations in `annotationsAmong` as well as in `hasAnnotation`.) If so  we're fine  as we handle `null` gracefully. (Of course  this assumes that silently ignoring an annotation that should have been inherited isn't a bad problem that should really lead to a crash. But there's only so much we can do in the presence of an incomplete classpath.) Or is it actually making the point that we want to try to look for `@Inherited` even if completing some _other_ part of the annotation has failed (if that is even possible) and hence we fall through instead of immediately returning `false`? If so  that doesn't currently work because we would already have bailed after the `null` return from `getSymbolFromString`. Anyway  whatever the comment is getting at  this CL is a no-op  so I'm just trying to avoid putting on blinders as I remove the surrounding code.)  PiperOrigin-RevId: 651828339
google,error-prone,3b9ffc2543080ad9150d41197aa9a3942e150849,https://github.com/google/error-prone/commit/3b9ffc2543080ad9150d41197aa9a3942e150849,Add support in ErrorProne for a new `ThreadSafeTypeParamerter` annotation that is replacing `ThreadSafe.TypeParameter`.  A follow-up CL will actually create the annotation once ErrorProne has this CL released to avoid premature dependencies on the new annotation.  This is the first step in open sourcing the threadsafe equivalent of `ImmutableTypeParameter`. After this an LSC will be performed to replace all usages of `ThreadSafe.TypeParameter` with `ThreadSafeTypeParameter` after which and remaining references to the former (e.g. in messages) will be removed.  PiperOrigin-RevId: 651816128
wiremock,wiremock,e03e4d2a91ba34777dcd3e3557b5fb06b9c05a98,https://github.com/wiremock/wiremock/commit/e03e4d2a91ba34777dcd3e3557b5fb06b9c05a98,Merge pull request #2981 from wiremock/xml-perf-improvements  Xml perf improvements
wiremock,wiremock,f76cb7f2f9e342df72ff780b35b6116f1bde6745,https://github.com/wiremock/wiremock/commit/f76cb7f2f9e342df72ff780b35b6116f1bde6745,perf: Reinstate improvements on EqualToXmlPattern
wiremock,wiremock,9ccb1c89a3f15db0c396af3999939fe464f87e48,https://github.com/wiremock/wiremock/commit/9ccb1c89a3f15db0c396af3999939fe464f87e48,improve performance of EqualToXmlPattern. (#2944)  removes use of DiffBuilder.ignoreComments which performs an xslt transform on the provided XML documents which is very memory intensive. now comments are ignored at read/parse time.  as part of this change  the Xml utility class was also refactored to (hopefully) make it more reusable.
wiremock,wiremock,df5b40798a464f9fabfa8dc6dcbf8f2c2f5b962b,https://github.com/wiremock/wiremock/commit/df5b40798a464f9fabfa8dc6dcbf8f2c2f5b962b,header checks should be case-insensitive  Update so the code performs a case-insensitive check on the headers
wiremock,wiremock,5610f61083dcc55664a1110569aeac5852689262,https://github.com/wiremock/wiremock/commit/5610f61083dcc55664a1110569aeac5852689262,Fix HttpClientBuilder сode and add options for Connection Management to increase the proxying performance (#2744)  * Fix code in HttpClientBuilder that override ConnectionManager parameters and add two command lines options (maxHttpClientConnections and disableConnectionReuse) to flexibly manage HttpClient  ---------  Co-authored-by: Tom Akehurst <tom@wiremock.org>
junit-team,junit5,aa0e34da5cc3e3fdce861b9028134571c86fdd8a,https://github.com/junit-team/junit5/commit/aa0e34da5cc3e3fdce861b9028134571c86fdd8a,Make phase that fails due to critical discovery issues configurable  Users of the Launcher API that perform a discovery without executing the resulting test plan are prone to accidentally hiding discovery issues from their users unless they implement their own `LauncherDiscoveryListener`. Therefore  the new `junit.platform.discovery.issue.failure.phase` configuration parameter allows configuring the `discovery` phase as the one that should fail in case critical discovery issues are encountered. By default  they will still be reported in the `execution` phase.
AutoMQ,automq,3a3d4a0c2e3d9825f4b1100234f5af7046986caf,https://github.com/AutoMQ/automq/commit/3a3d4a0c2e3d9825f4b1100234f5af7046986caf,feat: Added consumer percentage control for catch-up reading in perf test (#2573)  * Add documentation for --consumers-during-catchup parameter  This adds documentation comments for the new parameter that allows controlling the percentage of consumers activated during catch-up reads.  * Add --consumers-during-catchup parameter to PerfConfig  Introduces a new parameter to control what percentage of consumers should be activated during catch-up read scenarios. Default is 100%. Includes validation to ensure the value is between 0 and 100.  * Enhance ConsumerService to support partial consumer activation  Modifies resetOffset() and resume() methods to accept a percentage parameter  allowing only a subset of consumers to be activated during catch-up reads. Maintains backwards compatibility with existing method signatures.  * Update PerfCommand to use consumers-during-catchup parameter  Updates the catch-up read logic to use the new parameter for controlling the percentage of consumers that should be activated during catch-up reads.  * Fix various code style issues for whitespace and line breaks to comply with project standards  * fixed the two Checkstyle issues again 😭  * Implements a new --consumers-during-catchup parameter to control what percentage of topics have active consumers during catch-up reading phases. This parameter allows users to simulate realistic scenarios where only a subset of topics experience catch-up reads simultaneously.  * removes unnecessary pause
AutoMQ,automq,84ddd236d6e59a10d70b7a7ba11c0e5747d4d31c,https://github.com/AutoMQ/automq/commit/84ddd236d6e59a10d70b7a7ba11c0e5747d4d31c,perf(tool/perf): reduce the record header size (#2579)  * perf(tool/perf): reduce the record header size  Signed-off-by: Ning Yu <ningyu@automq.com>  * style: fix lint  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,7db3c74cd3d4e402c5912bf248d4fe6e6f22b75a,https://github.com/AutoMQ/automq/commit/7db3c74cd3d4e402c5912bf248d4fe6e6f22b75a,fix(perf): unify the time in different nodes (#2554) (#2556)  Signed-off-by: Robin Han <hanxvdovehx@gmail.com>
AutoMQ,automq,e83322518f816eb1372ed92112a2d1060a0717e6,https://github.com/AutoMQ/automq/commit/e83322518f816eb1372ed92112a2d1060a0717e6,feat: Add --catchup-topic-prefix for performance testing and solve the issue  (#2514)  feat: Add --catchup-topic-prefix for performance testing
AutoMQ,automq,c4bf688e36b50e418d1d79b2da856cd37d245fad,https://github.com/AutoMQ/automq/commit/c4bf688e36b50e418d1d79b2da856cd37d245fad,perf: Migrate to Monitored Thread Pools by Replacing Native ExecutorService (#2485)  * docs: Update IDE launch configuration in contribution guide  - Adjust the memory settings in VM Options and change `-Xmx1` to `-Xmx1G`  Signed-off-by: ZetoHkr <147681181+shawngao-org@users.noreply.github.com>  * refactor(thread): optimize thread pool closing logic  - Introduce the ThreadUtils class to uniformly handle thread pool shutdown  #2358  * perf(s3): Optimize TrafficRateLimiter test code import  * perf(s3): Optimize TrafficRateLimiter test code import  * perf: Migrate to Monitored Thread Pools by Replacing Native ExecutorService.  ---------  Signed-off-by: ZetoHkr <147681181+shawngao-org@users.noreply.github.com>
AutoMQ,automq,24cb55f02081b051850014c8a81ea7a86e090f1c,https://github.com/AutoMQ/automq/commit/24cb55f02081b051850014c8a81ea7a86e090f1c,perf: Standardize ExecutorService Shutdown with Safety Wrapper. (#2430)  * docs: Update IDE launch configuration in contribution guide  - Adjust the memory settings in VM Options and change `-Xmx1` to `-Xmx1G`  Signed-off-by: ZetoHkr <147681181+shawngao-org@users.noreply.github.com>  * refactor(thread): optimize thread pool closing logic  - Introduce the ThreadUtils class to uniformly handle thread pool shutdown  #2358  * perf(s3): Optimize TrafficRateLimiter test code import  * perf(s3): Optimize TrafficRateLimiter test code import  ---------  Signed-off-by: ZetoHkr <147681181+shawngao-org@users.noreply.github.com>
AutoMQ,automq,c41faffb2a2e658c2e065059434e42d217a711f3,https://github.com/AutoMQ/automq/commit/c41faffb2a2e658c2e065059434e42d217a711f3,perf: Replace Unsafe ScheduledExecutorService Initialization with Exception-Handling Wrapper (#2414)  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357  * perf: pr-issue-2357
AutoMQ,automq,d91dbfca87962c400569e68007aa19b84e1ef6e1,https://github.com/AutoMQ/automq/commit/d91dbfca87962c400569e68007aa19b84e1ef6e1,perf(s3stream/wal): add append timeout in block WAL (#2398)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,d34b8943cba1d4f83c6bd89fe635eb4bc2e6769f,https://github.com/AutoMQ/automq/commit/d34b8943cba1d4f83c6bd89fe635eb4bc2e6769f,perf(s3stream/objectstorage): unify the throttle criteria (#2386)  * perf(s3stream/objectstorage): unify the throttle criteria  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor(s3stream/objectstorage): retry on 403 responses  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor(objectstorage): use `TimeoutException` instead of `ApiCallAttemptTimeoutException`  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: supress the cause of `ObjectNotExistException`  Signed-off-by: Ning Yu <ningyu@automq.com>  * style: fix lint  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,14249c5786821aba071afaaf5454347b6a48daf3,https://github.com/AutoMQ/automq/commit/14249c5786821aba071afaaf5454347b6a48daf3,perf: add log level checks to optimize AutoMQ logging performance (#2378)  * perf: add log level checks to optimize AutoMQ logging performance  * fix: add @SuppressWarnings("NPathComplexity") to fix checkstyle issues
AutoMQ,automq,c4e2bcfa802f05ebfe2fe47c9fec7c6ff10e3cf4,https://github.com/AutoMQ/automq/commit/c4e2bcfa802f05ebfe2fe47c9fec7c6ff10e3cf4,perf(s3stream): limit write traffic to object storage (#2340)  perf(s3stream): limit write traffic to object storage (#2335)  * chore(objectstorage): log next retry delay    * feat: a `TrafficLimiter` to limit the network traffic    * feat: a `TrafficMonitor` to monitor the network traffic    * feat: record success and failed write requests    * feat: queued pending write tasks    * feat: run write tasks one by one    * feat: use a `TrafficRegulator` to control the rate of write requests    * feat: limit the inflight force upload tasks    * fix: correct retry count    * chore: fix commit object logs    * chore: log force uploads    * fix: catch exceptions    * style: fix lint    * fix: fix re-trigger run write task    * perf: increate if no traffic    * refactor: remove useless try-catch    * perf: ensure only one inflight force upload tasks    * refactor: move inner classes outside    * perf: increase rate limit slower    * chore: add a prefix in `AbstractObjectStorage#logger`    * chore: reduce useless logs    * perf: reduce the sample count on warmup    * feat: introduce `TrafficVolumeLimiter` base on IBM `AsyncSemaphore`    * feat: limit the inflight write requests    * chore: reduce useless logs    * fix: fix release size    * fix: release permits once the request failed    * perf: increase to max after 2 hours    * fix: limit the request size    * perf: adjust constants    ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,645db9d6b28d1522b5aa8b58151af6f0fdd71118,https://github.com/AutoMQ/automq/commit/645db9d6b28d1522b5aa8b58151af6f0fdd71118,fix(tool/perf): add admin properties in `ConsumerService#admin` (#2310)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,b0f021d7b7cf503f7eb08adef56b24af7523d17e,https://github.com/AutoMQ/automq/commit/b0f021d7b7cf503f7eb08adef56b24af7523d17e,feat(tool/perf): add option "--common-config-file" (#2308)  * refactor: rename "--common-configs" to "--admin-configs"  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: add option "--common-config-file"  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: use `Properties` rather than `Map` to pass configs  Signed-off-by: Ning Yu <ningyu@automq.com>  * style: fix lint  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: remove "--admin-config"  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,2a2d108fe28df522c544f0e9880ad5685940dba1,https://github.com/AutoMQ/automq/commit/2a2d108fe28df522c544f0e9880ad5685940dba1,fix(tools/perf): only delete test topics on reset (#2284)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,09d701e50042877f83331d0f7aec3a9ddda406da,https://github.com/AutoMQ/automq/commit/09d701e50042877f83331d0f7aec3a9ddda406da,fix(tools/perf): fix option name "--max-consume-record-rate"  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,df4c558618fda34985112a9fb968d61ab631097c,https://github.com/AutoMQ/automq/commit/df4c558618fda34985112a9fb968d61ab631097c,feat(tools/perf): support to limit the max poll rate of consumers (#2270)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,f2e858e946873b26eab252898717c50a519db31d,https://github.com/AutoMQ/automq/commit/f2e858e946873b26eab252898717c50a519db31d,perf(produce): fix validate compressed records alloc too many memory  Signed-off-by: Robin Han <hanxvdovehx@gmail.com>
AutoMQ,automq,339c75a2543c04112e6395253074492e421e8167,https://github.com/AutoMQ/automq/commit/339c75a2543c04112e6395253074492e421e8167,feat: only delete topics created by the perf tool  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,6a013d02ce9a83ab405edb100e8709ad64fc4b4b,https://github.com/AutoMQ/automq/commit/6a013d02ce9a83ab405edb100e8709ad64fc4b4b,feat(tools/perf): support schema message perf (#2226)  Signed-off-by: Robin Han <hanxvdovehx@gmail.com>
AutoMQ,automq,0265f44dbca0ab5e6c37bed00b36d05376652904,https://github.com/AutoMQ/automq/commit/0265f44dbca0ab5e6c37bed00b36d05376652904,fix(tools/perf): fix the perf tools await count (#2219)  Signed-off-by: Robin Han <hanxvdovehx@gmail.com>
AutoMQ,automq,b470ba4854a2c6d94c3059b3e8e7a5374c460701,https://github.com/AutoMQ/automq/commit/b470ba4854a2c6d94c3059b3e8e7a5374c460701,feat(backpressure): add metrics (#2198)  * feat(backpressure): log it on recovery from backpressure  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: add metric fetch_limiter_waiting_task_num  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: add metric fetch_limiter_timeout_count  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: add metric fetch_limiter_time  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: add metric back_pressure_state  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: add metric broker_quota_limit  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix(backpressure): run checkers with fixed delay  Signed-off-by: Ning Yu <ningyu@automq.com>  * style: fix lint  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf: drop too large values  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: record -1 for other states  Signed-off-by: Ning Yu <ningyu@automq.com>  * test: fix tests  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,89f2c6b206c84a53dd3eb6efa5d260be6e64164e,https://github.com/AutoMQ/automq/commit/89f2c6b206c84a53dd3eb6efa5d260be6e64164e,feat(quota): support to get current quota metric value... (#2170)  * fix: fix logs  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat(quota): support to get current quota metric value  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor(backpressure): remove `Regulator#minimize`  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(quota): increase the max of broker quota throttle time  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(backpressure): decrease cooldown time  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(quota): increase the max of broker quota throttle time  Signed-off-by: Ning Yu <ningyu@automq.com>  * docs: update comments  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,ff3b68ea07cad2bf3febc6ea0175637dd8455732,https://github.com/AutoMQ/automq/commit/ff3b68ea07cad2bf3febc6ea0175637dd8455732,feat(tools/perf): create topics in batch (#2166)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,cbaf3a29e44d952f5beaafa3243addbdf71c3003,https://github.com/AutoMQ/automq/commit/cbaf3a29e44d952f5beaafa3243addbdf71c3003,feat(tools/perf): run benchmark without consumer (#2135)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,8f5fcc5f5c2d06d1bcb68a0536fba5bb5a89b938,https://github.com/AutoMQ/automq/commit/8f5fcc5f5c2d06d1bcb68a0536fba5bb5a89b938,refactor(tools/perf): retry sending messages in when waiting topics ready (#2133)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,42debe7715cbd94ea02ae678c23970e374bc1e29,https://github.com/AutoMQ/automq/commit/42debe7715cbd94ea02ae678c23970e374bc1e29,perf(tools/perf): assuming all partitions have the same offset at the same time (#2127) (#2128)  * feat(tools/perf): log progress on resetting offsets    * fix: reset timeouts    * feat: increase the log interval    * perf(tools/perf): assuming all partitions have the same offset at the same time    * feat: limit the min of --backlog-duration    ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,a7250cd750dbf0716959128e334cbc323134d8ea,https://github.com/AutoMQ/automq/commit/a7250cd750dbf0716959128e334cbc323134d8ea,perf: limit the inflight requests (#2100)  * docs: add todos  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(network): limit the inflight requests by size  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(ReplicaManager): limit the queue size of the `fetchExecutor`s  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(KafkaApis): limit the queue size of async request handlers  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor(network): make "queued.max.requests.size.bytes" configurable  Signed-off-by: Ning Yu <ningyu@automq.com>  * style: fix lint  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix(network): limit the min queued request size per queue  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,bc63e6b6148a115bda49ccbeb0c29fe2ffa9a665,https://github.com/AutoMQ/automq/commit/bc63e6b6148a115bda49ccbeb0c29fe2ffa9a665,perf(s3stream/limiter): decrease the max tokens of network limiters (#2077)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,9ffe41de44442634f2ee3295f98785cba9676711,https://github.com/AutoMQ/automq/commit/9ffe41de44442634f2ee3295f98785cba9676711,perf(storage): increase the timeout to wait for the SEQ-0 in `ProducerStateManager` (#2031)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,e8e7aabc87b7d9c294ec7dbe2402541323e48d97,https://github.com/AutoMQ/automq/commit/e8e7aabc87b7d9c294ec7dbe2402541323e48d97,refactor(s3stream/wal): use bucket4j to limit the write rate and bandwidth (#2034)  * perf(s3stream/wal): limit iops by bucket4j  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix: make `pendingBlocks` and `currentBlock` volatile  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: no need to batch the `Block`  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix: leave some buffer for other write operations  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: limit the write bandwidth in `SlidingWindowService` rather than `WALBlockDeviceChannel`  Signed-off-by: Ning Yu <ningyu@automq.com>  * chore: add option "--bandwidth" in `WriteBench`  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix: limit the max refilling rate  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix: fix the case that `softLimit=0`  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf: increase the buckets' capacity  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,619138b92b123dc97281fab35eb3e5c059b90438,https://github.com/AutoMQ/automq/commit/619138b92b123dc97281fab35eb3e5c059b90438,fix(log): use the same view to calculate trim offsets (#1973)  * fix(log): use the same view to calculate trim offsets  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix: calculate trim offsets in the lock  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf: use `Iterator` to reduce the overhead caused by passing intermediate data  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf: merge `calTrimOffset` and `calStreamsMinOffset`  Signed-off-by: Ning Yu <ningyu@automq.com>  * test: fix tests  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,6259864445b2bef4489e0c0db60fc2cc25d220e2,https://github.com/AutoMQ/automq/commit/6259864445b2bef4489e0c0db60fc2cc25d220e2,perf(s3stream): optimize ByteBufAlloc.USAGE_STATS high cpu usage (#1959)  * perf(s3stream): replace ByteBufAlloc.USAGE_STATS to array  * perf(s3stream): replace ByteBufAlloc.USAGE_STATS to array  * perf(s3stream): replace ByteBufAlloc.USAGE_STATS to array
AutoMQ,automq,b8eeb23c39df228783f59b2eeeaddd9ec28e9c1e,https://github.com/AutoMQ/automq/commit/b8eeb23c39df228783f59b2eeeaddd9ec28e9c1e,perf(config): increase the default index interval from 4KiB to 1MiB (#1915)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,3bbaf81092063ba0313de4ce2a2011c2f54b6712,https://github.com/AutoMQ/automq/commit/3bbaf81092063ba0313de4ce2a2011c2f54b6712,perf(s3stream): optimize the critical section in s3 wal (#1880)  * perf(s3stream): optimize the critical section in s3 wal  Signed-off-by: SSpirits <admin@lv5.moe>  * fix(s3stream): fix check style  Signed-off-by: SSpirits <admin@lv5.moe>  * feat(s3stream): optimize  Signed-off-by: SSpirits <admin@lv5.moe>  * fix(s3stream): fix resource leak in test  Signed-off-by: SSpirits <admin@lv5.moe>  ---------  Signed-off-by: SSpirits <admin@lv5.moe>
AutoMQ,automq,09b685ef9f2a32595f122a8eb71a9677158ed1b8,https://github.com/AutoMQ/automq/commit/09b685ef9f2a32595f122a8eb71a9677158ed1b8,chore(tools/perf): increase the message sending rate during the warmup to accelerate JVM warmup (#1883)  chore(tools/perf): increase the message sending rate during the warmup to accelerate JVM warmup (#1881)  * perf: increase the message sending rate during the warmup to accelerate JVM warmup    * chore: rename "catchup-rate" to "send-rate-during-catchup"    * fix: fix compile error    ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,2d3a49669c6ddc0198462d97a01983191c6af212,https://github.com/AutoMQ/automq/commit/2d3a49669c6ddc0198462d97a01983191c6af212,perf(s3stream/wal): increase the size of the buffer pool as CPU cores (#1882)  perf: increase the size of the buffer pool as CPU cores  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,962fad347106c6111dde5e7d46fc8253e0331e8f,https://github.com/AutoMQ/automq/commit/962fad347106c6111dde5e7d46fc8253e0331e8f,perf(s3stream/wal): reuse the `ByteBuf` for record headers (#1877)  * refactor: manage the record headers' lifecycle in `Block`  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(s3stream/wal): reuse the `ByteBuf` for record headers  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf: remove the max size limit  Signed-off-by: Ning Yu <ningyu@automq.com>  * test: test `FixedSizeByteBufPool`  Signed-off-by: Ning Yu <ningyu@automq.com>  * revert: "perf: remove the max size limit"  This reverts commit ed6311210a77fd5547b45d4857cf2011fc072dc8.  * feat: use a separate `poolSize` to limit the size of the pool  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,70108bce99a1c3d2ba245c20a296f2ba92367a86,https://github.com/AutoMQ/automq/commit/70108bce99a1c3d2ba245c20a296f2ba92367a86,perf(s3stream): check the logger level before `trace` and `debug` (#1875)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,9ee6cfe3250aaab3bc612c86c4f4741dc6a394e9,https://github.com/AutoMQ/automq/commit/9ee6cfe3250aaab3bc612c86c4f4741dc6a394e9,perf: use a new `GrowableMultiBufferSupplier` to avoid memory waste (#1800)  * perf: use a new `GrowableMultiBufferSupplier` to avoid memory waste  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: always try the first buffer in cache  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,dd6b5c1c1e1cab1a9077272252dc5e535ed106f2,https://github.com/AutoMQ/automq/commit/dd6b5c1c1e1cab1a9077272252dc5e535ed106f2,fix(core): Revert "perf: use netty buffer to pool memory (#1788)" (#1797)  Revert "perf: use netty buffer to pool memory (#1788)"  This reverts commit 5317eff6d037726a8f278b80a4ad00677eecbc67.
AutoMQ,automq,5317eff6d037726a8f278b80a4ad00677eecbc67,https://github.com/AutoMQ/automq/commit/5317eff6d037726a8f278b80a4ad00677eecbc67,perf: use netty buffer to pool memory (#1788)  * perf: use netty buffer to pool memory  Signed-off-by: Ning Yu <ningyu@automq.com>  * test: test `NettyBufferSupplier`  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: only replace the `BufferSupplier` in `KRaftControlRecordStateMachine`  Signed-off-by: Ning Yu <ningyu@automq.com>  * docs: add automq inject comments  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: replace all `BufferSupplier` used by `RecordsIterator`  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,393b0d36f6f4e2a78fb50b20ce072b6af79382ab,https://github.com/AutoMQ/automq/commit/393b0d36f6f4e2a78fb50b20ce072b6af79382ab,perf(core): batch persistent meta when delete large scale segment (#1670)  * perf(core): reduce persistent meta when delete large scale segment  * perf(core): batch persistent meta when delete large scale segment  * perf(core): batch persistent meta when delete large scale segment
AutoMQ,automq,504886d4777891c8f6f8bbc7e85a47e45b6d014b,https://github.com/AutoMQ/automq/commit/504886d4777891c8f6f8bbc7e85a47e45b6d014b,refactor(s3stream/wal): allow `reset` without `recover` (#1676)  * refactor(s3stream/wal): allow `reset` without `recover`  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(s3stream/wal): use a big enough start offset to reset WAL  Signed-off-by: Ning Yu <ningyu@automq.com>  * test: test reset without recover  Signed-off-by: Ning Yu <ningyu@automq.com>  * test: fix tests  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,6ff51bc388dac55e04343fe4247b1080f1241dab,https://github.com/AutoMQ/automq/commit/6ff51bc388dac55e04343fe4247b1080f1241dab,KAFKA-17210: Broker fixes for smooth concurrent fetches on share partition (#16711)  Identified a couple of reliability issues with broker code for share groups  1. Broker seems to get stuck at times when using multiple share consumers due to a corner case where the second last fetch request did not contain any topic partition to fetch  because of which the broker could never complete the last request. This results in a share fetch request getting stuck.  2. Since persister would not perform any business logic around sending state batches for a share partition  there could be scenarios where it sends state batches with no AVAILABLE records. This could cause a breach on the limit of in-flight messages we have configured  and hence broker would never be able to complete the share fetch requests.  Reviewers:  Andrew Schofield <aschofield@confluent.io>  Apoorv Mittal <apoorvmittal10@gmail.com>   Manikumar Reddy <manikumar.reddy@gmail.com>
AutoMQ,automq,d2063aa256f822c213db5069762ae4681c0d9ec5,https://github.com/AutoMQ/automq/commit/d2063aa256f822c213db5069762ae4681c0d9ec5,perf(s3stream): increase the object overhead of `StreamRecordBatch` (#1661)  * perf(s3stream): increase the object overhead of `StreamRecordBatch`  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: make `OBJECT_OVERHEAD` private  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,835f8851071996f4385f4a0ece528e88bc05205a,https://github.com/AutoMQ/automq/commit/835f8851071996f4385f4a0ece528e88bc05205a,fix(perf): fix options in "automq-perf-test.sh" (#1605)  * fix(PerfConfig): split properties by "=" with limit 2  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat(perf): add option "--common-configs"  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,a2a2eae47c64b2cb3a17401903deacb5cba87ea4,https://github.com/AutoMQ/automq/commit/a2a2eae47c64b2cb3a17401903deacb5cba87ea4,feat(core): optimize indexing performance by using sparse index cache (#1585)  Signed-off-by: Shichao Nie <niesc@automq.com>
AutoMQ,automq,f1d01668635222cdb170d8d8756b9ff18cde2ab5,https://github.com/AutoMQ/automq/commit/f1d01668635222cdb170d8d8756b9ff18cde2ab5,perf(s3stream): batch delete object when compact Composite object (#1513)  * perf(s3stream): batch delete object when compact Composite object  * perf(s3stream): batch delete object when compact Composite object  * perf(s3stream): batch delete object when compact Composite object
AutoMQ,automq,76119686e5a467da8112046b839d42ec8f551b3c,https://github.com/AutoMQ/automq/commit/76119686e5a467da8112046b839d42ec8f551b3c,feat(s3stream): optimize get objects performance (#1512)  Signed-off-by: Shichao Nie <niesc@automq.com>
AutoMQ,automq,c8439fb9572bd80d91c38bd6b0652e70a617a9aa,https://github.com/AutoMQ/automq/commit/c8439fb9572bd80d91c38bd6b0652e70a617a9aa,perf(s3stream): use FastThreadLocal in WAL io executor (#1507)  perf(s3stream): use FastThreadLocal in io executor
AutoMQ,automq,27018debb3da583b32cb4428fc5d2602cf46ae72,https://github.com/AutoMQ/automq/commit/27018debb3da583b32cb4428fc5d2602cf46ae72,perf(s3stream): Avoid non-reuse TimerUtil object creation on write path (#1447)  * perf(s3stream): Avoid non-reuse TimerUtil object creation on write path  * perf(s3stream): Avoid non-reuse TimerUtil object creation on write path
AutoMQ,automq,69fd80b78d406362d776db98212e8ad4cc647569,https://github.com/AutoMQ/automq/commit/69fd80b78d406362d776db98212e8ad4cc647569,fix(tools/perf): discard invalid latency values (#1430)  * fix(tools/perf): discard invalid latency values  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat: use a random topic prefix as default  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,79613d36219820dfadac7d44aa24e9c10fd71a3d,https://github.com/AutoMQ/automq/commit/79613d36219820dfadac7d44aa24e9c10fd71a3d,perf(FileCache): avoid evicting too many blocks at once (#1425)  * refactor(FileCache): Rename `Value` to `Blocks`  Signed-off-by: Ning Yu <ningyu@automq.com>  * docs(FileCache): add more comments  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(FileCache): avoid evicting too many blocks at once  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,af52f63a04532addb12656724bce1803d464ea32,https://github.com/AutoMQ/automq/commit/af52f63a04532addb12656724bce1803d464ea32,perf(metadata): use Timeline for objects image (#1419)  perf(metadata): use Timeline for objects image (#1405)  Signed-off-by: Robin Han <hanxvdovehx@gmail.com>
AutoMQ,automq,30a12095ac0be9cb8d56687636d7622b45ac0344,https://github.com/AutoMQ/automq/commit/30a12095ac0be9cb8d56687636d7622b45ac0344,perf(s3stream): reduce lock granularity in `StreamMetadataManager` (#1411)  * perf(s3stream): reduce lock granularity in `StreamMetadataManager`  * perf(s3stream): reduce lock granularity in `StreamMetadataManager`  * perf(s3stream): reduce lock granularity in `StreamMetadataManager`  * perf(s3stream): reduce lock granularity in `StreamMetadataManager`  * perf(s3stream): reduce lock granularity in `StreamMetadataManager`  avoid return MetadataImage
AutoMQ,automq,68070c94a6bd5141c720aaff85fa545da3a7a1b5,https://github.com/AutoMQ/automq/commit/68070c94a6bd5141c720aaff85fa545da3a7a1b5,KAFKA-16724: Added support for fractional throughput and monotonic payload in kafka-producer-perf-test.sh  Added support for fractional throughput and monotonic payload in kafka-producer-perf-test.sh. https://cwiki.apache.org/confluence/display/KAFKA/KIP-932%3A+Queues+for+Kafka#KIP932:QueuesforKafka-kafka-producer-perf-test.sh  Reviewers: Andrew Schofield <aschofield@confluent.io>  Manikumar Reddy <manikumar.reddy@gmail.com>
AutoMQ,automq,71f805b3ae53cfab73165e30e4a517616325385f,https://github.com/AutoMQ/automq/commit/71f805b3ae53cfab73165e30e4a517616325385f,feat(tools/perf): adjust send rate during warmup and catchup (#1395)  * feat(tools/perf): adjust send rate during warmup  Signed-off-by: Ning Yu <ningyu@automq.com>  * feat(tools/perf): add option "catchupRate"  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,3835515feaf7cb5bb7de3c4d63794e79100eb62a,https://github.com/AutoMQ/automq/commit/3835515feaf7cb5bb7de3c4d63794e79100eb62a,KAFKA-16541 Fix potential leader-epoch checkpoint file corruption (#15993)  A patch for KAFKA-15046 got rid of fsync on LeaderEpochFileCache#truncateFromStart/End for performance reason  but it turned out this could cause corrupted leader-epoch checkpoint file on ungraceful OS shutdown  i.e. OS shuts down in the middle when kernel is writing dirty pages back to the device.  To address this problem  this PR makes below changes: (1) Revert LeaderEpochCheckpoint#write to always fsync (2) truncateFromStart/End now call LeaderEpochCheckpoint#write asynchronously on scheduler thread (3) UnifiedLog#maybeCreateLeaderEpochCache now loads epoch entries from checkpoint file only when current cache is absent  Reviewers: Jun Rao <junrao@gmail.com>
AutoMQ,automq,078dd9a311a5e9ddb9d0b49a1164c50dadde3a27,https://github.com/AutoMQ/automq/commit/078dd9a311a5e9ddb9d0b49a1164c50dadde3a27,KAFKA-16821; Member Subscription Spec Interface (#16068)  This patch reworks the `PartitionAssignor` interface to use interfaces instead of POJOs. It mainly introduces the `MemberSubscriptionSpec` interface that represents a member subscription and changes the `GroupSpec` interfaces to expose the subscriptions and the assignments via different methods.  The patch does not change the performance.  before: ``` Benchmark                                     (memberCount)  (partitionsToMemberRatio)  (topicCount)  Mode  Cnt  Score   Error  Units TargetAssignmentBuilderBenchmark.build                10000                         10           100  avgt    5  3.462 ± 0.687  ms/op TargetAssignmentBuilderBenchmark.build                10000                         10          1000  avgt    5  3.626 ± 0.412  ms/op JMH benchmarks done ```  after: ``` Benchmark                                     (memberCount)  (partitionsToMemberRatio)  (topicCount)  Mode  Cnt  Score   Error  Units TargetAssignmentBuilderBenchmark.build                10000                         10           100  avgt    5  3.677 ± 0.683  ms/op TargetAssignmentBuilderBenchmark.build                10000                         10          1000  avgt    5  3.991 ± 0.065  ms/op JMH benchmarks done ```  Reviewers: David Jacot <djacot@confluent.io>
AutoMQ,automq,6b4c777c02db6b6faf0a8647bf03d41cd5a53d11,https://github.com/AutoMQ/automq/commit/6b4c777c02db6b6faf0a8647bf03d41cd5a53d11,perf(s3stream/allocator): increase trunk size of the netty allocator (#1364)  * perf(allocator): increase the default chunk size  Signed-off-by: Ning Yu <ningyu@automq.com>  * perf(allocator): use a new huge buffer if the current buffer does not satisfy the request  Signed-off-by: Ning Yu <ningyu@automq.com>  * style: fix lint  Signed-off-by: Ning Yu <ningyu@automq.com>  * fix: fix IndexOutOfBoundException  Signed-off-by: Ning Yu <ningyu@automq.com>  * refactor: return `Optional<Integer>` in `ByteBufAlloc#getChunkSize`  Signed-off-by: Ning Yu <ningyu@automq.com>  * test: fix tests  Signed-off-by: Ning Yu <ningyu@automq.com>  ---------  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,abc3a6c14ff5b837bac612876e6384407be6f6f4,https://github.com/AutoMQ/automq/commit/abc3a6c14ff5b837bac612876e6384407be6f6f4,fix(s3stream): temporarily disable percentile metrics for performance issue (#1353)  Signed-off-by: Shichao Nie <niesc@automq.com>
AutoMQ,automq,979f8d9aa3e8840951a151ab83eb006f8e7c1314,https://github.com/AutoMQ/automq/commit/979f8d9aa3e8840951a151ab83eb006f8e7c1314,MINOR: Small refactor in TargetAssignmentBuilder (#16174)  This patch is a small refactoring which mainly aims at avoid to construct a copy of the new target assignment in the TargetAssignmentBuilder because the copy is not used by the caller. The change relies on the exiting tests and it does not really have an impact on performance (e.g. validated with TargetAssignmentBuilderBenchmark).  Reviewers: Chia-Ping Tsai <chia7712@gmail.com>
AutoMQ,automq,741ff9ecd692489c2142dcf573512ac889c5e3de,https://github.com/AutoMQ/automq/commit/741ff9ecd692489c2142dcf573512ac889c5e3de,fix(perf): increase request timeout of the admin client (#1336)  Signed-off-by: Ning Yu <ningyu@automq.com>
AutoMQ,automq,c8af740bd44dae92bbe68254114c0fd7f7c32345,https://github.com/AutoMQ/automq/commit/c8af740bd44dae92bbe68254114c0fd7f7c32345,Improve producer ID expiration performance (#16075)  Skip using stream when expiring the producer ID. This can improve the performance significantly when the count is high. Before  Benchmark                                        (numProducerIds)  Mode  Cnt      Score       Error  Units ProducerStateManagerBench.testDeleteExpiringIds             10000  avgt    3    101.253 ±    28.031  us/op ProducerStateManagerBench.testDeleteExpiringIds            100000  avgt    3   2297.219 ±  1690.486  us/op ProducerStateManagerBench.testDeleteExpiringIds           1000000  avgt    3  30688.865 ± 16348.768  us/op After  Benchmark                                        (numProducerIds)  Mode  Cnt     Score     Error  Units ProducerStateManagerBench.testDeleteExpiringIds             10000  avgt    3    39.122 ±   1.151  us/op ProducerStateManagerBench.testDeleteExpiringIds            100000  avgt    3   464.363 ±  98.857  us/op ProducerStateManagerBench.testDeleteExpiringIds           1000000  avgt    3  5731.169 ± 674.380  us/op Also  made a change to the JMH testing which excludes the producer ID populating from the testing.  Reviewers: Artem Livshits <alivshits@confluent.io>  Justine Olshan <jolshan@confluent.io>
AutoMQ,automq,8d243dfbd415b5b34515695b72379d1489297a23,https://github.com/AutoMQ/automq/commit/8d243dfbd415b5b34515695b72379d1489297a23,KAFKA-15045: (KIP-924 pt. 12) Wiring in new assignment configs and logic (#16074)  This PR creates the new public config of KIP-924 in StreamsConfig and uses it to instantiate user-created TaskAssignors. If such a TaskAssignor is found and successfully created we then use that assignor to perform the task assignment  otherwise we revert back to the pre KIP-924 world with the internal task assignors.  Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>  Almog Gavra <almog@responsive.dev>
quartz-scheduler,quartz,316c5e78894fee76d0f7262cdaca3aa94bba54c5,https://github.com/quartz-scheduler/quartz/commit/316c5e78894fee76d0f7262cdaca3aa94bba54c5,Jobs get stuck when a Job Listener throws an exception #918  - Added new exception <codeJobExecutionProcessException</code> contains a link to the context of the task being performed <code>JobExecutionContext</code>  for the possibility of restoring the task  Signed-off-by: mplotnikov <maximplotnikov748@gmail.com>
quartz-scheduler,quartz,1207b03e593ffe0de6829356d9c1b71a810ecf75,https://github.com/quartz-scheduler/quartz/commit/1207b03e593ffe0de6829356d9c1b71a810ecf75,Jobs get stuck when a Job Listener throws an exception #918  - We veto the execution of the task if an error occurred while executing <code>TriggerListenerSupport#vetoJobExecution()</code> - Added to the <code>Scheduler Exception</code> a link to the context of the task being performed <code>JobExecutionContext</code>  for the possibility of restoring the task  Signed-off-by: mplotnikov <maximplotnikov748@gmail.com>
quartz-scheduler,quartz,375562579e954fa08b1d7e28a524c026595128ef,https://github.com/quartz-scheduler/quartz/commit/375562579e954fa08b1d7e28a524c026595128ef,Jobs get stuck when a Job Listener throws an exception #918  - We veto the execution of the task if an error occurred while executing <code>TriggerListenerSupport#vetoJobExecution()</code> - Added to the <code>Scheduler Exception</code> a link to the context of the task being performed <code>JobExecutionContext</code>  for the possibility of restoring the task  Signed-off-by: mplotnikov <maximplotnikov748@gmail.com>
quartz-scheduler,quartz,eb576ff498541c141edfa0a532ecf370f388f827,https://github.com/quartz-scheduler/quartz/commit/eb576ff498541c141edfa0a532ecf370f388f827,performance: replace  fo loop by bulk operation with .addAll() method  Signed-off-by: Carlos Garcia <bcode@protonmail.com>
quartz-scheduler,quartz,2bccc097601b550e2eb494eca8d608d909e184f8,https://github.com/quartz-scheduler/quartz/commit/2bccc097601b550e2eb494eca8d608d909e184f8,performance: use StringBuilder instead of String concatenation  Signed-off-by: Carlos Garcia <bcode@protonmail.com>
Col-E,Recaf,9f8a4a7890a28ef9558281aa10e0204303f48828,https://github.com/Col-E/Recaf/commit/9f8a4a7890a28ef9558281aa10e0204303f48828,Replace transformer collections of ClassPathNode with identity based collections over hash based collections  This massively improves performance due to the expensive nature of path hashing.  The creation of these collections is such that no duplicate paths should ever be attempted to go into the same collection. Additionally the user is not expected to do path based lookups in these maps. They are for primarily intended for iteration/inspection only.
Col-E,Recaf,570dafb8839a95c352194b8885ddac129fa2cd7b,https://github.com/Col-E/Recaf/commit/570dafb8839a95c352194b8885ddac129fa2cd7b,Remove dirs.dev dependency for performance concerns
Col-E,Recaf,3ec9804fcf2204e5dada7e4185bee03a96c4ef04,https://github.com/Col-E/Recaf/commit/3ec9804fcf2204e5dada7e4185bee03a96c4ef04,Migrate listener collections to CopyOnWrite implementations  For thread safety  these aren't often written to (or iterated over) so perf is not a big concern with these
jOOQ,jOOQ,db62f6c1076be4bf151d4118daa767cec2f3be08,https://github.com/jOOQ/jOOQ/commit/db62f6c1076be4bf151d4118daa767cec2f3be08,Merge pull request #17670 from ggsurrel/main  Performance improvement in BatchSingle class
jOOQ,jOOQ,daa947f1ce9baff14e359014e8299c6a177e51f3,https://github.com/jOOQ/jOOQ/commit/daa947f1ce9baff14e359014e8299c6a177e51f3,Performance improvement in BatchSingle.java
hazelcast,hazelcast,2d2918ff9e3c4cb3f0fd895de96d1a1127a8cfe8,https://github.com/hazelcast/hazelcast/commit/2d2918ff9e3c4cb3f0fd895de96d1a1127a8cfe8,Improve `Stream` `sum` performance by avoiding redundant boxing (micro optimization) (#4262)  When `sum`ming a `Stream`  calling `reduce` adds unnecessary object allocation overhead via boxing.  Updated to make faster and more efficient.  <details> <summary>JMH Benchmark</summary>  ```java import org.openjdk.jmh.annotations.*; import java.util.Collection; import java.util.concurrent.TimeUnit; import java.util.stream.*;  @BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.NANOSECONDS) @Warmup(iterations = 5  time = 1  timeUnit = TimeUnit.SECONDS) @Measurement(iterations = 3  time = 1  timeUnit = TimeUnit.SECONDS) @State(Scope.Benchmark) public class SumStreamBenchmark { private static final Collection<Long> values = LongStream.range(1  100000) .boxed() .collect(Collectors.toSet());  @Benchmark public long reduce() { return values.stream() .reduce(0L  Long::sum); }  @Benchmark public long mapToLong() { return values.stream() .mapToLong(Long::longValue) .sum(); } } ``` </details>  | Benchmark           | Time (ms/op) | Allocation Rate (B/op) | |---------------------|--------------|------------------------| | `reduce` (before)   | 0.719        | 2 400 000              | | `mapToLong` (after) | 0.483        | 233                    |  GitOrigin-RevId: 7d76911bdcd53cf0c483ab766f11a5834062a432
hazelcast,hazelcast,8ca36349f45bb169562d6797bdcb57cfcf26616a,https://github.com/hazelcast/hazelcast/commit/8ca36349f45bb169562d6797bdcb57cfcf26616a,Improve Metrics performance by replacing `AtomicLongFieldUpdater#lazySet` with `VarHandle#setOpaque` (#4074)  In https://github.com/hazelcast/hazelcast/pull/24620 we updated `SwCounter` to use `VarHandle#setOpaque` instead of `AtomicLongFieldUpdater#lazySet`.  We can apply this same change to other metrics  too.  This (micro-optimisation) shows a ~75% improvement in [benchmarks](https://github.com/hazelcast/internal-benchmarks/pull/56): | Method                              	| ns/op 	| |-------------------------------------	|-------	| | `AtomicIntegerFieldUpdater#lazySet` 	| 0.61 	| | `VarHandle#setOpaque`               	| 0.35	|  GitOrigin-RevId: d7538028b203110a3b98d102321b870332cf7b3e
hazelcast,hazelcast,576bed00fbae560622d3446a87ff9477fc3f6d62,https://github.com/hazelcast/hazelcast/commit/576bed00fbae560622d3446a87ff9477fc3f6d62,Correct ByteArrayObjectDataOutput capacity for large inputs [HZG-346] (#4099)  There is a demonstrable large performance hit (100x) when an input length of more than `Integer.MAX_VALUE / 2` is provided. This is due to resizing being done in increments of `len` passed to `ensureAvailable(int len)` when doubling the current buffer length with a bit shift would overflow and become a negative value.  This means that the buffer is potentially continuously extended by small increments  instead of a single large increment (which is the intention with a doubling strategy).  This PR resolves the issue by catching the overflow case and setting the buffer's length to a `MAX_ARRAY_SIZE` value  which is approximately the largest supported array size possible.  This is the simplest solution to the problem encountered  and fits with the current doubling strategy within the limits of the JVM. We could overhaul our buffering mechanism entirely and use a segmented approach  trading some additional overheads for a more robust solution that supports larger inputs - this is definitely a more complex solution  and is likely overkill for Hazelcast's use-case. I discussed these options with @gbarnett-hz and he agreed that the simple approach is sufficient for now.  This PR also includes a test case that increases the existing `ByteArrayObjectDataOutput` test coverage.  Fixes https://hazelcast.atlassian.net/browse/HZG-346 Closes https://github.com/hazelcast/hazelcast/issues/26422  GitOrigin-RevId: aaf54ed54a7f6c9238858e854938bdfd1d66a01a
hazelcast,hazelcast,8b9a0fc4055e30d1ed58c9a49b23311b811c99c7,https://github.com/hazelcast/hazelcast/commit/8b9a0fc4055e30d1ed58c9a49b23311b811c99c7,Use dedicated thread pool for vector collection search [AI-214] (#4084)  Introduces new `hz:vector-query` executor with default size equal to number of physical CPUs. This implements the decision from ADR-00046. This change will decrease vector search performance in default configuration but will avoid very high CPU utilisation. For achieving best performance adjusting of `hz:vector-query` pool size is recommended.  Small generalisation of `HealthMonitor` was needed to provide vector query executor statistics in Enterprise version only if vector collection license is present.  `SamplingNodeExtension` did not forward `getLicense()` invocation to wrapped `NodeExtension`. Apparently this has not yet caused any problems but could cause problems in the future  so it was fixed.  Fixes https://hazelcast.atlassian.net/browse/AI-214  Breaking changes: * different thread pool will be used for vector searches: `hz:vector-query` instead of `hz:query`. If `hz:query` executor was customised/tuned for vector collection the settings will have to be migrated to `hz:vector-query`.  GitOrigin-RevId: e34987e321fdfbd356a03b1a63a60cc0b2ade8b9
hazelcast,hazelcast,ff783f3b3ff5fd796deda27007f710417a993e65,https://github.com/hazelcast/hazelcast/commit/ff783f3b3ff5fd796deda27007f710417a993e65,Replace `"" + i` with `String.valueOf(i)` (#4068)  It's easier to read and _probably_ more performant - updated where sensible.  GitOrigin-RevId: 05f69ff74bf7ce47dd15741865d528eac68d08d5
hazelcast,hazelcast,9b1d5f63e7c0c65cb1881dd0fd69cb0e968fed40,https://github.com/hazelcast/hazelcast/commit/9b1d5f63e7c0c65cb1881dd0fd69cb0e968fed40,Add detailed vector search metrics [AI-296] (#3877)  Adds 2 useful metrics for understanding vector search performance: - `searchIndexVisitedNodes` - number of visited nodes in graph which is equal to number of distance calculations  this in turn is one of the most time-consuming operations during search - `searchIndexQueryCount` - number of searches on vector index. Can be useful to understand retries due to concurrent modifications. Without retires and PartitionPredicate filters `searchIndexQueryCount = searchCount * partitionCount` (non-empty partitions only)  anything more than that indicates that there were some retries.  Fixes https://hazelcast.atlassian.net/browse/AI-296  GitOrigin-RevId: 61c987790399f20539aa610ca8d13f6700defb5a
hazelcast,hazelcast,51c6f26c53e6604152148a1d2c100f3db7be7b5a,https://github.com/hazelcast/hazelcast/commit/51c6f26c53e6604152148a1d2c100f3db7be7b5a,Add support for blocking backup operations [AI-192] (#3846)  This PR introduces support for blocking and optionally offloaded backup operations.  ### Blocking backup operations  Up until now the only available mechanism for long-running backup operations was offloading  optionally using Step Engine implemented for IMap. Blocking backup operations introduced in this PR use some of Step Engine support code (in particular delayed sending of backup ACK) and use `OperationParker` to implementing waiting. `OperationParker` was extended to be aware of backup operations. Before its logic was well suited only for primary operations.  Blocking backup operation have some unique traits: - backup operation should not be cancelled when original member or client disconnects  unlike for primary operations - they can be parked both on source and destination on migration. Primary operations can ever be parked only on migration source - they execute on the owner and owner is never a destination of migration. - some migrations are backup->backup (changing replica index). In some such cases parked operations should be kept on the migration source (but not destination) as partition data is not updated by migration on source. This uses similar checks to the ones determining if the partition data should be retained or deleted after migration commit and is implemented by `BlockingBackupOperation.shouldKeepAfterMigration` - the operations should be properly cleared during migration to avoid executing stray operations when the partition/replica is migrated again to the same node - the operations should be cleared also after replica sync on destination - if the operation is rejected from execution (eg. by `metWithPreconditions` or other exception) it should mark replica as dirty to trigger anti-entropy task (note that it is not triggered immediately). This is similar to offloaded backup operations. This is the last resort if the backup operation cannot be executed properly and in order. This should happen rarely and is quite costly as it triggers replication of entire fragment partition (usually partition of single data structure). - backup operations must preserve order or alternatively be commutative (rare case). This affects requirements on `shouldWait` implementation - it is not allowed to return `true` for given operation once it returned `false` - because of the ordering requirements all non-commutative blocking backup operations for given data structure partition must block on the same condition and `WaitNotifyKey`  Additionally blocking backups can also be offloaded  but offloaded backups need to take care for ACK sending and error handling on their own if not using Step Engine.  With blocking backup operations `isClusterSafe`/`isMemberSafe` checks are not fully reliable. They will return true even if there are some parked blocking backups. Such backup operations can be lost in case of owner crash. This is hard to observe and can happen only for invocations that did not get their backup ACK in time.  Note: proper discarding of backup operations during migration and promotion and ensuring that the replicas are not silently out-of-sync is the most tricky and hard to test part of this change.  ### Impact on existing workloads  Currently no data structure uses blocking backups  so there will be no direct impact.  However some impact on migration and replication performance is possible in cases with large number of blocking operations. After the change the list of parked operations will have to be traversed in more cases than before  not only after migration on destination (as originally) but also on source and after replica sync on backup. `WaitSet` is not partitioned based on partitionId or replicaIndex or namespace.  Part of https://hazelcast.atlassian.net/browse/AI-192  GitOrigin-RevId: 4cbf961ac0d7ddb88448c8a00961ba62bc925dd6
hazelcast,hazelcast,694e349f4251fe3643d818e4235b211219c35338,https://github.com/hazelcast/hazelcast/commit/694e349f4251fe3643d818e4235b211219c35338,Log basic statistics after backup promotions [AI-293] (#3848)  The statistics can be useful for understanding timing and performance of backup promotions. We already log statistics for migrations. These statistics will be logged only in case of member leaving the cluster ungracefully so should have a minimal impact on normal operation.  Fixes: https://hazelcast.atlassian.net/browse/AI-293 GitOrigin-RevId: 070c61c850b1f71504c2c685f7c9544023afa834
hazelcast,hazelcast,82925e2c6da852347e8a12a1e15475e812a0524a,https://github.com/hazelcast/hazelcast/commit/82925e2c6da852347e8a12a1e15475e812a0524a,Unwrap OperationFactoryWrapper in InvocationProfiler and OperationProfiler output [HZG-258] (#3655)  Currently when enabled the `InvocationProfiler` and `OperationProfiler` `DiagnosticPlugins` record latency distributions of invocations and operations. Unfortunately some operations end up wrapped in the `OperationFactoryWrapper` class. This makes it hard to precisely understand what the cluster is doing and to diagnose performance issues.  This change will incur a small increase in heap usage to store the finer-grained `LatencyDistribution` objects when the profiler is enabled. Analysis shows each `LatencyDistribution` only retains a few hundred bytes so overall impact on node memory usage will be negligible.  Fixes https://hazelcast.atlassian.net/browse/HZG-258  Checklist: - [x] Labels (`Team:`  `Type:`  `Source:`  `Module:`) and Milestone set - [x] Add `Add to Release Notes` label if changes should be mentioned in release notes or `Not Release Notes content` if changes are not relevant for release notes - [x] Architecture Design Record (ADR) reviewed and signed-off if this PR represents a significant architectural change - [x] Request reviewers if possible - [ ] Send backports/forwardports if fix needs to be applied to past/future releases GitOrigin-RevId: a0b64502534c9f287b67db90f5e2fc0ae2e234e0
hazelcast,hazelcast,324dd593fba6c5c903d3b166fe7280c5dbaab418,https://github.com/hazelcast/hazelcast/commit/324dd593fba6c5c903d3b166fe7280c5dbaab418,Avoid passing explicit default value to new `Atomic` constructors to improve performance (#3535)  `new AtomicInteger(0)` is equivalent to `new AtomicInteger()`  but has a greater overhead due to an additional (redundant) `volatile` write.  Using the implicit default in these scenarios offers a performance improvement [benchmarked at ~27%](https://github.com/hazelcast/internal-benchmarks/pull/52) - however  our usage of these constructors is typically limited to fields inside objects which means the throughput improvement to the application is unlikely to be significant.  Updated [`Atomic` classes](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/package-frame.html) constructors' to use an implicit default definition  _where possible_  using: ``` find . -name "*.java" -exec sed -i '' \ -e 's/new AtomicBoolean(false)/new AtomicBoolean()/g' \ -e 's/new AtomicInteger(0)/new AtomicInteger()/g' \ -e 's/new AtomicIntegerArray(null)/new AtomicIntegerArray()/g' \ -e 's/new AtomicLong(0)/new AtomicLong()/g' \ -e 's/new AtomicLong(0l)/new AtomicLong()/g' \ -e 's/new AtomicLong(0L)/new AtomicLong()/g' \ -e 's/new AtomicLongArray(null)/new AtomicLongArray()/g' \ -e 's/new AtomicReference(null)/new AtomicReference()/g' \ -e 's/new AtomicReferenceArray(null)/new AtomicReferenceArray()/g' \ {} + ```  This was [originally raised](https://github.com/hazelcast/hazelcast/pull/22373) by @dreis2211  so full credit to Christoph for flagging this. Due to a delay in reviewing  the PR could not be merged: - too many conflicts - pre-`hazelcast-mono`  I've re-implemented this in a new PR to update `hazelcast-mono`'s sources as well (rather than just `hazelcast`).  Fixes: https://github.com/hazelcast/hazelcast/pull/22373  Co-authored-by: Christoph Dreis <6304496+dreis2211@users.noreply.github.com> GitOrigin-RevId: 4c9b504342ca2e6fad6f23cfe34730802153a8bf
hazelcast,hazelcast,8563cd401bccd84c63ad971412524cc8511dc726,https://github.com/hazelcast/hazelcast/commit/8563cd401bccd84c63ad971412524cc8511dc726,Do not destroy UCN NodeEngine context in OperationExecutorImpl [HZG-193] (#3453)  It is safe to remove this cleanup as it will be performed in `DefaultNodeExtension#onThreadStop` anyway. The only way the `NodeEngine` can be invalidated is by restarting the JVM.  In the in `OperationExecutorImpl#shutdown` method we are explicitly waiting for some threads to shutdown (not terminating)  so we shouldn't clear context at this stage.  This issue was discovered during development of https://github.com/hazelcast/hazelcast-mono/pull/3386 which terminates an instance on it's own operation thread to simulate a coordinator crash during Jet snapshot operations.  Checklist: - [x] Labels (`Team:`  `Type:`  `Source:`  `Module:`) and Milestone set - [x] Add `Add to Release Notes` label if changes should be mentioned in release notes or `Not Release Notes content` if changes are not relevant for release notes - [x] Architecture Design Record (ADR) reviewed and signed-off if this PR represents a significant architectural change - [x] Request reviewers if possible - [x] New public APIs have `@Nonnull/@Nullable` annotations - [x] New public APIs have `@since` tags in Javadoc - [ ] Send backports/forwardports if fix needs to be applied to past/future releases GitOrigin-RevId: 6ab998c52b5b94d657b5d62590cb842e389089fb
hazelcast,hazelcast,0a75bf11300a8d325ee6ef4dbb9838149476cd44,https://github.com/hazelcast/hazelcast/commit/0a75bf11300a8d325ee6ef4dbb9838149476cd44,Add control of search list size in similarity search [AI-140] (#3229)  The most important parameter that affects search results quality is search list size. Before this change it was always equal to `topK` or `partitionLimit` (if supplied)  which produced poorer quality results with smaller values.  Explicit control of that parameter is introduced via `efSearch` hint. This should improve in particular searches with small topK (e.g. 10)  where in some benchmarks it was hard to get >95% precision at all.  `partitionLimit` hint remains unchanged and can be used for further performance tuning. During initial tests it was observed that with good `efSearch` value  `partitionLimit` can be smaller than `topK` without compromising the quality.  Benchmarking was performed using `dbpedia-openai-1M-1536` dataset which has default topK = 10 on Apple M1 laptop  JDK 21 and Panama enabled. Single-member cluster with `-Xmx10g` and 16 partitions was used. Index parameters were max_degree = 16 and 32 (this is after upgrade to JVector 3  equivalent to 8 and 16 respectively in previous benchmarks)  ef_construction = 128 and disabled deduplication.  ![image](https://github.com/user-attachments/assets/462d2502-5865-4301-a063-6a4ad61c77a1)  ![image](https://github.com/user-attachments/assets/040f2121-730a-4be1-9b2b-f5093a3651df)  Labels on the line are `efSearch` value.  The results show that precision depends predominantly on `efSearch`. `partitionLimit` has very small impact when `efSearch` is sufficiently large. For reference  results with topK=100 are provided (previously recommended way to improve quality in low-topK searches). Quality is similar for the same effective search list size  but searches with inflated `topK` are slower.  Increasing `efSearch` past 100 brings very little improvement in quality but significant increase in run time (in this case).  Important to note about these benchmarks:  1. the results are quite noisy  likely due to running on a laptop  but they prove the main point about precision. Proper benchmarks will be done later. 2. search with `partitionLimit=10` is faster (!) than with `partitionLimit=5`. This however may be an artifact of benchmark methodology  lack of warmup  JIT compilation. This will be investigated further. 3. from initial tests is seems that `efSearch` value around 50-100 should be a good choice  but the sample is very small (only a few tests) so this may not turn out to be true. 4. `partitionLimit=3` results in slightly but visibly decreased quality  even though 3*16=48 entries in total are fetched from partitions for topK=10 query. It seems that the probability of omitting good candidate (because it is 4th best in partition) is non-negligible in this case. GitOrigin-RevId: a9e809c711d825f81a3f6b548ec0ee95c68a37d6
hazelcast,internal-benchmarks,00e5df77344b093bca9e361d6f7cc8cbe7d4454c,https://github.com/hazelcast/internal-benchmarks/commit/00e5df77344b093bca9e361d6f7cc8cbe7d4454c,Improve float array serialization performance in client protocol (#2334)  Use `ByteBuffer.asFloatBuffer()` to serialize float array more efficiently in client protocol codecs in similar way to https://github.com/hazelcast/hazelcast-mono/pull/2099.  Benchmarks:
hazelcast,hazelcast,80cd36c76c96f100c59b67d932682d85d909333f,https://github.com/hazelcast/hazelcast/commit/80cd36c76c96f100c59b67d932682d85d909333f,`0For` - original implementation - `1ByteBuf` - implementation from this PR - `serialization2Codec` and `deserialization2Codec` use codec from this PR and demonstrate overhead of allocation and zeroing on top copying floats  ``` Benchmark                                                       (arraySize)  Mode  Cnt    Score    Error  Units FloatArrayClientSerializationBenchmark.deserialization0For                1  avgt    6    2.731 ±  0.035  ns/op FloatArrayClientSerializationBenchmark.deserialization0For                2  avgt    6    3.536 ±  0.122  ns/op FloatArrayClientSerializationBenchmark.deserialization0For                8  avgt    6    7.811 ±  0.079  ns/op FloatArrayClientSerializationBenchmark.deserialization0For               16  avgt    6   11.726 ±  0.302  ns/op FloatArrayClientSerializationBenchmark.deserialization0For              100  avgt    6   57.087 ±  0.463  ns/op FloatArrayClientSerializationBenchmark.deserialization0For             1000  avgt    6  539.035 ±  3.802  ns/op FloatArrayClientSerializationBenchmark.deserialization1ByteBuf            1  avgt    6    2.085 ±  0.030  ns/op FloatArrayClientSerializationBenchmark.deserialization1ByteBuf            2  avgt    6    2.885 ±  0.060  ns/op FloatArrayClientSerializationBenchmark.deserialization1ByteBuf            8  avgt    6    2.886 ±  0.042  ns/op FloatArrayClientSerializationBenchmark.deserialization1ByteBuf           16  avgt    6    2.992 ±  0.034  ns/op FloatArrayClientSerializationBenchmark.deserialization1ByteBuf          100  avgt    6    8.307 ±  0.472  ns/op FloatArrayClientSerializationBenchmark.deserialization1ByteBuf         1000  avgt    6   75.898 ± 12.185  ns/op FloatArrayClientSerializationBenchmark.deserialization2Codec              1  avgt    6    3.175 ±  0.057  ns/op FloatArrayClientSerializationBenchmark.deserialization2Codec              2  avgt    6    4.167 ±  0.384  ns/op FloatArrayClientSerializationBenchmark.deserialization2Codec              8  avgt    6    4.322 ±  0.043  ns/op FloatArrayClientSerializationBenchmark.deserialization2Codec             16  avgt    6    5.595 ±  0.056  ns/op FloatArrayClientSerializationBenchmark.deserialization2Codec            100  avgt    6   16.472 ±  3.107  ns/op FloatArrayClientSerializationBenchmark.deserialization2Codec           1000  avgt    6  183.161 ±  8.089  ns/op FloatArrayClientSerializationBenchmark.serialization0For                  1  avgt    6    2.892 ±  0.056  ns/op FloatArrayClientSerializationBenchmark.serialization0For                  2  avgt    6    3.626 ±  0.015  ns/op FloatArrayClientSerializationBenchmark.serialization0For                  8  avgt    6    7.967 ±  0.028  ns/op FloatArrayClientSerializationBenchmark.serialization0For                 16  avgt    6   14.022 ±  0.081  ns/op FloatArrayClientSerializationBenchmark.serialization0For                100  avgt    6   71.627 ±  0.836  ns/op FloatArrayClientSerializationBenchmark.serialization0For               1000  avgt    6  659.696 ± 13.669  ns/op FloatArrayClientSerializationBenchmark.serialization1ByteBuf              1  avgt    6    2.030 ±  0.004  ns/op FloatArrayClientSerializationBenchmark.serialization1ByteBuf              2  avgt    6    2.921 ±  0.118  ns/op FloatArrayClientSerializationBenchmark.serialization1ByteBuf              8  avgt    6    2.899 ±  0.017  ns/op FloatArrayClientSerializationBenchmark.serialization1ByteBuf             16  avgt    6    3.015 ±  0.014  ns/op FloatArrayClientSerializationBenchmark.serialization1ByteBuf            100  avgt    6    8.579 ±  0.253  ns/op FloatArrayClientSerializationBenchmark.serialization1ByteBuf           1000  avgt    6   81.885 ± 10.505  ns/op FloatArrayClientSerializationBenchmark.serialization2Codec                1  avgt    6    4.384 ±  0.029  ns/op FloatArrayClientSerializationBenchmark.serialization2Codec                2  avgt    6    5.101 ±  0.085  ns/op FloatArrayClientSerializationBenchmark.serialization2Codec                8  avgt    6    7.063 ± 10.935  ns/op FloatArrayClientSerializationBenchmark.serialization2Codec               16  avgt    6    7.294 ±  0.114  ns/op FloatArrayClientSerializationBenchmark.serialization2Codec              100  avgt    6   18.160 ±  0.392  ns/op FloatArrayClientSerializationBenchmark.serialization2Codec             1000  avgt    6  191.175 ±  8.991  ns/op ```  Improvement is clearly visible. New implementation boils down more-or-less to allocate+zero+memcpy. There is still redundant target buffer zeroing (about 40% of total time with ByteBuf implementation)  which could be avoided by using `Unsafe.allocateUninitializedArray`. Compiler does not eliminate zeroing automatically in any of the the implementations.  GitOrigin-RevId: c55cd251fdfb678b9b4196e94cb3129445f529da
hazelcast,hazelcast,c7d012c34077b8a36b7f124943e02dd3b6b6e7e4,https://github.com/hazelcast/hazelcast/commit/c7d012c34077b8a36b7f124943e02dd3b6b6e7e4,Improve float array serialization performance (#2099)  Use `ByteBuffer. asFloatBuffer()` to serialize float array more efficiently. Inspired by https://github.com/hazelcast/hazelcast-mono/pull/1402/commits/5cbb2dbd038aa4fc52b48640fa24c09df220a82c  Important caveats: - only `ByteArrayObjectDataInput` and `Output` have optimised implementation. Unsafe and Fixed variants and `ObjectDataOutputStream` have unchanged implementation - some performance tests should be performed  in particular if this implementation behaves has really better performance for different array sizes (big and small - vectors are expected to be a on the order of 1k floats) and if it works equally well for different combinations of native and requested endianness - this implementation is used in many places  eg. compact serialization  so the impact may be broad  but float array is probably not often used - part of the performance improvement might be caused by invoking `ensureAvailable` once instead of for each float. Also  with the default buffer size being 4kB  for common vector sizes at least 1 resize is expected (with factor 2) - other arrays (int  long  etc.) were not analyzed. They might benefit from similar operations as well  ## Performance tests  Synthetic benchmark shows that 1 reallocation during serialization is ~2x more costly than entire serialization itself  so the buffer size was chosen to avoid reallocation. This may be interesting tuning parameter eg. to avoid reallcation when serializing vector collection search request.  Benchmark settings - running on Apple M1:  ``` # JMH version: 1.37 # VM version: JDK 21.0.2  OpenJDK 64-Bit Server VM  21.0.2+13-58 # Blackhole mode: compiler (auto-detected  use -Djmh.blackhole.autoDetect=false to disable) # Warmup: 3 iterations  10 s each # Measurement: 3 iterations  10 s each # Timeout: 10 min per iteration # Threads: 1 thread  will synchronize iterations # Benchmark mode: Average time  time/op ```  ### Serialization  #### Before  ``` Benchmark                                       (arraySize)  (nativeOrder)  Mode  Cnt     Score   Error  Units FloatArraySerializationBenchmark.serialization            1           true  avgt    6     3.677 ± 0.011  ns/op FloatArraySerializationBenchmark.serialization            2           true  avgt    6     4.977 ± 0.012  ns/op FloatArraySerializationBenchmark.serialization            8           true  avgt    6    12.558 ± 0.026  ns/op FloatArraySerializationBenchmark.serialization           16           true  avgt    6    22.719 ± 0.024  ns/op FloatArraySerializationBenchmark.serialization          100           true  avgt    6   128.823 ± 0.476  ns/op FloatArraySerializationBenchmark.serialization         1000           true  avgt    6  1268.861 ± 2.724  ns/op  FloatArraySerializationBenchmark.serialization            1          false  avgt    6     3.702 ± 0.010  ns/op FloatArraySerializationBenchmark.serialization            2          false  avgt    6     4.989 ± 0.007  ns/op FloatArraySerializationBenchmark.serialization            8          false  avgt    6    12.656 ± 0.102  ns/op FloatArraySerializationBenchmark.serialization           16          false  avgt    6    22.709 ± 0.031  ns/op FloatArraySerializationBenchmark.serialization          100          false  avgt    6   128.768 ± 0.337  ns/op FloatArraySerializationBenchmark.serialization         1000          false  avgt    6  1280.513 ± 3.518  ns/op ```  #### After  ``` Benchmark                                       (arraySize)  (nativeOrder)  Mode  Cnt    Score   Error  Units FloatArraySerializationBenchmark.serialization            1           true  avgt    6    4.739 ± 0.011  ns/op FloatArraySerializationBenchmark.serialization            2           true  avgt    6    5.673 ± 0.009  ns/op FloatArraySerializationBenchmark.serialization            8           true  avgt    6    5.896 ± 0.012  ns/op FloatArraySerializationBenchmark.serialization           16           true  avgt    6    6.107 ± 0.023  ns/op FloatArraySerializationBenchmark.serialization          100           true  avgt    6   11.906 ± 0.441  ns/op FloatArraySerializationBenchmark.serialization         1000           true  avgt    6   81.133 ± 8.741  ns/op  FloatArraySerializationBenchmark.serialization            1          false  avgt    6    4.774 ± 0.011  ns/op FloatArraySerializationBenchmark.serialization            2          false  avgt    6   14.922 ± 0.161  ns/op FloatArraySerializationBenchmark.serialization            8          false  avgt    6   15.273 ± 0.106  ns/op FloatArraySerializationBenchmark.serialization           16          false  avgt    6   16.174 ± 0.051  ns/op FloatArraySerializationBenchmark.serialization          100          false  avgt    6   25.014 ± 3.435  ns/op FloatArraySerializationBenchmark.serialization         1000          false  avgt    6  121.114 ± 1.065  ns/op ```  ### Deserialization  #### Before  ``` Benchmark                                           (arraySize)  (nativeOrder)  Mode  Cnt    Score    Error  Units FloatArrayDeserializationBenchmark.deserialization            1           true  avgt    6   12.173 ±  0.238  ns/op FloatArrayDeserializationBenchmark.deserialization            2           true  avgt    6   18.845 ±  0.323  ns/op FloatArrayDeserializationBenchmark.deserialization            8           true  avgt    6   20.399 ±  0.979  ns/op FloatArrayDeserializationBenchmark.deserialization           16           true  avgt    6   26.874 ±  7.764  ns/op FloatArrayDeserializationBenchmark.deserialization          100           true  avgt    6   73.798 ±  0.220  ns/op FloatArrayDeserializationBenchmark.deserialization         1000           true  avgt    6  731.697 ± 10.041  ns/op  FloatArrayDeserializationBenchmark.deserialization            1          false  avgt    6   11.945 ±  0.306  ns/op FloatArrayDeserializationBenchmark.deserialization            2          false  avgt    6   16.146 ±  0.560  ns/op FloatArrayDeserializationBenchmark.deserialization            8          false  avgt    6   20.968 ±  0.384  ns/op FloatArrayDeserializationBenchmark.deserialization           16          false  avgt    6   24.412 ±  0.733  ns/op FloatArrayDeserializationBenchmark.deserialization          100          false  avgt    6   73.652 ±  0.106  ns/op FloatArrayDeserializationBenchmark.deserialization         1000          false  avgt    6  729.102 ±  5.406  ns/op ```  #### After  ``` Benchmark                                           (arraySize)  (nativeOrder)  Mode  Cnt    Score     Error  Units FloatArrayDeserializationBenchmark.deserialization            1           true  avgt    6   14.846 ±   0.505  ns/op FloatArrayDeserializationBenchmark.deserialization            2           true  avgt    6   23.800 ±   0.100  ns/op FloatArrayDeserializationBenchmark.deserialization            8           true  avgt    6   16.897 ±   1.230  ns/op FloatArrayDeserializationBenchmark.deserialization           16           true  avgt    6   24.230 ±   0.556  ns/op FloatArrayDeserializationBenchmark.deserialization          100           true  avgt    6   34.877 ±   0.348  ns/op FloatArrayDeserializationBenchmark.deserialization         1000           true  avgt    6  216.405 ±   7.042  ns/op  FloatArrayDeserializationBenchmark.deserialization            1          false  avgt    6   16.093 ±   0.110  ns/op FloatArrayDeserializationBenchmark.deserialization            2          false  avgt    6   29.848 ±   0.072  ns/op FloatArrayDeserializationBenchmark.deserialization            8          false  avgt    6   30.159 ±   0.351  ns/op FloatArrayDeserializationBenchmark.deserialization           16          false  avgt    6   30.666 ±   0.305  ns/op FloatArrayDeserializationBenchmark.deserialization          100          false  avgt    6   41.933 ±   0.262  ns/op FloatArrayDeserializationBenchmark.deserialization         1000          false  avgt    6  314.484 ± 112.088  ns/op ```  ## Observations  - Byte-by-byte serialisation: - is nicely inlined (no method calls) but is not optimised to int/long operations nor vectorized (at least in JVM21 on aarch64 - Apple M1). This is highly inefficient when endianness matches native. This explains also lack of difference between matching and mismatched endianness (before change). - Additionally  there are extra range checks for each byte (see `array[(int) offset]` in `ByteArrayAccessStrategy`). They are optimised using uncommon trap  but int range comparison remains. - ByteBuffer implementation: - depends on endianness  calls to appropriate native memory copy routines  depending if endianness matches or not. - There is some initial cost (maybe alignment  extra checks or pre and post loops in native implementation or possibly some Java implementation used for small sizes) which makes it more costly for small array sizes. Around 8 (endianness matches) or 16 (mismatch) `ByteBuffer` starts outperforming original implementation - Significant stdev is observed for optimised 1000-element array in native order. This effect is repeatable. The theory is that there may be some threshold or alignment which is different on each iteration (CPU cache line alignment  saturated memory bandwidth?): when it is good  the results are ~20% better than otherwise. Another theory is that some random memory activity during the test slows is down. This does not invalidate the general conclusion that new implementation performs better. - Serialisation benchmark does not create any garbage  while deserialisation benchmark creates a new float array for each invocation. That might explain less spectacular gains (2-4x) than for serialization (5-10x).  ## Conclusions  The general trend is that for expected vector sizes for vector search use of the new serialization implementation is ~5-10x faster in serialization and ~2-4x faster in deserialization (not that this is synthetic benchmars  VectorValues and VectorDocument ser/de handles more that just float array). We might consider adding a threshold of minimum size (8-16) below which iterative implementation is used  but that can be done later if small float arrays are found significant in practice and the difference is not very big.  ## Benchmark code  https://github.com/hazelcast/internal-benchmarks/pull/49  GitOrigin-RevId: 71db0212e49bd411ccba00cb92496bded576f902
micronaut-projects,micronaut-core,1a8a8f0e5f2bda5536d3a950ab7d97621f9ec820,https://github.com/micronaut-projects/micronaut-core/commit/1a8a8f0e5f2bda5536d3a950ab7d97621f9ec820,Fix duplicate error in HTTP client (#11727)  When there's two errors  the DefaultHttpClient would try to complete the same future twice  leading to a logged message. This change adds a state transition so that the second exception is handled by AfterContent  which leads to pool-level handling instead.  A test is not possible because the only thing fixed here is a superfluous log statement.
micronaut-projects,micronaut-core,6cbbbd8924145fb1d99d42a7f09f2c7682e18d58,https://github.com/micronaut-projects/micronaut-core/commit/6cbbbd8924145fb1d99d42a7f09f2c7682e18d58,ThreadLocal bean improvements (#11685)  Improve performance and lifecycle management of ThreadLocal beans.  - Add a benchmark - Cache AbstractInitializableBeanDefinition.getScope. This is the majority of the performance improvement - Implement ThreadLocalCustomScope without the AbstractConcurrentCustomScope superclass. This leads to a minor performance improvement (no more locks) - Add optional advanced lifecycle handling. Can be turned on with the ThreadLocal.lifecycle flag. There are two cleanup paths: When a thread terminates  its beans are destroyed through a Cleaner on GC. Separately  when the app context terminates  all those cleaner tasks are invoked immediately.  There's still some performance left on the table  see the benchmark  but it's much better than before.  Fixes #11293
micronaut-projects,micronaut-core,5fdb596ebe12882117aa8d06816cd6f2667e0345,https://github.com/micronaut-projects/micronaut-core/commit/5fdb596ebe12882117aa8d06816cd6f2667e0345,Improve event loop locality of client requests (#11300)  If a request is made on an event loop that is part of the same event loop group that the client is configured to use  prefer making the connection on that same event loop. This keeps all the relevant processing on the same thread and may improve performance.If a request is made on an event loop that is part of the same event loop group that the client is configured to use  prefer making the connection on that same event loop. This keeps all the relevant processing on the same thread and may improve performance.
micronaut-projects,micronaut-core,1369f561f40bb7139d3f60801894378fca6e9a73,https://github.com/micronaut-projects/micronaut-core/commit/1369f561f40bb7139d3f60801894378fca6e9a73,Minor perf optimizations (#11322)  * Minor perf optimizations
micronaut-projects,micronaut-core,05601d6a4ee877d499099014e9548cbe768ac73d,https://github.com/micronaut-projects/micronaut-core/commit/05601d6a4ee877d499099014e9548cbe768ac73d,Inner classes with builder don't get introspected (#11313)  When an inner class is postponed to the next round the binary class name is used to perform a lookup of the refreshed class when processing gets to the next round. This is an issue because the canonical class name needs to be use to lookup elements. This changes the code to use `getCanonicalName()` which revealed another bug where the implementation of `JavaClassElement.getCanonicalName()` was not correct.  ---------  Co-authored-by: Graeme Rocher <graeme.rocher@oracle.com>
micronaut-projects,micronaut-core,bb68d7c72f96276636acbc5599d7eee62d8f1b79,https://github.com/micronaut-projects/micronaut-core/commit/bb68d7c72f96276636acbc5599d7eee62d8f1b79,Type pollution test (#10918)  Add a test to the benchmarks module that runs certain JMH benchmarks with the redhat type pollution agent ( https://github.com/RedHatPerf/type-pollution-agent ) and triggers a test failure if there is excessive type pollution.  This works by adding a new test task which includes the agent  and then using JFRUnit to access the JFR events emitted by the agents. If there are too many thrash events for a single concrete type  details are logged and the test fails.  Additionally: - Add a JMH benchmark that mimics the TechEmpower benchmark - Fix various type pollution issues that were found using the FullHttpStackBenchmark
micronaut-projects,micronaut-core,7fae86545edaaa0ce514cbd457a9422d5be45cda,https://github.com/micronaut-projects/micronaut-core/commit/7fae86545edaaa0ce514cbd457a9422d5be45cda,Uri template rewrite (#10921)  This is an attempt to get rid of the old UriTemplate  UriMatchTemplate  and UriTypeMatchTemplate (not used in Core). The implementation of matching/expanding is very confusing and hard to understand  with all the things being changed in the inherited constructor methods  etc. It does not allow adding any changes that we might need.  The new implementation is split into a parser  an expander  and an alternative URI template matcher. The matcher now matches segments of the path  allowing to match without always using the regexp; this should improve the performance of matching basic /hello or /hello/{world}. In the future  we can implement routing  which will combine similar segments into one  reducing the complexity for N to something better  considering most routes share the same prefix.  The new matcher passes previous tests and is now used to match the routes. However  there are still some cases where the old template is used: resolving conflicts and the URI builder. Next  I will investigate what needs to be aligned with the JAX-RS implementation.  Unfortunately  the classes are public and cannot be removed or changed  so we have a new implementation until v5.
micronaut-projects,micronaut-core,f9551ce2b3a1f9302da4a3a12b2e0b9bf3a2eaa4,https://github.com/micronaut-projects/micronaut-core/commit/f9551ce2b3a1f9302da4a3a12b2e0b9bf3a2eaa4,Fix some instance check type pollution (#10893)  Measured using https://github.com/RedHatPerf/type-pollution-agent in the techempower benchmarks.  This patch fixes some of the instanceof type pollution that could cause scalability issues with many CPUs  and also some instanceof misses that are less problematic but still worth avoiding.
haifengl,smile,600e9731cef2b3377c38b45994a076d1b7828d82,https://github.com/haifengl/smile/commit/600e9731cef2b3377c38b45994a076d1b7828d82,perfer pca/random initialization on large datasets
haifengl,smile,9b8e07b358960f43d36f9b1bca622428fd343e66,https://github.com/haifengl/smile/commit/9b8e07b358960f43d36f9b1bca622428fd343e66,switch to Map as PerfectHash may not exist
haifengl,smile,62129f804637da2e3d95de176bb06902b09da972,https://github.com/haifengl/smile/commit/62129f804637da2e3d95de176bb06902b09da972,refactor StructType to record and use PerfectHash for index lookup
haifengl,smile,2f8197b304d035089fc7d56793169b4e890cbdbe,https://github.com/haifengl/smile/commit/2f8197b304d035089fc7d56793169b4e890cbdbe,code analyzers like Sonar perfer 'static final' over 'final static'
vespa-engine,vespa,8c9d3d9a1d9301682ca0bbac822471f98edc0cd7,https://github.com/vespa-engine/vespa/commit/8c9d3d9a1d9301682ca0bbac822471f98edc0cd7,Merge pull request #33947 from vespa-engine/bratseth/use-performance-optimized-costs  Prioritize flavors by performance optimized cost
vespa-engine,vespa,5083587e953b6f7ba6f6db6ba6656f4a72c01fb2,https://github.com/vespa-engine/vespa/commit/5083587e953b6f7ba6f6db6ba6656f4a72c01fb2,Prioritize flavors by performance optimized cost
vespa-engine,vespa,9e3f277a568397c03cbc35d14806e337a690b9b1,https://github.com/vespa-engine/vespa/commit/9e3f277a568397c03cbc35d14806e337a690b9b1,Only overcommit on bare metal  Prerequisite to tracking performance differences on non-bare metal.
vespa-engine,vespa,b85ad806c49628eb4f65764106c78c803c6aa057,https://github.com/vespa-engine/vespa/commit/b85ad806c49628eb4f65764106c78c803c6aa057,Support weakand configuration for a rank profile.  Makes it possible to configure the following for improved performance: - stopword-limit: Specifies what the weakand considers to be a stopword and dropped during evaluation. - adjust-target: Used to adjust the initial score threshold of the weakand heap.
vespa-engine,vespa,d79c181312f5d44ed573d0a7816dc1ee29368e4e,https://github.com/vespa-engine/vespa/commit/d79c181312f5d44ed573d0a7816dc1ee29368e4e,fix: performance improvements by removing calls to positionAddOffset and removing redundant parsing
vespa-engine,vespa,3a0c22d8744d2070cbb11a7d10609072606677fc,https://github.com/vespa-engine/vespa/commit/3a0c22d8744d2070cbb11a7d10609072606677fc,feat: simple function completion and minor performance improvements
vespa-engine,vespa,528b6bd2d6dcc16ae06e3cd99ae23f3e02879050,https://github.com/vespa-engine/vespa/commit/528b6bd2d6dcc16ae06e3cd99ae23f3e02879050,Merge pull request #31734 from vespa-engine/jonmv/allow-private-endpoints-in-dev-perf  Allow private endpoints (and no publics) in dev and perf
vespa-engine,vespa,5e5c545bdb69a164c3fd22b16d4cd81d980fe552,https://github.com/vespa-engine/vespa/commit/5e5c545bdb69a164c3fd22b16d4cd81d980fe552,Allow private endpoints (and no publics) in dev and perf
graphql-java,graphql-java,0ad655c9ff9f37385447900c4289080fc5108a54,https://github.com/graphql-java/graphql-java/commit/0ad655c9ff9f37385447900c4289080fc5108a54,Merge pull request #3930 from graphql-java/remove-optional-streams-style-code  Removing some of the Optional.map() and .stream() for performance reasons
graphql-java,graphql-java,6fcb38196f6e372eaba3041d6eb3419c2da54a5c,https://github.com/graphql-java/graphql-java/commit/6fcb38196f6e372eaba3041d6eb3419c2da54a5c,Merge pull request #3932 from graphql-java/fpkit-no-longer-uses-streams  FpKit now longer uses streams for performance reasons
graphql-java,graphql-java,24b955a4d926a13b47f746c465ae6ae8fdb4ccf8,https://github.com/graphql-java/graphql-java/commit/24b955a4d926a13b47f746c465ae6ae8fdb4ccf8,Merge pull request #3948 from graphql-java/reduce-performance-forks  reduce forks to 2 to make perf tests faster
graphql-java,graphql-java,2863a126b4c2f0672ce17b6812274d59404581a1,https://github.com/graphql-java/graphql-java/commit/2863a126b4c2f0672ce17b6812274d59404581a1,reduce forks to 2 to make perf tests faster
graphql-java,graphql-java,ade6100ff573e35e8f3e55ba3570be624ad24e5d,https://github.com/graphql-java/graphql-java/commit/ade6100ff573e35e8f3e55ba3570be624ad24e5d,This is a performance improvement for property data fetchers to not create `DataFetcherFactoryEnvironment` objects for simple property fetchers
graphql-java,graphql-java,c81cee8e93f6a4646c9defe51f080c366b57ddbd,https://github.com/graphql-java/graphql-java/commit/c81cee8e93f6a4646c9defe51f080c366b57ddbd,FpKit now longer uses streams for performance reasons - tweak - removed unused code
graphql-java,graphql-java,36fc91c482a22b3119bddad60f74b827d1521ffe,https://github.com/graphql-java/graphql-java/commit/36fc91c482a22b3119bddad60f74b827d1521ffe,FpKit now longer uses streams for performance reasons - tweak
graphql-java,graphql-java,26bb00178f5e146a4820cde488db4924112d732e,https://github.com/graphql-java/graphql-java/commit/26bb00178f5e146a4820cde488db4924112d732e,Revert "Revert "FpKit now longer uses streams for performance reasons""  This reverts commit 262ff2f5a5fcec6df6f39970577af6f803aaab0d.
graphql-java,graphql-java,262ff2f5a5fcec6df6f39970577af6f803aaab0d,https://github.com/graphql-java/graphql-java/commit/262ff2f5a5fcec6df6f39970577af6f803aaab0d,Revert "FpKit now longer uses streams for performance reasons"  This reverts commit 5e7ce823c9f33ab3491d0f5fad129b96359135c8.
graphql-java,graphql-java,5e7ce823c9f33ab3491d0f5fad129b96359135c8,https://github.com/graphql-java/graphql-java/commit/5e7ce823c9f33ab3491d0f5fad129b96359135c8,FpKit now longer uses streams for performance reasons
graphql-java,graphql-java,66d527c23b9e367ec6f98a72c5ab120bb93cbccb,https://github.com/graphql-java/graphql-java/commit/66d527c23b9e367ec6f98a72c5ab120bb93cbccb,Removing some fo the Optional.map() and .stream() for performance reasons.
graphql-java,graphql-java,f1730ccc4d666deb62169cc6841e4f56e91d7c51,https://github.com/graphql-java/graphql-java/commit/f1730ccc4d666deb62169cc6841e4f56e91d7c51,Large in memory query benchmark - moving class to performance
graphql-java,graphql-java,1f4d18e51bd46c5bf00ed0fc5c417e5867b349d8,https://github.com/graphql-java/graphql-java/commit/1f4d18e51bd46c5bf00ed0fc5c417e5867b349d8,some performance naming cleanup  docs
graphql-java,graphql-java,c2d1a3b7a277ffb797f13873463e4c5566e6e28c,https://github.com/graphql-java/graphql-java/commit/c2d1a3b7a277ffb797f13873463e4c5566e6e28c,add ENF performance tests
graphql-java,graphql-java,7e576b1eaad6db2804bdb7f71527a44067ae521c,https://github.com/graphql-java/graphql-java/commit/7e576b1eaad6db2804bdb7f71527a44067ae521c,Implement equals/hashCode for GraphqlErrorImpl  We have a great many tests that verify that the errors emitted from a `DataFetcher` fit a certain shape.  However  verifying equality of these errors is fiddly and error-prone  as we must directly check each individual attribute on every error - this is painful especially when we are trying to perform assertions on a `List` of `GraphQLError`s.  To this end  we add `#equals` / `#hashCode` methods on `GraphqlErrorImpl`. However  it is important to note that `equals` will return `true` if all the attributes match  even if the implementing class is _not_ a `GraphqlErrorImpl`. This is done so that different implementations may be swapped in and out with causing test failures.
graphql-java,graphql-java,ec8bafb0ef8931466b9ec52d59aa1c05bffb2c96,https://github.com/graphql-java/graphql-java/commit/ec8bafb0ef8931466b9ec52d59aa1c05bffb2c96,fix sorting improve performance for calc lower bounds costs
hibernate,hibernate-orm,b7cfe004ca3cf9f468b067a05530cc0cc45b80d8,https://github.com/hibernate/hibernate-orm/commit/b7cfe004ca3cf9f468b067a05530cc0cc45b80d8,use LockTimeoutException instead of PessimisticLockException on Postgres and h2  this is perhaps not *perfectly* correct for Postgres  but I believe it's good enough for our purposes (this error code can occur in some other situations  but they are very unlikely to affect us  it seems to me)
hibernate,hibernate-orm,6877c201b7b9a96e538ccb360deb4d1f4816f3cd,https://github.com/hibernate/hibernate-orm/commit/6877c201b7b9a96e538ccb360deb4d1f4816f3cd,HHH-19208 Adapt javadoc of QuerySettings.QUERY_PLAN_CACHE_ENABLED  More info see: https://discourse.hibernate.org/t/after-upgrade-to-hibernate-6-slower-performance-when-executing-query-the-first-time-due-to-antlr/10614/8
hibernate,hibernate-orm,2eaa094d52c82c48ecd75b9abed1df391041619b,https://github.com/hibernate/hibernate-orm/commit/2eaa094d52c82c48ecd75b9abed1df391041619b,HHH-19063 - Drop forms of SchemaNameResolver performing reflection
hibernate,hibernate-orm,68e1eccd94d1dda7e817c66bd2e42c827bf03beb,https://github.com/hibernate/hibernate-orm/commit/68e1eccd94d1dda7e817c66bd2e42c827bf03beb,remove a superfluous overload of createMutationQuery()
hibernate,hibernate-orm,5ffff1b5b9882ebf98c492d5ecb1bc2347c04e6e,https://github.com/hibernate/hibernate-orm/commit/5ffff1b5b9882ebf98c492d5ecb1bc2347c04e6e,HHH-18976 Get rid of dynamic array instantiation in MultiEntityLoaderStandard  MultiEntityLoaderStandard is used for arbitrary ID types  including IdClass  making it very problematic to instantiate T[] where T is the ID type: in native images  it requires registering T[] for reflection for every T that can possibly be used as an ID type.  Fortunately  MultiEntityLoaderStandard does not  in fact  need concrete-type arrays: Object[] works perfectly well with this implementation  and only the other implementation  MultiIdEntityLoaderArrayParam  actually needs concrete-type arrays. We're truly in a lucky streak  because MultiIdEntityLoaderArrayParam is only used for well-known  basic types such as Integer  which can easily be registered for reflection in native images  and likely will be for other reasons anyway.  Some of the dynamic instantiations were originally introduced to fix the following issue:  * HHH-17201 -- tested in MultiIdEntityLoadTests  The corresponding tests still pass after removing these dynamic array instantiations.
hibernate,hibernate-orm,7ba3e8d1f85c2055b8f6872ab0f779797c6aec7c,https://github.com/hibernate/hibernate-orm/commit/7ba3e8d1f85c2055b8f6872ab0f779797c6aec7c,HHH-18976 Use a clone-based implementation for all array mutability plans  It's more consistent  and happens to get rid of ArrayMutabilityPlan  which involved an unnecessary use of Array.newInstance.  I've also seen claims that clone() performs better than Array.newInstance() due to not having to zero-out the allocated memory  but I doubt that's relevant here.
hibernate,hibernate-orm,0a2c229c49304395337f0882336de8f0d983b6d5,https://github.com/hibernate/hibernate-orm/commit/0a2c229c49304395337f0882336de8f0d983b6d5,HHH-19010 get rid of inheritance of SessionFactoryImplementor from query creation context stuff  Let's make the NodeBuilder the thing that is the SqmCreationContext  and introduce a new thing that performs a similar role for SqlAstCreationContext. The reason here is twofold:  - it is part of my long struggle to decouple query parsing/typing from the SessionFactory  in order to make HibernateProcessor less of a hack and I guess more robust - SessionFactoryImplementor was already starting to be polluted with weird little operations that didn't belong there. That will only get worse with time.
hibernate,hibernate-orm,4c77552746efffb640c37f88edd3614e8c388dbf,https://github.com/hibernate/hibernate-orm/commit/4c77552746efffb640c37f88edd3614e8c388dbf,do more caching of commonly-used services for performance and convenience  expose them via BootstrapContext and SessionFactoryImplementor
hibernate,hibernate-orm,05b8d0dbe92fc8e70d32d55a41f7af82f9882130,https://github.com/hibernate/hibernate-orm/commit/05b8d0dbe92fc8e70d32d55a41f7af82f9882130,minor change  for possibly better performance
hibernate,hibernate-orm,eeaf27e97f14baa28718f5142d6b71b2adf2aa89,https://github.com/hibernate/hibernate-orm/commit/eeaf27e97f14baa28718f5142d6b71b2adf2aa89,HHH-18750 : @OneToMany with @Any mapped in secondary table KO (ClassCastException)  Proposed solution: Follow the same approach as @ManyToOne for the second pass.  For your information : @OneToMany with @ManyToOne that uses a @SecondaryTable works perfectly.  https://hibernate.atlassian.net/browse/HHH-18750
hibernate,hibernate-orm,6f0a6c3b358f8318317482127e4b689e0e389240,https://github.com/hibernate/hibernate-orm/commit/6f0a6c3b358f8318317482127e4b689e0e389240,remove return value of Callback.performCallback  since it was always true and ignored
hibernate,hibernate-orm,cbf4ac803c6cab8a8fc05a8f6a362f8002fe9803,https://github.com/hibernate/hibernate-orm/commit/cbf4ac803c6cab8a8fc05a8f6a362f8002fe9803,HHH-15848 completely new impl of DefaultDirtyCheckEventListener  this is definitely not perfect yet  but it's certainly a much better foundation than the ancient implementation which was bad and side-effecty in all sorts of ways
hibernate,hibernate-orm,aa6e46eb79d0b4346118ccff6b1eda1827d5236a,https://github.com/hibernate/hibernate-orm/commit/aa6e46eb79d0b4346118ccff6b1eda1827d5236a,get rid of use of tracev()  - at most one of these is worse without it - there was even a performance bug due to missing isTraceEnabled()
hibernate,hibernate-orm,9828ad7b3371eb248c85dd78332e8c03f3fbd0c1,https://github.com/hibernate/hibernate-orm/commit/9828ad7b3371eb248c85dd78332e8c03f3fbd0c1,document performance implications of id batching i.e. BatchSize  Signed-off-by: Gavin King <gavin@hibernate.org>
hibernate,hibernate-orm,94b444b4d8e0ac7ec5817642ca711f626ee327f3,https://github.com/hibernate/hibernate-orm/commit/94b444b4d8e0ac7ec5817642ca711f626ee327f3,HHH-18506 Improve flush performance by reducing itable stubs
apache,flink-cdc,250ab43e185ee39b23c4fb6376b84ea9e6741f58,https://github.com/apache/flink-cdc/commit/250ab43e185ee39b23c4fb6376b84ea9e6741f58,[FLINK-37741][cdc-runtime] Fix transform operator performance degradation  This closes  #4007
apache,flink-cdc,77218abd68999efd8cf9ad46f7e830caa024fd7c,https://github.com/apache/flink-cdc/commit/77218abd68999efd8cf9ad46f7e830caa024fd7c,[FLINK-37539][pipeline-connector/paimon] Replace stream with parallelStream to optimize the performance  This closes  #3966
apache,flink-cdc,602abde36ffae5a7395b922e7ac50db6214a3df3,https://github.com/apache/flink-cdc/commit/602abde36ffae5a7395b922e7ac50db6214a3df3,[FLINK-37278][cdc-runtime] Optimize regular schema evolution topology's performance  This closes  #3912.
apache,flink-cdc,fc71888d7a9a84b73f1f6a16f7c755e2b1b40c02,https://github.com/apache/flink-cdc/commit/fc71888d7a9a84b73f1f6a16f7c755e2b1b40c02,[hotfix][cdc-common] Remove duplicated code to improve performance  This closes #3840.  Co-authored-by: zhangchaoming.zcm <zhangchaoming.zcm@antgroup.com>
apache,flink-cdc,2a5828c0ac445461c12a6d387a7b8f12f4dca158,https://github.com/apache/flink-cdc/commit/2a5828c0ac445461c12a6d387a7b8f12f4dca158,[FLINK-35291][runtime] Improve the ROW data deserialization performance of DebeziumEventDeserializationScheme (#3289)  Co-authored-by: liuzeshan <liuzeshan@bytedance.com>
apache,incubator-kie-drools,23ff9dc9ffc3c0bd7a8e91fd65af5ffc2d0d3d5f,https://github.com/apache/incubator-kie-drools/commit/23ff9dc9ffc3c0bd7a8e91fd65af5ffc2d0d3d5f,[incubator-kie-drools-6007] Executable model doesn't report an error when duplicated (#6013)  * removing kie-ci from dependency  because it causes a test failure in KieBaseIncludeTest  * Use canonicalKieModule.getKiePackages() rather than getKieBase()  * null check for kiePackage  * move populateIncludedRuleNameMap out of packages loop  * removed unused FileManager  * performance improvement. Use getModelForKBase instead of getKiePackages  * Fit into build phases  * clean up
robolectric,robolectric,ed81f7e6e73984ac7f66e96ff7a95d0bb85b414c,https://github.com/robolectric/robolectric/commit/ed81f7e6e73984ac7f66e96ff7a95d0bb85b414c,Fix `VerifyError` in `ViewAnimationTest`  The lambda in `HardwareRenderingScreenshot` was extract out of the class  so the API check was not performed. This commit removes the intermediate `ThreadedRenderer` to avoid this issue.
robolectric,robolectric,51ad387dd4eef06f0d8d4762c280c3515310eda8,https://github.com/robolectric/robolectric/commit/51ad387dd4eef06f0d8d4762c280c3515310eda8,Update PackageManager shadows to correctly perform getApplicationInfo() contract when th package is not present.  - throw if called with Package name - return a default icon if called with application info  PiperOrigin-RevId: 738277726
robolectric,robolectric,95d48c6e07138e37730306b4f4c08a364925612e,https://github.com/robolectric/robolectric/commit/95d48c6e07138e37730306b4f4c08a364925612e,Add an API to disable Robolectric's ShadowView.AnimationRunner  When using the Robolectric simulator  the ShadowView.AnimationRunner mechanism prevents View animations from properly functioning. Add an option to disable it.  In real Android  View animations  as set by View.startAnimation  are driven by rendering traversals that occur every draw frame. However  in Robolectric  these rendering traversals do not occur  so ShadowView.AnimationRunner was the solution used to simulate these animations. However  with the simulator  which does perform rendering traversals each frame  ShadowView.AnimationRunner needs to be disabled for animations to function properly.  PiperOrigin-RevId: 738078518
robolectric,robolectric,0d813b7ebaff90b82b96869173c3e445295025da,https://github.com/robolectric/robolectric/commit/0d813b7ebaff90b82b96869173c3e445295025da,Set the default Choreographer frame delay in Robolectric to 15 ms (60 fps)  Previously  the default Choreographer frame delay was 1ms  which approximated 1000fps. This was overkill and caused an excessive number of animation frames to be run  which caused performance issues when running tests that contained long animations. Reduce the default frame delay to 15ms  which approximates 60ps.  This only applies to paused looper mode. Also  this value is still configurable by a system property  since some tests depend on the 1ms value.  Note if your test fails as a result of this CL  just add:  jvm_flags = ["-Drobolectric.defaultFrameDelayMs=1"]  to your android_local_test target(s) that are failing.  PiperOrigin-RevId: 727008546
robolectric,robolectric,cae7375dbbdcc3d3b57d742b656641867e6d5206,https://github.com/robolectric/robolectric/commit/cae7375dbbdcc3d3b57d742b656641867e6d5206,Make the Choreographer frame delay configurable with a system property  The default 1ms frame delay is overkill  it causes an unnecessarily large number of animation frames. Add an option to configure the default frame delay. This is part of a larger effort to increaser the frame delay to something that approximates 60fps. The benefit of this is that animations will require much fewer frames  which will result in better performance.  PiperOrigin-RevId: 725688137
robolectric,robolectric,da246cf14092ebc191765dadfdec1a993a6dbd2b,https://github.com/robolectric/robolectric/commit/da246cf14092ebc191765dadfdec1a993a6dbd2b,Add consistent performance measurement for binary vs native resources.  Add three different performance measurements for the suspected resources hotspots: - load apk assets - get resource value - apply style PiperOrigin-RevId: 682027218
robolectric,robolectric,f5ca3616204d76f2dc88699f36f27e2c6f7a566b,https://github.com/robolectric/robolectric/commit/f5ca3616204d76f2dc88699f36f27e2c6f7a566b,Intercept ShadowNativeAllocationRegistry constructor calls to make classLoader parameter nonnull.  The newly introduced constructor (in version V) now performs a null check on the classLoader argument. We intercept the constructor calls so as to pass a non-null classloader. The classloader would be Robolectric's SandboxClassloader  but the value itself does not affect the behavior of actual class.  PiperOrigin-RevId: 667778649
robolectric,robolectric,65abe75a74c365abdb616731d5621d16754a23e6,https://github.com/robolectric/robolectric/commit/65abe75a74c365abdb616731d5621d16754a23e6,Add perf stat for Choreographer.doFrame for S+  There are some active performance investigations related to Choreographer and animations. Previously we were only keeping track of doFrame for R and below. Add metric collections for S and above.  PiperOrigin-RevId: 665899605
robolectric,robolectric,60322cc18cd43cf212a50a890a260678f5b9ad6f,https://github.com/robolectric/robolectric/commit/60322cc18cd43cf212a50a890a260678f5b9ad6f,Expose SimplePerfStatsReporter#finalReport.  This will allow generating reports at arbirary points if desired. And is especially useful for environments where runtime hook generated reports do not work.  PiperOrigin-RevId: 650330586
robolectric,robolectric,154e85b0968bc118118dfeade7a12996933c2f4d,https://github.com/robolectric/robolectric/commit/154e85b0968bc118118dfeade7a12996933c2f4d,Add support for native SQLiteRawStatement in Android V  Android V introduced a new class  SQLiteRawStatement  that simplifies the API for performing custom SQLite queries.  PiperOrigin-RevId: 637946838
vavr-io,vavr,67866dabffb6bb0704710abeaacd01cd2eaee2a9,https://github.com/vavr-io/vavr/commit/67866dabffb6bb0704710abeaacd01cd2eaee2a9,Replace synchronized method/block with reentrant lock  (#2842) (#2845)  Virtual threads suffer when performing a blocking operation inside a `synchronized` method/block  and it is recommended that the `synchronized` method/block be replaced with a `ReentrantLock`.  More details: https://docs.oracle.com/en/java/javase/21/core/virtual-threads.html#GUID-04C03FFC-066D-4857-85B9-E5A27A875AF9  ----  related: https://github.com/vavr-io/vavr/issues/2760  Co-authored-by: karnsa <karnsa@vmware.com>
vavr-io,vavr,86d1bf3f27fdff24cf78c3baf2db457c70b334d3,https://github.com/vavr-io/vavr/commit/86d1bf3f27fdff24cf78c3baf2db457c70b334d3,Faster LinkedHashMap tail() (#2725)  Change: LinkedHashMap tail() is now constant-time  from being O(size) time and space.  Motivation: We have a use case which looks somewhat like the following.  1. It's effectively a Queue use case (fifo). 2. Iteration order needs to be stable. 3. Elements in the queue frequently need to be verified as present in the queue. 4. Occasionally  removals must occur  which are usually from the head of the queue although occasionally randomly.  We started with a Queue and eventually found performance issues from too much linear scanning. This was not obviously going to be a bottleneck since our queues are usually of very small size  but ended up so. The next type we considered was a LinkedHashSet  using the `head` and `tail` methods to implement a dequeue-like operation and the set operations being sufficient for the other operations. Unfortunately  it seems that `tail` at present copies the entire queue despite not seemingly needing to.  We moved to our own hand-crafted HashSet+Queue structure  but this PR seems like it'd generally be beneficial for users of Vavr!
vavr-io,vavr,2e2373d9f3f9cb6997fc399b51237d20e15aad2b,https://github.com/vavr-io/vavr/commit/2e2373d9f3f9cb6997fc399b51237d20e15aad2b,Performance improvement for List::unfold  List::unfoldLeft (#2689)
MuntashirAkon,AppManager,4c2070e69ebdaf347c946f2fc56b3e6df7170c41,https://github.com/MuntashirAkon/AppManager/commit/4c2070e69ebdaf347c946f2fc56b3e6df7170c41,[Finder] Add support for logical expressions  Each filter option is assigned a unique ID consisting of its type and a number. These unique IDs acts as operands or variables in the expression. The expression supports the following operations in order: - Brackets: Any operand or expression inside the first bracket will be evaluated first. Brackets can also be nested. - And: Logical "and" operation can be performed by separating multiple operands by & (ampersand) symbol. This is the default behavior when no expression is explicitly specified. - Or: Logical "or" operation can be performed using the | (pipe) symbol.  All of above follows the standard Boolean algebraic laws for evaluation. Not operator is not supported  because it already has implicit support in each filter.  Example: Consider the following expression:  apk_size_1 & (app_type_2 | bloatware_3)  For each application item  app_type_2 filter will be matched first as it is the first item inside the brackets. If it returns true (i.e.  the filter matched)  it shall skip checking bloatware_3 filter due to short-circuit and collapse the expression as stated below. If it returns false  it will evaluate bloatware_3 and collapse the expression.  apk_size_1 & true  In this simplified form  first apk_size_1 will be evaluated. If it returns true  the second operand (i.e.  true) will be evaluated and the expression will return true. If it returns false  the second operand will not be evaluated and the expression will return false.  Signed-off-by: Muntashir Al-Islam <muntashirakon@riseup.net>
MuntashirAkon,AppManager,28dd32f24c8ff7e479319d14ae955c6061cdcdc3,https://github.com/MuntashirAkon/AppManager/commit/28dd32f24c8ff7e479319d14ae955c6061cdcdc3,[LogViewer] Add UID and package filtering in Android 7 onwards  UID filtering can be utilized using the `uid` keyword. It supports both numeric UID and its named owner. For example  to filter logs from the Shell user  either `uid:2000` or `uid=:shell` can be used (equal sign is for exact match). If the argument is an non-regex integer  an exact match is performed. Otherwise the typical rules apply.  Package name filtering can be utilized using the `pkg` keyword (equivalent to `package` keyword in Android Studio) which retains all the typical rules. For example  `pkg=:io.github.muntashirakon.AppManager` can be used to filter the logs produced by App Manager release variant.  Signed-off-by: Muntashir Al-Islam <muntashirakon@riseup.net>
MuntashirAkon,AppManager,4b7748c321e7659885f39584197ddc0aab73a979,https://github.com/MuntashirAkon/AppManager/commit/4b7748c321e7659885f39584197ddc0aab73a979,[AppInfo] Perform long operations in the background in onPrepareMenu  Signed-off-by: Muntashir Al-Islam <muntashirakon@riseup.net>
apache,camel,65123ab6ccd5b62c69edb81aff52bb2b3c54245f,https://github.com/apache/camel/commit/65123ab6ccd5b62c69edb81aff52bb2b3c54245f,CAMEL-21845: camel-sql - Improve performance of batch inserts (#17390)  * CAMEL-21845: camel-sql - Improve performance of batch inserts
apache,camel,5e3074215a60a97b511abca9d6b195f30db3edd1,https://github.com/apache/camel/commit/5e3074215a60a97b511abca9d6b195f30db3edd1,CAMEL-21663: fix NPE hurting sjms/sjms2 performance
apache,camel,9b609a1a18e17957d8cd05aebfd42c1fe5d3afb5,https://github.com/apache/camel/commit/9b609a1a18e17957d8cd05aebfd42c1fe5d3afb5,CAMEL-21406: camel-sql - Fix so configuring RowMapperFactory can refer to a bean or class
apache,camel,f617e42d6dc67827475b5dea509de51d5b1d5ebf,https://github.com/apache/camel/commit/f617e42d6dc67827475b5dea509de51d5b1d5ebf,Iterate over the "entrySet" instead of the "keySet"  it improves performance in case there are a lot of beans and routes registered  Signed-off-by: Aurélien Pupier <apupier@redhat.com>
apache,camel,632e5ece55a0d207afcacec851f6904a0bcbb8de,https://github.com/apache/camel/commit/632e5ece55a0d207afcacec851f6904a0bcbb8de,CAMEL-20884 - Replace this call to "replaceAll()" by a call to the "replace()" method.  The underlying implementation of String::replaceAll calls the java.util.regex.Pattern.compile() method each time it is called even if the first argument is not a regular expression. This has a significant performance cost and therefore should be used with care.  Signed-off-by: Aurélien Pupier <apupier@redhat.com>
graphhopper,graphhopper,18bfcc9d530ea0386e7f78ff761a4d5c839382d8,https://github.com/graphhopper/graphhopper/commit/18bfcc9d530ea0386e7f78ff761a4d5c839382d8,gtfs: make rt performant again
apache,hudi,129c0da70c2d68b3c95ba093182222f0c24645dc,https://github.com/apache/hudi/commit/129c0da70c2d68b3c95ba093182222f0c24645dc,[HUDI-9437] Improve Flink Clustering Performance Using Row Reader (#13344)
apache,hudi,6fd14638c36b81ef5024873f4b42ec1af31328f8,https://github.com/apache/hudi/commit/6fd14638c36b81ef5024873f4b42ec1af31328f8,[HUDI-9364] Improve FileSystemView loading performance with large partitions (#13247)
apache,hudi,7dbd6f3c77004c72f8525efbb7662d1c3797c84b,https://github.com/apache/hudi/commit/7dbd6f3c77004c72f8525efbb7662d1c3797c84b,[HUDI-9322] Optimizing performance for hoodie log files writing (#13170)  * [HUDI-9322] Optimizing performance for hoodie log files writing  * eliminate the byte[] copy for avro data block serialization * add BaseAvroPayload#getRecordBytes * refactor the #bufferRecord of HoodieAppendHandle to make it more clear  ---------  Co-authored-by: danny0405 <yuzhao.cyz@gmail.com>
apache,hudi,d2ea523645856dd0cc1873caa7aabd89f6052e14,https://github.com/apache/hudi/commit/d2ea523645856dd0cc1873caa7aabd89f6052e14,[HUDI-9311] Revert HUDI-7146 which causes perf overhead for merging MDT log files (#13136)
apache,hudi,024ecd8729668b36fb12c97e10dd7a34a4ae5135,https://github.com/apache/hudi/commit/024ecd8729668b36fb12c97e10dd7a34a4ae5135,[HUDI-9152] Improve read/write/compaction performance by reusing avro schema (#12949)  1. Introduce JVM level caching for avro schema to reduce the cost of schema comparison.  NOTE: Use cache to cache references to the schema on key links where the schema may be created repeatedly. This ensures that only one variable instance of the same Schema will be used during a JVM lifetime  thus reducing the overhead of schema comparison on important io paths. For most of the cases  we only need to compare whether it is the same reference  there is no need to call the `Schema::equals` method.  2. Cache the frequently reused Schema on the IO code path.  ---------  Signed-off-by: TheR1sing3un <chaoyang@apache.org>
apache,hudi,257979fc3e2951054f7b8e7d9afe611f1415f34c,https://github.com/apache/hudi/commit/257979fc3e2951054f7b8e7d9afe611f1415f34c,[HUDI-8800] Introduce SingleSparkConsistentBucketClusteringExecutionStrategy to improve performance (#12537)
apache,hudi,24f0db68904b78ef10c7594b26660ddbcb0c00c7,https://github.com/apache/hudi/commit/24f0db68904b78ef10c7594b26660ddbcb0c00c7,[HUDI-8678] feat: improve consistent-bucket resizing performance by reducing unnecessary record collecting (#12451)  * feat: improve consistent-bucket resizing performance by reducing unnecessary record collecting * refactor: remove ConsistentHashingBucketInsertPartitioner  ---------  Signed-off-by: TheR1sing3un <chaoyang@apache.org>
apache,hudi,79bcb69361203adc75f7b387bfa91eaa02993bb5,https://github.com/apache/hudi/commit/79bcb69361203adc75f7b387bfa91eaa02993bb5,[HUDI-8787] Improve compaction performance by reducing unnecessary disk access (#12531)  * perf: improve compaction performance by avoid unnecessary disk visiting  1. improve compaction performance by avoid unnecessary disk visiting 2. support push down predicate to `ExternalSpillableMap`  Signed-off-by: TheR1sing3un <chaoyang@apache.org>  * Cosmetic changes  * feat: support RocksDbDiskMap::iterator(filter)  1. support RocksDbDiskMap::iterator(filter) 2. refactor RocksDBDao::iterator to return key-value pairs rather than only values  Signed-off-by: TheR1sing3un <chaoyang@apache.org>  * refactor: remove unused changes  1. remove unused changes  Signed-off-by: TheR1sing3un <chaoyang@apache.org>  ---------  Signed-off-by: TheR1sing3un <chaoyang@apache.org> Co-authored-by: danny0405 <yuzhao.cyz@gmail.com>
apache,hudi,9da3221a79465f3326ae3ac206b08d60864ddcaa,https://github.com/apache/hudi/commit/9da3221a79465f3326ae3ac206b08d60864ddcaa,[HUDI-8781] Optimize executor memory usage during executing clustering (#12515)  * perf: optimize executor memory usage during executing clustering  1. optimize executor memory usage during executing clustering  Signed-off-by: TheR1sing3un <chaoyang@apache.org>  * Cosmetic changes  ---------  Signed-off-by: TheR1sing3un <chaoyang@apache.org> Co-authored-by: danny0405 <yuzhao.cyz@gmail.com>
apache,hudi,76dbdaa65e3e2865d250f862ff23ab0039679e87,https://github.com/apache/hudi/commit/76dbdaa65e3e2865d250f862ff23ab0039679e87,[HUDI-8622] Fix performance regression of tag when written into consistent bucket index table (#12389)  * fix: fix performance regression of tag when written into consistent bucket index table  1. fix performance regression of tag when written into consistent bucket index table 2. unified the tag logic of the bucket index and lazily loaded the required mapper information  ---------  Signed-off-by: TheR1sing3un <chaoyang@apache.org> Co-authored-by: danny0405 <yuzhao.cyz@gmail.com>
apache,hudi,00709d2928d95d36146c23ff6a1ae0d1537a51a0,https://github.com/apache/hudi/commit/00709d2928d95d36146c23ff6a1ae0d1537a51a0,[HUDI-8676] Improve ValidationUtils performance by lazy appending msg (#12450)
apache,hudi,36db1317318a024f6fdd2e356a7c3f792af6a6e5,https://github.com/apache/hudi/commit/36db1317318a024f6fdd2e356a7c3f792af6a6e5,[HUDI-8573] Fix performance degradation in RowDataKeyGen (#12325)  [HUDI-8573] Fix performance degradation in RowDataKeyGen
apache,hudi,0cd28e52873e0c6ef61184047953d5a3feac895b,https://github.com/apache/hudi/commit/0cd28e52873e0c6ef61184047953d5a3feac895b,[MINOR] HoodieAvroUtils performance optimization of createFullName method (#11747)
apache,hudi,cbd2573bdeb2de97aba77f5ba9702c87f26e4ab7,https://github.com/apache/hudi/commit/cbd2573bdeb2de97aba77f5ba9702c87f26e4ab7,[MINOR] Improve performance of generateBucketKey (#11748)  Co-authored-by: Sergey Troshkov <troshkov.sergey@huawei.com>
apache,hudi,98b3d3bac0f31219e5b93b7528516b27b87ea699,https://github.com/apache/hudi/commit/98b3d3bac0f31219e5b93b7528516b27b87ea699,[HUDI-7980] Optimize the configuration content when performing clustering with row writer (#11614)
apache,hudi,8e36fe91715d96785fab63f51b3ab6ae61f2b53c,https://github.com/apache/hudi/commit/8e36fe91715d96785fab63f51b3ab6ae61f2b53c,[HUDI-7924] Capture Latency and Failure Metrics For Hive Table recreation (#11498)  Added latency and failure metrics for recreate table on meta sync failure. Results in pushing new metrics to prometheus which helps in monitoring the performance of recreating table.  ---------  Co-authored-by: Vamsi <vamsi@Vamsis-MacBook-Pro.local> Co-authored-by: Vamsi <vamsi@onehouse.ai>
apache,iotdb,56158b46eab7788770ce4c0387d472ac8cbfe5d1,https://github.com/apache/iotdb/commit/56158b46eab7788770ce4c0387d472ac8cbfe5d1,perf: Optimize the code structure and add comments for CaseWhen
apache,iotdb,7ba971748598654a82c401e0a4aafa94afc70581,https://github.com/apache/iotdb/commit/7ba971748598654a82c401e0a4aafa94afc70581,Optimize partition cache getRegionReplicaSet interface performance by batching (#15396)  * finish  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * enhance  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix ci  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix new arrayList redundantly  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix new arrayList redundantly  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * refine code  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  ---------  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>
apache,iotdb,74a0d306f8138c14406d4fb2ccfc120b9adf5183,https://github.com/apache/iotdb/commit/74a0d306f8138c14406d4fb2ccfc120b9adf5183,perf: avoid reading tsfile on distinct aggregation on tag/attribute column
apache,iotdb,ef8dc52e89f8fa074c9beb5c814b8780aa972ef0,https://github.com/apache/iotdb/commit/ef8dc52e89f8fa074c9beb5c814b8780aa972ef0,perf: add encodeBatch interface to accelerate flushing for sequential insert (#15243)  * perf: batch encode to improve insert performance  * fix IoTDBSimpleQueryIT  * move batchEncodeInfo / times as MemTableFlushTask member
apache,iotdb,da246d7ae06d36852f3bdae4b4910b301c3567ad,https://github.com/apache/iotdb/commit/da246d7ae06d36852f3bdae4b4910b301c3567ad,perf: various types of iterator for multiple tvlists in memchunk (#15114)  * perf: batch iterator for multiple TVLists/alignedTVList  * fix bug 1  * current time  * single TVList/alignedTVList iterator  * fix: queryAlignChuckWithDeletionTest  * fix: pr review  * fix: test cases  * remove clone in sortTvListForFlush
apache,iotdb,cc0a99a456f818dc189cb5c3175bf42a1fe94036,https://github.com/apache/iotdb/commit/cc0a99a456f818dc189cb5c3175bf42a1fe94036,Pipe: Optimize realtime performace when pipe starts after long time stop with heavy data backlog (#15048) (#15057)
apache,iotdb,c557e3e42fcf7687ea772b0822e083d70ab41710,https://github.com/apache/iotdb/commit/c557e3e42fcf7687ea772b0822e083d70ab41710,perf: more adjustment for memtable/tvlist  (#15035)  * fix: no need to synchronize list since sort is already an synchronized method  * fix: optimize mininum time update in TVList  * perf: special case for merge sort iterator when no handover occurs
apache,iotdb,1adc74dffd2a032306aa536be97d879369fd9386,https://github.com/apache/iotdb/commit/1adc74dffd2a032306aa536be97d879369fd9386,fix: memtable enhancement issues (#14994)  * perf: improve seq inserting  * rename MergeSortTvListIterator to MergeSortTVListIterator  * fix: concurrent indices modification during query sort and flush sort
apache,iotdb,b498285c4f413251e2fe3ebe52a5b1530a0b762f,https://github.com/apache/iotdb/commit/b498285c4f413251e2fe3ebe52a5b1530a0b762f,Pipe: Modify MaxAllowedPinnedMemTableCount to adapt to changes in the number of DRs & Modify the implementation of the poll method in PipeRealtimePriorityBlockingQueue to reduce commit queue backlog & Adjust the default thread count related to Pipe for better performance & Significantly reduce pipeMemoryAllocateRetryIntervalMs & Provide a switch for memory control of ConnectorReadFileBuffer (#14917)
apache,iotdb,85326097340d22813446140060ffd3396ce4c536,https://github.com/apache/iotdb/commit/85326097340d22813446140060ffd3396ce4c536,Memtable enhancement for query (#14591)  * Split non_aligned charge text chunk  * dev non_aligned  * dev aligned chunk split  * new type  * dev aligned binary chunk split  * Fix binary size calculatation  * fix IT  * update IoTDBDuplicateTimeIT.java  * fix pipe IT  * change method names  * add ut  * add UT  * remove useless methods  * fix UT  * fix /FileReaderManagerTest  * fix win UT  * add binary test  * Add Aligned UTs  * fix win ut  * improve coverage  * fix comments  * fix windows UT  * fix review  * fix review  * fix review  * target chunk size count non binary  * fix compile  * fix UT  * Tvlist feat new (#14616)  * null bitmap for int tvlist  * update min/max timestamp and sequential part of tvlist during insert  * mutable & immutable tvlists in writable memchunk  * copy-on-write array list  * review comments part 1  * fix unit test errors  * review comments part 2  * push down global time filter  * fix MemPageReaderTest case  * fix memory page offsets error  * synchronized sort & MergeSortTvListIterator bug  * tvlist_sort_threshold config property  * bug fix: * out of mempage bounds check * overlapped data error during query  * optimize TVListIterator & MergeSortTvListIterator  * retrofit encode when tvlist_sort_threshold is zero  * delay sort & statistic generation to query execution  * fix: skip deleted data during encode  * aligned time series part  * fix: MemAlignedChunkReader page offset  * performance issue: * change some list to array * remember row count in tvlist iterator  * fix: memory chunk reader may read more points than expected in one page  * update chunk & page statistic for aligend memchunk by column  * revert: getAlignedValueForQuery  * fix: * CopyOnWriteArrayList for AlignedTVList bitmaps * memory control of column access  * refactor: Tim/Quick/Backward TVList  * refactor: synchronized tvlist method: sort  putXXX  * refactor: change list to array in AlignedTVList iterator  * revert: remove CopyOnWriteArrayList  * refactor: clone MergeSort iterator from ReadOnlyChunk  * fix: clone working tvlist during flush if there is query on it  * fix: writable mem chunk flush conditions  * refactor: add annotation and variable/function rename  * fix: * remove delete method in BinaryTVList * filter deleted data in WritableMemChunk encode  * fix: remove getSortedTvListForQuery in SeriesRegionScan  * fix: TsFileProcessorTest unit test  * fix: IoTDBNullIdQueryIT.noMeasurementColumnsSelectTest  * fix: delete column of aligned time series  * fix: aligned timeseries encode bug  * fix: IoTDBGroupByNaturalMonthIT  * remove avgSeriesPointNumberThreshold setting  * fix: IoTDBDeleteAlignedTimeseriesIT & AlignedTVListTest  * fix: Copy globalTimeFilter due to GroupByMonthFilter  * reset tmpLength for backward sort  * * fix TVList clear * bitmap mark * sequence row count  * hot-load TVLIST_SORT_THRESHOLD  * fix: isNullValue caller  * fix unit test  * refactor: abstract prepareTvListMapForQuery method  * refactor:  clear/clone/expand indices and bitmap  * merge sort using min heap  * fix: WritableMemChunk deserialize  * feat: add index mem cost for TVList  * fix: hot-load tvlist_sort_threshold setting  * remove needless line in property template  ---------  Co-authored-by: shizy <shizy04@gmail.com>
apache,iotdb,f84a52eca0f1b5c25262b4209f501c6f95624136,https://github.com/apache/iotdb/commit/f84a52eca0f1b5c25262b4209f501c6f95624136,Pipe: Fix potential NPE from WALEntryHandler#getInsertNodeViaCacheIfPossible & Improve performance for pipe slightly (#14312)
apache,iotdb,c7e2d8e67c5efafaf7b16bddb576b998881106cc,https://github.com/apache/iotdb/commit/c7e2d8e67c5efafaf7b16bddb576b998881106cc,Optimize insertRelationalTablet performance (#14197)  * Optimize single device insert relation tablet performance  * optimize ttl check
apache,iotdb,22cf9449c40dbd5376f6b4a52f64897b41544065,https://github.com/apache/iotdb/commit/22cf9449c40dbd5376f6b4a52f64897b41544065,Pipe: Improve performance when syncing table data between clusters with param 'table-name' = '.*' or with param 'database-name' = '.*' by reducing unnecessary tsfile parse & Fix data filter with both tree and table patterns (#14150)  Co-authored-by: Steve Yurong Su <rong@apache.org>
apache,iotdb,63da4a42c941bc0e8135769365eac52808362e98,https://github.com/apache/iotdb/commit/63da4a42c941bc0e8135769365eac52808362e98,Table model data deletion (#13878)  * temp save  * temp save  * Change modification format  * update tsfile version  * fix deviceId match  * fix identitySinOperatorTest  * refactor interface hierachy  * refactor package structure  * spotless  * support table deletion  * fix test  * remove v1 mod file  * fix read empty mod  * add table deletion IT  * Fix nullability check in buildTsBlock().  * fix partialPath type in TreeDeletionEntry  * add predicate ut  * allow multiple mods in a plan node  * implment deleteDataForDropTable & fix IT  * fix log level  * ignore one test  * Added table IT (#13978)  * Update IoTDBTableIT.java  * Update DataNodeInternalRPCServiceImpl.java  * fix RelationalDeleteDataPlan serialization  * spotless  * add license  * Support more compicated deletion predicate  * fix tests  * update tsfile version & update tests  * fix modEntry merge  * parallel file deletion & fix ut  * add performance test remove redundant force optimize the procdedure of writing modfile  * all shared mod file framework  * fix TsFileResourceSerialization  * add mod file manager  * fix TsFileResource deserialization  * fix exception handle  * spotless  * fulfill deletion framework  * ignore perf test  * use buffered stream to read mods  * fix ut  * spotless  * add mod file exists marker and log condition  * fix tests  * fix test  * Drop column adaptation (#14073)  * drop column  * Update IoTDBTableIT.java  * Update IoTDBTableIT.java  * adaptation (#14077)  * fix comments  * Fixed the adaptation of delete device (#14081)  * adaptation  * Fix  * Update DeleteDevice.java  * Update AnalyzeUtils.java  * Update IoTDBDeviceIT.java  * update maxTime in TVList after deletion  * fix comment  * fix comment  ---------  Co-authored-by: Caideyipi <87789683+Caideyipi@users.noreply.github.com>
apache,iotdb,c372cabd8066c53ca2bb459b7659665588bb00a3,https://github.com/apache/iotdb/commit/c372cabd8066c53ca2bb459b7659665588bb00a3,Pipe: Disable sloppyPattern in PipeHistoricalDataRegionTsFileAndDeletionExtractor & Fix TsFileInsertionEventTableParserTabletIterator does not perform type conversion when parsing data of Date type (#13934)
apache,iotdb,80cd9f702845464d4cb37f0b28667e97b3628378,https://github.com/apache/iotdb/commit/80cd9f702845464d4cb37f0b28667e97b3628378,Perfect the aggregation queries when there is no devices or no data partitions.
apache,iotdb,1b7d159d07dac2531ee7367f4d5f349fe2859cbe,https://github.com/apache/iotdb/commit/1b7d159d07dac2531ee7367f4d5f349fe2859cbe,Pipe: perform deep copy for incoming progress index when constructing and updating progress index & fix hash code of progress index (#13441)
apache,iotdb,124a25e0efe6dee66dc9bd8a740feb2318f7252e,https://github.com/apache/iotdb/commit/124a25e0efe6dee66dc9bd8a740feb2318f7252e,Support update attribute on standalone version & Enable cache update / mlog writing of createOrUpdate device & Improved the performance / semantic of schema device query & Introduce limit/offset of show device
apache,iotdb,ded0abb7bb156bbbe9d04b170c4556c74c2e80a0,https://github.com/apache/iotdb/commit/ded0abb7bb156bbbe9d04b170c4556c74c2e80a0,Perfect impl of dispatch cost in ExplainAnalyze
apache,iotdb,ce4ed6a5024af15411f9a2fe3221c8eaab2e2c0c,https://github.com/apache/iotdb/commit/ce4ed6a5024af15411f9a2fe3221c8eaab2e2c0c,Perfect the print result of dispatch and timeseries metadata modification in explain analyze
apache,iotdb,7d552c7887fc0b2663fb2b93e5115c46ba5ac561,https://github.com/apache/iotdb/commit/7d552c7887fc0b2663fb2b93e5115c46ba5ac561,Pipe / Load: Improved the performance of invalidating last cache when loading tsFile (#12833)
apache,iotdb,c0743d98c2c249958d6063dfacd86d5ca0c3fb43,https://github.com/apache/iotdb/commit/c0743d98c2c249958d6063dfacd86d5ca0c3fb43,Enhance wal compression (#12830)  * use directbuffer to optimize performance  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * optimize log  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  * fix review  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>  ---------  Signed-off-by: OneSizeFitQuorum <tanxinyu@apache.org>
apache,iotdb,2e1ebf46e930f49c47fa1ff54da902d97cfc65b1,https://github.com/apache/iotdb/commit/2e1ebf46e930f49c47fa1ff54da902d97cfc65b1,Pipe: Intoduce TsFileInsertionScanDataContainer to read data from tsfile sequentially to improve pattern parse performance when filter rate is high (#12781)  Co-authored-by: Steve Yurong Su <rong@apache.org>
apache,iotdb,ab9347bfde9053d574466303f298273ecd1cb5b8,https://github.com/apache/iotdb/commit/ab9347bfde9053d574466303f298273ecd1cb5b8,Perfect methods of IAnalysis to adapt the write process of table model
apache,pinot,56f1ce734bce3b46636d457f04e5bb72081a3d2a,https://github.com/apache/pinot/commit/56f1ce734bce3b46636d457f04e5bb72081a3d2a,Verify if skipped submodule POMs contain hardcoded versions (#15816)  * yml file created  * Set up the environment and added Java logic to perform validation checks  * code clean up  * DepVerifier test hardcoded version within POM  * minor changes  * minor changes  * Complete Java logic that enforces dep guidelines + added scala-2.13 version in root POM  * test isInsideTagBlock  * test isMaven  * test actual pom files  * fix on yml file  * yml file created  * Set up the environment and added Java logic to perform validation checks  * code clean up  * DepVerifier test hardcoded version within POM  * minor changes  * minor changes  * Complete Java logic that enforces dep guidelines + added scala-2.13 version in root POM  * test isInsideTagBlock  * test isMaven  * test actual pom files  * fix on yml file  * class cannot find  * fix environment build  * Customize Maven Enforcer Plugin  * comment out pinotCustomDependencyVersionRule  * yml fix  * yml fix  * yml fix 1  * yml fix 2  * yml fix 3  * yml fix 4  * yml fix 5  * commented  * commented entire enforcer  * add back commented section  * delete yml and sh files  * Unit test done  * reorder pinot-dependency-verifier in modules list  * addressed Tianle's comments  * addressed remaining comments  * minor changes  * minor fix  * add `mvn clean install` in linter.sh  * batch 4  * batch 5  * batch 6  * add README  comments  set property true by default  * add license  * 2 phase build  * First PR: Install pinot-dependency-verifier before running full build  * Remove README.md  * Minor fix  * Verify if skipped submodule POMs contain hardcoded versions  * create a helper function that verifies hardcoded versions  * Remove if condition in helper function
apache,pinot,f164da11c335cc424aecdab89f7ca723ef2d7a73,https://github.com/apache/pinot/commit/f164da11c335cc424aecdab89f7ca723ef2d7a73,Add Maven Enforcer Rule to automatically enforce Dependency Management Guidelines during PR check-in (Part 2) (#15795)  * yml file created  * Set up the environment and added Java logic to perform validation checks  * code clean up  * DepVerifier test hardcoded version within POM  * minor changes  * minor changes  * Complete Java logic that enforces dep guidelines + added scala-2.13 version in root POM  * test isInsideTagBlock  * test isMaven  * test actual pom files  * fix on yml file  * yml file created  * Set up the environment and added Java logic to perform validation checks  * code clean up  * DepVerifier test hardcoded version within POM  * minor changes  * minor changes  * Complete Java logic that enforces dep guidelines + added scala-2.13 version in root POM  * test isInsideTagBlock  * test isMaven  * test actual pom files  * fix on yml file  * class cannot find  * fix environment build  * Customize Maven Enforcer Plugin  * comment out pinotCustomDependencyVersionRule  * yml fix  * yml fix  * yml fix 1  * yml fix 2  * yml fix 3  * yml fix 4  * yml fix 5  * commented  * commented entire enforcer  * add back commented section  * delete yml and sh files  * Unit test done  * reorder pinot-dependency-verifier in modules list  * addressed Tianle's comments  * addressed remaining comments  * minor changes  * minor fix  * add `mvn clean install` in linter.sh  * batch 4  * batch 5  * batch 6  * add README  comments  set property true by default  * add license  * 2 phase build  * First PR: Install pinot-dependency-verifier before running full build  * Remove README.md  * Minor fix  * Full build with pinot-dependency-verifier + README  * 2 phase build in yml + add enforcer profile  * Update README  minor fix on error message  * Remove full build CLI + minor tweak in README  * Remove `run.dependency.verifier` property
apache,pinot,37e5f8e492e401753175f9cf7d4cffaf404705ae,https://github.com/apache/pinot/commit/37e5f8e492e401753175f9cf7d4cffaf404705ae,Add Maven Enforcer Rule to automatically enforce Dependency Management Guidelines during PR check-in (#15739)  * yml file created  * Set up the environment and added Java logic to perform validation checks  * code clean up  * DepVerifier test hardcoded version within POM  * minor changes  * minor changes  * Complete Java logic that enforces dep guidelines + added scala-2.13 version in root POM  * test isInsideTagBlock  * test isMaven  * test actual pom files  * fix on yml file  * yml file created  * Set up the environment and added Java logic to perform validation checks  * code clean up  * DepVerifier test hardcoded version within POM  * minor changes  * minor changes  * Complete Java logic that enforces dep guidelines + added scala-2.13 version in root POM  * test isInsideTagBlock  * test isMaven  * test actual pom files  * fix on yml file  * class cannot find  * fix environment build  * Customize Maven Enforcer Plugin  * comment out pinotCustomDependencyVersionRule  * yml fix  * yml fix  * yml fix 1  * yml fix 2  * yml fix 3  * yml fix 4  * yml fix 5  * commented  * commented entire enforcer  * add back commented section  * delete yml and sh files  * Unit test done  * reorder pinot-dependency-verifier in modules list  * addressed Tianle's comments  * addressed remaining comments  * minor changes  * minor fix  * add `mvn clean install` in linter.sh  * batch 4  * batch 5  * batch 6  * add README  comments  set property true by default  * add license  * 2 phase build  * First PR: Install pinot-dependency-verifier before running full build  * Remove README.md  * Minor fix
apache,pinot,43630fdfb43d38a7317a4c42638ce217005fcb93,https://github.com/apache/pinot/commit/43630fdfb43d38a7317a4c42638ce217005fcb93,Log query exceptions in query runner perf tool (#15572)
apache,pinot,290914c4f2b907bcd52a3527d28a99787651da70,https://github.com/apache/pinot/commit/290914c4f2b907bcd52a3527d28a99787651da70,Normalize excessive whitespaces in sql to avoid regex performance issues (#15498)
apache,pinot,2c7b0ddc72b69f8bb080158d1b20de27c1d69431,https://github.com/apache/pinot/commit/2c7b0ddc72b69f8bb080158d1b20de27c1d69431,Improve performance of multi-stage queries with large IN clauses (#14615)
apache,pinot,31441164c74b5661563d51136d6ba22a1039952d,https://github.com/apache/pinot/commit/31441164c74b5661563d51136d6ba22a1039952d,Adding perf benchmark logic for GroupIdGenerator hash map (#14992)
apache,pinot,8b2d5b6d9ee980ee8004d44edb0c0e32ad5f7db0,https://github.com/apache/pinot/commit/8b2d5b6d9ee980ee8004d44edb0c0e32ad5f7db0,Improve JSON_MATCH performance. (#15049)
apache,pinot,5f220b398cda1fde87a286492e05521d48923634,https://github.com/apache/pinot/commit/5f220b398cda1fde87a286492e05521d48923634,Add support for performing pre-checks for TableRebalance (#15029)  * Add support for performing pre-checks for TableRebalance
apache,pinot,060018502b4c07849c94051fee36f1fad09d1755,https://github.com/apache/pinot/commit/060018502b4c07849c94051fee36f1fad09d1755,[perf] Run Inverted Index Before Other Operators (#14764)
apache,pinot,381097dc1ba990839f43fe40fc9144fb9c879769,https://github.com/apache/pinot/commit/381097dc1ba990839f43fe40fc9144fb9c879769,Added support to perform task validations for plug-in tasks (#14340)  * Added support to perform task validations for plug-in tasks  * trigger build2
apache,pinot,8774d320c4f789772f3a512fd290f6182625c140,https://github.com/apache/pinot/commit/8774d320c4f789772f3a512fd290f6182625c140,Add and use CLPMutableForwardIndexV2 by default to improve ingestion performance and efficiency (#14241)  * Add the initial implementation of CLPMutableForwardIndexV2  * Upgraded clp-ffi.version from 0.4.6 to 0.4.7  * Updated comments  * Address code review concerns  * Enable CLPMutableForwardIndexV2 to be compatible with CLPMutableForwardIndex during mutable->immutable segment conversion.  * Change Pinot to use CLPMutableForwardIndexV2 by default  * Fix integration issues related to dictionary encoding.  * Fix left-over bugs introduced by class name changes.  * Fixed small bug in CLPMutableForwardIndexV2#appendEncodedMessage related to Pinot null value handling.  * Fix style
apache,pinot,e5df02c594e52ca85ae98800052dddafe0a5e533,https://github.com/apache/pinot/commit/e5df02c594e52ca85ae98800052dddafe0a5e533,Improve performance of DataBlock serde (#13303)  This commit includes several changes in the code that builds  serializes and deserializes DataBlocks in order to improve the performace.  Changes here should not change the binary format (test included verify that). Instead we've changed how the code to reduce allocation and copies.
apache,pinot,cf52567dadbaec2673904162f88077d4c2426632,https://github.com/apache/pinot/commit/cf52567dadbaec2673904162f88077d4c2426632,Improve null handling performance for nullable single input aggregation functions (#13791)  Modify aggregation functions that were not extending NullableSingleInputAggregationFunction to do so and optimize their code to use the methods included there.
logisim-evolution,logisim-evolution,5146a8f22ac76ab02483dff5825e2abbb7aa48da,https://github.com/logisim-evolution/logisim-evolution/commit/5146a8f22ac76ab02483dff5825e2abbb7aa48da,Improve performance of regtab. HC:b310161
logisim-evolution,logisim-evolution,cd3a09e84f727a402b7d8c39d71646411c142429,https://github.com/logisim-evolution/logisim-evolution/commit/cd3a09e84f727a402b7d8c39d71646411c142429,Remove performance hostspot in ram simulation. HC:8d83b0d
logisim-evolution,logisim-evolution,4298b73ee79f2646783b69b2b9b0f35a7a17fbe6,https://github.com/logisim-evolution/logisim-evolution/commit/4298b73ee79f2646783b69b2b9b0f35a7a17fbe6,Minor performance improvement. HC:c6f6175
logisim-evolution,logisim-evolution,ac9119da9453ee1353e2dc2175399df07e271809,https://github.com/logisim-evolution/logisim-evolution/commit/ac9119da9453ee1353e2dc2175399df07e271809,Minor performance tweaks. HC:de8cd66
apache,hive,208488089df71e8a0dafd95b2812e5e4864baed9,https://github.com/apache/hive/commit/208488089df71e8a0dafd95b2812e5e4864baed9,HIVE-27102: Upgrade Calcite version from 1.25.0 to 1.33.0 (#5196)  1. Upgrade Calcite version from 1.25.0 to 1.33.0 and update code to address breaking changes and avoid compilation failures: * Add HiveJdbcImplementor#visit(JdbcTableScan) to address breaking change from CALCITE-4640. * Add HiveRuleConfig to address the removal of RelRule.Config.EMPTY by CALCITE-4839. This allows to instantiate RelRule subclasses without relying on the immutables annotation processor. * Adapt RelNode constructors and pass empty hints to address breaking change from in CALCITE-4640. * Simplify HiveDruidRules by using the config from respective rules in Calcite. * Adapt HiveRelFactories due to changes in ProjectFactory API (CALCITE-4199  CALCITE-5127). 2. Upgrade Avatica version from 1.12.0 to 1.23.0 (required by Calcite). 3. Add explicit dependency to org.immutables:value-annotations to avoid compilation failures due to missing Value annotation. Since we don't want to propagate the dependency to dependent projects we declared it at provided scope. The problem appears cause annotation pre-processing is enabled. 4. Add explicit dependency to org.locationtech.jts:jts-core and declare it at runtime scope. The jts-core is a transitive dependency used by calcite-core. In order for CBO to function properly the jar must be present in the classpath. If the jar is missing  Hiveserver2 will fail during startup with ClassNotFoundException. Normally we wouldn't need to explicitly declare transitive dependencies  but we are forced to do so cause calcite is partially shaded in hive-exec module. We are already doing the same for various other calcite deps such as janino  avatica  etc. 5. Register HiveIn as SqlKind.OTHER_FUNCTION and distinguish it from SqlKind.IN which is now reserved for usage inside RexSubQuery 6. Replace checks for SqlKind.IN in RexCall  which can no longer appear  with checks on HiveIn operator directly. 7. Implement new rules and converters to transform/expand the internal SEARCH operator to traditional conjunctions/disjunctions of standard comparisons operators  IN  and BETWEEN. 8. Copy and use Lopt classes from Calcite with the fix for (CALCITE-6737) to avoid changes in join order when queries contain self-joins. 9. Move rollup aggregation logic from HiveRelBuilder to the appropriate SqlAggFunction subclass and remove redundant overrides in HiveMaterializedViewRule. CALCITE-4342 made rollup strategy a first class citizen of the SqlAggFunction interface so each function can return its own rollup if available. It's no longer necessary to pass from the RelBuilder and basically whenever we need to obtain a rollup we have to ask directly the SqlAggFunction. As part of CALCITE-4342  the MaterializedViewRule#getRollup method was deprecated so this refactoring is necessary for the Hive MV rules to function properly. 10. Make HivePreFilteringRule a reduction rule to address fixpoint bug (StackOverflowError) triggered by changes in RelMetadataQuery#getPulledUpPredicates (CALCITE-5036). 11. Add HiveTypeFactory to avoid redundant nullability CAST to ARRAY that in some cases (lateral_view_outer.q) leads to wrong results. The redundant CAST is a side-effect from fixing CALCITE-4603. 12. Handle ROW type literals in RexNodeConverter#transformInToOrOperands After the upgrade a ROW RexCall with constants is reduced/folded to a RexLiteral of ROW types so we must adapt the IN transformation logic otherwise the ROW specific code will not kick in. Although this is merely an optimization it has impact on correctness (due to existing Hive bugs) so it's necessary to keep it working. 13. Enhance FilterSelectivityEstimator to provide better estimates for NOT 14. Use RelBuilder instead of RexBuilder in TestHivePushdownSnapshotFilterRule. Code is more readable and avoids creating invalid expressions with wrong types as it was the case before. 15. Disable RelBuilder simplifications in TestHivePointLookupOptimizerRule to avoid the introduction of SEARCH in the input plans. The rule is meant to handle disjunctions (OR) so we want to keep the input plans intact in order to properly test the rule. If the input plan is simplified to SEARCH or something else then test coverage will change that is not desired.  The plan and behavior changes that occur as part of the upgrade can be summarized into the following categories. An additional (R*) classifier  is used to denote subtle regressions that will be fixed by follow-up patches. Each change is associated with some represantive .q.out files that illustrate the overall impact on query plans.  Category I: Changes that are mostly cosmetic and do not affect the performance or behavior of the queries.  1. Changes in the output of EXPLAIN EXTENDED (OPTIMIZED SQL) * Less parentheses [auto_join_reordering_values.q.out] * Less spaces when formatting IN  ARRAY  etc. [alter_partition_coltype.q.out  autoColumnStats_5a.q.out] * VALUES clause with one entry becomes SELECT with literals [acid_nullscan.q.out] * New OPTIMIZED SQL entries since various bugs in RelToSqlConverter were fixed [pcs.q.out] * Explicit CROSS JOIN clause instead of comma separated sub-queries [tez_fixed_bucket_pruning.q.out] 2. Different formatting (toString) for DOUBLE literals [rule_exclusion_config.q.out] 3. Extra "fields" attributes in the output of EXPLAIN FORMATTED (R*) [concat_op.q.out] 4. IN clause literals sorted alphabetically [dynamic_partition_pruning_2.q.out] 5. Different aliases for expressions inside projects (e.g.  lots of EXPR$ in CBO plans) [cardinality_preserving_join_opt.q.out] 6. Result changes due to missing ORDER BY in queries [partition_wise_fileformat2.q.out]  Category II: Changes that can slightly affect the performance of queries increasing/decreasing CPU cycles.  1. Predicate order changes in filters [auto_join5.q.out] 2. Redundant IS NOT NULL predicates (R*) [explainuser_4.q.out  pointlookup5.q.out] 3. Redundant CAST around COALESCE (R*) [cbo_aggregate_reduce_functions_rule.q.out] 3. Pull-up constants from nullsafe predicates [is_distinct_from.q.out  join_nullsafe.q.out  orc_predicate_pushdown.q.out] 4. Residual filter predicates in joins (R*) [ppd_join.q.out]  Category III: Changes that can lead to significant changes in query performance.  1. Expression simplifications in project/filter/join * IN  BETWEEN  and comparison operators (< > >= <= == <>) are simplified together by exploiting the SEARCH operator [bucketsortoptimize_insert_7.q.out] * x NOT BETWEEN 10 AND 20 is transformed to x < 10 OR x > 20 [udf_between.q.out] * x <> 10 is transformed to x > 10 OR x < 10 (R*) [ppd_join3.q.out] * Simplifications with arithmetic operators [materialized_view_rewrite_8.q.out  tez_vector_dynpart_hashjoin_2.q.out] 2. Disjunctive predicate pushdown for joins (R*) [cbo_join_transitive_pred_loop_1.q.out  correlationoptimizer8.q.out] 3. LEFT OUTER to INNER joins due to CALCITE-5247 [subquery_ANY.q.out] 4. Better orders for LEFT/RIGHT/ANTI joins due to CALCITE-4208 [auto_join30.q.out  subquery_join_rewrite.q.out] 5. Unconditional OR to IN tranformations [in_typecheck_char.q.out  pointlookup2.q.out] 6. Extra partition pruning due to simplifications [select_unquote_or.q.out]  Co-authored-by: Stamatis Zampetakis <zabetak@gmail.com> Co-authored-by: Krisztian Kasa <kasakrisz2@gmail.com>
apache,hive,dd5b05e4be89a3c705675871450821b66b8ec4d7,https://github.com/apache/hive/commit/dd5b05e4be89a3c705675871450821b66b8ec4d7,HIVE-28725: Sorting is performed when order by position is disabled when CBO is enabled (Krisztian Kasa  reviewed by Shohei Okumiya)
apache,hive,88e2175b2c04403e35b404634306f60d599c3d83,https://github.com/apache/hive/commit/88e2175b2c04403e35b404634306f60d599c3d83,HIVE-28700: MRCompactor may cause data loss when performing the major compaction (#5603) (Zhihua Deng  reviewed by Denys Kuzmenko   Marta Kuczora)
apache,hive,e8ccca82be80611e79178c1ab1acc75c5d62b2e0,https://github.com/apache/hive/commit/e8ccca82be80611e79178c1ab1acc75c5d62b2e0,HIVE-27874: Support datatype conversion on fetch threads (Kurt Deschler  reviewed by Attila Turoczy  Denys Kuzmenko)  This patch provides a mechanism to move expensive datatype conversions (i.e.  Timestamp) to the fetch threads where the work can be done in parallel. This can substantially improve performance in cases where the client thread is the bottleneck and resources are available for multiple fetch threads. Implementation is in form of ConvertedResultSet  which is agnostic to the underlying protocol result and can be dynamically substituted into the fetch path.  Closes #4902
apache,hive,7871199a49c692deef70628bc12ef022f4099bd3,https://github.com/apache/hive/commit/7871199a49c692deef70628bc12ef022f4099bd3,HIVE-27872: Support multi-stream parallel fetch in JDBC driver (Kurt Deschler  reviewed by Attila Turoczy  Denys Kuzmenko)  This patch enables JDBC to open multiple sockets to an HS2 service and performance concurrent fetch results for a single query. This can significantly speed up fetching of large results that are bottlenecked on Thrift serialization  de-serialization  and string conversion. With adequate threads  fetch performance will now only be limited by the single-threaded client-side result processing and server-size row materialization.  Added JDBC Client parameter fetchThreads to control the number of threads allocated for fetching. Setting fetchThreads=1 will pipeline the Fetch using the existing connection asynchronously. Setting fetchThreads>1 will cause an additional Thrift connection to be opened to the server for each thread. Care should be taken not to over-allocate connections to the server.  Added new HiveConf parameter hive.jdbc.fetch.threads to allow config of fetchThrads from server conf.  Closes #4902
apache,hive,6166da46337a75d0131c591e4b3aa339961514d2,https://github.com/apache/hive/commit/6166da46337a75d0131c591e4b3aa339961514d2,HIVE-27873: Fix getOperationStatus and optimize fetch (Kurt Deschler  reviewed by Attila Turoczy  Denys Kuzmenko)  This patch fixes a major performance issue fetching result from Impala. The problem was that Impala does not set isHasResultSet during getOperationStatus() calls  resulting in that RPC getting called and logging a completion message for every row fetched. Optimizes the fetch path to minimize conditional checks in the fast path.  Closes #4902
apache,hive,d4cef17160a3137dd43b98312a0e550e823ed05e,https://github.com/apache/hive/commit/d4cef17160a3137dd43b98312a0e550e823ed05e,HIVE-28596: Skip ColumnAccessInfo collection when not needed to speed-up compilation (Stamatis Zampetakis reviewed by Dmitriy Fingerman  Soumyakanti Das  Butao Zhang)  Avoid paying the perf-overhead of collecting ColumnAccessInfo via the trimmer when it is not necessary. Currently  we only use the ColumnAccessInfo when we need to perform column based authorization or when it is requested explicitly by the user via the hive.stats.collect.scancols property.  Close apache/hive#5517
apache,hive,454415b4798dbb7accf5e5de23db3dc4801b7d68,https://github.com/apache/hive/commit/454415b4798dbb7accf5e5de23db3dc4801b7d68,HIVE-28571: Basic UNIONTYPE support in CBO (Stamatis Zampetakis reviewed by Soumyakanti Das  Attila Turoczy  Alessandro Solimando  Shohei Okumiya)  Support UNIONTYPE in the CBO path to take advantage of the powerful optimizations that are performed in the CBO layer and avoid relying on the fallback mechanism. The changes do not aim to cover new use-cases but just to ensure that existing queries that involve UNIONTYPE can exploit the CBO.  1. Model union as struct at the CBO layer 2. Remove CalciteSemanticException from TypeConverter APIs (no longer thrown) 3. Update q.out files since queries with UNIONTYPE can now use CBO  The plan changes in existing tests are trivial and expected. * join_thrift: Extra Select Operator; minor perf impact and does not alter result * union21/unionDistinct_3: COUNT(1) becomes COUNT() but both are equivalent in terms of semantics/performance * udf_isnull_isnotnull: Select expressions simplify to true; valid simplification based on the filter predicates  Close apache/hive#5497
apache,hive,b6c191db42c7bc9911ce566e5f28b38dde591718,https://github.com/apache/hive/commit/b6c191db42c7bc9911ce566e5f28b38dde591718,HIVE-28259: Common table expression detection and rewrites using CBO (Stamatis Zampetakis reviewed by Alessandro Solimando  Aman Sinha)  1. Add `applyCteRewriting` phase in `CalcitePlanner` for detecting and using CTEs; ensure rewrite logic is consistent with existing `hive.optimize.cte.materialize.*` properties. 2. Model CTEs as materialized views (MVs) and add utility method in `HiveMaterializedViewUtils` for mapping a CTE to a `RelOptMaterialization`. 3. Refactor core MV rewrite logic in `CalcitePlanner` to use during CTE rewrite and exploit CTEs in a cost-based manner. 4. Add `HiveTableSpool` operator to represent CTEs and handle them in the plan using new rules: `TableScanToSpoolRule` and `RemoveUnusedCteRule`. 5. Add `TableScanToSpoolRule`  and `RemoveInfrequentCteRule` to add/remove spools from the plan. 6. Enhance/Enrich metadata handlers for handling the Spool operator. 7. Add `AggregatedColumns` metadata (and respective handler and metadata query)  for controlling if a CTE is a "full aggregate" at the CBO (RelNode) level to ensure consistent behavior with `hive.optimize.cte.materialize.full.aggregate.only` property. 8. Add `HiveSqlTypeUtil.containsSqlType` for detecting and skipping the creation of CTEs with untyped nulls since they are not supported (HIVE-11217). 9. Add `hive.optimize.cte.suggester.class` and CommonTableExpressionSuggester interface to provide pluggable CTE detection logic. Given that CTE detection logic can range from basic tree traversal algorithms to complex workload analysis frameworks this part needs to be configurable since there is no one-size-fits-all implementation. The configuration property also allows proprietary algorithms to be integrated in HiveServer2 by implementing the necessary APIs and adding the jars in the classpath. 10. Add prototype implementation for CTE detection logic in CommonTableExpressionIdentitySuggester using CommonRelSubExprRegisterRule and CommonTableExpressionRegistry. Although the implementation is rather simple it can discover various interesting CTEs as demonstrated by the tests and can be indeed useful in a prod environment. 11. Map spool(s) to `WITH` clauses during the RelNode to AST conversion (ASTConverter  ASTBuilder  PlanModifierForASTConv) to exploit existing CTE materialization feature (HIVE-11752). 12. Modify (slighly) `SemanticAnalyzer`/`CalcitePlanner` to enable AST-based CTE materialization (getMetadata) post CBO run. 13. Tests for: * CTE detection logic using the `CommonTableExpressionPrintSuggester` (`TestTezTPCDS30TBPerfCliDriver`); use only in cbo_query* tests to avoid redundancy. * demonstrating (end-to-end) the CTE feature (cte_cbo_rewrite_0.q) * verify coherence of CTE rewrite with `hive.optimize.cte.materialize.full.aggregate.only` (cte_mat_12.q) * spool JSON serialization (cte_cbo_plan_json.q)  Close apache/hive#5249
apache,hive,673ca384fe7f4fe63b58fc2cb5eae99a3f1790cc,https://github.com/apache/hive/commit/673ca384fe7f4fe63b58fc2cb5eae99a3f1790cc,HIVE-28428: Performance regression in map-hash aggregation (Ryu Kobayashi  reviewed by Denys Kuzmenko)  Closes #5380
apache,hive,07cc9c3faebf790284ed0b3b2811f83394575ff5,https://github.com/apache/hive/commit/07cc9c3faebf790284ed0b3b2811f83394575ff5,HIVE-28285: Exception when querying JDBC tables with Hive/DB column types mismatch (Stamatis Zampetakis reviewed by Zhihua Deng)  1. Revert changes from HIVE-27487 for fetching types from the database (RS) and restore the old behavior. 2. Enrich JDBC type conversion test matrix for Derby a. Increase type combinations b. Increase test row/values c. Test with CBO on/off  When Hive types and database (DB) column types are different there is an implicit type conversion that must be done. The JdbcRecordIterator drives the type conversion  and the way we extract values from the ResultSet largely determines the result. In order to perform the conversion the iterator must use the Hive DDL types and not the database DDL types obtained from the result set.  Close apache/hive#5274
JSQLParser,JSqlParser,517fe72c55cdddeca2745d1456a34d42ad152596,https://github.com/JSQLParser/JSqlParser/commit/517fe72c55cdddeca2745d1456a34d42ad152596,chore: minor clean-ups around the performance optimisations  Signed-off-by: Andreas Reichel <andreas@manticore-projects.com>
JSQLParser,JSqlParser,93ca6610425fc8ac99095ba74161c69095682f06,https://github.com/JSQLParser/JSqlParser/commit/93ca6610425fc8ac99095ba74161c69095682f06,Exasol support (#2046)  * feat: Support REGEXP_LIKE as LikeExpression  Implement support of REGEXP_LIKE as LikeExpression as described here: https://docs.exasol.com/db/latest/sql_references/predicates/not_regexp_like.htm  * feat: Support sub select as part of function parameters  Allow unparenthesesed sub selects being part of multiple function parameters  * fix: Readd mistakenly removed K_TEXT_LITERAL  * revert: Revert code formatting changes  * fix: Fix choice conflicts  * refactor: Rename test methods  * refactor: Apply  on changed files  * feat: Introduce allowUnparenthesizedSubSelects feature  * refactor: Apply formatApply  * test: add test for the standard grammar  expected to fail  * test: make the performance tests more robust regarding the time-outs  * style: reformat code  Signed-off-by: Andreas Reichel <andreas@manticore-projects.com>  * fix: Revert default keyword changes  * fix: Revert default keyword changes  ---------  Signed-off-by: Andreas Reichel <andreas@manticore-projects.com> Co-authored-by: Stefan Steinhauser <stefan.steinhauser@arz.at> Co-authored-by: Andreas Reichel <andreas@manticore-projects.com>
JSQLParser,JSqlParser,82470e55aaf915778e07cb39f091fc2fb1c08192,https://github.com/JSQLParser/JSqlParser/commit/82470e55aaf915778e07cb39f091fc2fb1c08192,feature/fix: parsing inserts/updates/delete within CTEs (#2055)  * feature parsing inserts/updates/delete within CTEs  * removing System lines  * fixing codacy issues  * reducing the looping in NestedBracketsPerformanceTest to just 6  * formatting fixes via spotlessApply
redis,lettuce,d3123e8c4b93d399899d362534defd77e8b56992,https://github.com/redis/lettuce/commit/d3123e8c4b93d399899d362534defd77e8b56992,Improve the performance of obtaining write connections through double-check locks. (#3228)  * Improves the performance of obtaining write connections.  Signed-off-by: c00887447 <c00887447@huawei2.com>  * Polishing  * Formatter carelessly forgotten  ---------  Signed-off-by: c00887447 <c00887447@huawei2.com> Co-authored-by: c00887447 <c00887447@huawei2.com> Co-authored-by: Tihomir Mateev <tihomir.mateev@gmail.com>
redis,lettuce,50878081f7f735b2fb6b98c16ef5350ed7dd6cc4,https://github.com/redis/lettuce/commit/50878081f7f735b2fb6b98c16ef5350ed7dd6cc4,Optimize string concatenation in getNodeDescription() (#3262)  Replace String.join with Collectors.joining to avoid intermediate collection creation  improving performance by eliminating the unnecessary List creation step.
redis,lettuce,a127d5ab26d636048ee89c981f46e840df66b404,https://github.com/redis/lettuce/commit/a127d5ab26d636048ee89c981f46e840df66b404,Fix SimpleBatcher apparent deadlock #2196 (#3148)  * Fix SimpleBatcher Concurrency Issue (#2196)  ### Problem: - **Flush Operation Conflict:** - When one thread (T1) performs a flush  it may read and dispatch batched commands before resetting the `flushing` flag. - If another thread (T2) adds a command and forced flush at this moment. Command might be added to the queue but does not trigger a flush. - As a result  the command remains in the queue until the next flush request  causing a delay in dispatching.  - **Flag Reset Between Iterations:** - During a default flush operation  if multiple batches are processed  the `flushing` flag is reset between iterations. - This allows another thread to take over  potentially causing the initial thread to return `BatchTasks.EMPTY` instead of properly processed commands.  1. T1 -> batch(command  CommandBatching.flush() 2. T1 -> flushing.compareAndSet(false  true) == true 3. T1 -> flush()->doFlush() 4. T2 -> batch(command  CommandBatching.flush() 5. T2 -> flushing.compareAndSet(false  true) == false  #already flushing will skip doFlush  and command remain not dispatched 6. T1 -> batch() completes 7. T2 -> batch() completes  ### Fix: If force flush is requested while flushing  perform additional flush iteration after ongoing completes  * format  * Update src/main/java/io/lettuce/core/dynamic/SimpleBatcher.java  Co-authored-by: Tihomir Krasimirov Mateev <tihomir.mateev@redis.com>  * Update src/main/java/io/lettuce/core/dynamic/SimpleBatcher.java  Co-authored-by: ggivo <ivo.gaydajiev@gmail.com>  ---------  Co-authored-by: Tihomir Krasimirov Mateev <tihomir.mateev@redis.com>
alibaba,jetcache,984aa2a5fafadbcbbc61eafc22c9904f1d3f6dde,https://github.com/alibaba/jetcache/commit/984aa2a5fafadbcbbc61eafc22c9904f1d3f6dde,perf: improve performance
alibaba,jetcache,26b9139c586850d8ae2c8c43e650153646178bba,https://github.com/alibaba/jetcache/commit/26b9139c586850d8ae2c8c43e650153646178bba,perf: fix encode/decode performance issue with lettuce #908
spring-projects,spring-ai,d619e25cb56d2d2ddfceae70311bfa29b0a5edd5,https://github.com/spring-projects/spring-ai/commit/d619e25cb56d2d2ddfceae70311bfa29b0a5edd5,Self-contained prompt templates in advisors  The built-in advisors that perform prompt augmentation have been updated to use self-contained templates. The goal is for each advisor to be able to perform templating operations without affecting nor being affected by templating and prompt decisions in other advisors.  * QuestionAnswerAdvisor * PromptChatMemoryAdvisor * VectorStoreChatMemoryAdvisor  Documentation and upgrade notes have been updated accordingly  Signed-off-by: Thomas Vitale <ThomasVitale@users.noreply.github.com>
spring-projects,spring-ai,2ea518686f33046daeeb541515c94fb9031a114a,https://github.com/spring-projects/spring-ai/commit/2ea518686f33046daeeb541515c94fb9031a114a,Support DocumentPostProcessors in RAG Advisor  The DocumentPostProcessor is one of the modular RAG components introduce in M8. You can now use this API from within the RetrievalAugmentationAdvisor to post-process the retrieved documents before passing them to the model. For example  you can use such an interface to perform re-ranking of the retrieved documents based on their relevance to the query  remove irrelevant or redundant documents  or compress the content of each document to reduce noise and redundancy.  Signed-off-by: Thomas Vitale <ThomasVitale@users.noreply.github.com>
spring-projects,spring-ai,faa8778b6a1b4198998f253d066f9a7950367c3f,https://github.com/spring-projects/spring-ai/commit/faa8778b6a1b4198998f253d066f9a7950367c3f,Make ChatClient and Advisor APIs more robust - Part 2  * ChatClient observations now include the full prompt content instead of just the userText and systemText. Furthermore  they include consistent telemetry for the tools passed via the ChatClient and a first-class conversation ID when using memory advisors. Incomplete or unsafe attributes have been deprecated. * Adopted the new robust Advisor APIs for BaseAdvisor and RetrievalAugmentationAdvisor. * Improved the prompt augmentation facilities in ChatClientRequest and Prompt for performance and immutability. * Fixed integration test racing condition. * Updated the documentation for ChatClient and Observability accordingly. * Documented changes in upgrade notes. * Introduced `prompt.augmentUserMessage(String text)` to directly replace the user message content. * Added `prompt.augmentUserMessage(Function<UserMessage  UserMessage> augmenter)` for more granular updates using the `userMessage.mutate()` pattern  allowing modification of text  media  and metadata.  Relates to gh-2655  Signed-off-by: Thomas Vitale <ThomasVitale@users.noreply.github.com>
spring-projects,spring-ai,0d748e2b877f1ad532ffd7c00b8297cf82c68f1a,https://github.com/spring-projects/spring-ai/commit/0d748e2b877f1ad532ffd7c00b8297cf82c68f1a,feat(tool): Add ToolExecutionEligibilityChecker interface  Introduce a new ToolExecutionEligibilityChecker interface to provide a more flexible way to determine when tool execution should be performed based on model responses. This abstraction replaces the hardcoded logic previously scattered across the codebase.  - Adds a new ToolExecutionEligibilityChecker interface in spring-ai-core - Integrates the checker into OpenAiChatModel with appropriate defaults - Updates OpenAiChatAutoConfiguration to support the new interface - Provides a default implementation that maintains backward compatibility  Signed-off-by: Christian Tzolov <christian.tzolov@broadcom.com>  refactor: Replace ToolExecutionEligibilityChecker with ToolExecutionEligibilityPredicate  - Replacing ToolExecutionEligibilityChecker with ToolExecutionEligibilityPredicate - Changing from Function<ChatResponse  Boolean> to BiPredicate<ChatOptions  ChatResponse> - Adding a DefaultToolExecutionEligibilityPredicate implementation - Updating AnthropicChatModel and OpenAiChatModel to use the new predicate - Updating auto-configurations to inject the new predicate - Adding comprehensive tests for the new predicate implementation  The new approach provides a cleaner and more consistent way to determine when tool execution should be performed based on both prompt options and chat responses.  Add Bedrock Converse support  add mistral support  Add ollama and vertex gemini  add ToolExecutionEligibilityPredicate docs  Signed-off-by: Christian Tzolov <christian.tzolov@broadcom.com>
spring-projects,spring-ai,e128a39ec34831915501183f246bac55fd435c69,https://github.com/spring-projects/spring-ai/commit/e128a39ec34831915501183f246bac55fd435c69,Updates the `SearchRequest` class to be non-final and adds a `MilvusSearchRequest` subclass that includes Milvus-specific fields for native expressions and search parameters.  - Updated `SearchRequest.java` to make the class non-final. - Added `MilvusSearchRequest` with specific Milvus parameters such as `nativeExpression` and `searchParamsJson`. - Modified `doSimilaritySearch` method in `MilvusVectorStore` to handle these new fields from `MilvusSearchRequest`.  Add unit tests for MilvusVectorStore and MilvusSearchRequest  Introduce comprehensive unit tests to validate the functionality of MilvusVectorStore and MilvusSearchRequest  including scenarios for native and filter expressions. Refactor MilvusVectorStore to improve filter expression handling by introducing a helper method for converted expressions.  Add detailed documentation for MilvusSearchRequest usage  Introduced sections explaining MilvusSearchRequest's parameters  `nativeExpression`  and `searchParamsJson`  with examples for enhanced clarity. This update provides guidance on leveraging Milvus-specific features for precise filtering and optimal search performance.  Signed-off-by: waileong <wai_leong1015@hotmail.com>
spring-projects,spring-ai,b525309e8c6b1868f51fa5a79cf8c334258e464b,https://github.com/spring-projects/spring-ai/commit/b525309e8c6b1868f51fa5a79cf8c334258e464b,Introduce PII marker for logging sensitive data  Introduce a utility class `LoggingMarkers` providing an SLF4J marker for tagging log entries with Personally Identifiable Information (PII). Update `BeanOutputConverter` to use the `PII_MARKER` in error logs for invalid JSON conversions. Enhance tests to verify PII marker usage in logging.  - Set Java version dynamically and configure Kotlin compiler  - Updated Maven configurations to dynamically reference the Java version using `${java.version}`. Added Kotlin compiler settings  including `jvmTarget` alignment with Java version and enabling `javaParameters`. This ensures consistency and better compatibility across builds.  - Fix log assertion in BeanOutputConverterTest to use Java 17  - Updated the test to assert log size explicitly before accessing the first log entry. This ensures the test is more robust and avoids potential issues with accessing logs unexpectedly.  - Use placeholders in logger.error to prevent string concatenation.  - Replaced string concatenation with a placeholder in the logger.error call to improve performance and maintain consistency with logging best practices. This helps avoid unnecessary overhead when logging is disabled.  - Update logging markers and improve data classification  - Replaced `PII_MARKER` with `SENSITIVE_DATA_MARKER`. Introduced `RESTRICTED_DATA_MARKER`  `REGULATED_DATA_MARKER` and `PUBLIC_DATA_MARKER`  - Updated associated logging logic and tests to reflect these changes.  - Fix punctuation in Javadoc comments for LoggingMarkers.  - Added missing periods to improve consistency and clarity in the Javadoc comments. This change ensures proper formatting and adheres to standard writing conventions.  Signed-off-by: Konstantin Pavlov <{ID}+{username}@users.noreply.github.com>
spring-projects,spring-ai,329e6c02191dfaa6e34d59e0898c2a23a1a14a52,https://github.com/spring-projects/spring-ai/commit/329e6c02191dfaa6e34d59e0898c2a23a1a14a52,Remove superfluous batchingStrategy implementation in CosmosDBVectorStore
spring-projects,spring-ai,0c5455e4cdb95df2e290bdd7cac57694956cb34f,https://github.com/spring-projects/spring-ai/commit/0c5455e4cdb95df2e290bdd7cac57694956cb34f,Improve context formatting in QuestionAnswerAdvisor  - Add line breaks and clarify context boundaries in user text advice. This improve the performance of a Llama3.x - Update corresponding test to reflect new formatting
spring-projects,spring-ai,c205c7d5cad5d6278be9c56d3babe6ed9af87ae9,https://github.com/spring-projects/spring-ai/commit/c205c7d5cad5d6278be9c56d3babe6ed9af87ae9,Fix interleaved output in JsonReader's parseJsonNode method  Replace parallelStream with stream to prevent thread-unsafe appends to the shared StringBuilder. This fixes the issue of intermingled key-value pairs in the generated Document content. Also  replace StringBuffer with StringBuilder for better performance in single-threaded context.  The change ensures correct ordering of extracted JSON keys and their values in the resulting Document  improving the reliability and readability of the parsed output.
spring-projects,spring-ai,2ecffc10c73404e7d1512c12d7fdaa30443280b5,https://github.com/spring-projects/spring-ai/commit/2ecffc10c73404e7d1512c12d7fdaa30443280b5,Refactor data filtering in RelevancyEvaluator  Replace Objects::nonNull and instanceof checks with StringUtils::hasText for more efficient and cleaner content filtering. This change simplifies the stream operation in the getContent method  improving readability and potentially performance.
spring-projects,spring-ai,202148d45bf9c226a04768f7ff9836a89e0bee9c,https://github.com/spring-projects/spring-ai/commit/202148d45bf9c226a04768f7ff9836a89e0bee9c,Prevent timeouts with configurable batching for PgVectorStore inserts  Resolves https://github.com/spring-projects/spring-ai/issues/1199  - Implement configurable maxDocumentBatchSize to prevent insert timeouts when adding large numbers of documents - Update PgVectorStore to process document inserts in controlled batches - Add maxDocumentBatchSize property to PgVectorStoreProperties - Update PgVectorStoreAutoConfiguration to use the new batching property - Add tests to verify batching behavior and performance  This change addresses the issue of PgVectorStore inserts timing out due to large document volumes. By introducing configurable batching  users can now control the insert process to avoid timeouts while maintaining performance and reducing memory overhead for large-scale document additions.
spring-projects,spring-ai,8d31a57698f86842535fdad0c97901c43e728993,https://github.com/spring-projects/spring-ai/commit/8d31a57698f86842535fdad0c97901c43e728993,Add metadataFieldsToFilter property for MongoDB store  Introduce a new property for the MongoDB vector store: spring.ai.vectorstore.mongodb.metadata-fields-to-filter  This property accepts comma-separated values specifying which metadata fields can be used for filtering when querying the vector store. It ensures that metadata indexes are created if they don't already exist.  This addition enhances query performance and flexibility by allowing users to define filterable fields in advance.  Co-authored-by: Eddú Meléndez <eddu.melendez@gmail.com>
spring-projects,spring-ai,0927bd197db7ed4f8ac7fa7d99c9cbfdc24b3c27,https://github.com/spring-projects/spring-ai/commit/0927bd197db7ed4f8ac7fa7d99c9cbfdc24b3c27,Fix MiniMax model function call implementation  Implement function call capability for MiniMax model and add unit tests based on new tool classes. Address most scenarios  but note limitations in complex English contexts with multiple function calls. Weather query example: may stop prematurely when querying multiple locations due to single-location parameter limit. This behavior stems from model performance constraints.  Streaming function calling is not passing tests  will be address seperately.  Resolves #1077  Implement function call capability for the Moonshot model. Include unit tests to verify the new functionality. This feature addresses the requirements outlined in issue #1058. fix: MiniMax function call  review
spring-projects,spring-ai,036093a42b719d2506b93bec9dfea3cc977d730a,https://github.com/spring-projects/spring-ai/commit/036093a42b719d2506b93bec9dfea3cc977d730a,Enhance vector store observability support  * Consolidate usage of “db.collection.name” attribute to track table name  collection name  index name  document name  or whatever concept a vector database uses to store data. Removed “db.index” that was use sometimes instead of “db.collection.name”. This usage is in line with the OpenTelemetry Semantic Conventions. * Configure query response content to be included as a “span event” instead of a “span attribute” if the backend system supports that  similar to how we do for the model observations. * Structure vector store observation attributes in dedicated enums  including one for the Spring AI Kinds to avoid hard-coding the same value in a lot of places. This follows the OpenTelemetry Semantic Conventions as much as possible. Also  adopt Spring usual non-null-by-default strategy as much as possible. * Align vector store conventions to the chat model ones  and follow alphabetical order for values. This is particularly useful for the convention classes  for which the Micrometer performance of exporting telemetry data improves when key values are added already sorted to the context. * Fix flaky test in Mistral AI. * Improve Qdrant integration tests.  Signed-off-by: Thomas Vitale <ThomasVitale@users.noreply.github.com>
apache,hbase,53e3aa902d234b17076a751c73a215464669e102,https://github.com/apache/hbase/commit/53e3aa902d234b17076a751c73a215464669e102,HBASE-28984 Refactor cache update logic to use new PermissionCache instances (#6485)  - This PR replaced the clear operation on existing PermissionCache with logic to create a new PermissionCache instance during updates. - This change ensures that readers have consistent and uninterrupted access to the cache data  minimizing the risk of race conditions. - By creating a new cache instance for each update instead of clearing the existing one  we improve data integrity and avoid potential issues related to concurrent access. - This approach aligns with the Copy-On-Write pattern  optimizing read performance by isolating update operations.  Co-authored-by: Wu Peiming[ 呉培銘 ] <peiming.wu@linecorp.com> Signed-off-by: Duo Zhang <zhangduo@apache.org>
apache,hbase,670deaa1d6020b1f603819504f95e016c1e06dbb,https://github.com/apache/hbase/commit/670deaa1d6020b1f603819504f95e016c1e06dbb,HBASE-29040 Fix description of "sampleRate" of PerformanceEvaluation (#6558)  Signed-off-by: Duo Zhang <zhangduo@apache.org>
apache,hbase,6ebd48e477ff0af5900b4149a3cecc43c8449bc6,https://github.com/apache/hbase/commit/6ebd48e477ff0af5900b4149a3cecc43c8449bc6,HBASE-29013 Make PerformanceEvaluation support larger data sets (#6509)  Use 8-byte long integers in the code to prevent integer overflows.  Signed-off-by: Duo Zhang <zhangduo@apache.org> Reviewed-by: Peng Lu <lupeng_nwpu@qq.com>
apache,hbase,84f4fb3b4210f9d7d50533eed5471bbab381d8c9,https://github.com/apache/hbase/commit/84f4fb3b4210f9d7d50533eed5471bbab381d8c9,HBASE-28432 Refactor tools which are under test packaging to a new module hbase-diagnostics (#6258)  - Move PerformanceEvaluation  LoadTestTool  HFilePerformanceEvaluation  ScanPerformanceEvaluation  LoadBalancerPerformanceEvaluation  and WALPerformanceEvaluation to a new module: hbase-diagnostics  Signed-off-by: Istvan Toth <stoty@apache.org> Signed-off-by: Nick Dimiduk <ndimiduk@apache.org>
apache,nifi,c1403db4f04a47b601073049a1efbddac91274e9,https://github.com/apache/nifi/commit/c1403db4f04a47b601073049a1efbddac91274e9,NIFI-14586  NIFI-14587: Expose Processors' Performance Metrics in the UI as part of the Status History; in doing so  I discovered a bug in which the GC time was not being tracked properly and fixed it. Sorted counter values lexicographically.  Signed-off-by: Pierre Villard <pierre.villard.fr@gmail.com>  This closes #9963.
apache,nifi,7c6a741e6bb8a19ff6520b542fbf84f0bd48d348,https://github.com/apache/nifi/commit/7c6a741e6bb8a19ff6520b542fbf84f0bd48d348,NIFI-14375 Refactored Kafka Components to Improve Partitioning  Performance  and Tests (#9807)  Co-authored-by: Pierre Villard <pierre.villard.fr@gmail.com> Co-authored-by: Jordan Sammut <jordan.sammut@gig.com> Signed-off-by: David Handermann <exceptionfactory@apache.org>
apache,nifi,e8e53c244cc6b7470c2486bea7d0a21b76850267,https://github.com/apache/nifi/commit/e8e53c244cc6b7470c2486bea7d0a21b76850267,NIFI-14077 Added Processing Performance Gauges to Flow Metrics (#9577)  Signed-off-by: David Handermann <exceptionfactory@apache.org>
apache,nifi,6bc0e384f40cdbb6e0b1e9155a6907a90d6fd5e1,https://github.com/apache/nifi/commit/6bc0e384f40cdbb6e0b1e9155a6907a90d6fd5e1,NIFI-13978 Improved Performance of Record Date Time Parsing  - Replaced DateTimeFormatter.parseBest() with custom TemporalQuery that checks for a ZoneId  This closes #9497.  Signed-off-by: Peter Turcsanyi <turcsanyi@apache.org>
apache,nifi,e9ce40d2091e5a5d98c37dcf752cb56a072339ba,https://github.com/apache/nifi/commit/e9ce40d2091e5a5d98c37dcf752cb56a072339ba,NIFI-12992 Reset Validation State when Updating Process Group Execution Engine (#9187)  - Do not reset validation status for a component if it is not in a state where validation will be performed  Signed-off-by: David Handermann <exceptionfactory@apache.org>
apache,nifi,69b99390e637de0d489b0764c1290825d6f521ca,https://github.com/apache/nifi/commit/69b99390e637de0d489b0764c1290825d6f521ca,NIFI-13800 Fixed Missing Registry Client Handling when Inheriting Cluster Flow (#9309)  When inheriting Registry Clients from a cluster's flow  any missing registry clients are removed at the end instead of the beginning of the synchronization logic. This allows those registry clients to still be referenced while performing synchronization. Also found that if a Processor is missing from cluster's flow but is running in local flow  on startup we get an error indicating that the processor does not belong to the associated process group so fixed that in tandem. Finally  noticed while verifying the fix that we check if the proposed flow is empty with a null check instead of using the isFlowEmpty() method - this could result in inheriting an empty flow from cluster even when a flow is loaded locally.  Signed-off-by: David Handermann <exceptionfactory@apache.org>
apache,nifi,bdaf514018d580adf37e20b586f5b451eaa1684b,https://github.com/apache/nifi/commit/bdaf514018d580adf37e20b586f5b451eaa1684b,NIFI-13688 Checked Validation Status before Fetching Parameters  - Added perform validation for Controller-level Services - Added wait on Parameter Provider validation status to allow for Controller Service enabling  Signed-off-by: Pierre Villard <pierre.villard.fr@gmail.com>  This closes #9210.
apache,nifi,f262e741704d1ab8bd314772445f5cbac609d533,https://github.com/apache/nifi/commit/f262e741704d1ab8bd314772445f5cbac609d533,NIFI-13439 Added Performance Status to Group Status responses  - Added ProcessingPerformanceStatus to nifi-api - Added Performance Status to Process Group and Processor Sources for Query NiFi Reporting Task  This closes #9014  Signed-off-by: David Handermann <exceptionfactory@apache.org>
apache,nifi,a039bc2b76b34172dbe705f6fa01e0d9377b1d3c,https://github.com/apache/nifi/commit/a039bc2b76b34172dbe705f6fa01e0d9377b1d3c,NIFI-13380: When determining if Record Type A is 'wider' than Record Type B  and both have a RECORD with the same name but different schemas  instead of determining that A is not wider than B  perform a recursive comparison to check if the RECORD within A's schema is wider than the RECORD within B's schema.  Signed-off-by: Matt Burgess <mattyb149@apache.org>  This closes #8948
201206030,novel,cd3a7206a99e482d43a1c61ddc4bc2c849845dab,https://github.com/201206030/novel/commit/cd3a7206a99e482d43a1c61ddc4bc2c849845dab,perf: instanceof 智能转型
201206030,novel,9d8709ed2d97d920345ba2a128db43c098c7a310,https://github.com/201206030/novel/commit/9d8709ed2d97d920345ba2a128db43c098c7a310,perf: 提前创建数据库连接池 Spring Boot 新版本默认会在第一次请求数据库时创建连接池
201206030,novel,60488258f5d36215c23327bacc699badeddce6c3,https://github.com/201206030/novel/commit/60488258f5d36215c23327bacc699badeddce6c3,perf: 提高接口第一次访问速度  Spring Boot 新版本默认会在第一次访问数据库时才创建连接池
00-Evan,shattered-pixel-dungeon,cc3c434d35369b9a48eda7e9da1ca4adcd44284d,https://github.com/00-Evan/shattered-pixel-dungeon/commit/cc3c434d35369b9a48eda7e9da1ca4adcd44284d,v2.5.0: improved text input performance regarding copy/paste
mik3y,usb-serial-for-android,9911e141a74d64ac587d320eed284c5aca08e54e,https://github.com/mik3y/usb-serial-for-android/commit/9911e141a74d64ac587d320eed284c5aca08e54e,01. Refactored SerialInputOutputManager (#615)  Used separate threads for reading and writing  enhancing concurrency and performance.  Note: before was possible to start `SerialInputOutputManager` with `Executors.newSingleThreadExecutor().submit(ioManager)`. Now you have to use `ioManager.start()`
CaffeineMC,sodium,c83c3fb30df0a6529c482c98f8ceff0501018644,https://github.com/CaffeineMC/sodium/commit/c83c3fb30df0a6529c482c98f8ceff0501018644,Combine draw commands to improve rendering performance (#2421)
CaffeineMC,sodium,0d4ab847519dfa7be84c562d6fe4e24761c96e7d,https://github.com/CaffeineMC/sodium/commit/0d4ab847519dfa7be84c562d6fe4e24761c96e7d,Do not cache ambient brightness at initialization  The world may not be assigned to the renderer at initialization  which is the case for non-terrain rendering (i.e. block entities.)  Likely  there is no performance benefit to caching this data in the first place  so the easiest solution is to just remove the code.
CaffeineMC,sodium,03aa768fce2ebbe98ead56576d914f571953bdd2,https://github.com/CaffeineMC/sodium/commit/03aa768fce2ebbe98ead56576d914f571953bdd2,Sort render lists for regions and sections after traversal (#2780)  Render sections and regions are sorted after the graph traversal is performed. This decouples their ordering from the graph  which isn't entirely correct for draw call sorting.  Fixes #2266
CaffeineMC,sodium,5e7c8ac4d78d7085041f3c8d477774480dba7dd4,https://github.com/CaffeineMC/sodium/commit/5e7c8ac4d78d7085041f3c8d477774480dba7dd4,Use alternative workaround for NVIDIA drivers  The NVIDIA driver enables a driver feature called "Threaded Optimizations" when it finds Minecraft  which causes severe performance issues and sometimes even crashes.  Newer versions of the driver seem to use a slightly different heuristic which our workaround doesn't address.  So  instead  use an alternative method that enables GL_DEBUG_OUTPUT_SYNCHRONOUS. This seems to reliably disable the functionality *even if* the user has configured it otherwise in their driver settings.  Additionally  on Windows  we now always indicate to the driver that Minecraft is running  so that users with hybrid graphics don't see regressed performance.
CaffeineMC,sodium,e7ea6f7dd5207c8fd0dac584a1134a16025a85de,https://github.com/CaffeineMC/sodium/commit/e7ea6f7dd5207c8fd0dac584a1134a16025a85de,Block the Overwolf Overlay due to graphical corruption  The overlay does not correctly restore the texture unit state in OpenGL  which causes problems when Minecraft thinks a texture has already been bound to a slot.  Since disabling the OpenGL state cache globally is not an acceptable solution (it would severely hurt performance) and their software doesn't give us any method to detect the problematic version  we block all versions.  gep_minecraft.dll is the payload they actually inject  which has no version information or description.  Fixes #2862
CaffeineMC,sodium,0939130bef947b78f0e2d6e62126026f39aad504,https://github.com/CaffeineMC/sodium/commit/0939130bef947b78f0e2d6e62126026f39aad504,Combine the vertex position attributes (#2753)  This improves terrain rendering performance significantly on Intel Xe-LP graphics under Linux.
CaffeineMC,sodium,cd18e7f9a19e0925ce80a1f43ce70d926366fd28,https://github.com/CaffeineMC/sodium/commit/cd18e7f9a19e0925ce80a1f43ce70d926366fd28,Use hardware copy for render target blits where possible (#2676)  Minecraft performs a blit using a fragment shader  which is unnecessary when blending is not used. Using the fixed function hardware to perform the blit is much faster and doesn't utilize the rasterization engine.
bitcoinj,bitcoinj,90963ee7302f0b9af196824cbc86d5461088d81e,https://github.com/bitcoinj/bitcoinj/commit/90963ee7302f0b9af196824cbc86d5461088d81e,MemoryFullPrunedBlockStore: remove type parameters from class `TransactionalMultiKeyHashMap`  Also  rename it to `TransactionalFullBlockMap` and update the JavaDoc.  Removing the type parameters and "de-genericizing" will make the code easier to read  enable further improvements  and likely performance optimizations that are specific to the "full block map" use case.
microsoft,typespec,a862ac7f18e66b1d93c9870bd8e50ad77b2ee07e,https://github.com/microsoft/typespec/commit/a862ac7f18e66b1d93c9870bd8e50ad77b2ee07e,http-client-java  fix eclipse languageserver when tmp folder is corrupted (#5307)  ### Situation I somehow encountered a folder corruption situation that the tmp folder for language server is there  but with no Jar file. Language server didn't start correctly  but the exception is thrown only when JSON RPC to the server is performed. <img width="1406" alt="Screenshot 2024-12-11 at 14 23 52" src="https://github.com/user-attachments/assets/1654fabc-bb71-4b81-aab7-b4c24b8afae3">   ### This PR - In case the language server did not start correctly  and user did not explicitly provide a language server path  force a re-download. - If the server failed to start anyway  throw with the process output.  ### Test #### Normal first download <img width="1372" alt="Screenshot 2024-12-09 at 20 12 06" src="https://github.com/user-attachments/assets/9126e436-cad2-4a03-9d7c-cb31d5bc4a95">   #### Normal downloaded <img width="1251" alt="Screenshot 2024-12-09 at 20 09 22" src="https://github.com/user-attachments/assets/2deb6fb3-acef-4d4b-8365-8a6ebf3ab3f8">   #### Corrupted folder  we do a re-download <img width="1306" alt="Screenshot 2024-12-09 at 20 08 35" src="https://github.com/user-attachments/assets/d8d56d6f-fd97-4b06-8327-08045cbadbff">  #### Server failed to start anyway  we throw with server output <img width="1383" alt="Screenshot 2024-12-10 at 16 48 25" src="https://github.com/user-attachments/assets/36f56e7d-0b58-4820-8c6f-a33dc559c902">
reactor,reactor-core,46773730a2e6ba02b93822a423023ceca142fcf3,https://github.com/reactor/reactor-core/commit/46773730a2e6ba02b93822a423023ceca142fcf3,Remove redundant return in Mono#subscribe (#3966)  The return statement at the end of the `subscribe(Subscriber)` method body was superflous.  Signed-off-by: Aleexender <alexkjh2011@gmail.com>
reactor,reactor-core,ebded61a33055f7ca9731b54285fcd17b436829c,https://github.com/reactor/reactor-core/commit/ebded61a33055f7ca9731b54285fcd17b436829c,Skip Automatic Context Propagation in special operators (#3845)  This change should improve performance of Automatic Context Propagation in certain cases when doOnDiscard  onErrorContinue  and onErrorStop are used.  The context-propagation integration requires contextWrite and tap operators to be barriers for restoring ThreadLocal values. Some internal usage of contextWrite does not require us to treat the operators the same way and we can skip the ceremony of restoring ThreadLocal state as we know that no ThreadLocalAccessor can be registered for them. Therefore  a private variant is introduced to avoid unnecessary overhead when not required.  Related #3840
osmandapp,OsmAnd,6b69d68f3551ff3fdde165ce39dd1a1f960ec4cd,https://github.com/osmandapp/OsmAnd/commit/6b69d68f3551ff3fdde165ce39dd1a1f960ec4cd,Add debug option to show tile performance metrics (#22574)
osmandapp,OsmAnd,a4cfda0ae68d92eaf6a20c7a5efef4f9774013b6,https://github.com/osmandapp/OsmAnd/commit/a4cfda0ae68d92eaf6a20c7a5efef4f9774013b6,Merge pull request #20062 from osmandapp/Performance_fix_19804  Performance fix 19804
osmandapp,OsmAnd,2b761dfadf85e1d545105810bd898fee7c34bc11,https://github.com/osmandapp/OsmAnd/commit/2b761dfadf85e1d545105810bd898fee7c34bc11,Revert for performance reasons
liquibase,liquibase,f8bb5963cb4e45e70192f348e135eac3c815e933,https://github.com/liquibase/liquibase/commit/f8bb5963cb4e45e70192f348e135eac3c815e933,feat: Add PostgreSQL Index Function Support via USING Clause (#6901)  * feat: Add PostgreSQL Index Type Support via USING Clause  This commit adds support for defining PostgreSQL index types through the USING clause. The implementation:  1. Adds a `using` property to `CreateIndexChange` and `Index` classes 2. Enhances JDBC snapshot capabilities to capture index types from PostgreSQL 3. Updates `CreateIndexGeneratorPostgres` to include the USING clause in SQL generation 4. Extends schema definitions by adding the `using` attribute to createIndex in XML 5. Updates related generators and statement classes to support the new property  This enhancement allows specifying index types like btree  gin  gist  etc. when creating/snapshoting indexes in PostgreSQL  providing better control over index behavior and performance.  * feat: add unit and integration tests  * chore: rename column to match metadata  * chore: rename column to match metadata  * Update liquibase-standard/src/main/java/liquibase/sqlgenerator/core/CreateIndexGenerator.java  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com>  * fix copilot fix  ---------  Co-authored-by: Copilot <175728472+Copilot@users.noreply.github.com> Co-authored-by: rberezen <ruslan.berezenskyi@gmail.com> Co-authored-by: Alex <abrackx@gmail.com>
liquibase,liquibase,7863041bf261a25ed5e9400fcfc9c25737c035eb,https://github.com/liquibase/liquibase/commit/7863041bf261a25ed5e9400fcfc9c25737c035eb,feat: new tagDatabase parameter to keep tag on rollback (#6835)  * feat: Keep tags during changelog rollback operations  Implement functionality to preserve tags when performing rollback operations in Liquibase. This ensures tag information is maintained in the database change history during rollback scenarios  providing better traceability and consistency for database version control.  * feat: rename methods + tests  * fix tests  * Rollback integration test added.  * chore: fix help message  ---------  Co-authored-by: Daniel Mallorga <dmallorga@liquibase.com>
liquibase,liquibase,d534727e88c6ecd17678ee92fe27276deb5c865a,https://github.com/liquibase/liquibase/commit/d534727e88c6ecd17678ee92fe27276deb5c865a,Fixes #6685 Performance issue of v4.30.0 (#6686)  Fixes #6685 Performance issue of an incorrect extract-variable refactoring of commit 89a3c3bc0bd211c8c5862af54d7fb9a22a5577ca  -- The original author extracted replacedSnapshotControl as a local variable but then failed passing it down the line. Instead the original snapshotControl object got passed which seems to operate on the entire catalog instead of just the example type in question.
liquibase,liquibase,946f4e47be62f710458e1adf29745301e0e7c95a,https://github.com/liquibase/liquibase/commit/946f4e47be62f710458e1adf29745301e0e7c95a,improve startup performance (DAT-18327) (#6366)
liquibase,liquibase,da5dfed11ad4b7ea64c8f4fcaf427b7dd60425b1,https://github.com/liquibase/liquibase/commit/da5dfed11ad4b7ea64c8f4fcaf427b7dd60425b1,update-to-tag fixes when provided tag doesn't exist or there is not tagDatabaseChange in the changelog (#6169)  * - Introduce STRICT usage for updateToTag. - Integration tests added.  * Moved tagDatabase check fields inside of STRICT if block.  * Disable update-to-tag warning message when using STRICT mode.  * Apply review comment and move tagChange validation from StatusChangeLogIterator to UpdateToTagCommandStep.  * UpdateToTag code refactoring done for Strict validations added.  * chore: switching to h2 for performance improvement  * chore: switching to h2 for performance improvement; changed tests order (!?); fxied exception message  * Move Strict tests to a new test class.  ---------  Co-authored-by: filipe <flautert@liquibase.org>
liquibase,liquibase,a964357fcffa050356947093e20a5ce6e23b83a6,https://github.com/liquibase/liquibase/commit/a964357fcffa050356947093e20a5ce6e23b83a6,Fixed performance issue with valueBlobFile on Postgres databases introduced in v4.18.0 (#6097)  Fixed performance issue with valueBlobFile on Postgres databases
liquibase,liquibase,6c018c37c8afe5346d438ed424912b84ba25fc4c,https://github.com/liquibase/liquibase/commit/6c018c37c8afe5346d438ed424912b84ba25fc4c,Added liquibase:snapshot command to the liquibase-maven-plugin (#1379)  * Added liquibase:snapshot command to the liquibase-maven-plugin  * - Update performLiquibaseTask implementation by replacing usage of SnapshotCommand by SnapshotCommandStep.  * review fixes  clean unused code  * fix NPE  * change snapshotFormat parameter description  ---------  Co-authored-by: Daniel Mallorga <dmallorga@liquibase.com>
OpenTSDB,opentsdb,93867ca30a16377b7012312bff1c71c940c67b17,https://github.com/OpenTSDB/opentsdb/commit/93867ca30a16377b7012312bff1c71c940c67b17,Add support for splitting rollup queries (#1853)  * Add an SLA config flag for rollup intervals  Adds a configuration option for rollup intervals to specify their maximum acceptable delay. Queries that cover a time between now and that maximum delay will need to query other tables for that time interval.  * Add global config flag to enable splitting queries  Adds a global config flag to enable splitting queries that would hit the rollup table  but the rollup table has a delay SLA configured. In that case  this feature allows splitting a query into to; one that gets the data from the rollups table until the time where it's guaranteed to be available  and the rest from the raw table.  * Add a new SplitRollupQuery  Adds a SplitRollupQuery class that suports splitting a rollup query into two separate queries. This is useful for when a rollup table is filled by e.g. a batch job that processes the data from the previous day on a daily basis. Rollup data for yesterday will then only be available some time today. This delay SLA can be configured on a per-table basis. The delay would specify by how much time the table can be behind real time.  If a query comes in that would query data from that blackout period where data is only available in the raw table  but not yet guaranteed to be in the rollup table  the incoming query can be split into two using the SplitRollupQuery class. It wraps a query that queries the rollup table until the last guaranteed to be available timestamp based on the SLA; and one that gets the remaining data from the raw table.  * Extract an AbstractQuery  Extracts an AbstractQuery from the TsdbQuery implementation since we'd like to reuse some parts of it in other Query classes (in this case SplitRollupQuery)  * Extract an AbstractSpanGroup  * Avoid NullPointerException when setting start time  Avoids a NullPointerException that happened when we were trying to set the start time on a query that would be eligible to split  but due to the SLA config only hit the raw table anyway.  * Scale timestamps to milliseconds for split queries  Scales all timestamps for split queries to milliseconds. It's important to maintain consistent units between all the partial queries that make up the bigger one.  * Fix starting time error for split queries  Fixes a bug that would happen when the start time of a query aligns perfectly with the time configured in the SLA for the delay of a rollup table. For a defined SLA  e.g. 1 day  if the start time of the query was exactly 1 day ago  the end time of the rollups part of the query would be updated and then be equal to its start time. That isn't allowed and causes a query exception.
LWJGL,lwjgl3,924d3ea2b3e96b77c37370fa5a32b65dfe355a0b,https://github.com/LWJGL/lwjgl3/commit/924d3ea2b3e96b77c37370fa5a32b65dfe355a0b,perf(core) restore custom loop in Java 17 memcpy
LWJGL,lwjgl3,216e13c3b833b3d000a356014c6f8b5bd1d05393,https://github.com/LWJGL/lwjgl3/commit/216e13c3b833b3d000a356014c6f8b5bd1d05393,perf(libdivide) support more JDK intrinsics  Added support for:  - Long.divideUnsigned - Long.remainderUnsigned - Math.unsignedMultiplyHigh  via multi-release MathUtil implementations. All libdivide functions now use JDK intrinsics internally and are significantly more efficient.
GeyserMC,Geyser,6452d11d951aa93b2a449f9e0bffd9820f0a7ab1,https://github.com/GeyserMC/Geyser/commit/6452d11d951aa93b2a449f9e0bffd9820f0a7ab1,Merge remote-tracking branch 'origin/fix-fabric-world-manager-performance' into feature/1.21.2
GeyserMC,Geyser,fb634e8528a1d9644c981151e89702447c508a33,https://github.com/GeyserMC/Geyser/commit/fb634e8528a1d9644c981151e89702447c508a33,Merge branch 'master' into fix-fabric-world-manager-performance
GeyserMC,Geyser,8122983765e85a108e80c85d18bb953d25dca273,https://github.com/GeyserMC/Geyser/commit/8122983765e85a108e80c85d18bb953d25dca273,Merge branch 'master' into fix-fabric-world-manager-performance
GeyserMC,Geyser,4c3ea83e20e4876788f6105d3c88d2459f4abb48,https://github.com/GeyserMC/Geyser/commit/4c3ea83e20e4876788f6105d3c88d2459f4abb48,Merge branch 'master' into fix-fabric-world-manager-performance
GeyserMC,Geyser,73f7259b6dd508680582418bbb7963ad0b460907,https://github.com/GeyserMC/Geyser/commit/73f7259b6dd508680582418bbb7963ad0b460907,Add proper text component parsing from NBT (#5029)  * Attempt creating a simple NBT text component parser  * Fix style merging  * Rename TextDecoration to ChatDecoration  use better style deserialization in ChatDecoration  * Remove unused code  * containsKey optimisations  update documentation  improve getStyleFromNbtMap performance slightly  more slight tweaks  * Remove unnecessary deserializeStyle method
GeyserMC,Geyser,0834c70b3ff1e5cb334f2ca58b0c93e7f5902329,https://github.com/GeyserMC/Geyser/commit/0834c70b3ff1e5cb334f2ca58b0c93e7f5902329,Merge branch 'master' into fix-fabric-world-manager-performance
GeyserMC,Geyser,f18a163eaa4a8a3eda3cff199ac04ad1c68d5315,https://github.com/GeyserMC/Geyser/commit/f18a163eaa4a8a3eda3cff199ac04ad1c68d5315,Merge branch 'master' into fix-fabric-world-manager-performance
GeyserMC,Geyser,293bad55bc5aa6e3a583663868f33766fb7ead18,https://github.com/GeyserMC/Geyser/commit/293bad55bc5aa6e3a583663868f33766fb7ead18,Merge branch 'master' into fix-fabric-world-manager-performance
cinit,QAuxiliary,b1c490da79285e6c3c429cc9991f3de4850dda42,https://github.com/cinit/QAuxiliary/commit/b1c490da79285e6c3c429cc9991f3de4850dda42,fix: standalone: do not perform CHA lookup in invokeOriginalMethod
cinit,QAuxiliary,4298e2203a403924c73d52d18ac6f2335b4c8503,https://github.com/cinit/QAuxiliary/commit/4298e2203a403924c73d52d18ac6f2335b4c8503,chore: override findClass instead of loadClass for better performance
runelite,runelite,194bf6c4893834c52b7529e2f160991a12020e43,https://github.com/runelite/runelite/commit/194bf6c4893834c52b7529e2f160991a12020e43,client: Fix WildcardMatcher regex DoS  Before this commit  wildcard strings with multiple repeated asterisks would be expanded to a string similar to ".*.*.*" before invoking `String#matches()` using the result. While this has no change in behavior  expanding enough asterisks would eventually cause this matching to perform badly and hang the client. Instead  expanding groups of asterisks to a single regex ".*" avoids this problem.
runelite,runelite,210c0c3d8fa1ac5a9d2f04e6f3d09a47cfa03bba,https://github.com/runelite/runelite/commit/210c0c3d8fa1ac5a9d2f04e6f3d09a47cfa03bba,entityhider: selectively hide party members  I placed party members above friends  since it is a logically more restrictive group than your entire friends list. It is likely to only contain the people you are actively playing with  and so you might want to hide everyone else  including friends.  PartyService#getMemberByDisplayName performs jagex name sanitization  so it may be desirable to use a cached form of this list if performance is a concern. Let me know and I can add it. I didn't notice any performance impact even at busy places like the GE.
runelite,runelite,3aa3f8ea58e6fe889867fc39b2a03ddcb0c7aa73,https://github.com/runelite/runelite/commit/3aa3f8ea58e6fe889867fc39b2a03ddcb0c7aa73,timers buffs: remove superfluous time and unit values
runelite,runelite,c36527d0f9c9ccef79fb900ec7d4a7c4af8f63ee,https://github.com/runelite/runelite/commit/c36527d0f9c9ccef79fb900ec7d4a7c4af8f63ee,menu swapper: fix crash performing Use swap on items with submenus  This was calling swap("use") on a submenu menu entry  findIndex() was finding the "Use" from the top level menu  calling swap() which then was passing the submenu MenuEntry to client.setMenuEntries()  optionIndexes used to be heavily used  but now it is only used for built in swaps and for item "Use" swap. Change it to be lazily computed and be per-menu.  Pass the Menu around to the various functions and use it instead of assuming the top level menu.
oshi,oshi,9556b9df09a7e9438205ca1fde73b97a62e818a5,https://github.com/oshi/oshi/commit/9556b9df09a7e9438205ca1fde73b97a62e818a5,Reduce redundant logging on perf counter failures (#2725)
oshi,oshi,422da23413990b22464eea440b94f5af666b9157,https://github.com/oshi/oshi/commit/422da23413990b22464eea440b94f5af666b9157,Do not log error on macOS for hw.nperflevels (#2711)  when the systctl call fails.
line,armeria,2bc4f1727a4675fcdb5b4d821e0ac56016c00d8c,https://github.com/line/armeria/commit/2bc4f1727a4675fcdb5b4d821e0ac56016c00d8c,Introduce `BraveRpcService` (#6115)  Motivation:  The motivation for this PR is better described in #6084  The changeset in this PR attempts to: - Expose `ArmeriaHttpServerParser` and `ArmeriaRpcServerParser` - By doing so  users can choose what information must be extracted from `BraveService` or `BraveRpcService`. This can provide more flexibility on whether to use only `BraveService`  only `BraveRpcService`  or both `BraveService` and `BraveRpcService` - Introduce `BraveRpcService` which allows users to perform sampling or request/response parsing based on `RpcRequest`  Modifications:  - Added `BraveRpcService`. By default  armeria-specific tags/annotations recorded by `BraveRpcService` is the same as `BraveService` - Exposed `ArmeriaHttpServerParser` and `ArmeriaRpcServerParser` to allow users to easily construct a `RpcTracing` or `HttpTracing`  Result:  - Users may use `RpcRequest`  `RpcResponse` to apply sampling/tags/annotations  <!-- Visit this URL to learn more about how to write a pull request description:  https://armeria.dev/community/developer-guide#how-to-write-pull-request-description -->  ---------  Co-authored-by: Trustin Lee <trustin@linecorp.com>
line,armeria,c3b22a1f7dca346991e82ab72ca76774dbabf04e,https://github.com/line/armeria/commit/c3b22a1f7dca346991e82ab72ca76774dbabf04e,Fix the `VirtualHost#normalizeHostnamePattern` method to not perform unnecessary operations (#6208)  Motivation:  I found that the `VirtualHost#normalizeHostnamePattern` method was performing unnecessary operations.  Modifications:  - Moved the code to remove the wildcard from `hostnamePattern` inside the `if then` scope.  Result:  - I expect that operations to be performed only if `hostnamePattern` is not a wildcard.  <!-- Visit this URL to learn more about how to write a pull request description:  https://armeria.dev/community/developer-guide#how-to-write-pull-request-description -->
line,armeria,d7481584954c4953cdf6019d333ccf01f78a88f1,https://github.com/line/armeria/commit/d7481584954c4953cdf6019d333ccf01f78a88f1,Fixed a bug where the default setting for DNS over TCP was unintentionally disabled (#6202)  Motivation:  DNS over TCP configuration should be enabled by default in DNS resolvers. However  the default setting was unintentionally disabled during the implementation of #6127.  Motifications:  - Revert the default settings to re-enable TCP fallback in DNS resolver.  Result:  The default DNS resolver now correctly performs TCP fallback.
line,armeria,65629276366b5eb7c1094bb492759789094f437a,https://github.com/line/armeria/commit/65629276366b5eb7c1094bb492759789094f437a,Exclude attributes from `Endpoint.toString()` (#6061)  Motivation:  We observed `OutOfMemoryError` in internal CI tests when a new `Endpoint` is created. ``` com.linecorp.armeria.xds.client.endpoint.EndpointsPool$$Lambda$754/1708334230@3a58f44e java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOfRange(Arrays.java:3664) at java.lang.String.<init>(String.java:207) at java.lang.StringBuilder.toString(StringBuilder.java:412) at com.google.protobuf.TextFormat$Printer.printToString(TextFormat.java:593) at com.google.protobuf.AbstractMessage.toString(AbstractMessage.java:87) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:136) at com.linecorp.armeria.internal.shaded.guava.collect.AbstractMapEntry.toString(AbstractMapEntry.java:70) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:136) at com.linecorp.armeria.internal.shaded.guava.collect.Iterators.toString(Iterators.java:294) at com.linecorp.armeria.common.ImmutableAttributes.toString(ImmutableAttributes.java:183) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:136) at com.linecorp.armeria.client.Endpoint.generateToString(Endpoint.java:304) at com.linecorp.armeria.client.Endpoint.<init>(Endpoint.java:268) at com.linecorp.armeria.client.Endpoint.replaceAttrs(Endpoint.java:747) at com.linecorp.armeria.client.Endpoint.withAttr(Endpoint.java:707) at com.linecorp.armeria.xds.client.endpoint.EndpointsPool.withTimestamp(EndpointsPool.java:103) at com.linecorp.armeria.xds.client.endpoint.EndpointsPool.cacheAttributesAndDelegate(EndpointsPool.java:71) at com.linecorp.armeria.xds.client.endpoint.EndpointsPool.lambda$updateClusterSnapshot$1(EndpointsPool.java:62) at com.linecorp.armeria.xds.client.endpoint.EndpointsPool$$Lambda$754/1708334230.run(Unknown Source) ``` When an `Endpoint` is initicated  it pre-generates a string representation for caching and reuse. In our tests  it contained many attributes related to xDS for service discovery. It might be a good idea to remove attributes whose size is hard to predict from `toString()` and prevent unintended OOM.  In addition  the result size of `toString()` gets small  so I doubt that the pre-generated cache is useful for performance.  Modifications:  - Remove attributes from `toString()`. - Lazily build the string representation when `toString()` is called and don't cache the result.  Result:  `Endpoint` no longer includes attributes in `.toString()`.
line,armeria,79112f537eb851aa2aba3b534777a608f2c71e0d,https://github.com/line/armeria/commit/79112f537eb851aa2aba3b534777a608f2c71e0d,Fix race condition where `log.whenComplete` may not complete (#5986)  Motivation:  It has been reported that `log.whenComplete` is not completing in some cases in #5981. The cause seems to be a race condition between `DefaultRequestLog#endRequest` and `DefaultRequestLog#requestContent`. Completion of `log.whenComplete` is important because 1) it is semantically bound to an HTTP request 2) users (including us) have clean up logic using `log.whenComplete`.  I propose that simply `endRequest` only sets `name` if content is not deferred  and `requestContent` sets `name` if content is deferred. The logic is easier to reason about  and there are minimal performance implications since a lock is held anyways.  Modifications:  - `#endRequest` sets `name` if `requestContent` isn't deferred - `#requestContent` sets name if `requestContent` is deferred  Result:  - Closes #5981  <!-- Visit this URL to learn more about how to write a pull request description:  https://armeria.dev/community/developer-guide#how-to-write-pull-request-description -->
line,armeria,93088a30891ce0c1ce2503d51bc7e3f73158d30a,https://github.com/line/armeria/commit/93088a30891ce0c1ce2503d51bc7e3f73158d30a,Avoid redundant fromObject(value) calls in addObject methods (#5962)  Motivation:  In the addObject and addObjectAndNotify methods  the fromObject(value) function is being called redundantly. This results in unnecessary computations  which could affect performance and reduce code readability. By removing the redundant calls to fromObject(value)  we can improve the efficiency and clarity of the code.  Modifications:  - Modified the addObject method to pass the original value directly to the addObjectAndNotify method without calling fromObject(value). - Updated the addObjectAndNotify method to perform the fromObject(value) call once  ensuring it is not called multiple times for the same value.  Result:  - Improved code readability and maintainability. - Eliminated the redundant fromObject(value) calls  enhancing performance.  <!-- Visit this URL to learn more about how to write a pull request description:  https://armeria.dev/community/developer-guide#how-to-write-pull-request-description -->
line,armeria,c1d547518c8e14ddd5e723d02de7bb2a63f9c003,https://github.com/line/armeria/commit/c1d547518c8e14ddd5e723d02de7bb2a63f9c003,Fix a leak in `HttpEncodedResponse` (#5858)  Motivation:  An `HttpData` produced in `HttpEncodedResponse.beforeComplete()` is not collected by `CollectingSubscriberAndSubscription` but is leaked.  ```java Hint: {10B  pooled  <unknown>} com.linecorp.armeria.common.HttpData.wrap(HttpData.java:110) com.linecorp.armeria.server.encoding.HttpEncodedResponse.beforeComplete(HttpEncodedResponse.java:163) com.linecorp.armeria.common.stream.FilteredStreamMessage.lambda$collect$0(FilteredStreamMessage.java:201) java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934) java.base/java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:950) java.base/java.util.concurrent.CompletableFuture.handle(CompletableFuture.java:2340) com.linecorp.armeria.common.stream.FilteredStreamMessage.collect(FilteredStreamMessage.java:142) ```  `CollectingSubscriberAndSubscription` was designed to only apply `filter()` to the `upstream.collect()`. I didn't consider that an object could be published via `onNext()` in `beforeComplete()`. The purpose of `CollectingSubscriberAndSubscription` was to provide an optimized code path for unary calls. it didn't seem the code provides a trivial performance improvement but the implementation was complex and error-prone.  I was able to fix the code not to leak the data but I didn't want to additional complexity to it. It might be cleaner to use the Reactive Streams API instead of keeping the custom `collect()` implementation. There will be no change in performance for normal message sizes.  Modifications:  - Remove the custom `collect()` implemtation in `FilteredStreamMessage`.  Result:  Fix a potential leak when sending compressed responses.
line,armeria,ea194006ada6b0c6785cee2dcc972324a5dbae0d,https://github.com/line/armeria/commit/ea194006ada6b0c6785cee2dcc972324a5dbae0d,Support colons in path for server-side (#5676)  Motivation:  Currently  gRPC verb suffix is supported by introducing a `VerbSuffixPathMapping`. There were several issues with this approach: - `VerbSuffixPathMapping` was applied inconsistently. - For instance  `RouteBuilder path(String pathPattern)` applies `VerbSuffixPathMapping`  but `RouteBuilder path(String prefix  String pathPattern)` doesn't. - In the context of `ExactPathMapping`  `VerbSuffixPathMapping` was acting as a workaround to support colons in the path. Additionally the support for colons is incomplete since only the last colon is correctly handled. - Another side-effect of this is route collisions are incorrectly reported since the verb suffix isn't taken into account. - Continued support of `VerbSuffixPathMapping` makes supporting colons natively difficult. - Misc) A path such ending with an escape character failed to start up the server: `/path\\`  The original motivation for #5613 was that the following patterns weren't supported. - /path/{param}:verb - /path/literal:verb  I propose that the first case be handled via using `REGEX` `PathMapping` types  and the second case be handled by natively supporting colons in our path parameters. Note that colons are supported per the [rfc3986](https://datatracker.ietf.org/doc/html/rfc3986#section-3.3)  and we already have an issue https://github.com/line/armeria/issues/4577.  1. Supporting `/path/{param}:verb`  I propose that we support this simply by introducing a new `PathMappingType.REGEX`. We can simply check if the last `PathSegment` is a `VerbPathSegment`. If it is a `VerbPathSegment`  then we can just use `PathMappingType.REGEX`.  2. Supporting `/path/literal:verb`  Internally  we represent colons as parameterized variables both in `ParameterizedPathMapping` and `RoutingTrieBuilder`. This makes it difficult to support colons  so I propose that we modify the internal representation to a different character (`\0`). This character isn't a valid path character per [rfc3986](https://datatracker.ietf.org/doc/html/rfc3986#section-3.3)  so I believe the impact is minimal.  One side effect of this approach is that `ParameterizedPathMapping#paths` will return null character delimited paths. e.g. `/v0/:/path` -> `/v0/\0/path` Having said this  I believe the normal user path doesn't really use this value so it shouldn't matter.  3. Supporting colons in general  When one calls `RouteBuilder#path(String)`  we determine whether to use `ParameterizedPathMapping` or `ExactPathMapping` depending on whether a colon is used.  https://github.com/line/armeria/blob/8ab42847c146b481c72f000dbc1c4d77dc009220/core/src/main/java/com/linecorp/armeria/server/RouteBuilder.java#L546  - If the colon does not start the segment (e.g. `/a:b`)  it is trivial to assume that the colon should be matched exactly. - If the colon does start the segment (e.g. `/:param`)  it is ambiguous whether the user intended this to be a literal or parameter.  For this case  I propose the following: - If a colon is used as-is  `/:param`  then the segment will be used to represent a parameter - If a colon is escaped  `/\\:param`  then the segment will be treated as a literal  Optimization) `ExactPathMapping` is more performant than `ParameterizedPathMapping` because a simple equality check is done. I propose as an optimization we modify the condition to check if `/:` is contained instead of `:`. As a downside of this approach  the colon escape logic needs to also be added to `ExactPathMapping`. This can be undone if this logic seems too complicated though.  Modifications:  - When `VerbPathSegment` is used  use `RegexPathMapping` for gRPC transcoding. - Removed `VerbSuffixPathMapping` and related changes in `RoutingTrieBuilder`  `RouteBuilder`  and `ParameterizedPathMapping` - Modified `RoutingTrieBuilder` and `ParameterizedPathMapping` to use `\0` instead of ':' to represent parameterized path segments. This allows us to use ':' in most `PathMapping` types. - Relaxed `ParameterizedPathMapping` to act like `ExactPathMapping` when a path segment isn't capturing a path parameter. - Replace `/\\:` to `/:` in `ParameterizedPathMapping` and `ExactPathMapping` to give users an option to use the first colon as a literal.  Result:  - https://github.com/line/armeria/issues/4577 is closed - Logic related to gRPC verb suffixes is generalized and cleaned up  <!-- Visit this URL to learn more about how to write a pull request description: https://armeria.dev/community/developer-guide#how-to-write-pull-request-description -->
apache,ignite,36ec141e7a7f29d33e6e11e3e63e1f82cc2a4d99,https://github.com/apache/ignite/commit/36ec141e7a7f29d33e6e11e3e63e1f82cc2a4d99,IGNITE-25376 PerfStat: fix reading empty unsupported system views (#12059)
apache,ignite,15d8fb31f6bcc0e8f545bbf55492493efb8820fc,https://github.com/apache/ignite/commit/15d8fb31f6bcc0e8f545bbf55492493efb8820fc,IGNITE-24287 Ignore node.attributes system view to record in PerfStat (#12045)
apache,ignite,9697e0e104b8733e755225c630abefc3a0b6e60e,https://github.com/apache/ignite/commit/9697e0e104b8733e755225c630abefc3a0b6e60e,IGNITE-24286 Recording systemViews to perfStart report (#11826)
apache,ignite,10ea05c9885be94f73cd67981f7c791b8de6a0d7,https://github.com/apache/ignite/commit/10ea05c9885be94f73cd67981f7c791b8de6a0d7,IGNITE-24284 SQL Calcite: Fix flaky SqlDiagnosticIntegrationTest.testPerformanceStatistics - Fixes #12039.  Signed-off-by: Aleksey Plekhanov <plehanov.alex@gmail.com>
apache,ignite,6b67d839cdd54299d938b4317fa8c027925a5d44,https://github.com/apache/ignite/commit/6b67d839cdd54299d938b4317fa8c027925a5d44,IGNITE-24777 Fix reading performance statistics report due to miss-cached strings (#11949)  Thank you for submitting the pull request to the Apache Ignite.  In order to streamline the review of the contribution we ask you to ensure the following steps have been taken:  ### The Contribution Checklist - [ ] There is a single JIRA ticket related to the pull request. - [x] The web-link to the pull request is attached to the JIRA ticket. - [ ] The JIRA ticket has the _Patch Available_ state. - [x] The pull request body describes changes that have been made. The description explains _WHAT_ and _WHY_ was made instead of _HOW_. - [x] The pull request title is treated as the final commit message. The following pattern must be used: `IGNITE-XXXX Change summary` where `XXXX` - number of JIRA issue. - [ ] A reviewer has been mentioned through the JIRA comments (see [the Maintainers list](https://cwiki.apache.org/confluence/display/IGNITE/How+to+Contribute#HowtoContribute-ReviewProcessandMaintainers)) - [x] The pull request has been checked by the Teamcity Bot and the `green visa` attached to the JIRA ticket (see [TC.Bot: Check PR](https://mtcga.gridgain.com/prs.html))  ### Notes - [How to Contribute](https://cwiki.apache.org/confluence/display/IGNITE/How+to+Contribute) - [Coding abbreviation rules](https://cwiki.apache.org/confluence/display/IGNITE/Abbreviation+Rules) - [Coding Guidelines](https://cwiki.apache.org/confluence/display/IGNITE/Coding+Guidelines) - [Apache Ignite Teamcity Bot](https://cwiki.apache.org/confluence/display/IGNITE/Apache+Ignite+Teamcity+Bot)  If you need any help  please email dev@ignite.apache.org or ask anу advice on http://asf.slack.com _#ignite_ channel.
apache,ignite,86d85f9c191ee8a11f18504c17aabdbd186b83f3,https://github.com/apache/ignite/commit/86d85f9c191ee8a11f18504c17aabdbd186b83f3,IGNITE-23901 Added performance statistics for putAllConflict  removeAllConflict operations (#11793)
apache,ignite,fab69a781090291d7b08a02f29356959171a6a8e,https://github.com/apache/ignite/commit/fab69a781090291d7b08a02f29356959171a6a8e,IGNITE-24385 Fixed performance drop introduced by SQL plan history system view - Fixes #11848.  Signed-off-by: Aleksey Plekhanov <plehanov.alex@gmail.com>
apache,ignite,8ee63d7fc2c6a4965c992a27c5660972bd9921e0,https://github.com/apache/ignite/commit/8ee63d7fc2c6a4965c992a27c5660972bd9921e0,IGNITE-24168 Fixed performance drop caused by IGNITE-22375 (#11797)
apache,ignite,add61eb58874a6199b4f23f739efad55155f6bcd,https://github.com/apache/ignite/commit/add61eb58874a6199b4f23f739efad55155f6bcd,IGNITE-22964 Java thin: fix client init hang on unreachable discovered address (#11486)  * Do not perform cluster discovery synchronously while initializing the client - do `applyOnDefaultChannel` before checking `channelsCnt.get() == 0` in `channelsInit` * Do not disconnect active channels when performing discovery  even if those addresses are not in the new list to avoid unnecessary reconnects
jindrapetrik,jpexs-decompiler,680044eef558fdacb9f438ba6b9f0325ff172f54,https://github.com/jindrapetrik/jpexs-decompiler/commit/680044eef558fdacb9f438ba6b9f0325ff172f54,spelling: perform  Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
bisq-network,bisq,a136f79bd148c6bdf1713bca58c6cbab9f38f99c,https://github.com/bisq-network/bisq/commit/a136f79bd148c6bdf1713bca58c6cbab9f38f99c,Add new payment method "Faster Payments System (SBP)" for Russian Ruble  This is the standard P2P payment method in Russia to perform funds transfers and payments between Russian bank accounts in Russian Rubles. There is no chargeback risk. Recipient bank account is located using telephone number and bank name  and sender receives recipients first name  middle name  and initial of last name to confirm the phone number entered is correct before sending. Adding this new payment method has been discussed at length on the GitHub 'growth' channel at: https://github.com/bisq-network/growth/issues/288
apache,calcite,8f1dce91b61309db67355fa62372cff45d9803e9,https://github.com/apache/calcite/commit/8f1dce91b61309db67355fa62372cff45d9803e9,[CALCITE-6966] Change JoinConditionOrExpansionRule name and accept more predicates that will allow the expansion to be performed
apache,calcite,c8a513b96185374345ea24e64deebfdbcd126268,https://github.com/apache/calcite/commit/c8a513b96185374345ea24e64deebfdbcd126268,[CALCITE-6720] Refactor cross product logic in RelMdUniqueKeys#getPassedThroughCols using Linq4j#product  The RelMdUniqueKeys#getPassedThroughCols method exists only for performing a cross product of the various mappings between input and output "pass through" columns.  The entire method can be replaced by exploiting the built-in Linq4j#product API and few other utility methods.  After the refactoring the code is easier to follow and potentially more efficient since the result is computed gradually and we don't have to retain the entire cross product result in memory.
apache,calcite,2d6e9a7dda70313bbfcf0e39f4cffc1404f1f521,https://github.com/apache/calcite/commit/2d6e9a7dda70313bbfcf0e39f4cffc1404f1f521,[CALCITE-6471] Improve performance of SqlToRelConverter by preventing unconditional conversion of sqlNodes to string for null-check messages
apache,calcite,1cdf864e9f853284f05aa3bf8515951a018ae22e,https://github.com/apache/calcite/commit/1cdf864e9f853284f05aa3bf8515951a018ae22e,[CALCITE-6454] Implement array comparison operators  * <  <=  >  >= now work for arrays and rows * Can also sort arrays and rows * Comparison is performed along corresponding indexes * Longer arrays are considered greater * null is considered greater than anything * Cannot change whether nulls are first or last
SPLWare,esProc,bd2221985d446368e25118b19ec7cb67b460f33a,https://github.com/SPLWare/esProc/commit/bd2221985d446368e25118b19ec7cb67b460f33a,Do performance optimization.
SPLWare,esProc,4fe1387ae2ee97094a01ca43057846ff6b4bc8b9,https://github.com/SPLWare/esProc/commit/4fe1387ae2ee97094a01ca43057846ff6b4bc8b9,Added functions such as AES  DES  DESEDE  and RSA to perform corresponding encryption and decryption operations.
SPLWare,esProc,3e24ea5f4b6d20e5c19522105bc496775d0c7bf3,https://github.com/SPLWare/esProc/commit/3e24ea5f4b6d20e5c19522105bc496775d0c7bf3,Added functions such as AES  DES  DESEDE  and RSA to perform corresponding encryption and decryption operations.
apache,maven,4fc2ea08d4f7cd94126bc1b70d9ba59ff91c0420,https://github.com/apache/maven/commit/4fc2ea08d4f7cd94126bc1b70d9ba59ff91c0420,[MNG-8615] [MNG-8616] Maven core extensions handling improvements (#2147)  It all started with MNG-8615 to not swallow DI problems while loading core extensions. Then tried to add origin as well  but it turns out there is lack of context. Then turned out models are not location tracking. Then came MNG-8616 as Maven was too rigid and did not apply precedence for extensions making project hopping problematic  if not impossible...  Changes: * do not swallow DI issues happened during core extension loading; belly up instead. * report which extension caused DI issue * make core extension models and IO location tracking * alter CLI api to carry all extensions "by origin" (project  user  installation) * parser should be plain dumb  all it does is load extensions in precedence order and validates them (validity: they must be GA unique) * DI capsule performs now "selection" of extensions (based on precedence) and loads them as before * finally: we have now DEBUG logs which all extensions were considered and which were effectively loaded  something I missed a lot * new UTs revealed MavenInvokerUT problem: ClassWorlds were not cleaned up properly  Behavioural change since 4.0.0-rc-3: * conflict within same source (same file) makes Maven fail - as before * conflict spanning across sources is warning only; precedence is applied to select extension to be loaded  ---  https://issues.apache.org/jira/browse/MNG-8615 https://issues.apache.org/jira/browse/MNG-8616
apache,maven,f2bc813529b112f19526b70b5063464bc98c7eb7,https://github.com/apache/maven/commit/f2bc813529b112f19526b70b5063464bc98c7eb7,[MNG-8575] Replace a list with O(N²) performance by O(N) at least during iteration. (#2092)  * Replace a list with O(N²) performance by O(N) at least during iteration. * Remove a comment which is not true anymore. * Replace `CopyOnWriteArrayList` by `LinkedHashSet` for avoiding to iterate over all previous values every time that a new value is added. * Short-circuit for `List.isEmpty()`: stop at the first element found  without iterating over all elements.
micrometer-metrics,micrometer,86bb7508199e7364a8b84819c785b54bb1daa652,https://github.com/micrometer-metrics/micrometer/commit/86bb7508199e7364a8b84819c785b54bb1daa652,Improve average performance of LongTaskTimer for out-of-order stopping (#5591)  * Improve average performance of LongTaskTimer for out-of-order stopping  * Polish DefaultLongTaskTimer and its tests  ---------  Co-authored-by: Jonatan Ivanov <jonatan.ivanov@gmail.com>
micrometer-metrics,micrometer,d77b7be4484cc31bd85475c3c9d7ca8d33e82ab2,https://github.com/micrometer-metrics/micrometer/commit/d77b7be4484cc31bd85475c3c9d7ca8d33e82ab2,Fix concurrency issue with Exponential Histogram (#5749)  Synchronize writes to exponential histogram. An alternative would be to use explicit locks  but this made the code more complex and did not yield significant performance improvements over using a synchronized method. Also adds a concurrency test that reproduced the reported issue and verifies this fixes it.  Fixes gh-5740
micrometer-metrics,micrometer,3625a9eae0a0275eded5237468be078e300c7936,https://github.com/micrometer-metrics/micrometer/commit/3625a9eae0a0275eded5237468be078e300c7936,Fix performance regression in MeterRegistry#remove (#5750)  Adds a reverse look-up for the pre-filter meter ID for use when removing a Meter. This avoids the need to iterate over the meters in the cache (preFilterIdMeterMap)  which scales linearly with the number of meters  and is problematic because it does this while holding the meterMap lock needed to add new meters. Also adds benchmarks for measuring the performance of the remove method with different numbers of meters registered.  Resolves gh-5466
micrometer-metrics,micrometer,3c252c25477c5d875d7e78d29f375bb9a8cfb3a6,https://github.com/micrometer-metrics/micrometer/commit/3c252c25477c5d875d7e78d29f375bb9a8cfb3a6,Add benchmarks for DefaultLTT start/stop (#5595)  * Add benchmarks for DefaultLTT start/stop  Start and stop are called on the critical path. We should have benchmarks for them to evaluate changes that may affect their performance.  * Stop a random sample instead of middle sample  We should get a better idea of average performance with a random sample rather than a sample in a fixed position in the active tasks collection.  * AverageTime BenchmarkMode instead of single-shot  Using the invocation-level setup method  we can use average-time rather than single shot.
micrometer-metrics,micrometer,1d498f6b6266a547862c8727060cf36122bb6257,https://github.com/micrometer-metrics/micrometer/commit/1d498f6b6266a547862c8727060cf36122bb6257,Apply performance improvement from Tags to KeyValues  See gh-4959.  Resolves gh-5140
micrometer-metrics,micrometer,1482cdf46babc40273d24ebf572b0b15aec7bfd2,https://github.com/micrometer-metrics/micrometer/commit/1482cdf46babc40273d24ebf572b0b15aec7bfd2,Tags merge optimization (#4959)  Improves the performance of merging two Tags instances by taking advantage of the fact that their internal representation of tags is always sorted and deduplicated. Therefore  they can be merged more efficiently than a collection of tags that may not be sorted and deduplicated. Added benchmark to measure Tags.and(Tags) operation.  See gh-5140
micrometer-metrics,micrometer,bb2ff45562a79f810bb1aafe24d3f55cb1df551a,https://github.com/micrometer-metrics/micrometer/commit/bb2ff45562a79f810bb1aafe24d3f55cb1df551a,Support for ExponentialHistogram in OTLP Registry (#3959)  Adds support for https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/metrics/data-model.md#exponentialhistogram. Math used for index calculation is re-used from the OTEL specification which lays down the formula/techniques to be considered for index calculation also keeping performance in mind.  Some new configuration options are added to `OtlpConfig` for controlling the behavior. A `histogramFlavor` method controls whether the existing explicit bucket histograms will be used or exponential histograms. The max scale and max bucket count of the exponential histograms can also be configured for the registry via the corresponding methods added to `OtlpConfig`.  Resolves gh-3861
nextcloud,android,efd437c5901f6a9069274d22cd623ed8e528ce11,https://github.com/nextcloud/android/commit/efd437c5901f6a9069274d22cd623ed8e528ce11,Merge pull request #14829 from nextcloud/performance  Fix Performance Warnings
nextcloud,android,59de186cdad59b5986db3e10ee0485cc39883418,https://github.com/nextcloud/android/commit/59de186cdad59b5986db3e10ee0485cc39883418,fix performance issues  Signed-off-by: Alex Knop <alexknoptech@protonmail.com>
nextcloud,android,385c424567966a7aa3f5a3b5cc8e24b9ff1fe438,https://github.com/nextcloud/android/commit/385c424567966a7aa3f5a3b5cc8e24b9ff1fe438,fix performance issues  Signed-off-by: Alex Knop <alexknoptech@protonmail.com>
nextcloud,android,7a7b31072e52b572254cf85790bc2147a2477e3e,https://github.com/nextcloud/android/commit/7a7b31072e52b572254cf85790bc2147a2477e3e,fix performance issues  Signed-off-by: Alex Knop <alexknoptech@protonmail.com>
nextcloud,android,0a7399e9d17d0e4e3fcac1d24d5bd2288269a9bf,https://github.com/nextcloud/android/commit/0a7399e9d17d0e4e3fcac1d24d5bd2288269a9bf,fix java.lang.IllegalStateException: Can not perform this action after onSaveInstanceState at com.owncloud.android.ui.fragment.OCFileListFragment.lambda$openActionsMenu$9(OCFileListFragment.java:702)  Signed-off-by: alperozturk <alper_ozturk@proton.me>
nextcloud,android,a7433351e56d39e658ed4192c5ee60b037ab7f5d,https://github.com/nextcloud/android/commit/a7433351e56d39e658ed4192c5ee60b037ab7f5d,Merge pull request #13700 from Onkar755/improve-bitmaputils  Improve BitmapUtils: Bitmap Handling and Image Processing for Modern APIs and Performance Enhancements
nextcloud,android,ffc0528605019fafe1faced38d19144cd1c7899f,https://github.com/nextcloud/android/commit/ffc0528605019fafe1faced38d19144cd1c7899f,refactor(ui): coroutine based user status retrieval.  Replaced ancient async task with a coroutine based alternative. This comes with numerous advantages.  - Improved performance due to the nature of coroutines and the reduced overhead associated with AsyncTasks and threads. Approx a %50 performance increase was measured during testing. - Passing references to fragment lifecycle context to an AsyncTask is generally a bad idea.
xinnan-tech,xiaozhi-esp32-server,7c5f2d95a40fed2c0b2a5de7fb50678ffe80fc4d,https://github.com/xinnan-tech/xiaozhi-esp32-server/commit/7c5f2d95a40fed2c0b2a5de7fb50678ffe80fc4d,🎈 perf: 更新文档+优雅的退出 (#292)  * 🎈 perf: 更新文档+优雅的退出  * update:gsv代码可以使用gpt_sovits_v2适配器代码拓展，不需要单独。如果需要定义模型名称，可以在gpt_sovits_v2代码中添加扩展  ---------  Co-authored-by: hrz <1710360675@qq.com>
oracle,opengrok,05519d9ae437bb0f4bf29ed175c64a73846c1f7e,https://github.com/oracle/opengrok/commit/05519d9ae437bb0f4bf29ed175c64a73846c1f7e,bubble indexParallel exceptions up to the indexer main  also get rid of the capability to perform per directory reindex  fixes #4696
oracle,opengrok,f7dbff57fb9fdb17cd4032c69adafbbf2459d47d,https://github.com/oracle/opengrok/commit/f7dbff57fb9fdb17cd4032c69adafbbf2459d47d,History based reindex vs. git revert (#4609)  avoid processing untouched files when performing history based reindex  fixes #4607
deepjavalibrary,djl,85d09ba667c021610edd3965965ffcbeff3af8e5,https://github.com/deepjavalibrary/djl/commit/85d09ba667c021610edd3965965ffcbeff3af8e5,[api] Adds FUSE based repository support (#3695)  * [api] Adds FUSE based repository support  1. No cloud SDK dependencies 2. No need to download the entire bucket 3. High performance  * Update api/src/test/java/ai/djl/repository/FuseRepositoryTest.java  Co-authored-by: Frank Liu <frankfliu2000@gmail.com>  ---------  Co-authored-by: Frank Liu <frankfliu2000@gmail.com>
deepjavalibrary,djl,41f75681aab8708c375e94f0a99ad7673a74f7ae,https://github.com/deepjavalibrary/djl/commit/41f75681aab8708c375e94f0a99ad7673a74f7ae,[api] Improve listModel performance (#3641)
deepjavalibrary,djl,f91a696fdbab2037e86aa1d91741dcd973c7ef62,https://github.com/deepjavalibrary/djl/commit/f91a696fdbab2037e86aa1d91741dcd973c7ef62,[api] Optimized text embedding post processing performance (#3459)
deepjavalibrary,djl,c895909f47595b4f544acd67fd18b963d9d4c872,https://github.com/deepjavalibrary/djl/commit/c895909f47595b4f544acd67fd18b963d9d4c872,[enhancement] Optimize memory copy overhead to enhance performance. (#3289)
ant-media,Ant-Media-Server,ec14d51720bcc9d6a05f1e93f2f11afe45bbb9bf,https://github.com/ant-media/Ant-Media-Server/commit/ec14d51720bcc9d6a05f1e93f2f11afe45bbb9bf,Merge pull request #7185 from ant-media/mongoQueryOptimizations  Use separate locks for MongoDB queries to perform faster
dromara,dynamic-tp,7cae3db4d14db504caa38cd0fb049ffe0a542a83,https://github.com/dromara/dynamic-tp/commit/7cae3db4d14db504caa38cd0fb049ffe0a542a83,Merge pull request #537 from yanhom1314/master  optimize adapter  perfect log and check
dromara,dynamic-tp,8f585de916851f298b0febd94bf8300082ad4962,https://github.com/dromara/dynamic-tp/commit/8f585de916851f298b0febd94bf8300082ad4962,Merge pull request #510 from KamToHung/perfect_JreEnum  [ISSUE #509] JreEnum lessThan and greaterThan method
pytorch,serve,c7bbf2cf74b2cf34870f2ddee959dbcee38725bb,https://github.com/pytorch/serve/commit/c7bbf2cf74b2cf34870f2ddee959dbcee38725bb,Torchserve support for Intel GPUs (#3132)  * Merge changes from intel-sandbox/serve  * ipex_gpu_enable - New config in config.properties  * Instructions for IPEX GPU support  * Final Commits 1  * Style: Refactor code formatting  * Readme Updated  * Code Refactoring  * Code Refactoring  * Final Commit  * self.device mapping to XPU  * Code Refactoring  * Mulitple GPU device engagement enabled  * Remove unused changes  * Revert "Remove unused changes"  This reverts commit 9188deb4911633a1d2a3f7f97e80ff7343fbc492.  * Add performance gain info for GPU  * Update README.md  * Add units to table  * Update metric reading configuration.  * Update system metrics script path.  * Update ConfigManager.java  * Reformat ConfigManager.java  * Fix spelling issues  * Fix lint changed file.  ---------  Co-authored-by: root <root@DUT4039DG2FRD.fm.intel.com> Co-authored-by: Kanya-Mo <167922169+Kanya-Mo@users.noreply.github.com> Co-authored-by: Mo  Kanya <kanya.mo@intel.com> Co-authored-by: Anup Renikunta <anup.renikunta@intel.com> Co-authored-by: Ankith Gunapal <agunapal@ischool.Berkeley.edu>
gsantner,markor,b2f44074a8858a11d3e254ce2b4cdf9816a11eb7,https://github.com/gsantner/markor/commit/b2f44074a8858a11d3e254ce2b4cdf9816a11eb7,Quick filter all files  Share files into markor (PR #2521)  * Separate options - aborting * Switching to dialog based sort * Cleanups to dialog * New approach to dialog using change callback * Fixes  wrap improvements * Invisible instead of gone * Tweak to wrap state * Fixed scroll to cursor * File search perf and all files * Window options * Fixes  using compat * Fix for jumping to position * Proper handling of title bar * Share to copy  long pres to open folder * Fixed how we update virtual folders  ---------  Co-authored-by: Gregor Santner <gsantner@users.noreply.github.com>
gsantner,markor,ccb2c6a69a8e4101ea0500e003ef969ad76f8c55,https://github.com/gsantner/markor/commit/ccb2c6a69a8e4101ea0500e003ef969ad76f8c55,Improve text rendering performance  closes #2478  closes #2515 (#2509)  * Invisible instead of gone  * Fixes  * tweaks  * Fixed states  * Tweak to wrap state  * Fixed scroll to cursor  * File search perf and all files  * Window options  * Fixes  using compat  * Reverted changes to file search  * Undo file loads  * Improving layout further  * Fix dialog title  * Switching to -1  pulling fixes  * Fixed issues with last line  * reformat code  ---------  Co-authored-by: Gregor Santner <gsantner@mailbox.org>
gsantner,markor,f19f8b2969cbebf26c7a046458d867ad75467fe2,https://github.com/gsantner/markor/commit/f19f8b2969cbebf26c7a046458d867ad75467fe2,Keyboard TAB key handling  syntax highlighting performance  filebrowser navigation (PR #2487 closes #2469 #2484)  * Keypress handler * Restored option  fixed duplicate calls to loadFolder on resume * Made some small changes for highlighting performance ---------  Co-authored-by: Gregor Santner <gsantner@mailbox.org>
gsantner,markor,c5fe529515830dd16ba5dea6e14eadd016b1a1bf,https://github.com/gsantner/markor/commit/c5fe529515830dd16ba5dea6e14eadd016b1a1bf,Navigation and highlighting improvements (PR #2377 closes #2409 closes #2410)  * Many fixes to navigation and go back * Cleanups to fragment handling * Tweaks to showing keyboard * Final tweaks to activity stacks * Removed unnecessary 'synchronized' calls for perf * Tweaks to highlighting * Reverted some highlighting changes * Fix template cursor start * Improvements to display of file browser dialog / fragment * Initial set of changes for async * Using canonical paths and and static members * Defensive logic for indices * Batching fixup for reduced computation * Switching back to start end as it made no difference * Tweaks to launching; file paths -> canonical * Fixed wikitext link formatting
crate,crate,e33210befaeeb3c134a624d8d78603b5c914c3e3,https://github.com/crate/crate/commit/e33210befaeeb3c134a624d8d78603b5c914c3e3,Remove synced flush  Follow up to https://github.com/crate/crate/pull/17364  Nodes after 4.4. always used a normal flush:  final boolean preferNormalFlush = clusterService.state().nodes().getMinNodeVersion().onOrAfter(Version.V_4_4_0); if (preferNormalFlush) { performNormalFlushOnInactive(indexShard); } else if (indexShard.routingEntry().primary()) {  Given that <4.4 nodes can't join a 6.0 cluster we can remove the synced flush functionality.
crate,crate,5ce8a91e21cd41dc374b846daf4fe277d6f46204,https://github.com/crate/crate/commit/5ce8a91e21cd41dc374b846daf4fe277d6f46204,Always load singleton string primary key from binary dv  If a table has a single primary key of type STRING  Crate will now store it in a BinaryDocValues field. These are more efficient than SortedDocValues for sequential access  and should both improve the performance of joins on primary keys and allow the removal of the Crate DocValuesFormat fork  which will re-enable compression on lower-cardinality string columns and open up the possibility of using sparse indexes in future.  This change shows significant speedups when streaming large numbers of primary key values:  Q: select count(t1."sourceIP") from uservisits_large t1 inner join uservisits_small t2 on t1."sourceIP" = t2."sourceIP" C: 1 | Version |         Mean ±    Stdev |        Min |     Median |         Q3 |        Max | |   V1    |    25393.936 ±  339.451 |  24746.715 |  25338.120 |  25668.752 |  26095.559 | |   V2    |    11375.290 ±  378.957 |  10383.943 |  11328.199 |  11476.776 |  12147.357 | ├---------┴-------------------------┴------------┴------------┴------------┴------------┘ |               -  76.25%                           -  76.42% There is a 100.00% probability that the observed difference is not random  and the best estimate of that difference is 76.25% The test has statistical significance  Q: select count(t1."sourceIP") from uservisits_large t1 inner join uservisits_small t2 on t1."sourceIP" = t2."sourceIP" C: 5 | Version |         Mean ±    Stdev |        Min |     Median |         Q3 |        Max | |   V1    |    27992.866 ±  348.264 |  27397.459 |  27871.027 |  28083.560 |  28950.416 | |   V2    |    14367.342 ±  428.816 |  14025.410 |  14237.366 |  14344.758 |  15561.470 | ├---------┴-------------------------┴------------┴------------┴------------┴------------┘ |               -  64.33%                           -  64.76% There is a 100.00% probability that the observed difference is not random  and the best estimate of that difference is 64.33% The test has statistical significance
crate,crate,326971a6e9d61300977090e55d162305f291b85d,https://github.com/crate/crate/commit/326971a6e9d61300977090e55d162305f291b85d,Use bigger buffer size for OpenDAL's OperatorOutputStream  Helps to avoid BlockCountExceedsLimit and improves upload performance
crate,crate,2f9c95e884496199096a05f5e63a7f212635668e,https://github.com/crate/crate/commit/2f9c95e884496199096a05f5e63a7f212635668e,Avoid BooleanQuery at NotPredicate if not needed  This seems not to have a performance impact but at least improves readability of query string representation.
crate,crate,104ef15f984b477cfa9faf1463e0c24eac71febd,https://github.com/crate/crate/commit/104ef15f984b477cfa9faf1463e0c24eac71febd,Avoid double negation when negating a lucene query by `Queries.not`  Relates to queries which negates inner queries twice like e.g. `IS NOT DISTINCT` -> not(not(eqQuery). This seems not to have a performance impact but at least improves readability of query string representation.  Relates to a discussion at https://github.com/crate/crate/pull/16621
crate,crate,e78efc5c1922a26282a50f0c811fe34df5a1cfa6,https://github.com/crate/crate/commit/e78efc5c1922a26282a50f0c811fe34df5a1cfa6,Add lucene query generation for `IS DISTINCT` queries when possible  This should improve performance a lot when using `IS (NOT)? DISTINCT` inside the ``WHERE`` clause as it will utilize lucene indexes or docvalues.  Closes #16537.
crate,crate,409b3963f47c81f30c6c429c67cd8dea2ed5940f,https://github.com/crate/crate/commit/409b3963f47c81f30c6c429c67cd8dea2ed5940f,Fix issue with LIKE/ILIKE on INDEX OFF cols with empty pattern  When the pattern is empty  a different code path is followed  to use a more performant `TermsQuery`  but if the column is declared with `INDEX OFF`  the `TermsQuery` wouldn't match any row. Fallback to a `GenericFunctionFilter` in that case.  Fixes: #16567
crate,crate,2a095d41da40a77b8131840084368ef8e62182e7,https://github.com/crate/crate/commit/2a095d41da40a77b8131840084368ef8e62182e7,Minor code improvements & warnings fix  - Make `Identifiers` a final utility class - `FunctionName` is a record  so `final` is superfluous - Minor code improvements in `UserDefinedFunctionsMetadataTest`
crate,crate,6c63ebf5feee7ef8db1a6a3158e416d98053166d,https://github.com/crate/crate/commit/6c63ebf5feee7ef8db1a6a3158e416d98053166d,Fix array_length function for input arrays containing nulls  Introduce '_array_length_' field to index array columns' lengths for performance improvements  see the PR for details
camunda,camunda-bpm-platform,3151c40896c5447ed14c378b1d9ee3b2dd02d354,https://github.com/camunda/camunda-bpm-platform/commit/3151c40896c5447ed14c378b1d9ee3b2dd02d354,fix(mssql): replace or with union in external task to improve performance
bepass-org,oblivion,f2a221bf2442a24cd5e43c86ab045dcb4fd4a00f,https://github.com/bepass-org/oblivion/commit/f2a221bf2442a24cd5e43c86ab045dcb4fd4a00f,use newSingleThreadScheduledExecutor for performConnectionTest  Signed-off-by: Mark Pashmfouroush <mark@markpash.me>
bepass-org,oblivion,e686c693784ba10897d5ce69e7d00d2c31c27902,https://github.com/bepass-org/oblivion/commit/e686c693784ba10897d5ce69e7d00d2c31c27902,Ready to release V5 (#376)  Fixed Hot issue bugs related to Android API. Add enhanced logging logic. Add support IPV4/6 endpoint. Add support multi-endpoint. Improved performance. Add Turkish Language.
bepass-org,oblivion,9fd7a400129b845ce83a79b05d9b3e4b9f8c9d4b,https://github.com/bepass-org/oblivion/commit/9fd7a400129b845ce83a79b05d9b3e4b9f8c9d4b,Fixed battery issue  improved VpnService performance
201206030,novel-plus,a4d6272a4f1927451e64a5639fcb096af1180fd5,https://github.com/201206030/novel-plus/commit/a4d6272a4f1927451e64a5639fcb096af1180fd5,perf(novel-crawl): 减小失效爬虫的CPU占用
201206030,novel-plus,415bf8a64c4d75b019381bacba7d22c3d340e7b0,https://github.com/201206030/novel-plus/commit/415bf8a64c4d75b019381bacba7d22c3d340e7b0,perf: 设置小说推荐缓存时间
201206030,novel-plus,d4fa0abc4e30b9ef8ccfd38065bff9a78a46412e,https://github.com/201206030/novel-plus/commit/d4fa0abc4e30b9ef8ccfd38065bff9a78a46412e,perf: 使用流式响应处理AI生成文本，提高用户体验
201206030,novel-plus,4b1507b2d1b17b1d40b60e9190a2a796dc0c2b34,https://github.com/201206030/novel-plus/commit/4b1507b2d1b17b1d40b60e9190a2a796dc0c2b34,perf: 连接池统一创建
201206030,novel-plus,82658f3b5f5ddac4a4fa4e5180823f4a34af38af,https://github.com/201206030/novel-plus/commit/82658f3b5f5ddac4a4fa4e5180823f4a34af38af,perf: 兼容其它数据源
201206030,novel-plus,acf9c76757c194b37498f5b3442985e41fc35cc3,https://github.com/201206030/novel-plus/commit/acf9c76757c194b37498f5b3442985e41fc35cc3,perf: 提前创建数据库连接池 Spring Boot 新版本默认会在第一次请求数据库时创建连接池
201206030,novel-plus,4b00ea68a9127086d9b57a5ed92316db7abb0e11,https://github.com/201206030/novel-plus/commit/4b00ea68a9127086d9b57a5ed92316db7abb0e11,perf: 提高第一次登录速度
201206030,novel-plus,85b64bbc10e3188d8fa0b28ea2ae9a7a49081fde,https://github.com/201206030/novel-plus/commit/85b64bbc10e3188d8fa0b28ea2ae9a7a49081fde,perf: 爬虫采集流程优化
201206030,novel-plus,6d0ab337579c10e90f8b61f50c578a33dc349427,https://github.com/201206030/novel-plus/commit/6d0ab337579c10e90f8b61f50c578a33dc349427,perf: 爬虫分类规则优化
201206030,novel-plus,89992dc781d8189f25ec58bca0558e22ae3dafff,https://github.com/201206030/novel-plus/commit/89992dc781d8189f25ec58bca0558e22ae3dafff,perf(novel-crawl): 增加小说内容过滤
alibaba,fastjson2,c12ef42a7e4eded00188ff16eed828c60b0c7acb,https://github.com/alibaba/fastjson2/commit/c12ef42a7e4eded00188ff16eed828c60b0c7acb,refactor(JSONWriterUTF8): Optimize string encoding methods  Refactored the string encoding logic by removing redundant variable assignments and modifying method visibility for better performance and maintainability.
alibaba,fastjson2,a632e14b37cf01fa78202b0dbbbcc4a886140620,https://github.com/alibaba/fastjson2/commit/a632e14b37cf01fa78202b0dbbbcc4a886140620,fix(JSONReader): Add reference detection disable flag  Add support for disabling reference detection via a new feature flag (MASK_DISABLE_REFERENCE_DETECT). This improves performance in cases where reference tracking is unnecessary.
alibaba,fastjson2,3016e80649b0788adeedde510427518279b1f411,https://github.com/alibaba/fastjson2/commit/3016e80649b0788adeedde510427518279b1f411,Improve some code  fix some bugs (#3320)  * Unified setting file encoding to UTF-8  * Fix issue#3283  * Aligning with new features of JDK8  * Simply some redundant code  * Improve Map handle performance  * Fix some potential bugs  * Improve some String processing performance  * Remove unnecessary protected modifier from final classes  because they are NOT inheritable  * Aligning with new features of JDK8  * Optimize FastJsonJsonView  * Fix conditional branch duplication  * Revert "Unified setting file encoding to UTF-8"  This reverts commit 66f1238da8371886a38473dbe8d5374e87e85381.
alibaba,fastjson2,0a0014b5fc7c8e7cfdd928dbf7cab6810e14097a,https://github.com/alibaba/fastjson2/commit/0a0014b5fc7c8e7cfdd928dbf7cab6810e14097a,Improve some code  fix some bugs (#3317)  * Unified setting file encoding to UTF-8  * Fix issue#3283  * Aligning with new features of JDK8  * Simply some redundant code  * Improve Map handle performance  * Fix some potential bugs  * Improve some String processing performance  * Remove unnecessary protected modifier from final classes  because they are NOT inheritable  * Aligning with new features of JDK8  * Optimize FastJsonJsonView  * Fix conditional branch duplication
geoserver,geoserver,8bdc0ff167782b6682a1336e6ca4e09c462b6502,https://github.com/geoserver/geoserver/commit/8bdc0ff167782b6682a1336e6ca4e09c462b6502,[GEOS-11786] Longitudinal profile process: general performance improvements
geoserver,geoserver,d8f10b65f10643149b5f8774ee27ff72504bf116,https://github.com/geoserver/geoserver/commit/d8f10b65f10643149b5f8774ee27ff72504bf116,[GEOS-11284] Promote community module "datadir catalog loader" to core  Promote the "datadir catalog loader" community module to the GeoServer core  improving startup performance for deployments with large data directories.  * Make the new loader log objects added to the catalog the same way DefaultGeoServerLoader does  and enhance GeoServerLoader's logging of timing with the count of objects added as the new loader does. * Add upgrade docs for optimized data directory loader Added a section to the upgrade guide explaining improvements to the configuration loading process. Included considerations for compatibility  along with configuration details on how to disable the new loader if needed. * Add more data directory loader tests and multi-instance improvements * Explicitly declares bean dependencies to ensure proper initialization order by making GeoServerLoaderProxy depend on essential components (extensions  dataDirectory  securityManager  configLock). * Guard sanitizing writes with GeoServerConfigurationLock (e.g. when adding default styles or estblishing the default workspace). This ensures consistency when multiple instances start off a shared data diretory such as in the case of GeoServer Cloud.
geoserver,geoserver,f1eb501c1b045925c7f3870ad32f7582ebb40be2,https://github.com/geoserver/geoserver/commit/f1eb501c1b045925c7f3870ad32f7582ebb40be2,[GEOS-11768] Reduce thread contention in Catalog operations  Improve performance by reducing thread contention in catalog operations.  Main improvements include: - Using fine-grained locking instead of broad synchronization blocks - Separating read/write locking patterns to allow concurrent reads - Only synchronizing when actually necessary for specialized operations  These changes should improve performance in multi-threaded environments while maintaining thread safety.
geoserver,geoserver,8372e1118ae8250b2f3dcf4b265c9f26c880d63b,https://github.com/geoserver/geoserver/commit/8372e1118ae8250b2f3dcf4b265c9f26c880d63b,[GEOS-11757] Optimize ConfigurationPasswordEncryptionHelper to Cache Encrypted Fields by Store Type  The `ConfigurationPasswordEncryptionHelper` class has been updated to improve performance by caching encrypted fields based on the StoreInfo type when the class cache is hit.  Previously  when a DataStoreInfo object lacked a type property  the method would repeatedly invoke `getCatalog().getResourcePool().getDataStoreFactory(info)`  causing unnecessary factory lookups and expensive serialization/deserialization operations within `ResourcePool`.  `decode(StoreInfo info)` has been updated to only call `securityManager.loadPasswordEncoders()` only when necessary. This makes a significant difference performance improvement when loading a large data directory  about 10% with the new DataDirectoryGeoServerLoader  and about 3% with DefaultGeoServerLoader.
geoserver,geoserver,1f86e8853e983479f47511dd403432b5462a820b,https://github.com/geoserver/geoserver/commit/1f86e8853e983479f47511dd403432b5462a820b,[GEOS-11758] datadir-catalog-loader: Improve reliability and code quality  Refactors the datadir-catalog-loader module in preparation for moving it to core:  - Avoid possible deadlocks in the main thread during startup due to spring holding a lock on the singletons registry. Force parsing Stores  Layers  and LayerGroups in the calling thread while loading the file contents in the ForkJoinPool. It is rather unpredictable when something deep inside XStreamReader will end up calling GeoServerExtensions.bean()/extensions(). - Make configuration parameters consistent with GeoServer naming conventions - Extend DefaultGeoServerLoader tapping into readCatalog() and readConfiguration()  hence fully sharing the loading workflow of the super classes. - Move from catalog to config package to follow gs-main's GeoServerLoader package naming - Use ForkJoinPool.ManagedBlocker to load file contents for better I/O parallelism - Replace test faker library with direct object creation - Replace weak reflection by the com.github.stefanbirkner:system-rules test dependency for environment variables related tests - Fix catalog resolution in StyleInfo  StoreInfo and ResourceInfo instances - Increase test coverage to +90%  This improves startup performance significantly for large catalogs  especially when using network filesystems like NFS.
geoserver,geoserver,26ed5955b7e78fb993b0949fb1a6791af5356eee,https://github.com/geoserver/geoserver/commit/26ed5955b7e78fb993b0949fb1a6791af5356eee,[GEOS-11607] KML WMS GetMap is performing a heavy database load query
geoserver,geoserver,1caac1c2a400938e56c88b91e1a8a1f80700cbb5,https://github.com/geoserver/geoserver/commit/1caac1c2a400938e56c88b91e1a8a1f80700cbb5,[GEOS-11580] Improve embedded GWC meta-tiling performance: review  QA  docs  GUI
geoserver,geoserver,2fbb244fbd8d5c6e7bc9fe06af41576e023c4cd6,https://github.com/geoserver/geoserver/commit/2fbb244fbd8d5c6e7bc9fe06af41576e023c4cd6,[GEOS-11580] Improve embedded GWC meta-tiling performance
geoserver,geoserver,93ff3a1e9b1c0bd50e69d9734db7ac39213c3c51,https://github.com/geoserver/geoserver/commit/93ff3a1e9b1c0bd50e69d9734db7ac39213c3c51,remove superfluous semicolons  use isEmpty() instead of size==0 on collections  fix some try-with-resources
geoserver,geoserver,e360b0fbd96701f6b2da15c12e7e29dea33cb5ad,https://github.com/geoserver/geoserver/commit/e360b0fbd96701f6b2da15c12e7e29dea33cb5ad,[GEOS-11424] Speed up web-ui WorkspaceAdminComponentAuthorizer  Logging in as a user other than `admin`  and having a large amount of workspaces results in potentlially substantial delays in loading almost every webui wicket page  as `WorkspaceAdminComponentAuthorizer` will be run several times  each of them potentially doing a full scan of workspaces  to determine if the user has admin privileges in at least one workspace.  This patch addresses the long-standing TODO comment in `WorkspaceAdminComponentAuthorizer` by caching the result in the `RequestAttributes`  hence performing the check once per request.  Also  moves the logic to a new default method in the `ResourceAccessManager` interface  giving implementations a chance to provide a more efficient check.
knowm,XChange,98a3ec00020659702e50a0024bc726d6350d08c7,https://github.com/knowm/XChange/commit/98a3ec00020659702e50a0024bc726d6350d08c7,Merge pull request #4974 from lvxiao1/develop  [fix]When using a proxy with WebSocket  the DNS resolution should be performed within the proxy server.
knowm,XChange,3336c5434c3c97eafbd8b9a9b22a4b60770ed3b0,https://github.com/knowm/XChange/commit/3336c5434c3c97eafbd8b9a9b22a4b60770ed3b0,Merge pull request #4999 from ivelkov/develop  [Kraken] - Improve performance of checksum computation
knowm,XChange,efce27c9319fa22854e0b69e8477635faeba5e29,https://github.com/knowm/XChange/commit/efce27c9319fa22854e0b69e8477635faeba5e29,[Kraken] - Improve performance of checksum computation
jMonkeyEngine,jmonkeyengine,d080f5411d5fdcb3ecca80c025a2ed33ea06ef69,https://github.com/jMonkeyEngine/jmonkeyengine/commit/d080f5411d5fdcb3ecca80c025a2ed33ea06ef69,Refactor: Remove redundant checkAlError call for performance  removes the repeated call to the `checkAlError` method. The repeated string creation and the associated overhead of the error checking can negatively impact performance  especially in a frequently executed game loop. By removing this redundant check  we aim to improve overall performance and reduce garbage collection pressure.
jMonkeyEngine,jmonkeyengine,05e496a87f33f7e927bb966b10f97a144ac5845f,https://github.com/jMonkeyEngine/jmonkeyengine/commit/05e496a87f33f7e927bb966b10f97a144ac5845f,Prefer ArrayDeque over ArrayList  You should consider using ArrayDeque when:  You need a collection that efficiently supports adding and removing elements from both ends. This makes it ideal for implementing data structures like queues and stacks. You frequently perform removals from the beginning of the collection. see ALAudioRenderer.newChannel() and ALAudioRenderer.freeChannel()
jMonkeyEngine,jmonkeyengine,eee43564c4002675a3dbaed7743b755a62659175,https://github.com/jMonkeyEngine/jmonkeyengine/commit/eee43564c4002675a3dbaed7743b755a62659175,Fix for XMLExporter issues in #2310 (#2313)  * #2176 Make LWJGLBufferAllocator use nmemCalloc() instead of nmemAlloc()  https://github.com/jMonkeyEngine/jmonkeyengine/issues/2176 LWJGLBufferAllocator.allocate() now always returns zero-initialized buffers.  * Added unit tests for JmeExporter/JmeImporter implementations  Tests all write* and read* methods of OutputCapsule and InputCapsule respectively.  * Fixed XMLExporter/XMLImporter BitSets  Previously DOMOutputCapsule was writing indices of set bits and DOMInputCapsule was reading values of each bit.  Changed DOMOutputCapsule to match the expected behavior of DOMInputCapsule.  * Fixed DOMInputCapsule.readString() returning defVal for empty strings  org.w3c.dom.Element.getAttribute() returns an empty string for attributes that are not found.  It looks like DOMInputCapsule.readString() was interpreting an empty string as the attribute not existing  and returning defVal even when the attribute did exist and was an empty string.  Now it checks explicitly whether the attribute exists.  * Deprecated DOMSerializer in favor of javax.xml.transform.Transformer  DOMSerializer contains several edge-case issues that were only partially worked around with the encodeString() and decodeString() helper methods.  Java has a robust built-in solution to serializing Document objects  and using that instead fixes several bugs.  * Fixed NullPointerException when XMLExporter writes a String[] with null  Also refactored all primitive array write and read methods to be more readable and reduce duplicate code.  * Made DOM capsules reuse write/read primitive array methods for buffers  Further reduces duplicate code  * Fixed DOMOutputCapsule.write(Savable[][]) NullPointerException  Refactored write and read methods for Savables and 1 and 2 dimensional Savable arrays.  Fixed NullPointerException when writing a 2d Savable array containing a null element in the outer array.  * Added Savable reference consistency test to InputOutputCapsuleTest  * Fixed DOMInputCapsule throwing NullPointerException when reading list  DOMInputCapsule used to throw a NullPointerException when reading an Arraylist containing a null element.  Also refactored list write and read methods to clean up a bit and accidentally also fixed an unrelated bug where reading ArrayList<ByteBuffer> would return a list containing all null elements.  * Made XMLExporter save and load buffer positions properly.  * Cleanup and formatting for XMLExporter related classes  * Undid XMLExporter saving buffer positions.  Not saving positions is intentional https://github.com/jMonkeyEngine/jmonkeyengine/issues/2312#issuecomment-2359149509  * Fixed infinite recursion with XMLExporter  Writing a Savable containing a reference loop caused infinite recursion due to bookkeeping being performed after the recursive call instead of before.  Also added a unit test for this to InputOutputCapsuleTest.
jetty,jetty.project,c064dd5d1b8f27172b2909e232b4afedb226c14f,https://github.com/jetty/jetty.project/commit/c064dd5d1b8f27172b2909e232b4afedb226c14f,Fixes #12496 - MultiPartFormData.Parser question. (#12500)  * Deprecated MultiPartFormData.Parser.parse() with 2 Promises  a leftover from previous experiments. * Added @deprecated javadoc tag to deprecated methods  indicating what to use instead. * Updated documentation to use non-deprecated APIs. * Updated source code to use non-deprecated APIs. * Updated class javadocs with usage examples that were outdated. * Made Blocker.Promise extends Promise.Invocable  so it can be use to block while using asynchronous APIs. * Added missing methods that perform asynchronous operation using a Promise rather than CompletableFuture. * Added servlet-to-handler examples for form fields and multipart.  Signed-off-by: Simone Bordet <simone.bordet@gmail.com>
jetty,jetty.project,fa143fa62a4098f9f4e5b5f32f25cfb22031d2e0,https://github.com/jetty/jetty.project/commit/fa143fa62a4098f9f4e5b5f32f25cfb22031d2e0,Improvements to HttpSender. (#12111)  * Changed ContentSender demand from iterate()+IDLE to succeeded()+SCHEDULED. This ensures that there is no re-iteration in case a 100 Continue response arrives. This  in turn  avoids that the demand is performed multiple times  causing ISE to be thrown. * Changed the 100 Continue action of the proxy Servlet/Handler  that provides the request content  to be executed by the HttpSender  rather than by the HttpReceiver.  Signed-off-by: Simone Bordet <simone.bordet@gmail.com>
JabRef,jabref,fae38967f663324083a3b907e3b0d087f32f1168,https://github.com/JabRef/jabref/commit/fae38967f663324083a3b907e3b0d087f32f1168,Fixing performance issues when bulk deleting (#12926)  * Fixing performance issues  when bulk deleting  * Removing comments  * Update CHANGELOG.md  * Delete benchmark  Rmoved a benchmark that I was using to test the performance that was accidently pushed.  * Fixing JMH Config  * Checkstyle Fix
JabRef,jabref,4dea5fae711f9dd61c94f6a170b475b9e0633e22,https://github.com/JabRef/jabref/commit/4dea5fae711f9dd61c94f6a170b475b9e0633e22,11219 adapt UI font size (#12042)  * Migrate all CSS files to the Base.css  * fixed seprator and toggle button misalignment  * deleted the original css files and updated the css references  * Apply requested changes  * Add selectors to avoid styling errors  * Remove superfluous calls to class.getResource  * Extract and adapt hardcoded styles from ai chat  * Simplify ai gdpr dialog  * Remove unused hardcoded padding  * Adapt JournalInfo  * Fixed review notes  * CHANGELOG.md  * more CHANGELOG.md  * fix dark text color  * CHANGELOG.md  ---------  Co-authored-by: Christoph <siedlerkiller@gmail.com> Co-authored-by: Carl Christian Snethlage <50491877+calixtus@users.noreply.github.com>
JabRef,jabref,de81430c1472920304c8348207ee2af9ec525822,https://github.com/JabRef/jabref/commit/de81430c1472920304c8348207ee2af9ec525822,[OO] Major performance improvements & fixes (#12221)  * Transform \n-><p>  reduce LibreOffice API calls  improve documentation  * Remove unnecessary regex transform  improve trailing regex match  * Fix tests  * Minor fix - comment
JabRef,jabref,538f0eef4ba08410a403f0f30f8020b5f519404a,https://github.com/JabRef/jabref/commit/538f0eef4ba08410a403f0f30f8020b5f519404a,Improve performance with duplicate check on paste (#11843)  * Improve performance with duplicate check on paste  * changelog  * refine changelopg
JabRef,jabref,7da71d66591c55356870d175d913373681f4aa70,https://github.com/JabRef/jabref/commit/7da71d66591c55356870d175d913373681f4aa70,Fix deprecated java api and minor refactor (#11747)  * Rename "Show 'Ai Chat' tab" to "Show tab 'AI Chat'"  solve #11708  * Update Server.java  1. remove superfluous URISyntaxException 2. change System.out.printf to LOGGER.debug 3. use `createSSLContext(boolean)` instead of deprecated API `createSSLContext()`
JabRef,jabref,6af91b963dce2a4c9d0320ed8d0e1423b80f8a54,https://github.com/JabRef/jabref/commit/6af91b963dce2a4c9d0320ed8d0e1423b80f8a54,Lucene search (#11542)  * Use pattern matching for cast  Co-authored-by: Christoph <siedlerkiller@gmail.com>  * Fix pattern matching  * Fix merge  * Speed up switches between sorting/filtering modes  * Fixed merge errors  * Fixed small issues  * Removed obsolete tests  fixed some tests  * Fixed merge error in CHANGELOG.md  * Fixed checkstyle  * Fixed more tests  * Removed obsolete tests  * Fixes "Fixed merge error in CHANGELOG.md" by removing duplicate entries  This reverts commit 536ecfaaedf9b14b93e3d50b1a63166f4440661e.  * WiP on tests  * Checkstyle  * Checkstyle  * Update Java version  * Refine logging  * Fix compile error  * Add LuceneTest  * Update CHANGELOG.md  * Move search classes to pdf package  * Move search classes to search package  * rewriteRun  * Remove bibEntry from DocumentReader  * Rewrite LuceneIndexer  * Remove IndexingTaskManager  * Separate Bib fields index and LinkedFiles index  * Fix null LuceneManager in ExternalFilesEntryLinker  * Save as action  * Clear linkedFiles indexer when fullText indexing is disabled in preferences  * remove comments  * get indexed files on update  * Add LUCENE_MANAGERS map for accessing managers by databaseContext.getUid  * Move LuceneManager from search.indexing package to search  * Fix wrong order for import  * Move SearchQuery to model package  * Fix issue with opening multiple unsaved libraries  * Pass LuceneManager down to the entry editor  * Improve searching performance  * Change SearchFieldConstants to enum  * More performance improvements for searching  - Read document only one time - getHighlighterFragments only when the search results tab is opened  * Update FulltextSearchResultsTab.java  * Fix group union  intersection  * Fix backgroundtask  * Fix subscriptions  * Remove lastSearchQueryLogic  * Fix possible NPE  * Fix searchTask check  * Remove sort by score flag  * Fix score column sorting  * Fix modifier buttons listener  * Add search rank column  In floating mode entries will be ranked and sorted by it. Rank: (1= entry matches group and search  2= matches group but not search  3= matches search but not group  4= matches nothing)  * hide search rank column from preferences  * Add search_rank column to sort order by default  * Update CHANGELOG.md  * fix typo  * Change the order of the rank  1= entry matches group and search  2= matches search but not group  3= matches group but not search  4= matches nothing  * Use NGramAnalyzer for indexing  * Resolve conflicts  * update search matches with lucene  * PreviewViewer highlighting with Lucene  * Delete IndexingTaskManager.java  * SourceTab highlighting with Lucene  * Fix non-ASCII characters  * Extract query terms from search query  * Highlight regex queries  * return js highlight function  * Fix invalid search query throw exception  * Refactor Lucene indexer classes  * Refactor linked files indexer  * Update search matches when entries are added or updated  * Remove preferences from ActionHelper  * checkstyle  * comment out search tests  * OpenRewrite  * Fix Groups Parser/Serializer  * Localization  * Search groups  * Release `IndexSearcher` after completing search task  * Checkstyle  * Correct typo  * Remove GroupSearchQuery  * Remove EventBus from LuceneManager and use BibDatabase eventBus  * Fix number of matched entries in groups  * Fix search groups  * Localization  * Remove bib fields highlighter  * Pass LuceneManager to search groups  * Fix performance issues by caching matched entries  * Update GroupDialogViewModelTest.java  * Update main table matches  * Fix groups icon  * Restore Search.g4 and GrammarBasedSearchRule  * First version of search group migration  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Add groups field to the index  * Remove search rules  * Localization  * Add test cases  * Fix names  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Add some more functionality  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Always add "all" prefix  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Add comment for alternative implementation  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Mark library tab changed after migration  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Add another test for regular expression  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Small fixes  * Fix markBaseChanged  * Fix adding new entries did not update MatchCategory  * Fix searching for Non-ASCII characters  * Fix escaping special characters  Use WhitespaceTokenizer instead of StandardTokenizer https://stackoverflow.com/a/6119584/21694752  * Fix tests  Co-Authored-By: Oliver Kopp <kopp.dev@gmail.com>  * Add first draft of LatexToUnicodeFoldingFilter  Co-authored-by: Loay Ghreeb <52158423+LoayGhreeb@users.noreply.github.com>  * Fix LatexToUnicodeFoldingFilter  Co-Authored-By: Oliver Kopp <kopp.dev@gmail.com>  * Remove LatexToUnicode from SearchQuery  * Localization  * AllowedToUseLogic  * Update CHANGELOG.md  * Use sentence case for search result heading  * Add CHANGELOG for change in JabRefFrameViewModel  * Add more changes to CHANGELOG.md  * Add ADR-0038  * Rename "SCORE" to "MATCH_SCORE"  * Add link to ADR-0038  * Add another CHANGELOG.md entry  * Add CHANGELOG.md entry  * Revert change of filename  * Add JavaDoc comment  * Trying to find better names  * Discard changes to src/main/resources/tinylog.properties  * Remove commented out code  * Remove obsolete testing class  * Remove obsolete test  * Discard changes to src/test/resources/tinylog-test.properties  * Remove completely disabled code  * Rename "all" to "any"  * Catch thrown exception  Invalid regex queries throws an exception  * Remove groups field from the default field  https://github.com/JabRef/jabref/issues/7996  * Remove SearchGroupsListener  * Update Benchmarks.java  * Update module-info.java  * Fixes from code review on LibraryTab  * Remove regex button from search bar  * Use BibEntry.getId instead of System.identityHashCode  * Add BibEntry index map  * Readd option  * Add `@ADR` annotation  * Add some comment  * One more annotation  * Add CHANGELOG.md entry  * One more annotation  * Add CHANGELOG.md entry  * Revert "Add BibEntry index map"  This reverts commit 27ed10599d826af18995c0de61ff480c1ff90274.  * Use binary search to find the index of the entry  * openrewrite  * Tests for LinkedFilesIndexer  * Fix DatabaseSearcher  * LocalizationConsistencyTest  * DatabaseSearcherWithBibFilesTest  * Fix typo in CHANGELOG.md  * Fix typo  * Use parameterized test for DatabaseSearcherTest  * Fix DatabaseSearcherWithBibFiles tests  * Fix exportMatches test  * Remove regex check box from search groups dialog  * JavaDoc  * Fix SearchGroups test  * Remove closeAndWait methods and use CurrentThreadTaskExecutor  * Fix architecture test  * Allow to use logic  * Add debug logging for search  * Add more logging  * Assert with containsInAnyOrder  * Fix DatabaseSearcher test  * Global search dialog  * Rename method  * Improve code quality  - Maintain a map of BibEntryId to BibEntry. - Store search results within SearchQuery instead of using the map in StateManager. - Remove LuceneManager from SearchGroups. - Use a different Analyzer for PDFs.  * Use non-static preferences variables  * Update CHANGELOG.md  * Delete SearchGroupTest.java  * fix typo  * fix indentation  * Update matchedEntries on the UI thread  matchedEntries should be updated on the UI thread because the size binding of matchedEntries will be reflected in the UI.  * Discard changes to src/main/java/org/jabref/gui/importer/actions/GUIPostOpenAction.java  * Fix LoayGhreeb#12  * Sync search flags between search bar and global search bar  * Move VERSION_6_0_ALPHA const to SearchGroupsMigrationAction  * Refactor LuceneSearcher  * Use linked files analyzer for highlighting full-text results  * Fix line break  * Fix tests  * Use EnglishAnalyzer for indexing/searching linked files  https://github.com/apache/lucene/blob/68cc8734ca28a9db800e4192a636d3b490cfd41a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java#L101-L110  * Ask to wait for linked files indexing on shutdown  When closing JabRef  only ask users to wait for the linked files indexer to finish. The bib fields indexer is recalculated on startup  so it doesn't need to be completed before shutdown.  * Use EdgeNGram instead of NGram  * Return comment  * Update CHANGELOG.md  ---------  Co-authored-by: Benedikt Tutzer <btut@users.noreply.github.com> Co-authored-by: Christoph <siedlerkiller@gmail.com> Co-authored-by: Carl Christian Snethlage <50491877+calixtus@users.noreply.github.com> Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>
JabRef,jabref,db9f83cf77b1a5215e729d3c336371b4edef04a4,https://github.com/JabRef/jabref/commit/db9f83cf77b1a5215e729d3c336371b4edef04a4,Search floating mode (#11510)  * Search/Groups floating mode  * Hide search rank column  * Update query listener from global to library-specific  * Update selected groups listener from global to library-specific  * Move table-row CSS classes to Base.css  * Adapt tests  * Update JabRef_en.properties  * CHANGELOG  * Fix jumpToSearchKey  * Add shortcut to scroll to the next/prev rank  Left  right arrows  * Localization  * Fix scroll shortcut to handle rank gaps  * OpenRewrite  * Add temporary MappedBackedList implementation  * Use constants for rank values  * Update rank colors  * Add CustomFilteredList  * Improve group switching and search performance  - Update matches in the background - Prevent unnecessary search query rechecks when switching groups  * OpenRewrite  * Fix NPE  * Fix NPE  * Create a list of observables  * refilter the list after updateVisibility  * Add onUpdateCallback to the CustomFilteredList  * iterate over updated range  * Rename onUpdateCallback to onUpdate  * Register events to the row  * Update matches in the background  * Delete MappedBackedList.java  * Update matches in the background for global search  * Pass properties to MainTableDataModel instead of LibraryTab  * Remove search rank column from the preferences  * Add SearchRank enum  * EnumSet constructor  * Remove int value from SearchRank enum  * Move SearchRank enum to search package  * Remove FILTERING_SEARCH from search flags  * Remove KEEP_SEARCH_STRING from search flags  * Fix SearchPreferences constructor  * Move comment up  * Rename SearchRank to MatchCategory  * Update src/main/java/org/jabref/gui/util/CustomFilteredList.java  Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>  * Update src/main/java/org/jabref/gui/util/CustomFilteredList.java  Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>  * Update CustomFilteredList.java  * Update PreferencesMigrations.java  * Replace CustomFilteredList.java with reflection  Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>  * Minor stylistic fixes  * Fix reflection  Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>  * Correct typo  * Fix typo  * Fix unit tests  * CHANGELOG  ---------  Co-authored-by: Carl Christian Snethlage <50491877+calixtus@users.noreply.github.com> Co-authored-by: Oliver Kopp <kopp.dev@gmail.com>
JFormDesigner,FlatLaf,0ad3180b10341623ce3cf41a481e024b3f96469f,https://github.com/JFormDesigner/FlatLaf/commit/0ad3180b10341623ce3cf41a481e024b3f96469f,FileChooser: improved performance when navigating to large directories with thousands of files (issue #953)
uber,NullAway,6f4cda7c374965d670f28ec088fe07ad323e4013,https://github.com/uber/NullAway/commit/6f4cda7c374965d670f28ec088fe07ad323e4013,JSpecify: preserve explicit nullability annotations on type variables when performing substitutions (#1143)  Fixes #1091  Consider the following code: ```java abstract class Test { abstract <V> @Nullable V foo(Function<@Nullable V  @Nullable V> f); void testPositive(Function<String  String> f) { this.<String>foo(f); // error } } ``` The call to `foo` should not type check. Since the type of its parameter `f` is `Function<@Nullable V  @Nullable V>`  with explicit `@Nullable` annotations on the type variables  any `Function` passed to `foo` must have `@Nullable` type arguments. In typechecking this code  NullAway previously substituted the type arguments for the type variables in `foo` just using built-in `javac` routines. But  this would yield a formal parameter type `Function<String  String>`  as the `javac` routine would not retain the explicit type arguments in the right places. So we would miss reporting an error. This PR fixes the substitutions and re-introduces the annotations on type variables  so we get the type `Function<@Nullable String  @Nullable String>` for the formal parameter at the call  and report an error correctly. Substitutions were broken in other cases as well; substituting `@Nullable V` for `@Nullable V` (where `V` is a type variable) yielded just `V`  which led to false positives (like #1091).  The main logic changes are in `TypeSubstitutionUtils`. We add a new `RestoreNullnessAnnotationsVisitor` and use it to restore nullability annotations from type variables after performing a substitution.  We also extract the `TypeMetadataBuilder` logic to a top-level source file  and add new methods as needed for this PR. Some of this could have been split into a separate PR but it's a bit of a pain to extract it now.
uber,NullAway,a4ce24986429ad46af8171f198c550b7403a7b89,https://github.com/uber/NullAway/commit/a4ce24986429ad46af8171f198c550b7403a7b89, Refactored data clumps with the help of LLMs (research project) (#960)  Hello maintainers   I am conducting a master thesis project focused on enhancing code quality through automated refactoring of data clumps  assisted by Large Language Models (LLMs).   <details> <summary>Data clump definition</summary>  A data clump exists if 1. two methods (in the same or in different classes) have at least 3 common parameters and one of those methods does not override the other  or 2. At least three fields in a class are common with the parameters of a method (in the same or in a different class)  or 3. Two different classes have at least three common fields  See also the following UML diagram as an example ![Example data clump](https://raw.githubusercontent.com/compf/data_clump_eval_assets/main/data_clump_explained.svg) </details>   I believe these refactoring can contribute to the project by reducing complexity and enhancing readability of your source code.  Pursuant to the EU AI Act  I fully disclose the use of LLMs in generating these refactorings  emphasizing that all changes have undergone human review for quality assurance.   Even if you decide not to integrate my changes to your codebase (which is perfectly fine)  I ask you to fill out a feedback survey  which will be scientifically evaluated to determine the acceptance of AI-supported refactorings. You can find the feedback survey under https://campus.lamapoll.de/Data-clump-refactoring/en   Thank you for considering my contribution. I look forward to your feedback. If you have any other questions or comments  feel free to write a comment  or email me under tschoemaker@uni-osnabrueck.de .   Best regards  Timo Schoemaker Department of Computer Science University of Osnabrück  ---------  Co-authored-by: Manu Sridharan <msridhar@gmail.com>
tencentmusic,supersonic,f7fce0217f677c223cfc16fabe18be075829a031,https://github.com/tencentmusic/supersonic/commit/f7fce0217f677c223cfc16fabe18be075829a031,[improvement][chat] Use a generic thread pool to perform concurrent mapping. (#1965)
tencentmusic,supersonic,5c70607851cbe8b222f1fd63ad5e3dac40854fff,https://github.com/tencentmusic/supersonic/commit/5c70607851cbe8b222f1fd63ad5e3dac40854fff,[improvement][common] Field replacement is performed using a recursive approach  and it supports field replacement with complex expressions (#1859)
tencentmusic,supersonic,d9cf874536fae8ddce4f2b9d1bf2608b1be1bdf9,https://github.com/tencentmusic/supersonic/commit/d9cf874536fae8ddce4f2b9d1bf2608b1be1bdf9,[improvement][chat] In STRICT mode  embedingMapper does not perform mapping (#1858)
tencentmusic,supersonic,9302d1f6e4f11c7dd0e5782a7670f03134dd2022,https://github.com/tencentmusic/supersonic/commit/9302d1f6e4f11c7dd0e5782a7670f03134dd2022,[improvement][chat] The two characters need to be a perfect match (#1805)
tencentmusic,supersonic,0c70df12cac80f22c85412c18e718b0615016bf4,https://github.com/tencentmusic/supersonic/commit/0c70df12cac80f22c85412c18e718b0615016bf4,(improvement)(chat) If there are function operator fields  precise replacement must be performed. (#1554)
tencentmusic,supersonic,2f2f493d17f6c1e617dcc86fbdbb28065cefa7bd,https://github.com/tencentmusic/supersonic/commit/2f2f493d17f6c1e617dcc86fbdbb28065cefa7bd, (improvement)(headless) When performing semantic modeling  support simultaneous querying of both TABLE and VIEW. (#1551)
tencentmusic,supersonic,9a1fac5d4cef14cf5c673778d06ed0909bb8dcea,https://github.com/tencentmusic/supersonic/commit/9a1fac5d4cef14cf5c673778d06ed0909bb8dcea,(improvement)(chat) Reduce frequent loading of embedding models to improve loading performance. (#1478)
tencentmusic,supersonic,ae34c15c9595e1c7f8662b5e4badaa96c6b511d3,https://github.com/tencentmusic/supersonic/commit/ae34c15c9595e1c7f8662b5e4badaa96c6b511d3,(improvement)(chat) Improve vector recall performance. (#1458)
tencentmusic,supersonic,d64ed02df9d4550c20b22a533c8fe82100ddf023,https://github.com/tencentmusic/supersonic/commit/d64ed02df9d4550c20b22a533c8fe82100ddf023,(improvement)(chat) Remove langchain4j configuration file and perform all configuration for the large model through the UI interface. (#1442)
tencentmusic,supersonic,ea4aa3eacf4349c861d90b487387e65d00454745,https://github.com/tencentmusic/supersonic/commit/ea4aa3eacf4349c861d90b487387e65d00454745,(improvement)(headless)Remove unnecessary `performExecution` method from `ChatQueryService`.
tencentmusic,supersonic,db9a3fa056d04e03938e62844ea6fa337d87c6b4,https://github.com/tencentmusic/supersonic/commit/db9a3fa056d04e03938e62844ea6fa337d87c6b4,(improvement)(headless) performParsed skip translation (#1222)
MovingBlocks,Terasology,ba4d716488d333b240c49efd4ee8fa0ac08da61d,https://github.com/MovingBlocks/Terasology/commit/ba4d716488d333b240c49efd4ee8fa0ac08da61d,qa: address pmd findings (#5274)  * qa: fix PMD CollapsibleIfStatements finding  * qa: fix PMD SimplifiedTernary findings  * fix: formatting mistake  * qa: address PMD SystemPrintln findings  - suppress for LoggingContext.java and Terasology.java because logging not necessarily available yet there - refactor for PathManager.java as logger is already in use in that context  also see previous discussion in https://github.com/MovingBlocks/Terasology/pull/5036#discussion_r889518509  * qa: address PMD InvalidLogMessageFormat finding  - if I understand https://stackoverflow.com/questions/7102339/is-there-a-correct-way-to-pass-arguments-in-slf4j correctly  this should not be less performant but valid SLF4j format  * qa: address PMD AvoidBranchingStatementAsLastInLoop findings  * qa: address PMD ForLoopCanBeForeach findings  * chore: remove unused import  * qa: address PMD AvoidPrintStackTrace findings  * chore: remove unused imports  * chore: fix checkstyle DeclarationOrderCheck warning
apache,kylin,56a189a5856cf979135065b1385760ea414db6b8,https://github.com/apache/kylin/commit/56a189a5856cf979135065b1385760ea414db6b8,KYLIN-6009 fix api performance
apache,kylin,220a2dffd639af5a333b21e933f841791af5fc1e,https://github.com/apache/kylin/commit/220a2dffd639af5a333b21e933f841791af5fc1e,KYLIN-5952 Support recommendation for kylin5  **SQL-Based Model and Index Creation**  1. **Quick Model Creation:** Upload SQL files directly through the front-end interface to rapidly create models and generate corresponding indexes.  2. **Index Optimization Recommendations:** Leverage existing models to recommend and optimize indexes  improving query performance.  3. **Historical Query Analysis:** Generate index optimization suggestions based on query history. Select the most profitable recommendations through statistical analysis  which users can review and approve via the front-end interface.  4. **Unnecessary Index Elimination:** Use statistical insights to identify and recommend the removal of redundant indexes.  5. **Enhanced Left Join Models:** For left join models  add new dimension tables by configuring `kylin.smart.conf.model-opt-rule=append`.  6. **Optimized Aggregation Indexes:** For excluded columns  configure `kylin.smart.conf.propose-all-foreign-keys=false` to maximize the aggregation of recommended indexes.  7. **Further Configuration:** For additional recommended settings  refer to the full range of SmartConfig configurations and specific settings within the `KylinConfigBase` class.  --------- Co-authored-by: Yifei.Wu <vafuler945@gmail.com> Co-authored-by: Yifan Zhang <event.dimlas@gmail.com> Co-authored-by: Zhiting Guo <35057824+frearb@users.noreply.github.com> Co-authored-by: huangfeng1993 <715187657@qq.com> Co-authored-by: Junqing Cai <caicai121723@163.com> Co-authored-by: Jiale He <965374246@qq.com> Co-authored-by: Liang.Hua <36814772+jacobhua@users.noreply.github.com> Co-authored-by: lixiang <447399170@qq.com> Co-authored-by: Ruixuan Zhang <ruixuan.zhang@kyligence.io> Co-authored-by: lionelcao <whucaolu@gmail.com> Co-authored-by: Xuecheng Shan <shanxuecheng@gmail.com> Co-authored-by: Zhimin Wu <596361258@qq.com> Co-authored-by: feng.zhu <fishcus@outlook.com> Co-authored-by: Cheng Hao <chenghao2262@users.noreply.github.com> Co-authored-by: xinbei <xinbei.fu.gr@dartmouth.edu> Co-authored-by: bingfeng.guo <546745169@qq.com> Co-authored-by: lxian2shell <lxian2shell@gmail.com>
apache,kylin,64408cefd76fdc1a4d82e9c8f32d2e11aa09a016,https://github.com/apache/kylin/commit/64408cefd76fdc1a4d82e9c8f32d2e11aa09a016,KYLIN-5949 Kylin supports Delta Lake as Index storage  1. Support Delta Lake as Index storage. 2. When querying  you can choose to cache the Delta Log on the driver or in RDD Cache mode. 3. V1 and V3 storage is isolated at the model level. 4. Data storage is no longer divided into segments. 5. Query storage optimization can be performed at the index level.  Co-authored-by: Mingming Ge <7mming7@gmail.com>
apache,kylin,205a100e174a0481b9ae1404adaac838295713be,https://github.com/apache/kylin/commit/205a100e174a0481b9ae1404adaac838295713be,KYLIN-5899 Optimization of job table and transaction  1. Add index of project and model_id for job table. 2. Ops booter add async-profiler-lib. 3. transparent transaction should skip fetch memory lock. 4. Optimize the performance of accessing the job_info table.  --------- Co-authored-by: Xuecheng Shan <xuecheng.shan@kyligence.io> Co-authored-by: sibing.zhang <sibing.zhang@qq.com>
spotbugs,spotbugs,1b42dde322dc43e039a45208d1a689203fb4badb,https://github.com/spotbugs/spotbugs/commit/1b42dde322dc43e039a45208d1a689203fb4badb,fix: buffer ViewCFG's output to improve performance (#3407)
spotbugs,spotbugs,9ce74c0793af7725a9f183b98ae60bf4b25dfc5e,https://github.com/spotbugs/spotbugs/commit/9ce74c0793af7725a9f183b98ae60bf4b25dfc5e,perf: avoid calling File.getCanonicalPath twice to improve performance
spotbugs,spotbugs,8b6f8f863be4e71d09f65186270226b3f258287c,https://github.com/spotbugs/spotbugs/commit/8b6f8f863be4e71d09f65186270226b3f258287c,Perform Bug-Reporter deduplication based on target files (#3115)  * Perform Bug-Reporter deduplication based on target files  This is attempt number two to fix #2047  after the initial attempt broke stylesheet behavior and was reverted.  * Canonicalize target filenames to deduplicate output streams  Also includes minor updates to address pull request reviews.  * Remove unnecessary .toString() calls  * Incorporate PR suggestions  * remove unused import  ---------  Co-authored-by: Vogel612 <vogel612@gmx.net>
undertow-io,undertow,bcd28cd16da7281f0c996c397611286224286f9e,https://github.com/undertow-io/undertow/commit/bcd28cd16da7281f0c996c397611286224286f9e,Merge pull request #1721 from jasondlee/UNDERTOW-2547  UNDERTOW-2547 - Perform gathering write
undertow-io,undertow,afa896e03705942f5f0974d9edc9f3f9c5c7551c,https://github.com/undertow-io/undertow/commit/afa896e03705942f5f0974d9edc9f3f9c5c7551c,UNDERTOW-2547 - Perform gathering write  Modify conduit to perform a gather write to decrease latency
camunda,camunda,64203ddee76c0309dd82c53f551ba1ae84f4248f,https://github.com/camunda/camunda/commit/64203ddee76c0309dd82c53f551ba1ae84f4248f,refactor: reintroduce modify pi wrapper  Restore the ModifyProcessZeebeWrapper class to contain the custom Operate process instance modification code. This code is only used for the client-based adapter  and is meant to encapsulate all the logic necessary to perform a (Java) client-based process instance modification.
camunda,camunda,21e822f22e681c7162c54ab8719e78a375398a94,https://github.com/camunda/camunda/commit/21e822f22e681c7162c54ab8719e78a375398a94,refactor: remove pre-authorization annotation on rest services  Remove the Spring-based PreAuthorize check from the Decision and Process/ProcessInstance rest services in Operate. The authorization check is now performed in the service layer.
camunda,camunda,6de4a38ffec7511ad2a8e36f8f2ff0a5cb26e64c,https://github.com/camunda/camunda/commit/6de4a38ffec7511ad2a8e36f8f2ff0a5cb26e64c,feat: introduce process instance migration adapter on operate  Allow Operate to use the CamundaClient or the Camunda Service layer to perform process instance migrations. This allows Operate to run correctly in standalone (compatibility)  or single app mode.
camunda,camunda,05067210f9ff727d8c033c2478aa7abe10337689,https://github.com/camunda/camunda/commit/05067210f9ff727d8c033c2478aa7abe10337689,Perform Incident notification on the background (#31937)  ## Description  <!-- Describe the goal and purpose of this PR. --> Perform the process of notifying regarding Incidents in the Background task of `IncidentUpdateTask`. We already have access to the Incident's in that part of the code and it would be more efficient to batch the notifications instead of send one per exported Incident in the respective `IncidentHandler`.  In this PR the Process and Form caches are also being exposed by the `ExporterResourceProvider` so as to be reused for the background task  as I believe their implementation is not of a shared cache but distinct instances.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria) - [ ] enable backports [when recommended](https://github.com/camunda/camunda/wiki/CI-&-Automation#when-to-backport-ci-changes)  ## Related issues  closes #31937
camunda,camunda,bfb2d134038eba80e4c400c7ab99066296449b34,https://github.com/camunda/camunda/commit/bfb2d134038eba80e4c400c7ab99066296449b34,fix: perform incident notification through background task
camunda,camunda,e2fa252130abad8887c72b72bd021ce1b6b4e7a6,https://github.com/camunda/camunda/commit/e2fa252130abad8887c72b72bd021ce1b6b4e7a6,feat: make IncidentUpdateTask async (#28255)  ## Description The purpose of this PR is refactor the IncidentUpdateTask in a way that it does not block any threads with `.join()` calls and try to parallelize as much as possible. In order to perform causal ordering when processing incidents  only one incident is processed at a given time. A couple of tasks have been parallelized and are mostly about fetching necessary data before processing the incidents: - `searchForInstances > checkDataAndCollectParentTreePaths` : data is collected in parallel - `processIncident > createFlowNodeInstanceUpdates`: flow node instances are fetched in parallel if necessary  All async operation like `thenComposeAsync` are done in the provided executor  which is the executor created in `BackgroundManager`. The size of the executor has been kept the same even if the task is not blocking anymore  to avoid starving other threads. Considering that some tasks may be spawned in parallel  this task could use more than 1 thread  but in general it should be non-blocking so it should not use 1 thread in average.  A simple mechanism for trying to gracefully shutdown this tasks have been added: before rescheduling any task check if it's been cancelled and it if it  it will not reschedule itself. The executor is then terminated "gracefully" with `shutdown` first  giving some time for the tasks to stop gracefully before running `shutdownNow`  if they are not yet terminated. This mechanism can be removed if not deemed necessary  ### Comments The code is much more hard to follow and understand after this change and the parallelism that we gain is really just limited to a couple of places. So I think it would be ok if we decide to keep using a synchronous style except for those two places  where we can actually gain some performance.  ## Related issues closes #27894
camunda,camunda,94d32e7299406d8b2458124b7af0e0cfc153bb7f,https://github.com/camunda/camunda/commit/94d32e7299406d8b2458124b7af0e0cfc153bb7f,31055 add dedicated call hierarchy endpoint to v2 api (#31219)  ## Description  <!-- Describe the goal and purpose of this PR. --> This PR implements the process instance call hierarchy feature for the V2 API by addressing both the API layer for the ES/OS and RDBMS backends.  ### 🔍 What’s included: - Implements [issue #31055](https://github.com/camunda/camunda/issues/31055): Adds a dedicated REST API endpoint to retrieve the call hierarchy for a given process instance (using ES/OS data store). - Implements [issue #31061](https://github.com/camunda/camunda/issues/31061): Adds logic in the RDBMS exporter to generate and persist the treePath  enabling the call hierarchy data to be stored and queried.  The call hierarchy is essential for supporting features in Operate such as breadcrumb navigation and root instance cancellation. This functionality is intentionally separated from the main /process-instances/{key} endpoint to avoid performance degradation during standard queries.  ---  ### Note on Code Duplication  To integrate this feature with the RDBMS backend in a self-contained and milestone-friendly manner  I mirrored logic previously written for ES/OS. The following classes and methods were adapted and copied into the RDBMS module: - CachedProcessEntity - ExporterEntityCache interface - ProcessCacheUtil - createTreePath(...) from ProcessInstanceExporterHandler  While not ideal  this duplication was accepted for pragmatic reasons. A follow-up task is planned ([#31218](https://github.com/camunda/camunda/issues/31218)) to extract this shared logic into a reusable module (camunda-domain-common) to ensure long-term maintainability.  ### Circular Dependency Handling  Originally  these changes were split across two separate PRs (#31219 and #31291). However  due to mutual dependencies between the API contract and the exporter logic  they have now been merged into this single PR to enable a clean review and integration.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria) - [ ] enable backports [when recommended](https://github.com/camunda/camunda/wiki/CI-&-Automation#when-to-backport-ci-changes)  ## Related issues  closes #31055  #31061
camunda,camunda,1f123446fd20b2c32f00f7b1553036371b06cf21,https://github.com/camunda/camunda/commit/1f123446fd20b2c32f00f7b1553036371b06cf21,refactor: perform authorization check before role existence check
camunda,camunda,69e5c593fc71c566d5bd76202f0d23550cce9551,https://github.com/camunda/camunda/commit/69e5c593fc71c566d5bd76202f0d23550cce9551,refactor: retrieve `userTaskKey` from element instance  Changed the approach for retrieving the user task key during `CREATING` task listener job completion. Instead of extracting the key from the job's custom headers  it is now retrieved directly from the associated `ElementInstance` in the state.  Accessing the element instance state might cause an additional RocksDB lookup  but it may be cached internally and is unlikely to cause a noticeable performance impact.  If the element instance is missing (which should not happen in normal flow) or the user task key is invalid or uninitialized (<= 0)  an `IllegalStateException` is thrown to indicate an unexpected internal issue and to provide additional context for debugging.
camunda,camunda,5359590838a5ec46b44a01016e00e81fddcd53d0,https://github.com/camunda/camunda/commit/5359590838a5ec46b44a01016e00e81fddcd53d0,feat: make variable filtering more convenient (#31245)  ## Description  * Improves variable filtering in requests in the Java client  allowing consumers instead of filter objects. * Removes superficial and unused classes  methods  and attributes. * Simplifies search properties transformation. * Streamlines client API to REST DTO transformation.  ## Breaking changes  * Replaces the `ProcessInstanceVariableFilterRequest` and `UserTaskVariableFilterRequest` with a unified `VariableValueFilterRequest` in the OpenAPI spec. Custom REST API clients generating DTOs from the spec will need to be adjusted. The affected endpoints are alpha feature endpoints and thus already marked as subject to change in 8.6 and 8.7. * For naming consistency  replaces the `VariableUserTaskFilterRequest` with a more fitting `UserTaskVariableFilterRequest`. Custom REST API clients generating DTOs from the spec will need to be adjusted. The affected endpoints are alpha feature endpoints and thus already marked as subject to change in 8.6 and 8.7.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria) - [ ] enable backports [when recommended](https://github.com/camunda/camunda/wiki/CI-&-Automation#when-to-backport-ci-changes)  ## Related issues  closes #31362
camunda,camunda,fed98c2c788483292d1185ad1f71658dcd080ecd,https://github.com/camunda/camunda/commit/fed98c2c788483292d1185ad1f71658dcd080ecd,refactor: streamline search implementation  Uses consistent filter and property handling in all Java client request implementations.  Removes unused and superficial methods and attributes.
camunda,camunda,558aecaece644de507bdf0fe80a7425322e59131,https://github.com/camunda/camunda/commit/558aecaece644de507bdf0fe80a7425322e59131,[Backport main] add verbose flag to operate backup list API to improve performance if `verbose=false` (#30849)  # Description Backport of #30730 to `main`.  > [!WARNING] Because of the changes in the webapps backup module the backport has been mostly manual  - Added verbose flag to OpenAPI backup spec - Added tests in BackupRestoreIT checking that the backups are completed with both `verbose=false` & `verbose=true`  relates to #30695
camunda,camunda,008e80952483bde5f823db7e17cfa26b8af2c647,https://github.com/camunda/camunda/commit/008e80952483bde5f823db7e17cfa26b8af2c647,perf: use an `EnumMap`
camunda,camunda,50a967afff8c67739e8c0437ca194e351bf5046d,https://github.com/camunda/camunda/commit/50a967afff8c67739e8c0437ca194e351bf5046d,feat: add verbose flag to operate backup list API to improve performance  Adding verbose to the request sent to ES/OS speeds up the query by a lot  but we don't get startTime and metadata fields populated. startTime will be null  while metadata can be extracted from the snapshot name if metadata field is missing. closes #30695  (cherry picked from commit fe1e6e30eab1fb124d87d4a0a4811d0e24428f44)
camunda,camunda,464b6174dacd942d8493e070c9223adbbc85357f,https://github.com/camunda/camunda/commit/464b6174dacd942d8493e070c9223adbbc85357f,feat: remove a limiter on appends based on avg append latency (#30353)  ## Description The condition `(now() - (appendLatency.mean() / maxAppendsPerMember) >= appendTimeStart)` may be superfluous as we already limit the number of inflight requests to maxAppendsPerMember.  Looking into the implementation of `DescriptiveStatistics` class it's not been implemented with performance in mind  as it create a lot of temporary `double[] ` instead of using a fixed size circular buffer.  A better solution would be to implement #30379  but it's outside the scope of this PR  FYI @ChrisKujawa
camunda,camunda,819c0b70ac6356f1a49e0f7f3fb6ee33a8feee4c),[solution](https://github.com/camunda/camunda/commit/819c0b70ac6356f1a49e0f7f3fb6ee33a8feee4c),Improve archiving performance (#30684)  ## Description  It was [highlighted](https://camunda.slack.com/archives/C08D74J6HUG/p1740412731557659) that the archiving is too slow to catch up with exporting/processing. We were able to complete a lot of process instances  and also to export them to ES. But they haven't been moved to the dated indices quickly enough  causing to fill up ES at some point.  We can see in the following metric that we complete per partition ~50 process instances (on the medic benchmarks)  but only ~5% are actually moved to the dated indices  where they can be deleted by ILM later.  ![archiver-lagging-behind](https://github.com/user-attachments/assets/354d052e-abc1-42b1-8526-4d43b74c0e58)  We haven't had not much observability around this topic and missed some visualization (previously we added some panels via #30679). On top of this  the PR adds some more panels to better visualize the archiving  and how many instances are actually found to be archived.  This allowed better to pinpoint where the bottleneck was  querying the completed process instances. Especially that it is not easy to fine-tune related `rolloverBatchSize` as it would simply stop finding new results  ![2025-04-07_14-46](https://github.com/user-attachments/assets/5106cf19-621f-4711-b7e4-946bd963ed48)  It turned out to be an issue with the used aggregation  also highlighted by @lenaschoenburg [previously](https://camunda.slack.com/archives/C08D74J6HUG/p1744033725997889?thread_ts=1744030148.764179&cid=C08D74J6HUG).  With the new metric and the
camunda,camunda,d922905acbc13991967787159e097d1ba720a2c8,https://github.com/camunda/camunda/commit/d922905acbc13991967787159e097d1ba720a2c8,
camunda,camunda,e2ee8c3be6d1b228b76a90bb0f855741f3b0b1fc,https://github.com/camunda/camunda/commit/e2ee8c3be6d1b228b76a90bb0f855741f3b0b1fc,refactor: use search query instead of aggregation  * Similar to 0354a9693a092e81a50470b1ef0c2cff78bc6698 this commit affects the OS part of the archiving logic * This commit replaces the previous used aggregation with a normal search query to filter for process instances or batch operations by end date * The use case for aggregations are completly different to what we wanted to use it for * Benchmarks has shown that aggregations perform way slower than a normal search query * It is not clear why we used an aggregration before * Aggregation are limited by how many results it can return  at somepoint it simply failed without any results  and unclear error messages. This made it hard to fine-tune. * The general change was motivated by 819c0b70ac6356f1a49e0f7f3fb6ee33a8feee4c from @lenaschoenburg
camunda,camunda,84fab4994210d118064a9cafb262b0c98ba01d51,https://github.com/camunda/camunda/commit/84fab4994210d118064a9cafb262b0c98ba01d51,refactor: use search query instead of aggregation  * This commit replaces the previous used aggregation with a normal search query to filter for process instances or batch operations by end date * The use case for aggregations are completly different to what we wanted to use it for * Benchmarks has shown that aggregations perform way slower than a normal search query * It is not clear why we used an aggregration before * Aggregation are limited by how many results it can return  at somepoint it simply failed without any results  and unclear error messages. This made it hard to fine-tune. * The general change was motivated by 819c0b70ac6356f1a49e0f7f3fb6ee33a8feee4c from @lenaschoenburg
camunda,camunda,95b11c1aa811793e64a4a3d468aea7efc1c653a3,https://github.com/camunda/camunda/commit/95b11c1aa811793e64a4a3d468aea7efc1c653a3,feat: invalidate caches instead of updating them to avoid upfront performance cost
camunda,camunda,7d9aac336892e00c1fc474c2c5a2b817f6987798,https://github.com/camunda/camunda/commit/7d9aac336892e00c1fc474c2c5a2b817f6987798,feat: handle state cleanup and preparation on `UT.CANCELING` event  Extends `UserTaskCancelingV2Applier` to perform full cleanup and preparation when applying the `CANCELING` user task event:  - Deletes intermediate state and request metadata from previous transitions (if present). - Persists new intermediate state for the `canceling` transition.
camunda,camunda,c34066fc76ca0056378a929f7b1f1a9ed78f48cf,https://github.com/camunda/camunda/commit/c34066fc76ca0056378a929f7b1f1a9ed78f48cf,feat: remove a limiter on appends based on avg append latency  The condition (now() - (appendLatency.mean() / maxAppendsPerMember) >= appendTimeStart) may be superfluous as we already limit the number of inflight requests to maxAppendsPerMember.  Looking into the implementation of DescriptiveStatistics class it's not been implemented with performance in mind  as it create a lot of temporary double[] instead of using a fixed size circular buffer.
camunda,camunda,9700be4c48897cb59032f379a9c60a84ec3e45f2,https://github.com/camunda/camunda/commit/9700be4c48897cb59032f379a9c60a84ec3e45f2,feat: init batch operation (#29916)  ## Description  This PR introduces:  #### Batch operation execution scheduler This scheduler periodically looks for new batch operations. For any new batch operation it will perform a paged search using the respective searchClient to retrieve all the keys for the current partition. This needs to happen using paging because ES/OS only support 10K entries in the search result. This result will then be split into chunks (BatchOperationChunkRecord) of 400.000 entity keys if needed  to keep the record smaller than 4MB. To not block any stream processor  all this must happen in a separate `AsyncTaskGroup.BATCH_OPERATIONS`  #### DbBatchOperationState The internal state is enhanced to store all the entity keys of the batch operation. To not overload the RocksDB  the amount of keys will be split again here into smaller chunks of 3500 keys (smaller than the 32KB blocksize of RocksDB) and stored in a separate column family. This block-list is implemented as queue. New entityKeys are added to the head  when keys are being processed they are picked and removed from the tail of the queue.  ## Related issues  closes #29697
camunda,camunda,bb1e6a637310018153e7ad7e0a09794234d69fe2,https://github.com/camunda/camunda/commit/bb1e6a637310018153e7ad7e0a09794234d69fe2,Fix tenant duplicate entity  assignment error code (#30165)  ## Description  <!-- Describe the goal and purpose of this PR. --> This PR adjusts the error code provided when a duplicate entity assignment is performed for a tenant. The response now returns a `409` (Conflict) instead of the wrong `400` error.  Furthermore  the error messages for the different entities are now aligned. Previously  for the same error code (I.e. `409`)  the error message varied depending if the entity was a User  Group  or Mapping.  Finally  @maryarm and @Oleksiivanov (since I think both of you are working on Mapping-related tasks)  I needed to adjust the following to ensure that the Mapping-related IT tests work: - I adjusted the OpenAPI spec for the `PUR|DELETE /tenants/{tenantId}/mapping-rules/{mappingKey}` endpoints to use `mappingId`. - The `TenantController` is updated as well for the `PUT` and `DELETE` endpoints (`DELETE` isn't implemented yet in the processor) - The Java client `AssignMappingToTenant` interface and CommandImpl are updated (no delete command is implemented yet).  I haven't been able to find an issue about the mapping-related changes  so I can link to it. Let me know if there is one.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria) - [ ] enable backports [when recommended](https://github.com/camunda/camunda/wiki/CI-&-Automation#when-to-backport-ci-changes)  ## Related issues  closes #29400 related to #29534
camunda,camunda,8dcb9d7f21e0010e2c9ff688d05bb667ab63b3de,https://github.com/camunda/camunda/commit/8dcb9d7f21e0010e2c9ff688d05bb667ab63b3de,feat: introducing `deleteIntermediateStateIfExists` method  Reason: Allow deletion of intermediate state without additional checks in order to benefit performance.
camunda,camunda,2bedeeab814849b4830eb845d6573c08539155be,https://github.com/camunda/camunda/commit/2bedeeab814849b4830eb845d6573c08539155be,Remove hardcoded default tenant access (#29839)  ## Description  <!-- Describe the goal and purpose of this PR. --> If MT is enabled we would check if the provided tenant id was the default tenant. If this was true we'd always allow the user to perform the operation they are doing. This is not how it worked in old identity. In old identity  if MT is enabled  the user must be explicitly assigned to the default tenant to be able to interact with it. I have removed this entire check. Before this method is called we already check if MT is enabled or not.  I had to fix some tests in the process. I've also did some small refactorings. I extracted a few flags to fields to make the code simpler and I've slightly improved the error message if a user is not assigned to a tenant.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria) - [ ] enable backports [when recommended](https://github.com/camunda/camunda/wiki/CI-&-Automation#when-to-backport-ci-changes)  ## Related issues  closes #29518
camunda,camunda,57565138b90571f4bbcdc09fa325518bf6ade85a,https://github.com/camunda/camunda/commit/57565138b90571f4bbcdc09fa325518bf6ade85a,fix: remove tenant check when fetching mapping resource identifiers  Tenants are unrelated to the authorized resource identifiers. We shouldn't perform a tenant check here.
camunda,camunda,09b0a63e22ed0d8ec3985cc5820dfea92842b1a8,https://github.com/camunda/camunda/commit/09b0a63e22ed0d8ec3985cc5820dfea92842b1a8,feat: improve error message  The forbidden for tenant error message was semantically a bit strange. It would say there were insufficient permissions to perform an action for a tenant. Permissions here is odd  as tenants aren't about authorizations  but about data access. I've reworded this message.  Note there is a differences between deploy resource command and deploy command. This is because the deploy command uses gRPC  whereas the deploy resource command uses rest. In gRPC there is no forbidden status. Hence it is PERMISSION_DENIED here.
camunda,camunda,5f99c448063563a8d95f96cb88703acc8e42b0be,https://github.com/camunda/camunda/commit/5f99c448063563a8d95f96cb88703acc8e42b0be, fix: don't pollute the command cache  (#29769)  This fixes #29735 where the command cache was polluted with commands that were processed before they were added to the cache.  Instead of only persisting staged cache entries after a successful write  we now do so before trying to write. In rare cases where writing fails  we perform a rollback by removing all staged  and by now persisted  cache entries. There is a small possibility that rollback removes entries that were added by other cache users. This is safe though  the cache is only an optimization.  Sadly I didn't manage to write a regression test for this. I think that's okay though  the bug is well understood and relatively simple.
camunda,camunda,14000162ae35335ba3fbd9156dea48806b4a4fa3,https://github.com/camunda/camunda/commit/14000162ae35335ba3fbd9156dea48806b4a4fa3,perf: avoid singleton set when caching a single key
camunda,camunda,15b0d5ac6ce7036849a477c26ae4d6dd5993887f,https://github.com/camunda/camunda/commit/15b0d5ac6ce7036849a477c26ae4d6dd5993887f,perf: rollback only locks cache once
camunda,camunda,421f3eba2e1fdee3212eacc33941e898bec8e06e,https://github.com/camunda/camunda/commit/421f3eba2e1fdee3212eacc33941e898bec8e06e,fix: don't pollute the command cache  This fixes a bug where the command cache was polluted with commands that were processed before they were added to the cache.  Instead of only persisting staged cache entries after a successful write  we now do so before trying to write. In rare cases where writing fails  we perform a rollback by removing all staged  and by now persisted  cache entries. There is a small possibility that rollback removes entries that were added by other cache users. This is safe though  the cache is only an optimization.
camunda,camunda,515031d066dfb39dea2d019c366b7d498e4685ca,https://github.com/camunda/camunda/commit/515031d066dfb39dea2d019c366b7d498e4685ca,feat: new records for batch operation (#29665)  ## Description  Introduces new records and intents for managing batch operations in the engine.  There will be three new record types:  ### BatchOperationCreationRecordValue The initial record to create a batchOperation:  - batchOperationKey: the PK of the batch operation. on Intent.CREATE it will not contain a value - entityFilter: a generic filter object. This is a serialized JSON string representation of the filter object from the camunda-search-domain and can be applied to the respective search service - batchOperationType: an enum to specify the actual operation to execute on each entity  ### BatchOperationKeyChunkRecordValue A record containing a chunk of entity keys to perform the actual batch command on. This command/event can occur multiple times at the beginning of the batch operation. The total collection of keys will be sliced into smaller chunks of max. 495.000 entity keys  the max amount of 8-byte Long values in 4MB record size plus a bit overhead.  - batchOperationKey: the PK of the batch operation - entityKeys: A list of entity keys to be added to the batch operation  ### BatchOperationExecutionRecordValue The record representing a batch operation in progress. Some records will not have any entityKeys  only EXECUTING and EXECUTED.  - batchOperationKey: the PK of the batch operation - entityKeys: A list of entity keys to be added to the batch operation - batchOperationType: an enum to specify the actual operation to execute on each entity  ## Notes  Some changes in this PR do not relate directly to BatchOperations in the Engine. - the legacy ES/OS exporters have unit tests checking that each existing valueType can be handled by the exporter. Therefore this PR also changes code in the ES/OS exporter  ## Checklist  ## Related issues  closes #29669
camunda,camunda,d4b0f927d84bb64b82f3c844ac6e8edd006a57fd,https://github.com/camunda/camunda/commit/d4b0f927d84bb64b82f3c844ac6e8edd006a57fd,fix: return forbidden if user is not assigned to the tenant  Unauthorizd indicates that a client did not provide valid authentication. This is not the case. Forbidden indicates that the client has been authenticated  but it lacks the required permissions. This is the case when it's not authorized to perform operations for an unassigned tenant.
camunda,camunda,9981bb1b9505e419a63b3726e529ebbbf925d270,https://github.com/camunda/camunda/commit/9981bb1b9505e419a63b3726e529ebbbf925d270,migration testing preconditions (#29185)  ## Description  <!-- Describe the goal and purpose of this PR. --> Add some preconditions related to the migration testing setup. - CamundaVolume: Wrapper to perform correct extraction of the Zeebe volume to have data accessible on newer version - MigrationDatabaseChecks: Checks related to Importer's having flushed the `import-positions` and that the importers have finished importing - Move `withWorkingDirectory` to base class for Standalone test apps - Create DocumentBaseSearchClients based on existing `ConnectConfiguration` ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria) - [ ] enable backports [when recommended](https://github.com/camunda/camunda/wiki/CI-&-Automation#when-to-backport-ci-changes)  ## Related issues  split from: https://github.com/camunda/camunda/pull/28012 related to: https://github.com/camunda/camunda/issues/28166
camunda,camunda,c9eca7ec17c7fcb13dcecd0b53a49e595fae9408,https://github.com/camunda/camunda/commit/c9eca7ec17c7fcb13dcecd0b53a49e595fae9408,Raise incident at specific called process depth (#22735)  ## Description  <!-- Describe the goal and purpose of this PR. -->  This is the effort we made during our mob programming session. We added a test case with a failsafe to ensure the process instance doesn't overload the test engine. We also added a simple implementation to check the called process depth.  Still to do: - actually increment the depth - make the depth configurable - consider how to deal with existing child instances - configure the depth in the test for performance  ## Related issues  closes #16410
camunda,camunda,18a73e46433806806a942db06c4a2d114a3c3900,https://github.com/camunda/camunda/commit/18a73e46433806806a942db06c4a2d114a3c3900,perf: correctly declare number of properties  This helps avoid unnecessary allocations and memory copies for the declared properties.  Co-authored-by: Dmitriy Melnychuk <dmitriy.melnychuk@capgemini.com> Co-authored-by: Remco <remco@westerhoud.nl> Co-authored-by: Stephan Epping <stephan.epping@camunda.com> Co-authored-by: berkaycanbc <berkay.can@camunda.com>
camunda,camunda,9be05f33b7dc214568f8785be35c43da874bd363,https://github.com/camunda/camunda/commit/9be05f33b7dc214568f8785be35c43da874bd363,fix: start calledProcessDepth at 0  There's a reason to keep it at -1  if you want to differentiate between the calledProcessDepth not set  and it just being set by default.  The -1 case could've been useful to ensure we set the calledProcessDepth correctly for child process instance created after 8.7 that are part of root process instance created prior to 8.7. However  this comes at the cost of performance  as a lookup is needed for each called level.  Instead  we simply set the default to 0  to ensure that all element instances created prior to 8.7  have it set to 0.  This has the added benefit that we do not have to explicitly set the value in other places (like test code).  Co-authored-by: Dmitriy Melnychuk <dmitriy.melnychuk@capgemini.com> Co-authored-by: Remco <remco@westerhoud.nl> Co-authored-by: Stephan Epping <stephan.epping@camunda.com> Co-authored-by: berkaycanbc <berkay.can@camunda.com>
camunda,camunda,ed17ba8adfdce47a4a3431dd33d7ed82f40bd23f,https://github.com/camunda/camunda/commit/ed17ba8adfdce47a4a3431dd33d7ed82f40bd23f,feat: check for depth  This is the effort we made during our mob programming session. We added a test case with a failsafe to ensure the process instance doesn't overload the test engine. We also added a simple implementation to check the called process depth.  Still to do: - actually increment the depth - make the depth configurable - consider how to deal with existing child instances - configure the depth in the test for performance  Co-authored-by: berkaycanbc <berkay.can@camunda.com> Co-authored-by: ana.vinogradova <ana.vinogradova@camunda.com> Co-authored-by: Remco Westerhoud <remcowesterhoud@msn.com>
camunda,camunda,31604b79acea59b80f180735dda9497fb440722e,https://github.com/camunda/camunda/commit/31604b79acea59b80f180735dda9497fb440722e,feat: check if user has access to web application (#28438)  ## Description  Webapplication access needs to be granted via the [Application Permission](https://docs.google.com/document/d/1ikxw9ARCv-MBwASnj7WhW5uHCndCRk2B35xi-cB2N0M/edit?tab=t.0#heading=h.c0kx9z3moo6t). Thus when accessing a webapplication the authorization for the app needs to be checked and else a page displayed indicating a lack of permissions.  ## Checklist  - [ ] After a user is logged in to a web application the frontend performs a check whether the user has the `ACCESS` permission to that application e.g. via https://docs.camunda.io/docs/next/apis-tools/camunda-api-rest/specifications/find-authorizations/ - [ ] On no access  an error page is shown indicating the lack of authorization to access the application - see [Figma design](https://www.figma.com/design/0FhSUskdeATmYkEyXAj9S4/Identity-%26-Console-8.7?node-id=136-157115&t=sSiemgHtFmlqNAGa-4)  ## Related issues  closes #27885
camunda,camunda,c71f6acbf5dc739afa2cdb28d5702c8fa218eac9,https://github.com/camunda/camunda/commit/c71f6acbf5dc739afa2cdb28d5702c8fa218eac9,fix: perform delayed flush when wait for importers is disabled (#28154)  ## Description  Previously the delayed flush was not executed if wait for importers was disabled but actual importer indices with incomplete states existed. As the regular flush is already performed regardless of importer state  when the `shouldWaitForImporters` flag is set  this adopts the delayed flush to behave in the same way.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria) - [ ] enable backports [when recommended](https://github.com/camunda/camunda/wiki/CI-&-Automation#when-to-backport-ci-changes)
camunda,camunda,98967c46185af0d1d9f8247acf1600ad42e2dba0,https://github.com/camunda/camunda/commit/98967c46185af0d1d9f8247acf1600ad42e2dba0,fix: perform delayed flush when wait for importers is disabled  Previously the delayed flush was not executed when wait for importers was disabled but actual importer indices with incomplete states existed.
camunda,camunda,bcc389615bec1af9faac9ef3ae130f9b18ba6b22,https://github.com/camunda/camunda/commit/bcc389615bec1af9faac9ef3ae130f9b18ba6b22,fix: do not perform nested computeIfAbsent in SwimMembershipProtocolMetrics
camunda,camunda,e38e7456ab76156f69a0b5f53527365c16908fd7,https://github.com/camunda/camunda/commit/e38e7456ab76156f69a0b5f53527365c16908fd7,refactor: extract command finalization in `UserTaskUpdateProcessor`  Refactored `UserTaskUpdateProcessor` to separate command finalization logic into `onFinalizeCommand`. This allows waiting for all `updating` task listeners to be processed before writing the `UPDATED` intent.  Also  updated `userTaskRecord` to be modifiable in `onCommand` so that all updates performed by `UserTaskUpdateProcessor#onCommand` will be accessible during creation of the first `updating` listener job.
camunda,camunda,acd48487191a817c2572ee23ba87d4a5cd7ccd76,https://github.com/camunda/camunda/commit/acd48487191a817c2572ee23ba87d4a5cd7ccd76,deps: Update spring boot to v3.4.1 (main) (#25062)  This PR contains the following updates:  | Package | Change | Age | Adoption | Passing | Confidence | |---|---|---|---|---|---| | [org.springframework.boot:spring-boot-dependencies](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.5` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-dependencies/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-dependencies/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-dependencies/3.3.5/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-dependencies/3.3.5/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-maven-plugin](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-maven-plugin/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-maven-plugin/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-maven-plugin/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-maven-plugin/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter-webflux](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter-webflux/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter-webflux/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter-webflux/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter-webflux/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter-test](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter-test/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter-test/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter-test/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter-test/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-dependencies](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-dependencies/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-dependencies/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-dependencies/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-dependencies/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter-json](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter-json/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter-json/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter-json/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter-json/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter-actuator](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter-actuator/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter-actuator/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter-actuator/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter-actuator/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-configuration-processor](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-configuration-processor/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-configuration-processor/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-configuration-processor/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-configuration-processor/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-test](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-test/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-test/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-test/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-test/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-actuator-autoconfigure](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-actuator-autoconfigure/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-actuator-autoconfigure/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-actuator-autoconfigure/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-actuator-autoconfigure/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-autoconfigure](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-autoconfigure/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-autoconfigure/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-autoconfigure/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-autoconfigure/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-actuator](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-actuator/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-actuator/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-actuator/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-actuator/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | | [org.springframework.boot:spring-boot-starter-jersey](https://spring.io/projects/spring-boot) ([source](https://redirect.github.com/spring-projects/spring-boot)) | `3.3.4` -> `3.4.1` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter-jersey/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter-jersey/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter-jersey/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter-jersey/3.3.4/3.4.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) |  ---  > [!WARNING] > Some dependencies could not be looked up. Check the Dependency Dashboard for more information.  ---  ### Release Notes  <details> <summary>spring-projects/spring-boot (org.springframework.boot:spring-boot-dependencies)</summary>  ### [`v3.4.1`](https://redirect.github.com/spring-projects/spring-boot/compare/v3.4.0...v3.4.1)  ### [`v3.4.0`](https://redirect.github.com/spring-projects/spring-boot/releases/tag/v3.4.0)  ##### :star: New Features  - Add withDefaultRequestConfigCustomizer method to HttpComponentsClientHttpRequestFactoryBuilder [#&#8203;43139](https://redirect.github.com/spring-projects/spring-boot/issues/43139) - Fail JsonWriter if duplicate names are detected [#&#8203;43041](https://redirect.github.com/spring-projects/spring-boot/issues/43041) - Add JsonObjectDeserializer.nullSafeValue method that accepts a mapper Function [#&#8203;42972](https://redirect.github.com/spring-projects/spring-boot/issues/42972) - Support timeout property for GraphQL over SSE [#&#8203;42966](https://redirect.github.com/spring-projects/spring-boot/pull/42966) - Improve performance of ConfigurationPropertiesBinder by storing bind handlers on first access [#&#8203;42950](https://redirect.github.com/spring-projects/spring-boot/issues/42950) - Improve performance of ConcurrentReferenceCachingMetadataReaderFactory [#&#8203;42949](https://redirect.github.com/spring-projects/spring-boot/issues/42949) - Log warning in HikariCheckpointRestoreLifecycle if pool suspension isn't configured [#&#8203;42937](https://redirect.github.com/spring-projects/spring-boot/pull/42937) - Remove spring-boot-starter-aop dependency from spring-boot-starter-data-jpa and spring-boot-starter-integration [#&#8203;42934](https://redirect.github.com/spring-projects/spring-boot/issues/42934)  ##### :lady_beetle: Bug Fixes  - Jersey body handling is inconsistent with Spring Webflux and Spring MVC [#&#8203;43209](https://redirect.github.com/spring-projects/spring-boot/issues/43209) - Classes are accidentally named "structure logging" instead of "structured logging" [#&#8203;43203](https://redirect.github.com/spring-projects/spring-boot/pull/43203) - StructuredLoggingJsonProperties customizer should be a Class reference rather than a String [#&#8203;43202](https://redirect.github.com/spring-projects/spring-boot/issues/43202) - Cannot package OCI image when 'docker.io/paketobuildpacks/new-relic' is provided as a buildpack [#&#8203;43171](https://redirect.github.com/spring-projects/spring-boot/issues/43171) - Incorrect Type for 'management.endpoints.access.default' defined in additional-spring-configuration-metadata.json [#&#8203;43154](https://redirect.github.com/spring-projects/spring-boot/issues/43154) - WebServerPortFileWriter fails when using a portfile without extension [#&#8203;43117](https://redirect.github.com/spring-projects/spring-boot/issues/43117) - SslOptions.isSpecified() only returns true if ciphers and enabled protocols are set [#&#8203;43084](https://redirect.github.com/spring-projects/spring-boot/issues/43084) - SslHealthIndicator throws NullPointerException when using SslBundle with SslStoreBundle.NONE [#&#8203;43078](https://redirect.github.com/spring-projects/spring-boot/issues/43078) - JdkClientHttpRequestFactoryBuilder and JettyClientHttpRequestFactoryBuilder do not set Ciphers or Enabled Protocols [#&#8203;43077](https://redirect.github.com/spring-projects/spring-boot/issues/43077) - Root cause of errors is hidden when loading images from archive [#&#8203;43070](https://redirect.github.com/spring-projects/spring-boot/issues/43070) - mvn spring-boot:run fails on Windows with "Could Not Find or Load Main Class" when path contains non-ASCII characters [#&#8203;43062](https://redirect.github.com/spring-projects/spring-boot/issues/43062) - A `@SpyBean` on the output of a FactoryBean is not reset [#&#8203;43053](https://redirect.github.com/spring-projects/spring-boot/issues/43053) - Logback logging system does not process URLs with paths not ending in .xml [#&#8203;42990](https://redirect.github.com/spring-projects/spring-boot/issues/42990) - Bean-based conditions do not consider factory beans correctly when determining if they are a candidate [#&#8203;42970](https://redirect.github.com/spring-projects/spring-boot/issues/42970) - NPE in bootBuildImage when setting DOCKER_CONTEXT=default [#&#8203;42960](https://redirect.github.com/spring-projects/spring-boot/issues/42960) - Warning due to duplicate MockResolver extensions [#&#8203;42957](https://redirect.github.com/spring-projects/spring-boot/issues/42957) - HttpHostConnectException is thrown when using buildpacks with Gradle or Maven on Windows [#&#8203;42952](https://redirect.github.com/spring-projects/spring-boot/issues/42952) - build-info doesn't support seconds since the epoch from project.build.outputTimestamp [#&#8203;42936](https://redirect.github.com/spring-projects/spring-boot/issues/42936) - NPE in OnClassCondition.resolveOutcomesThreaded following thread interruption because firstHalf is null [#&#8203;42926](https://redirect.github.com/spring-projects/spring-boot/issues/42926) - Default WebSocketMessageBrokerConfigurer is always overriding custom channel executor [#&#8203;42924](https://redirect.github.com/spring-projects/spring-boot/issues/42924) - X-Registry-Auth header sent to Docker Engine API contains field "authHeader" [#&#8203;42915](https://redirect.github.com/spring-projects/spring-boot/issues/42915) - ApplicationContextRunner has inconsistent behaviour with duplicate auto-configuration class names [#&#8203;17963](https://redirect.github.com/spring-projects/spring-boot/issues/17963)  ##### :notebook_with_decorative_cover: Documentation  - Migrate class references to full javadoc links [#&#8203;43239](https://redirect.github.com/spring-projects/spring-boot/issues/43239) - Documentation for 'spring.datasource.type' is misleading [#&#8203;43199](https://redirect.github.com/spring-projects/spring-boot/issues/43199) - Update "Upgrading From" section to use "2.x" [#&#8203;43160](https://redirect.github.com/spring-projects/spring-boot/issues/43160) - Include spring-boot-loader in API documentation [#&#8203;43153](https://redirect.github.com/spring-projects/spring-boot/issues/43153) - Document how and where to add custom GraalVM configuration files [#&#8203;43074](https://redirect.github.com/spring-projects/spring-boot/issues/43074) - Rework DataSource configuration examples to separate defining an additional DataSource and defining a DataSource of a different type [#&#8203;43059](https://redirect.github.com/spring-projects/spring-boot/issues/43059) - Location of the layers schema is incorrect in the Maven Plugin's examples [#&#8203;43033](https://redirect.github.com/spring-projects/spring-boot/issues/43033) - Link to Eclipse setup instructions [#&#8203;42954](https://redirect.github.com/spring-projects/spring-boot/issues/42954) - Fix link to Checkpoint and Restore status page [#&#8203;42939](https://redirect.github.com/spring-projects/spring-boot/issues/42939)  ##### :hammer: Dependency Upgrades  - Upgrade to ActiveMQ 6.1.4 [#&#8203;43128](https://redirect.github.com/spring-projects/spring-boot/issues/43128) - Upgrade to Byte Buddy 1.15.10 [#&#8203;43097](https://redirect.github.com/spring-projects/spring-boot/issues/43097) - Upgrade to Couchbase Client 3.7.5 [#&#8203;43098](https://redirect.github.com/spring-projects/spring-boot/issues/43098) - Upgrade to Elasticsearch Client 8.15.4 [#&#8203;43129](https://redirect.github.com/spring-projects/spring-boot/issues/43129) - Upgrade to Flyway 10.20.1 [#&#8203;43130](https://redirect.github.com/spring-projects/spring-boot/issues/43130) - Upgrade to Groovy 4.0.24 [#&#8203;43099](https://redirect.github.com/spring-projects/spring-boot/issues/43099) - Upgrade to Hibernate 6.6.2.Final [#&#8203;43100](https://redirect.github.com/spring-projects/spring-boot/issues/43100) - Upgrade to HttpClient5 5.4.1 [#&#8203;43102](https://redirect.github.com/spring-projects/spring-boot/issues/43102) - Upgrade to Infinispan 15.0.11.Final [#&#8203;43131](https://redirect.github.com/spring-projects/spring-boot/issues/43131) - Upgrade to Jackson Bom 2.18.1 [#&#8203;43103](https://redirect.github.com/spring-projects/spring-boot/issues/43103) - Upgrade to Jetty 12.0.15 [#&#8203;43104](https://redirect.github.com/spring-projects/spring-boot/issues/43104) - Upgrade to jOOQ 3.19.15 [#&#8203;43105](https://redirect.github.com/spring-projects/spring-boot/issues/43105) - Upgrade to Kafka 3.8.1 [#&#8203;43106](https://redirect.github.com/spring-projects/spring-boot/issues/43106) - Upgrade to Lettuce 6.4.1.RELEASE [#&#8203;43185](https://redirect.github.com/spring-projects/spring-boot/issues/43185) - Upgrade to Logback 1.5.12 [#&#8203;43107](https://redirect.github.com/spring-projects/spring-boot/issues/43107) - Upgrade to Lombok 1.18.36 [#&#8203;43186](https://redirect.github.com/spring-projects/spring-boot/issues/43186) - Upgrade to Maven Dependency Plugin 3.8.1 [#&#8203;43108](https://redirect.github.com/spring-projects/spring-boot/issues/43108) - Upgrade to Maven Failsafe Plugin 3.5.2 [#&#8203;43109](https://redirect.github.com/spring-projects/spring-boot/issues/43109) - Upgrade to Maven Surefire Plugin 3.5.2 [#&#8203;43110](https://redirect.github.com/spring-projects/spring-boot/issues/43110) - Upgrade to Micrometer 1.14.1 [#&#8203;43187](https://redirect.github.com/spring-projects/spring-boot/issues/43187) - Upgrade to Micrometer Tracing 1.4.0 [#&#8203;43120](https://redirect.github.com/spring-projects/spring-boot/issues/43120) - Upgrade to MongoDB 5.2.1 [#&#8203;43111](https://redirect.github.com/spring-projects/spring-boot/issues/43111) - Upgrade to Netty 4.1.115.Final [#&#8203;43133](https://redirect.github.com/spring-projects/spring-boot/issues/43133) - Upgrade to Prometheus Client 1.3.3 [#&#8203;43112](https://redirect.github.com/spring-projects/spring-boot/issues/43112) - Upgrade to Pulsar Reactive 0.5.9 [#&#8203;43188](https://redirect.github.com/spring-projects/spring-boot/issues/43188) - Upgrade to Reactor Bom 2024.0.0 [#&#8203;43015](https://redirect.github.com/spring-projects/spring-boot/issues/43015) - Upgrade to Spring AMQP 3.2.0 [#&#8203;43016](https://redirect.github.com/spring-projects/spring-boot/issues/43016) - Upgrade to Spring Authorization Server 1.4.0 [#&#8203;43017](https://redirect.github.com/spring-projects/spring-boot/issues/43017) - Upgrade to Spring Batch 5.2.0 [#&#8203;43018](https://redirect.github.com/spring-projects/spring-boot/issues/43018) - Upgrade to Spring Data Bom 2024.1.0 [#&#8203;43019](https://redirect.github.com/spring-projects/spring-boot/issues/43019) - Upgrade to Spring Framework 6.2.0 [#&#8203;43020](https://redirect.github.com/spring-projects/spring-boot/issues/43020) - Upgrade to Spring HATEOAS 2.4.0 [#&#8203;43021](https://redirect.github.com/spring-projects/spring-boot/issues/43021) - Upgrade to Spring Integration 6.4.0 [#&#8203;43022](https://redirect.github.com/spring-projects/spring-boot/issues/43022) - Upgrade to Spring Kafka 3.3.0 [#&#8203;43023](https://redirect.github.com/spring-projects/spring-boot/issues/43023) - Upgrade to Spring LDAP 3.2.8 [#&#8203;43189](https://redirect.github.com/spring-projects/spring-boot/issues/43189) - Upgrade to Spring Pulsar 1.2.0 [#&#8203;43024](https://redirect.github.com/spring-projects/spring-boot/issues/43024) - Upgrade to Spring RESTDocs 3.0.3 [#&#8203;43025](https://redirect.github.com/spring-projects/spring-boot/issues/43025) - Upgrade to Spring Security 6.4.1 [#&#8203;43232](https://redirect.github.com/spring-projects/spring-boot/issues/43232) - Upgrade to Spring Session 3.4.0 [#&#8203;43027](https://redirect.github.com/spring-projects/spring-boot/issues/43027) - Upgrade to Testcontainers 1.20.4 [#&#8203;43243](https://redirect.github.com/spring-projects/spring-boot/issues/43243) - Upgrade to Tomcat 10.1.33 [#&#8203;43134](https://redirect.github.com/spring-projects/spring-boot/issues/43134) - Upgrade to Undertow 2.3.18.Final [#&#8203;43166](https://redirect.github.com/spring-projects/spring-boot/issues/43166) - Upgrade to WebJars Locator Lite 1.0.1 [#&#8203;43135](https://redirect.github.com/spring-projects/spring-boot/issues/43135)  ##### :heart: Contributors  Thank you to all the contributors who worked on this release:  [@&#8203;ahoehma](https://redirect.github.com/ahoehma)  [@&#8203;deki](https://redirect.github.com/deki)  [@&#8203;izeye](https://redirect.github.com/izeye)  [@&#8203;ngocnhan-tran1996](https://redirect.github.com/ngocnhan-tran1996)  [@&#8203;nosan](https://redirect.github.com/nosan)  [@&#8203;quaff](https://redirect.github.com/quaff)  and [@&#8203;wickdynex](https://redirect.github.com/wickdynex)  ### [`v3.3.7`](https://redirect.github.com/spring-projects/spring-boot/compare/v3.3.6...v3.3.7)  ### [`v3.3.6`](https://redirect.github.com/spring-projects/spring-boot/releases/tag/v3.3.6)  [Compare Source](https://redirect.github.com/spring-projects/spring-boot/compare/v3.3.5...v3.3.6)  ##### :warning: Noteworthy  - This release upgrades to OpenTelemetry 1.38.0  see [this issue comment](https://redirect.github.com/spring-projects/spring-boot/issues/43200#issuecomment-2486198324) for more details.  ##### :lady_beetle: Bug Fixes  - Spring Boot 3.3.x dependencies do not converge for Micrometer Tracing and OpenTelemetry [#&#8203;43200](https://redirect.github.com/spring-projects/spring-boot/issues/43200) - Cannot package OCI image when 'docker.io/paketobuildpacks/new-relic' is provided as a buildpack [#&#8203;43170](https://redirect.github.com/spring-projects/spring-boot/issues/43170) - WebServerPortFileWriter fails when using a portfile without extension [#&#8203;43116](https://redirect.github.com/spring-projects/spring-boot/issues/43116) - SslOptions.isSpecified() only returns true if ciphers and enabled protocols are set [#&#8203;43083](https://redirect.github.com/spring-projects/spring-boot/issues/43083) - Root cause of errors is hidden when loading images from archive [#&#8203;43069](https://redirect.github.com/spring-projects/spring-boot/issues/43069) - mvn spring-boot:run fails on Windows with "Could Not Find or Load Main Class" when path contains non-ASCII characters [#&#8203;43051](https://redirect.github.com/spring-projects/spring-boot/issues/43051) - Logback logging system does not process URLs with paths not ending in .xml [#&#8203;42989](https://redirect.github.com/spring-projects/spring-boot/issues/42989) - NPE in bootBuildImage when setting DOCKER_CONTEXT=default [#&#8203;42959](https://redirect.github.com/spring-projects/spring-boot/issues/42959) - build-info doesn't support seconds since the epoch from project.build.outputTimestamp [#&#8203;42935](https://redirect.github.com/spring-projects/spring-boot/issues/42935) - NPE in OnClassCondition.resolveOutcomesThreaded following thread interruption because firstHalf is null [#&#8203;42925](https://redirect.github.com/spring-projects/spring-boot/issues/42925) - X-Registry-Auth header sent to Docker Engine API contains field "authHeader" [#&#8203;42914](https://redirect.github.com/spring-projects/spring-boot/issues/42914) - A `@SpyBean` on the output of a FactoryBean is not reset [#&#8203;31204](https://redirect.github.com/spring-projects/spring-boot/issues/31204)  ##### :notebook_with_decorative_cover: Documentation  - Documentation for 'spring.datasource.type' is misleading [#&#8203;43198](https://redirect.github.com/spring-projects/spring-boot/issues/43198) - Update "Upgrading From" section to use "2.x" [#&#8203;43159](https://redirect.github.com/spring-projects/spring-boot/issues/43159) - Include spring-boot-loader in API documentation [#&#8203;43151](https://redirect.github.com/spring-projects/spring-boot/issues/43151) - Document how and where to add custom GraalVM configuration files [#&#8203;43073](https://redirect.github.com/spring-projects/spring-boot/issues/43073) - Rework DataSource configuration examples to separate defining an additional DataSource and defining a DataSource of a different type [#&#8203;43058](https://redirect.github.com/spring-projects/spring-boot/issues/43058) - Location of the layers schema is incorrect in the Maven Plugin's examples [#&#8203;43032](https://redirect.github.com/spring-projects/spring-boot/issues/43032) - Link to Eclipse setup instructions [#&#8203;42953](https://redirect.github.com/spring-projects/spring-boot/issues/42953) - Fix link to Checkpoint and Restore status page [#&#8203;42938](https://redirect.github.com/spring-projects/spring-boot/issues/42938) - Update HttpWebServiceMessageSenderBuilder javadoc [#&#8203;42893](https://redirect.github.com/spring-projects/spring-boot/issues/42893) - Move default value descriptions to "description" in logging property metadata [#&#8203;42881](https://redirect.github.com/spring-projects/spring-boot/issues/42881)  ##### :hammer: Dependency Upgrades  - Upgrade to ActiveMQ 6.1.4 [#&#8203;43146](https://redirect.github.com/spring-projects/spring-boot/issues/43146) - Upgrade to Groovy 4.0.24 [#&#8203;43095](https://redirect.github.com/spring-projects/spring-boot/issues/43095) - Upgrade to Infinispan 15.0.11.Final [#&#8203;43147](https://redirect.github.com/spring-projects/spring-boot/issues/43147) - Upgrade to Jackson Bom 2.17.3 [#&#8203;43036](https://redirect.github.com/spring-projects/spring-boot/issues/43036) - Upgrade to Jetty 12.0.15 [#&#8203;43093](https://redirect.github.com/spring-projects/spring-boot/issues/43093) - Upgrade to jOOQ 3.19.15 [#&#8203;43037](https://redirect.github.com/spring-projects/spring-boot/issues/43037) - Upgrade to Logback 1.5.12 [#&#8203;43038](https://redirect.github.com/spring-projects/spring-boot/issues/43038) - Upgrade to Lombok 1.18.36 [#&#8203;43181](https://redirect.github.com/spring-projects/spring-boot/issues/43181) - Upgrade to Micrometer 1.13.8 [#&#8203;43182](https://redirect.github.com/spring-projects/spring-boot/issues/43182) - Upgrade to Micrometer Tracing 1.3.6 [#&#8203;43000](https://redirect.github.com/spring-projects/spring-boot/issues/43000) - Upgrade to Netty 4.1.115.Final [#&#8203;43148](https://redirect.github.com/spring-projects/spring-boot/issues/43148) - Upgrade to Pulsar Reactive 0.5.9 [#&#8203;43183](https://redirect.github.com/spring-projects/spring-boot/issues/43183) - Upgrade to Reactor Bom 2023.0.12 [#&#8203;43002](https://redirect.github.com/spring-projects/spring-boot/issues/43002) - Upgrade to Spring AMQP 3.1.8 [#&#8203;43004](https://redirect.github.com/spring-projects/spring-boot/issues/43004) - Upgrade to Spring Data Bom 2024.0.6 [#&#8203;43006](https://redirect.github.com/spring-projects/spring-boot/issues/43006) - Upgrade to Spring Framework 6.1.15 [#&#8203;43008](https://redirect.github.com/spring-projects/spring-boot/issues/43008) - Upgrade to Spring Integration 6.3.6 [#&#8203;43010](https://redirect.github.com/spring-projects/spring-boot/issues/43010) - Upgrade to Spring Kafka 3.2.5 [#&#8203;43011](https://redirect.github.com/spring-projects/spring-boot/issues/43011) - Upgrade to Spring LDAP 3.2.8 [#&#8203;43184](https://redirect.github.com/spring-projects/spring-boot/issues/43184) - Upgrade to Spring Pulsar 1.1.6 [#&#8203;43012](https://redirect.github.com/spring-projects/spring-boot/issues/43012) - Upgrade to Spring RESTDocs 3.0.3 [#&#8203;43014](https://redirect.github.com/spring-projects/spring-boot/issues/43014) - Upgrade to Spring Security 6.3.5 [#&#8203;43013](https://redirect.github.com/spring-projects/spring-boot/issues/43013) - Upgrade to Tomcat 10.1.33 [#&#8203;43149](https://redirect.github.com/spring-projects/spring-boot/issues/43149)  ##### :heart: Contributors  Thank you to all the contributors who worked on this release:  [@&#8203;ahoehma](https://redirect.github.com/ahoehma)  [@&#8203;izeye](https://redirect.github.com/izeye)  [@&#8203;ngocnhan-tran1996](https://redirect.github.com/ngocnhan-tran1996)  [@&#8203;nosan](https://redirect.github.com/nosan)  [@&#8203;quaff](https://redirect.github.com/quaff)  and [@&#8203;wickdynex](https://redirect.github.com/wickdynex)  </details>  ---  ### Configuration  📅 **Schedule**: Branch creation - At any time (no schedule defined)  Automerge - At any time (no schedule defined).  🚦 **Automerge**: Enabled.  ♻ **Rebasing**: Whenever PR is behind base branch  or you tick the rebase/retry checkbox.  🔕 **Ignore**: Close this PR and you won't be reminded about these updates again.  ---  - [ ] <!-- rebase-check -->If you want to rebase/retry this PR  check this box  ---  This PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/camunda/camunda).  <!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzOS4xOS4wIiwidXBkYXRlZEluVmVyIjoiMzkuNzIuNSIsInRhcmdldEJyYW5jaCI6Im1haW4iLCJsYWJlbHMiOlsiYXV0b21lcmdlIl19-->
camunda,camunda,e1ee5d211fc4ae51f34d0fbf1f77b3d8a64fdaec,https://github.com/camunda/camunda/commit/e1ee5d211fc4ae51f34d0fbf1f77b3d8a64fdaec,Introduce a Map with 3 keys (Map3D) as a generic collection (#27759)  ## Description  As part of migrating the metrics to Micrometer  we have cases where we want to access individual meters by their tag combinations. With normal usage  this requires us to perform some allocation - either the `Meter.Id` in order to pick it up  or some option like a `Tuple` or `Tuple3` or `Triple`. @entangled90 and I benchmarked these approaches  and the one without allocations (using a map with 3 keys) is  as expected  much faster. Some quick JMH results:  When using strings for keys:  ``` Benchmark                                                          Mode    Cnt        Score    Error   Units MetricCachingTest.measureTimeToIncrementTuples                     thrpt   50       17.868 ±  0.531   ops/s MetricCachingTest.measureTimeToIncrementTuples:gc.alloc.rate       thrpt   50      51.116 ±  3.121  MB/sec MetricCachingTest.measureTimeToIncrementTuples:gc.alloc.rate.norm  thrpt   50  3000382.667 ± 21.107    B/op  Benchmark                                                         Mode    Cnt    Score   Error   Units MetricCachingTest.measureTimeToIncrementMap3D                     thrpt   50  27.923 ± 1.302   ops/s MetricCachingTest.measureTimeToIncrementMap3D:gc.alloc.rate       thrpt   50    0.006 ± 0.001  MB/sec MetricCachingTest.measureTimeToIncrementMap3D:gc.alloc.rate.norm  thrpt   50   237.517 ± 5.155    B/op ```  When using enum for keys:  ``` Benchmark                                                          Mode    Cnt        Score    Error   Units MetricCachingTest.measureTimeToIncrementTuples                     thrpt   50       50.179 ±  0.710   ops/s MetricCachingTest.measureTimeToIncrementTuples:gc.alloc.rate       thrpt   50      143.543 ±  2.031  MB/sec MetricCachingTest.measureTimeToIncrementTuples:gc.alloc.rate.norm  thrpt   50  3000142.014 ± 21.107    B/op MetricCachingTest.measureTimeToIncrementTuples:gc.count            thrpt   50        3.000           counts MetricCachingTest.measureTimeToIncrementTuples:gc.time             thrpt   50        4.000               ms  Benchmark                                                         Mode    Cnt    Score   Error   Units MetricCachingTest.measureTimeToIncrementMap3D                     thrpt   50  125.315 ± 1.302   ops/s MetricCachingTest.measureTimeToIncrementMap3D:gc.alloc.rate       thrpt   50    0.007 ± 0.001  MB/sec MetricCachingTest.measureTimeToIncrementMap3D:gc.alloc.rate.norm  thrpt   50   56.932 ± 7.755    B/op ```  > [!Note] > To keep things fair  we refactored our tuples to be records in the hope that both immutable values and using records would nudge the JVM to avoid allocations.  > [!Note] > The actual benchmark will be added in a comment below.  Since metrics are in the hot path quite often  I think it's worth ensuring we're keeping things fast/light  and avoiding allocations is one of the best way to do this.  Also interesting is you will note that the benchmarks where we use enums for keys are much  much faster  which can be attributed (though we did not dig into it) to having the comparison/hash code being much faster.  ## MArray/Map3D/Table  The PR introduces a new generic multi-dimensional array structure which is variadic. It's not meant to be used directly all the time  but you can put it behind an interface to enforce some types and leverage the compiler.  You can see how `Map3D` and `Table` both use it under the hood for their enum counter parts. It's a little more verbose  I'll admit  which I'm not too happy about.  ## Related issues  related to #26078
camunda,camunda,903872dbe3cac85e4088f144649e8873202dda2d,https://github.com/camunda/camunda/commit/903872dbe3cac85e4088f144649e8873202dda2d,Ensure `withAuthentication()` is called before doing a service call. (#27470)  ## Description  <!-- Describe the goal and purpose of this PR. -->  This ArchUnit test verifies that every call to a Service that is performed by a Controller adds the authentication by calling withAuthentication first. The test will fail if this is not the case.  This is essential  as not making this call could result in cases where forget to add it  or accidentally remove it (for example during a merge conflict). As a result the endpoint would not verify a user's authorization anymore. Potentially causing data-leaks.  It is important to note that this method assumes that the service field is only called once  and chained afterward. If there is multiple separate calls to the service field it will not be accurate and should be modified! Currently  we do not have such use-cases. I tried to make it all-encompassing but the code became too complex and it was not worth it imo.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  N/A
camunda,camunda,832a83eb1151f3b67ecdffa2de5b5248d566b2d8,https://github.com/camunda/camunda/commit/832a83eb1151f3b67ecdffa2de5b5248d566b2d8,fix: Improve user task assignment handling and `changedAttributes` population (#26877)  ## Description PR description:  This PR addresses the following issues related to user task assignment handling and ensures consistent updates to the `changedAttributes` property:  - **Fixed Issue 1**: Correcting the `assignee` to an empty string (`""`) now properly clears the `assignee` in Tasklist instead of assigning an empty string. - **Fixed Issue 2**: Resolved the issue where the `assignee` was shown multiple times as a corrected attribute in Operate after unassigning a user task.  ### Key Changes: 1. Updated Zeebe code to include the `assignee` as an entry in the `changedAttributes` property after performing **assign**  **unassign**  or **claim** operations. This behavior is consistent regardless of whether user task listeners are configured or not. 2. Modified `camunda-exporter/UserTaskHandler.java` to rely on `record.getValue().getChangedAttributes()` when processing the `ASSIGNED` event for user tasks  and properly set the value for `entity.assignee` property as `null` instead of empty string `""`. This ensures downstream applications like Operate and Tasklist receive consistent and accurate data.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  closes #26614 closes #26791
camunda,camunda,992afbf741f25b2d04d2ac1dc0254516a32b8add#diff-c871546cf084ee89e36386017b5ee7350014b13f14df4f466cbf867ba35ee449)):,mode](https://github.com/camunda/camunda/commit/992afbf741f25b2d04d2ac1dc0254516a32b8add#diff-c871546cf084ee89e36386017b5ee7350014b13f14df4f466cbf867ba35ee449)):,fix: ignore MIGRATED variable records (#26914)  ## Description  1. Variable MIGRATED Zeebe record does not provide the value  therefore we need to fully skip it when persisting in `list-view` index. More context: values store in `list-view` index are used to perform filtering by variable name and value in Processes tab. 2. I additionally reworked how the new variables are persisted (basically I brought back the code that was there[ before introducing and removing again of concurrency
camunda,camunda,52429c0ad2a889f96db3b3e42cd8cf8e238474c1,https://github.com/camunda/camunda/commit/52429c0ad2a889f96db3b3e42cd8cf8e238474c1,upsert query. This should improve performance.  ## Related issues  closes #26490
camunda,camunda,029314e6e45a8d451ed68996ee8d15478fdfff78,https://github.com/camunda/camunda/commit/029314e6e45a8d451ed68996ee8d15478fdfff78,fix: use username instead of key in user export handlers (#26863)  ## Description  <!-- Describe the goal and purpose of this PR. -->  This PR updates the exporters (both OS/ES and RDBMS) to use the username instead of the key as the ID of the user object during exports.  I've tested this locally and observed the exporters successfully exporting the ID of the record as the username  I was also able to successfully perform updates and deletes.  ## Related issues  closes https://github.com/camunda/camunda/issues/26857 https://github.com/camunda/camunda/issues/26858 https://github.com/camunda/camunda/issues/26859
camunda,camunda,3d34648460a533b30288c9142b49a0d2be15201a,https://github.com/camunda/camunda/commit/3d34648460a533b30288c9142b49a0d2be15201a,fix: improve performance of range queries (#26647)  ## Description  Created one range query for range operations for all applicable data types.  ## Related issues  closes https://github.com/camunda/camunda/issues/24195
camunda,camunda,66117bde8fe951f552f516c40ca01d89e38d3ddc,https://github.com/camunda/camunda/commit/66117bde8fe951f552f516c40ca01d89e38d3ddc,fix: improve performance of range queries
camunda,camunda,30e5376b780de4a46710538e5dd9f3e20f02446f,https://github.com/camunda/camunda/commit/30e5376b780de4a46710538e5dd9f3e20f02446f,feat: perform tenant checks on groups  With this commit we include the tenant ids of groups in the tenant checks. Tenants can be assigned to groups  and if a user is assigned to the group all the tenants of this group are indirectly assigned to the user.
camunda,camunda,a5aa922c6404e8444fb02a1b8e6dc6e3729237ba,,https://github.com/camunda/camunda/commit/a5aa922c6404e8444fb02a1b8e6dc6e3729237ba,,test: fix tasklist docker tests (#26414)  ## Description  <!-- Describe the goal and purpose of this PR. --> Since
camunda,camunda,0532a68c98d419440645fd649ecbedbe27cc2ac0,https://github.com/camunda/camunda/commit/0532a68c98d419440645fd649ecbedbe27cc2ac0,Describe the goal and purpose of this PR. --> Since Tasklist container in the test does not start with this error (it is missing the `CAMUNDA_DATABASE_URL` parameter) ``` 2024-12-24 15:17:58.470 [] [main] [] ERROR io.camunda.application - Failed to start application with message: Failed to execute ApplicationRunner java.lang.IllegalStateException: Failed to execute ApplicationRunner at org.springframework.boot.SpringApplication.lambda$callRunner$6(SpringApplication.java:797) ~[spring-boot-3.3.7.jar:3.3.7] at org.springframework.util.function.ThrowingConsumer.accept(ThrowingConsumer.java:66) ~[spring-core-6.1.14.jar:6.1.14] at org.springframework.util.function.ThrowingConsumer$1.accept(ThrowingConsumer.java:88) ~[spring-core-6.1.14.jar:6.1.14] at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:798) ~[spring-boot-3.3.7.jar:3.3.7] at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:786) ~[spring-boot-3.3.7.jar:3.3.7] at org.springframework.boot.SpringApplication.lambda$callRunners$3(SpringApplication.java:774) ~[spring-boot-3.3.7.jar:3.3.7] at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) ~[?:?] at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357) ~[?:?] at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510) ~[?:?] at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) ~[?:?] at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) ~[?:?] at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) ~[?:?] at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:?] at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) ~[?:?] at org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:774) [spring-boot-3.3.7.jar:3.3.7] at org.springframework.boot.SpringApplication.run(SpringApplication.java:342) [spring-boot-3.3.7.jar:3.3.7] at io.camunda.application.StandaloneTasklist.main(StandaloneTasklist.java:48) [camunda-zeebe-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] Caused by: java.util.concurrent.ExecutionException: io.camunda.migration.api.MigrationException: Failed to fetch last migrated process at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:?] at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:?] at io.camunda.application.commons.migration.MigrationsRunner.run(MigrationsRunner.java:41) ~[camunda-zeebe-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] at org.springframework.boot.SpringApplication.lambda$callRunner$4(SpringApplication.java:786) ~[spring-boot-3.3.7.jar:3.3.7] at org.springframework.util.function.ThrowingConsumer$1.acceptWithException(ThrowingConsumer.java:83) ~[spring-core-6.1.14.jar:6.1.14] at org.springframework.util.function.ThrowingConsumer.accept(ThrowingConsumer.java:60) ~[spring-core-6.1.14.jar:6.1.14] ... 15 more Caused by: io.camunda.migration.api.MigrationException: Failed to fetch last migrated process at io.camunda.migration.process.MigrationRunner.call(MigrationRunner.java:86) ~[process-migration-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] at io.camunda.migration.process.MigrationRunner.call(MigrationRunner.java:36) ~[process-migration-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?] at java.base/java.lang.Thread.run(Thread.java:1583) ~[?:?] Caused by: io.camunda.migration.api.MigrationException: Failed to fetch last migrated process at io.camunda.migration.process.adapter.es.ElasticsearchAdapter.readLastMigratedEntity(ElasticsearchAdapter.java:144) ~[process-migration-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] at io.camunda.migration.process.MigrationRunner.call(MigrationRunner.java:66) ~[process-migration-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] at io.camunda.migration.process.MigrationRunner.call(MigrationRunner.java:36) ~[process-migration-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?] at java.base/java.lang.Thread.run(Thread.java:1583) ~[?:?] Caused by: java.net.ConnectException: Connection refused at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:934) ~[elasticsearch-rest-client-8.13.4.jar:8.13.4] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:304) ~[elasticsearch-rest-client-8.13.4.jar:8.13.4] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:292) ~[elasticsearch-rest-client-8.13.4.jar:8.13.4] at co.elastic.clients.transport.rest_client.RestClientHttpClient.performRequest(RestClientHttpClient.java:91) ~[elasticsearch-java-8.13.4.jar:?] at co.elastic.clients.transport.ElasticsearchTransportBase.performRequest(ElasticsearchTransportBase.java:144) ~[elasticsearch-java-8.13.4.jar:?] at co.elastic.clients.elasticsearch.ElasticsearchClient.search(ElasticsearchClient.java:1923) ~[elasticsearch-java-8.13.4.jar:?] at io.camunda.migration.process.adapter.es.ElasticsearchAdapter.lambda$readLastMigratedEntity$15(ElasticsearchAdapter.java:141) ~[process-migration-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] at io.github.resilience4j.retry.Retry.lambda$decorateCallable$5(Retry.java:237) ~[resilience4j-retry-2.2.0.jar:2.2.0] at io.camunda.migration.process.util.AdapterRetryDecorator.decorate(AdapterRetryDecorator.java:40) ~[process-migration-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] at io.camunda.migration.process.adapter.es.ElasticsearchAdapter.readLastMigratedEntity(ElasticsearchAdapter.java:139) ~[process-migration-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] at io.camunda.migration.process.MigrationRunner.call(MigrationRunner.java:66) ~[process-migration-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] at io.camunda.migration.process.MigrationRunner.call(MigrationRunner.java:36) ~[process-migration-8.7.0-SNAPSHOT.jar:8.7.0-SNAPSHOT] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?] at java.base/java.lang.Thread.run(Thread.java:1583) ~[?:?] Caused by: java.net.ConnectException: Connection refused at java.base/sun.nio.ch.Net.pollConnect(Native Method) ~[?:?] at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682) ~[?:?] at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:973) ~[?:?] at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvent(DefaultConnectingIOReactor.java:174) ~[httpcore-nio-4.4.16.jar:4.4.16] at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:148) ~[httpcore-nio-4.4.16.jar:4.4.16] at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351) ~[httpcore-nio-4.4.16.jar:4.4.16] at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221) ~[httpasyncclient-4.1.5.jar:4.1.5] at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64) ~[httpasyncclient-4.1.5.jar:4.1.5] at java.base/java.lang.Thread.run(Thread.java:1583) ~[?:?] ```  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  closes #
camunda,camunda,2e7a9558d40266b49e380f8c9d1eb0d6cdb568d2,https://github.com/camunda/camunda/commit/2e7a9558d40266b49e380f8c9d1eb0d6cdb568d2,feat: msgpack array value that uses delta encoding to save space (#25459)  This uses a simple delta-encoding optimization for arrays of longs that saves space if the array values are relatively close to each other and the difference between one value and the next can be encoded as a smaller MsgPack integer type than the value itself.  For example  imagine we need to encode an array of three keys from the same partition. Using the normal `ArrayValue<LongValue>` to encode these  we'd expect an encoding size of 3*64 bits + type marker and array header overhead. If the keys are relatively close to each other  using the new `DeltaEncodedLongArrayValue` is more efficient because the encoding only requires 64 bits for the first value + 2 * 8 bits + type marker and array header overhead  assuming that the deltas fit in 8 bit.  This implementation is not perfectly efficient because we don't impose any restrictions on the usage of `DeltaEncodedLongArrayValue`. If we would require all deltas to be encodable with a certain integer type  we could save the type marker overhead for example. Instead we choose flexibility and a simple implementation that always works and _sometimes_ saves space.  The `writeJson` method does not use delta encoding. This is to keep this implementation mostly internal and not leak it in places where human readability and easy parsing is preferable over minuscule space savings.  This is a classic space-time tradeoff because calculating or reversing the delta encoding takes some additional instructions  1 extra addition or subtraction and one array look-behind per array element.  The values stored in memory are _not_ delta encoded and thus don't benefit from the space saving. However  just by specializing for `long` directly instead of using the generic `ArrayValue<LongValue>`  we also save some memory overhead  for example object headers and indirection through references.  Randomized property tests were added to ensure that the new type can encode and decode any arbitrary array losslessly. Some additional sanity checks were added by testing the expected JSON encoding and encoded size.
camunda,camunda,5dcd9da63a76839862c0ae627e62c540889c6fb2,https://github.com/camunda/camunda/commit/5dcd9da63a76839862c0ae627e62c540889c6fb2,refactor: write migrated event for sequence flow  We decided to write `ELEMENT_MIGRATED` event for the sequence flow instead of the joining gateway. A joining gateway does not have an instance in the state until all incoming sequence flows are taken. Sequence flows do not have an instance in the state. But  on `SEQUENCE_FLOW_TAKEN` event  all data related to the sequence flow is created. Additionally  the joining gateway will not have a key yet while we are migrating the instance. Additionally  in the applier  we are actually updating the same column family as we do while applying `SEQUENCE_FLOW_TAKEN` event.  In short  while migrating joining gateways  sequence flow data exist in the state but the joining gateway instance is not. Therefore  we chose to write migrated event for sequence flows.  refactor: add integrity check for active sequence flow ids  There is a warning on the `getActiveSequenceFlowIds()` method about usage: Warning  this method should not be used for process instances created before 8.6. It may provide incorrect information for such process instances.  To make sure we have complete active sequence flow ids  we need to make sure active sequence flow ids count matches with the active sequence flow count that exists before 8.6. Additionally  `size()` method is added to `ArrayProperty` to improve the performance of the count query.  refactor: use copyFrom  refactor: use record for sequence flow instead of Map.Entry  refactor: group related methods  refactor: simplify test  Revert "refactor: add integrity check for active sequence flow ids"  This reverts commit 5d476e421fafd320b28d9449c3aa80d9b9d16c10.  refactor: use visitTakenSequenceFlows to retrieve sequence flow ids  Previously we were using getSequenceFlowIds over the elementInstance but the result of that method could be invalid for process instances created before 8.6. Instead  we can use your newly created method ElementInstanceState.visitTakenSequenceFlows to visit them. This provides all the needed info reliably as it is build around the numberOfTakenSequenceFlowsColumnFamily which has been the state tracking this for the joining parallel gateway since "forever". This would also mean we don't need a safety check.  Note: `getSequenceFlowsToMigrate()` method will be simplified along with validations PR.  refactor: use correct taken sequence flows count  To detect if there are any gateways with taken sequence flows  we need to use the count on the column family `numberOfTakenSequenceFlowsColumnFamily`.
camunda,camunda,2de91b9de2ed46dd62340192d3f799d4272b05a7,https://github.com/camunda/camunda/commit/2de91b9de2ed46dd62340192d3f799d4272b05a7,Wrap the GroupRecord instead of making a full copy before insertion (#25898)  ## Description  <!-- Describe the goal and purpose of this PR. -->  Wrapping is more performant than copying. In this place we don't need a full copy as we immediately insert it in our ColumnFamily.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  relates to https://github.com/camunda/camunda/pull/25635#pullrequestreview-2481712329
camunda,camunda,d70f6ed15f66b5cdcffacb842c355fe75be345b4,https://github.com/camunda/camunda/commit/d70f6ed15f66b5cdcffacb842c355fe75be345b4,perf: wrap the GroupRecord instead of making a full copy  Wrapping is more performant than copying. In this place we don't need a full copy as we immediately insert it in our ColumnFamily.
camunda,camunda,72a982e304567f8834b7b4f2408de612cf637a8f,https://github.com/camunda/camunda/commit/72a982e304567f8834b7b4f2408de612cf637a8f,refactor: authorization checks in Tasklist (#25799)  ## Description  * Replaces the `hasPermission()` checks with the new authorization concept  i.e.  checking if a user/client is allowed to complete  assign  and unassign a task and creating an instance. * Implements two modes: * New Architecture: Instead of communicating with the Gateway  it directly executes broker requests. In that case  the engine will perform the authorization checks. * Old Architecture (compatibility): This mode ensures that the old backend makes requests to the Gateway to ensure a running dev environment and old test infrastructure. However  the old backend does the authorization checks using the `AuthorizationChecker`. * Provides integration tests.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  related to #24665
camunda,camunda,86e1b362dca9b41a6f7cceaa7f44668abbcaa649,https://github.com/camunda/camunda/commit/86e1b362dca9b41a6f7cceaa7f44668abbcaa649,feat: provide access to `action` and `priority` user task properties in task listeners (#25955)  ## Description  Task listener jobs currently provide headers for various user task properties (e.g.  `assignee`  `dueDate`  `candidateUsers`). However  they lacked the **`priority`** and **`action`** properties  which limited the ability of task listeners to make decisions or perform specific actions based on these properties.  This PR introduces support for including the **`priority`** and **`action`** properties in task listener job headers. By adding these properties  task listeners now have richer context  enabling better handling and decision-making during task lifecycle processing.  > [!NOTE] Additionally  this PR introduces a dedicated `Protocol.USER_TASK_KEY_HEADER_NAME` constant for the **`userTaskKey`** property. Previously  the `userTaskKey` property was added directly using a string expression. Defining it as a constant improves consistency  aligning it with how other headers are defined.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  closes #24102
camunda,camunda,e4654ed0bc1aa9b984f2ca33226ed1d2b803951d,https://github.com/camunda/camunda/commit/e4654ed0bc1aa9b984f2ca33226ed1d2b803951d,fix: create Operate message template/index in exporter (#26200)  ## Description  We have `operate-message` index which was introduce within https://github.com/camunda/operate/issues/5873 and is not used currently. We decided to keep it in index structure  as it existed in older versions and if we decide to not have it anymore  we will need to perform some steps to remove it in older versions.  This PR adds it to the list of other indices managed by Camunda exporter.
camunda,camunda,22ce434d1937e185ae3bf211be356079090af4d2,https://github.com/camunda/camunda/commit/22ce434d1937e185ae3bf211be356079090af4d2,fix: bypass precondition checks for retried user task commands  When retrying a user task command after resolving an incident caused by task listener expression evaluation failures  the `UserTaskCommandPreconditionChecker` now bypasses authorization and state checks if the command lacks request metadata.  This change ensures that for internally retried commands (indicated by missing request metadata)  only the existence of the user task record is verified. Since these checks were already performed during the initial command processing  redoing them is redundant.
camunda,camunda,e5c72b9e8abf0e6f54e18e123937e28dc7b8dab7,https://github.com/camunda/camunda/commit/e5c72b9e8abf0e6f54e18e123937e28dc7b8dab7,Use DocumentBasedSearchClient for dynamic index discovery (#25730)  ## Description In this PR a new method to retrieve index by alias has been added to DocumentBasedSearchClient.  Classes used by Optimize to detect "dynamic" indices have been migrated to using the new client and this new method. Because these classes only need `DocumentBasedSearchClient` they can now be easily instantiated by all webapps  making it possible to perform backups from any webapp.  The backup integration will be done in a followup PR  <!-- Describe the goal and purpose of this PR. -->  ## Related issues  relates  #25597
camunda,camunda,4e9783bc988f9f811094eaff77d208bb939511e1,https://github.com/camunda/camunda/commit/4e9783bc988f9f811094eaff77d208bb939511e1,fix: allow SchemaManager to update current template index mappings (#25978)  ## Description  <!-- Describe the goal and purpose of this PR. --> SchemaManager did not update the running index mappings when performing a Template update. Since all of our changes are new fields and not updates we can perform the update of the current index of a template without needing a reindexing.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  closes #
camunda,camunda,768a1980461f2d0edc6e90220be55ad5dea4de2f,https://github.com/camunda/camunda/commit/768a1980461f2d0edc6e90220be55ad5dea4de2f,feat: add all configured users to the IdentitySetupRecord  Instead of only adding a default user we now iterate over all the users in the configuration and add them all to the IdentitySetupRecord.  Note that I removed the duplication checks. This became too complex to do here  and these checks are duplicated anyway  as the processor of the command we write here also performs them.
camunda,camunda,07b8a31dfa440b8b4d118d8902aac486801d6058,https://github.com/camunda/camunda/commit/07b8a31dfa440b8b4d118d8902aac486801d6058,feat: add tenant checks in the AuthorizationCheckBehavior  Verify that the user is authorizated to perform the command for the provided tenant  if not the command will be rejected.
camunda,camunda,97bce18840217d6c297f78cdd32825dac3b65569,https://github.com/camunda/camunda/commit/97bce18840217d6c297f78cdd32825dac3b65569,feat: verify permissions of groups (#25634)  Includes the user's group authorizations when checking for permissions. Tests in `AuthorizationCheckBehaviorTest` are extended.  I've also included a minor performance improvement and a small refactoring to prefer functional composition but feel free to oppose them  they are optional and can be dropped.  Closes #24403
camunda,camunda,fd2275c9473f3224410c85246c7e27495166b88a,https://github.com/camunda/camunda/commit/fd2275c9473f3224410c85246c7e27495166b88a,Assign priority of indices based on new requirements (#25617)  ## Description Recap the changes that has been done:  Name | Indexes | Priority -- | -- | -- camunda_8.6.0-snapshot_part_1_of_7 | Operate-import-position-8.3.0_  tasklist-import-position-8.2.0_optimize-position-based-import-index_v3 | 1 camunda_8.6.0-snapshot_part_2_of_7 | Operate-list-view-8.3.0_  tasklist-task-8.5.0_ | 2 camunda_8.6.0-snapshot_part_3_of_7 | operate-list-view-8.3.0_*  -operate-list-view-8.3.0_tasklist-task-8.5.0_*  -tasklist-task-8.5.0_ | 2 w/ matchers camunda_8.6.0-snapshot_part_4_of_7 | operate-batch-operation-1.0.0_   operate-operation-8.4.1_ | 3 camunda_8.6.0-snapshot_part_5_of_7 | operate-decision-8.3.0_operate-decision-instance-8.3.0_operate-event-8.3.0_operate-flownode-instance-8.3.1_operate-incident-8.3.1_ operate-message-8.5.0_operate-post-importer-queue-8.3.0_Operate-sequence-flow-8.3.0_operate-user-task-8.5.0_operate-variable-8.3.0_  tasklist-draft-task-variable-8.3.0_*  -tasklist-draft-task-variable-8.3.0_  tasklist-task-variable-8.3.0_*  -tasklist-task-variable-8.3.0_ | 4 & 4 w/ matchers camunda_8.6.0-snapshot_part_6_of_7 | Operate-migration-steps-repository-1.1.0_ operate-web-session-1.1.0_ Operate-user-1.2.0_operate-decision-instance-8.3.0_operate-decision-requirements-8.3.0_operate-metric-8.3.0_operate-process-8.3.0_ tasklist-process-8.4.0_Tasklist-form-8.4.0  Identity-Authorizations  Identity-Users  Identity-Web-Session  Identity-Tenants  Identity-Roles  Identity-Groups  Identity-Mapping | 5 camunda_8.6.0-snapshot_part_7_of_7 | optimize-process-instance-process_05rl6n5_v8  optimize-single-decision-report_v10  optimize-process-overview_v2  optimize-instant-dashboard_v1  optimize-timestamp-based-import-index_v5  optimize-business-key_v2optimize-single-process-report_v11  optimize-combined-report_v5  optimize-dashboard_v8  optimize-dashboard-share_v4  optimize-variable-update-instance_v2-000001  optimize-variable-label_v1  optimize-process-instance-process_1jkjnqs_v8  optimize-terminated-user-session_v3  optimize-settings_v3  optimize-collection_v5  optimize-process-instance-customer_onboarding_en_v8  optimize-report-share_v3  optimize-decision-definition_v5  optimize-tenant_v3  optimize-metadata_v3  optimize-external-process-variable_v2-000001  optimize-process-definition_v6  optimize-alert_v4  optimize-process-instance-variables-test_v8 | 6 (or 5  as optimize can be done in parallel?)  Changes to be performed: - create a new BackupPriority: - `operate-batch-operation-1.0.0_   operate-operation-8.4.1_ ` will have priority 3 - all prio3 indices will have prio4 - all prio4 indices will have prio5 - Operate ImportIndex had prio3  now has prio1 - Tasklist TaskTemplate had prio3  now has prio2 - Operate DecisionIndex was done in the last group (prio4)  now it will be done in prio4  but with all the others that were previously prio3 - Identity indices have been added with prio5  >[!WARNING] > Optimize indices are not added to the BackupService yet  ## Checklist  ## Related issues  closes #24464
camunda,camunda,b03c09a48fe3a8908e2371f042f2dbca9dc695f6,https://github.com/camunda/camunda/commit/b03c09a48fe3a8908e2371f042f2dbca9dc695f6,perf: avoid collecting authorizations in temporary sets  When checking for permissions  authorizations were collected to a set just to be turned into a stream again. We can avoid the intermediate set and just work on a stream of authorizations directly.
camunda,camunda,4ffc1e7643127972c0f368ad1c59ae8f8fc6de78,https://github.com/camunda/camunda/commit/4ffc1e7643127972c0f368ad1c59ae8f8fc6de78,fix: update user task when formId is not set during importing (#25181)  ## Description  <!-- Describe the goal and purpose of this PR. --> Fix the issue that occurs when a Form is not present/indexed yet when a User Task is created.  When the UserTask is queried check if it's `formId` field is not set and if other conditions apply and perform an update on the UserTask setting the `formId`.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  closes #24771
camunda,camunda,0eb1396a693a99bb8358cdcc80175eadf1b14fb0,https://github.com/camunda/camunda/commit/0eb1396a693a99bb8358cdcc80175eadf1b14fb0,refactor: make tests easily extensible to other DBs (#25151)  ## Description  Refactors the `ElasticsearchIncidentUpdateRepositoryIT` into a single `IncidentUpdateRepositoryIT` with per-store sub classes  which will later let us extend it to OS very easily.  I know there are different ways of doing this:  1. `TestTemplate`. This is what's used for the other exporter tests. I opted against it here specifically for a few reasons. You can't compose extensions easily in Junit  so when you end up using a custom `TestTemplateInvocationContextProvider` extension  you can't reuse `Testcontainers` or `AutoCloseResources` or any other extension  which means you end up having to manually manage the lifecycle of other things yourself. You also can't really say this test should _only_ be for this context (e.g. a only OS test). Behavior of the lifecycle hooks and methods are not as obvious with `TestTemplate` either. 2. `ParameterizedTest` with specific context passed as argument. The main downside is you don't have a split between set up/tear down and the test execution  or not as nice anyway. You also can't easily reuse the normal lifecycle hooks  or any other extension points.  Using interfaces (or abstract class if you need to share state) is a slightly easier way  as the main class behaves just as a normal test class  but the sub-classes will let you parameterize it. It also lets you define DB specific tests in the sub class if you need it. And you can reuse and make use of any extension as you normally would. It's of course not perfect  but I think it's good compromise.  ## Related issues  related to #24930
camunda,camunda,6bf75506f8cb4e7dca0e41ccb67e1a2880323547,https://github.com/camunda/camunda/commit/6bf75506f8cb4e7dca0e41ccb67e1a2880323547,build: replace Jetty with Tomcat in Optimize (#24433)  ## Description  Currently  the Optimize backend is running on the web server Jetty. In order to facilitate the move towards the single-jar and single-application projects  we need to replace Jetty with Tomcat  because: * every application is using Tomcat * the only way to make Tomcat and Jetty coexist in a multi-module application would be to use ad-hoc profile to mutually exclude the server that is not in use  and this is not what we want  This PR removes Jetty and its dependencies and adapts the configuration to work using Tomcat instead.  **Important Notes:**  * custom error handler removed and not replaced with anything: I tried to understand what cases its code would solve but I couldn't find any. After consulting with Josh  I decided to remove it  as it doesn't seem to impact anything. If I am wrong about it  please feel free to let me know and it will be quick to reintroduce it.  * there are some slight differences between Jetty and Tomcat related to the order some things are computed. Because of some small differences in behavior  we are installing the filters on the pattern `/*` and then we are checking whether or not we apply the filter  directly within the filter  * the pipeline `e2e-cloud-test` has been disabled: it's correct functioning is impacted by Tomcat  as the new webserver doesn't allow for the auth cookie to be passed from our oauth server (https) to the local instance of the application (http) that is used to run the tests. A new issue has been created to fix this problem eventually.  # Testing  Before merging this PR  an extensive QA/testing should be performed  to make sure the application works as expected  identically to when Jetty was the web server. Some of the important things we want to make sure are the following: * regression tests on the UI * migrations shall work as expected * static files are served with the /external prefix  with the cluster ID and with the contextPath as expected * external APIs shall work as expected * with /external/api/... URLs * with /api/external/URLs * non-external  publicly accessible APIs shall work as expected * the sharing functionality shall work as expected * the user-facing server customizations shall work as expected * **(done @buccarel)** the Gzip compression shall work as expected * **(done @buccarel)** HTTP2 works as expected * the application works as expected when clusterId is present (cloud environment) * all of the things mentioned above shall work as expected even with a custom context path  as well as in SaaS  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  * closes https://github.com/camunda/camunda/issues/24278 * part of https://github.com/camunda/product-hub/issues/2289
camunda,camunda,d4324810141ad5596f0d54d1bebe10eb3cd6ad91,https://github.com/camunda/camunda/commit/d4324810141ad5596f0d54d1bebe10eb3cd6ad91,Support multi handler flush (#24859)  ## Description ### Context  > Current state: > - In the ExporterBatchWriter we pass records to all handlers that accept the value type and want to handle the given record > - The results of each handler are stored in a cache (with the corresponding handler) > - Results are identified by id and entity type > - On flush  we call for each cached entity the corresponding handler to flush > > **Expectations (previous):** > > - The export handlers have a one-to-one mapping to an entity. > - This means per entity we have one specific handler  and vice-versa > - It could happen that the same handler is called multiple times during a batch because we see multiple events related to the same entity (ACTIVIATED-COMPLETED  etc.) > > Problem: > > - We have implemented multiple handlers that reuse entity types  even if they set only a subset of properties. > - Consequence: > - As they use the same type  and in some cases even generate the same id only the last handler wins. Flush is only called on the last one  which consumed the record. >     - Only partial data is updated. > - On creation (missing document) this is not an issue because we use mostly upserts  which uses the complete entity object. Here all data is written. > > https://github.com/camunda/camunda/issues/24736#issuecomment-2485444919  We were approaching this by caching all handlers related to the entity.  > [!Important] > > We are aware that this will cause more upserts per entity  and this might have a potential performance impact. But  we have no actual data to prove that this is for sure the case. > > - As we want to fix this critical bug ASAP we **[accept](https://camunda.slack.com/archives/C06F0GLJNFM/p1732020688807309?thread_ts=1731426489.455309&cid=C06F0GLJNFM)** this risk. > - If we see performance issues/regressions we can reconsider the solution and solve it differently. > - As an additional task  we will extend our metric support  ### In this PR  * Added new tests (first failing) that verify multiple registered handlers on the BatchWriter are called and flushing entities * Store more handlers per entity  before we only stored one * Call all handlers for an entity to flush all related data  <!-- Describe the goal and purpose of this PR. -->  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  For context https://camunda.slack.com/archives/C06F0GLJNFM/p1732015209561459?thread_ts=1731426489.455309&cid=C06F0GLJNFM  closes https://github.com/camunda/camunda/issues/24736
camunda,camunda,ded1c39ddca01aa1516311211f628a1fd41e5d11,https://github.com/camunda/camunda/commit/ded1c39ddca01aa1516311211f628a1fd41e5d11,Communicate when importing done (#24671)  ## Description  Early iteration PR for importing done communication on the operate side for ES. Will implement for tasklist and OS if the approach is approved.  **Approach** Since records are stored sequentially in the log  when the importer receives an 8.7.x record then all the 8.6.x records have been written to the zeebe indices and we can mark that partition as "finished" importing. Then if a record reader for that partition (which is finished) gets five empty batch requests we say that record reader is done importing.  five empty batch requests are required to avoid a race condition.If records are written to zeebe indices and before a refresh  the record reader pulls the import batch  it is empty so it then says that the record reader is done when it is not as there are still records to process. To avoid this we can either - refresh before each import batch  this should be avoided as there is a heavy performance cost with manual refreshes. - when retrieving batches add the `refresh=wait_for` query parameter so the request will block and not return until after a scheduled refresh has occurred  this should also be avoided as it would add time to the already long delay. - Use two empty batches as this should catch the zeebe indices after a refresh  ## Related issues  closes #22953
camunda,camunda,5ce2cc83643d5bff4f26f78d6b111a39d37d226c,https://github.com/camunda/camunda/commit/5ce2cc83643d5bff4f26f78d6b111a39d37d226c,feat: store metadata of newly created deployments (#24692)  This creates a new applier for the Deployment/Created event that actually stores the record in the state. The records only contains the resource metadata.  Because we don't want to overwrite deployments  the processor now checks whether a deployment already exists before handling a distributed Deployment/Create command. If it does  we simply write a rejection. This also means that we no longer re-create the resources needlessly  so that's a small performance win too.  :thought_balloon: I wasn't sure what to do about the old deployment handling where we remove the deployment from the state once fully distributed. This mechanism is no longer used since we switched to generalized command distribution but the code for it is still there  so there's a chance that we can still remove deployments. Could we remove the old processors/appliers now?  closes #24683
camunda,camunda,9e646e0dbd5a804b5ac7597b549b6e25b09a1647,https://github.com/camunda/camunda/commit/9e646e0dbd5a804b5ac7597b549b6e25b09a1647,fix: only serialize relevant fields  We don't want to serialize a couple of fields  like the empty  encodedLength  and length fields. This is common amongst UnpackedObjects. We also don't want to serialize the DirectBuffer methods  which are only there for cases where we need the best performance.
camunda,camunda,bfd1690dda7aee0b71213ba8af4ecb936cbb5f54,https://github.com/camunda/camunda/commit/bfd1690dda7aee0b71213ba8af4ecb936cbb5f54,perf: don't read entire deployment record to check for existence
camunda,camunda,aa4b4e93b2110db2b1e75b7c9cdb5c92192fc16d,https://github.com/camunda/camunda/commit/aa4b4e93b2110db2b1e75b7c9cdb5c92192fc16d,Improve unauthorized error messages (#24643)  ## Description  <!-- Describe the goal and purpose of this PR. -->  The current unauthorized messages are a bit vague for users. It will let them know that they are lacking authorizations to perform action X on resource Y. However  it will not give any details regarding the resource. For example  we don't tell what the process id is if a user is unauthorized to complete a job. This is useful information for them to know  as this is the specific resource id they must provide permissions for if they run into this error. This PR adds these details where necessary.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  closes #24611
camunda,camunda,4e96bfbfbad19031a2f146fb97be92976b3150f0,https://github.com/camunda/camunda/commit/4e96bfbfbad19031a2f146fb97be92976b3150f0,perf: remove redundant synchronized (#24621)  ## Description  This was originally removed with bcc0ee4  but got reintroduced accidentally to 8.6.0 on main with f69acb4.  See https://camunda.slack.com/archives/C037W9NMATG/p1731082662618709?thread_ts=1731053567.928969&cid=C037W9NMATG
camunda,camunda,9eff46f708fde00edd508faa5320efafcffa4e9c,https://github.com/camunda/camunda/commit/9eff46f708fde00edd508faa5320efafcffa4e9c,perf: remove redundant synchronized  This was originally removed with bcc0ee4  but got reintroduced accidentally to main with f69acb4.
camunda,camunda,cf2dd2887c1f5ccbde7c22e755ad48105a14d825,https://github.com/camunda/camunda/commit/cf2dd2887c1f5ccbde7c22e755ad48105a14d825,Add ArchUnit test to ensure processors doing authorization checks (#24111)  ## Description  <!-- Describe the goal and purpose of this PR. -->  This test has 2 goals. The first is ensuring the authorization checks are not removed from a processor by accident. The second goal is ensuring devs are triggered to think about authorizations when they create a new processor. If they add a new processor and don't perform auth checks  this test will fail. To circumvent this the processors needs to be explicitly excluded from authorization checks.  I had to add he `UserDeleteProcessor` auth checks to make it succeed. This was missed and didn't have an issue. The ITs for it will be implemented in https://github.com/camunda/camunda/issues/21694  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  closes #22896
camunda,camunda,baea357beee75c78a53570b603b4773cbcdbd58c,https://github.com/camunda/camunda/commit/baea357beee75c78a53570b603b4773cbcdbd58c,perf: return early  We should not look up the banned instance state for every command  but only for those where they are process instance related.
camunda,camunda,0fc7c19bbf703e80c7cc33ab08f0085582ab7def,https://github.com/camunda/camunda/commit/0fc7c19bbf703e80c7cc33ab08f0085582ab7def,build: fix checkstyle errors in Optimize (#24012)  ## Description  The context for this task is defined in https://github.com/camunda/camunda/issues/22267  With this PR:  * since Optimize is not yet inheriting from zeebe-parent  in order to perform the checkstyle validation  a new pipeline is being introduced  named `Optimize Checkstyle`  that temporarily inherits from zeebe-parent  before running `mvn checkstyle:check`. The inheritance only happens in the runner and does not land into the actual repo. Once we inherit from zeebe-parent  we can remove this extra pipeline.  * the inner classes Fields are many  and they have many consumers. Even though they have checkstyle errors  I decided to suppress the errors for those  to save time. Please let me know if we want this to be addressed as well.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  closes https://github.com/camunda/camunda/issues/22267
camunda,camunda,5f9a8d9c16a353fab62c0f49685635672d4513b8,https://github.com/camunda/camunda/commit/5f9a8d9c16a353fab62c0f49685635672d4513b8,fix: role deletion should be performed without providing the name  The role name is retrieved from the state before performing the deletion
camunda,camunda,59356e0ab63cd5219c4bd9ab8ff8c863259582b5,https://github.com/camunda/camunda/commit/59356e0ab63cd5219c4bd9ab8ff8c863259582b5,Check permissions in `MessageCorrelationCorrelateProcessor` (#23353)  ## Description  <!-- Describe the goal and purpose of this PR. -->  It's a bit of an odd one since there's different permissions to check here. The current solution is likely not the most performant  but it is the easiest way to implement it at this time. If it becomes a problem we can improve upon this later.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  closes #22885
camunda,camunda,9cec871faf15b5fb9904c5f6bc57445d50c4b1ba,https://github.com/camunda/camunda/commit/9cec871faf15b5fb9904c5f6bc57445d50c4b1ba,feat: add authorization check to message correlation  It's a bit of an odd one since there's different permissions to check here. The current solution if likely not the most performant  but it is the easiest way to implement it at this time. If it becomes a problem we can improve upon this later.
camunda,camunda,e64ba90c2fb0e73796ea3f2ec47fefe9d34e0ce7,https://github.com/camunda/camunda/commit/e64ba90c2fb0e73796ea3f2ec47fefe9d34e0ce7,Support delete operations in the batch request (#23227)  ## Description  <!-- Describe the goal and purpose of this PR. -->  Identity has a requirement that we perform deletes by doc ID  this PR introduces the base `delete` method in the BatchRequest class such that it can be used later in the handlers to process deletes.
camunda,camunda,94b07cbb206f4d6619b22b4843bb66d70aa80c4a,https://github.com/camunda/camunda/commit/94b07cbb206f4d6619b22b4843bb66d70aa80c4a,perf: Add Backoff Mechanism to Handle ElasticSearch Unavailability in Tasklist Importer
camunda,camunda,1f50069dd83dfde85a6b6fa1241e60bf32101363,https://github.com/camunda/camunda/commit/1f50069dd83dfde85a6b6fa1241e60bf32101363,feat: added configuration to disable cluster health checks  - added property camunda.operate.healthCheck.enabled with default 'true' - added checks to not perform health checks on clusters when that property is false in IndicesHealthIndicator  ElasticsearchConnector and OpensearchConnector
camunda,camunda,276f528929e2cd28b254460b3275e286cfaf6fa8,https://github.com/camunda/camunda/commit/276f528929e2cd28b254460b3275e286cfaf6fa8,Metrics for Camunda exporter (#23090)  ## Description  Micrometer metrics for Camunda exporter  currently tracks - Failed flushes - Flush duration - Flush latency (time from first record in batch to flush command) - Bulk size (number of entities) - Bulk size memory  Currently there is no implementation of bulk size memory as the exporter cannot currently track the memory usage of records currently as not implemented.  Screenshots of grafana metrics at https://github.com/camunda/camunda/issues/22867#issuecomment-2397375347  **Success Criteria:**  - [x] Identify relevant metrics that helps us with performance tuning and debugging - [x] Add the metrics to the exporter - [x] Make the new metrics available in grafana dashboard  ## Related issues  closes #22867
camunda,camunda,8e9a072c9a105d9f637dbbd8420a2c147f148cae,https://github.com/camunda/camunda/commit/8e9a072c9a105d9f637dbbd8420a2c147f148cae,feat: decorate PI creation processors with AuthorizableCommandProcessor  We need to wrap these processors with the new AuthorizableCommandProcessor. This makes sure we perform the authorization checks before the command is processed.
camunda,camunda,69fbb9ade32f544442815832a4106360c9ef07b5,https://github.com/camunda/camunda/commit/69fbb9ade32f544442815832a4106360c9ef07b5,refactor: move AuthorizationRequest to AuthorizationCheckBehavior  This request is used to determine if a user is authorized to perform an action. It is best described by the AuthorizationCheckBehavior  as this is the class responsible for checking the authorizations.
camunda,camunda,788ba161cf5ef9fce1cf74ca1d8989d0cb68ba44,https://github.com/camunda/camunda/commit/788ba161cf5ef9fce1cf74ca1d8989d0cb68ba44,fix: do not throw Exception when GCS store is not accessible on startup (#23057)  ## Description  In an exception is throw at startup  just log it at warning so that all information necessary if available when reading the logs. Because this error can be transient  it's better to continue anyway the initialization of the broker. `GcsBackupStoreConfig`  performs validation in the constructor already. ## Related issues closes #14593
camunda,camunda,a0d897fe0dd04117a4871bd367a1ae24c2a3d280,https://github.com/camunda/camunda/commit/a0d897fe0dd04117a4871bd367a1ae24c2a3d280,feat: initial AuthorizationCheckBehavior  Adds a new AuthorizationCheckBehavior class. This behavior is responsible for verifying the user of the incoming command is authorized to perform a given action. For this it takes the command  resource type  permission type and a map of resource identifiers. The behavior will use these parameters to fetch the permissions of this user from the state. It will compare the resource identifiers we have there with the resource identifiers that are required. If any of them match  the user is permitted to perform this action. It none match  the user has insufficient permissions. The command should be rejected when this happens (not part of this commit).
camunda,camunda,97f8910b8eb352f4e259117191ea56899016448c,https://github.com/camunda/camunda/commit/97f8910b8eb352f4e259117191ea56899016448c,feat: Decision instance get by key - API implementaton (#22811)  ## Description  <!-- Describe the goal and purpose of this PR. --> Implement Decision instance get by key API `evaluatedInputs` and `matchedRules` are returned in the response  in opposite to the search service for which these fields are omitted for performance reasons  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  relates to [#22656](https://github.com/camunda/camunda/issues/22656)
camunda,camunda,37707bd33d9d8e65ea8bdf7c6e441d1a57f6cb2f,https://github.com/camunda/camunda/commit/37707bd33d9d8e65ea8bdf7c6e441d1a57f6cb2f,feat: implement user task query by variable (#22293)  ## Description  Implementation of `UserTaskVariableFilter` as an array that will be converted to `VariableFilterValue` (with `eq` operator)  The decision to use array by name/value is based on the future implementations: - Being able to apply advanced query in name and value fields - User the array as the possibility to add `or` operator for 2 or more variables conditions - We do not support `and` for multiple variables  once `Elasticsearch/OpenSearch` does not allow `a= "something"` AND `a="another thing"`  like the situation of `name`. This would imply on multiple queries  and it is not the goal  once it could lead performance issues (This can be rediscussed later)  The query logic is provided for support: - Search Task Variables - Search Process Variables - When the TaskVariable contains the ProcessVariable as input/output - The Task is returned by the current value of the Process Variable inside the Task  For subprocess: - Only Task Variables are supported (Input)  ## Related issues  closes #https://github.com/camunda/camunda/issues/21846 closes #https://github.com/camunda/camunda/issues/21849
camunda,camunda,4acc7965f160bf4a330ae0fa4b2616adb9e9e0c3,https://github.com/camunda/camunda/commit/4acc7965f160bf4a330ae0fa4b2616adb9e9e0c3,fix: return userKey on user create request  IT's convenient to have the userKey available in the response when a user is created. This allows the client to perform followup actions  such as adding permissions.
camunda,camunda,2b044e7d430d24f860be5097334f6dc231fcd4ca,https://github.com/camunda/camunda/commit/2b044e7d430d24f860be5097334f6dc231fcd4ca,fix: test that input collection variable can be evaluated correctly for parallel multi-instance too  Before #20958  we evaluated input collection only for sequential multi-instances Now that we do for parallel too  And since not being able to evaluate it throws an unresolvable incident in `afterExecutionPathCompleted` We should also perform the check for parallel ones in `beforeExecutionPathCompleted`
camunda,camunda,357c972c7736cfaaa1e1a57a6753f467d4b7a90d,https://github.com/camunda/camunda/commit/357c972c7736cfaaa1e1a57a6753f467d4b7a90d,refactor: record value and metadata length is always non-zero (#21311)  This is a performance improvement to avoid calculating the length of record value and metadata for every entry that's being appended.  Record values inherit from `ObjectValue` which always has non-zero length due to the map header. Record metadata is serialized via SBE which also requires a header and thus always has non-zero length.  closes #19225
camunda,camunda,17e57f88b7be1c36642ad44b40a058e79843a46a,https://github.com/camunda/camunda/commit/17e57f88b7be1c36642ad44b40a058e79843a46a,refactor: record value and metadata length is always non-zero  This is a performance improvement to avoid calculating the length of record value and metadata for every entry that's being appended.  Record values inherit from `ObjectValue` which always has non-zero length due to the map header. Record metadata is serialized via SBE which also requires a header and thus always has non-zero length.
camunda,camunda,4a10b2e98a7512de5e7d158fd4fbc7246e910e63,https://github.com/camunda/camunda/commit/4a10b2e98a7512de5e7d158fd4fbc7246e910e63,Add mapping functionality to Actor futures (#18990)  ## Description  This PR adds mapping functionality similar to `CompletionStage#thenApply`  allowing you to chain futures and map the result only on success  but short-circuiting the chain if any error occurs at any point.  Additionally refactors one place as an example of where/how to use `thenApply`.  I've keep the warning about performance  but honestly I'd like to challenge it. Creating an intermediate future is pretty much required if you're gonna chain things _and_ allow changing the type of the result. The other option is essentially building your own stream-like pipeline  but wouldn't allow for branching  just pipelining  the results. So there's definite advantages to intermediate futures  and I'm not sure it's sure a performance hit.  As for executing callbacks on the given executor  is that so terrible? :thinking:
camunda,camunda,6f0d49efa9cbd7a32db70d680b3ffe05f0d09949,https://github.com/camunda/camunda/commit/6f0d49efa9cbd7a32db70d680b3ffe05f0d09949,feat: perform license check on startup - re-add functionality (#20998)  ## Description  This PR readds the code from https://github.com/camunda/camunda/pull/20541 which was reverted in https://github.com/camunda/camunda/pull/20940  This PR can be merged once the license checker library has been moved to a public repo - Completed with https://github.com/camunda/camunda-license-check/pull/34 - Update license checker version to 2.9.0  the version which was uploaded to `camunda-bpm`  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  closes #
camunda,camunda,f7f4b4e22e2c6fcb2f000044afcb3f8386ff8bc4,https://github.com/camunda/camunda/commit/f7f4b4e22e2c6fcb2f000044afcb3f8386ff8bc4,Migrate timer boundary event subscriptions (#19965)  ## Description  <!-- Describe the goal and purpose of this PR. --> <!-- --> <!-- For structural or foundational CI changes request review from @cmur2 -->  Enables the specification of mapping instructions for timer boundary events during process instance migration. This feature ensures that the timer boundary event is migrated to the target process definition.  During migration  certain properties of the timer are adjusted using a new `MIGRATED` intent:  - `processDefinitionKey` is updated to the definition key of the target process - `targetElementId` is updated to the ID of the target boundary event - `interrupting` status is updated to match the interrupting type of the target boundary event (either interrupting or non-interrupting) - This adjustment ensures that when the timer is triggered  the boundary event will perform as defined in the target process.  During migration  the timer duration (due date) and expressions are **not** re-evaluated. This approach ensures alignment with [our documentation on expressions](https://docs.camunda.io/docs/next/components/concepts/process-instance-migration/#jobs-expressions-and-input-mappings) and provides a consistent experience across all migrations.  ## Related issues  closes #15918
camunda,camunda,4b489d6a4e1805e4de46f8c182f0bdae1a8e856f,https://github.com/camunda/camunda/commit/4b489d6a4e1805e4de46f8c182f0bdae1a8e856f,feat: perform license check on startup
camunda,camunda,0c34eba36322cf6acb5acb2a0292ff1353248abb,https://github.com/camunda/camunda/commit/0c34eba36322cf6acb5acb2a0292ff1353248abb,build: merge optimize in (#19381)  This PR places Optimize in the `camunda/camunda` repo  together with every other application of the Camunda stack.  **What's in scope**  * placing Optimize in the monorepo `camunda/camunda` * making sure that the repo is healthy after the introduction of camunda  **What's not in scope**  * making Optimize as part of the single JAR  Optimize is being introduced in `camunda/camunda` as a submodule  declared as such in the main `pom.xml` file  but it's **not** using the parent pom from `parent/pom.xml`. This will be changed later  in the [context of becoming part of the same single JAR](https://github.com/camunda/product-hub/issues/2289).  **Workflow statuses**  * The workflows that are marked as "skipped" have not been checked * At the time of marking this PR as ready for review  all of the intended workflows are healthy * The workflow `Commitlint / lint-commits (pull_request)` currently fails  because the git history brought in by`camunda/camunda-optimize` had less strict rules than `camunda/camunda` * There is a number of workflows that are failing. The failures are due to a known issue: https://github.com/camunda/camunda/issues/17926  **Review**  As this PR is introducing several thousands of commits in the repo  it's difficult to review it using the diff page provided by github. If a given file `$file` shall be reviewed  a visually-good way to do it would be to use the terminal as follows:  ```bash $ git checkout main $ git pull $ git checkout merge-optimize-in $ git pull $ git merge main $ file="zeebe/sample-file.md" $ git diff main -- "$file" ```  The content of this PR can be logically categorized as follows:  * files that introduce Optimize in the repo  under the `optimize/` directory * project changes in the various `pom.xml`s * changes in the CI files that were already present in the repo * introduction of new CI files that are related to Optimize * changes to other files that are not related to Optimize (currently only 1 test)  The first part is practically a mere copy of Optimize. It's under the directory `optimize/`. One note is that as `camunda/camunda-optimize` had different coding style rules  currently this not directory is not compliant with the rules of `camunda/camunda`. Nevertheless  as the parent `parent/pom.xml` is not being used by Optimize  this doesn't currently represent a problem.  The `pom.xml`s files that have been changed are identifiable as follows: ```bash $ git diff --name-only main | grep 'pom.xml' | grep -v 'optimize' dist/pom.xml parent/pom.xml pom.xml testing/camunda-process-test-java/pom.xml ``` From the result  the entry `testing/camunda-process-test-java/pom.xml` can be neglected.  The changes to the CI files that are not related to Optimize  can be found as follows: ```bash $ git diff --name-only main | grep '.github/workflows' | grep -v 'optimize' .github/workflows/ci.yml .github/workflows/operate-a11y.yml .github/workflows/operate-ci-test-reusable.yml .github/workflows/operate-docker-tests.yml .github/workflows/operate-e2e-tests.yml .github/workflows/operate-frontend.yml .github/workflows/operate-playwright.yml .github/workflows/tasklist-docker-tests.yml .github/workflows/tasklist-e2e-tests.yml .github/workflows/zeebe-ci.yml ``` Other changes were mostly related to timeouts and to runner changes  as there was the need for a more powerful machine to complete the build (Optimize will address the problem of having a time-consuming build in a later moment).  The CI files introduced by Optimize can be found as follows: ```bash $ git diff --name-only main | grep 'workflows/optimize' .github/workflows/optimize-backend-linting.yml .github/workflows/optimize-check-c4.yml .github/workflows/optimize-check-renovate-config.yml .github/workflows/optimize-ci.yml .github/workflows/optimize-code-scanning-java.yml .github/workflows/optimize-code-scanning-javascript.yml .github/workflows/optimize-command-assign.yml .github/workflows/optimize-command-eng.yml .github/workflows/optimize-command-help.yml .github/workflows/optimize-command-pm.yml .github/workflows/optimize-command-qa.yml .github/workflows/optimize-commands-dispatch.yml .github/workflows/optimize-create-release-branch.yml .github/workflows/optimize-deploy-artifacts.yml .github/workflows/optimize-e2e-test-cloud.yml .github/workflows/optimize-e2e-tests-sm.yml .github/workflows/optimize-engine-compatibility.yml .github/workflows/optimize-es-compatibility.yml .github/workflows/optimize-generate-cambpm-test-datasets.yml .github/workflows/optimize-generate-changelog.yml .github/workflows/optimize-generate-upgrade-plan.yml .github/workflows/optimize-hadolint.yml .github/workflows/optimize-health-status-report.yml .github/workflows/optimize-helm-integration.yml .github/workflows/optimize-import-static-data-performance.yml .github/workflows/optimize-issue-add-labels.yml .github/workflows/optimize-opened-issues-automation.yml .github/workflows/optimize-os-compatibility.yml .github/workflows/optimize-pmd.yml .github/workflows/optimize-preview-env-clean.yml .github/workflows/optimize-preview-env-deploy.yml .github/workflows/optimize-preview-env-teardown.yml .github/workflows/optimize-publish-c4.yml .github/workflows/optimize-release-optimize-c8-only.yml .github/workflows/optimize-run-java-checks.yml .github/workflows/optimize-sync-issues.yml .github/workflows/optimize-unit-tests.yml .github/workflows/optimize-zeebe-compatibility.yml ```  In addition to this  the following file has been changed for a test to pass: ``` zeebe/exporters/elasticsearch-exporter/src/test/java/io/camunda/zeebe/exporter/ElasticsearchExporterTest.java ```
camunda,camunda,b804e3fb6e59c81979730c900e836539ef36a1ba,https://github.com/camunda/camunda/commit/b804e3fb6e59c81979730c900e836539ef36a1ba,Revert perform license check on startup (#20940)  ## Problem Description  We run into issues with the change from https://github.com/camunda/camunda/pull/20541 as we were no longer build locally  or on some CI pipelines. Thus because the license-checker dependency is not publically available. It is only in Camunda repositories available  which might also cause issues for external contributors.  See related [slack thread conversation ](https://camunda.slack.com/archives/C06HTSPD5AP/p1723190152698859?thread_ts=1722457528.375399&cid=C06HTSPD5AP)  We will need to revert the changes  until we find another solution like making the artifacts publicly available. \cc @cmur2 might be something you can help with. Furthermore  we might want to have a check in our CI that the builds are working also with public repos only.  ### What the PR does to overcome this  Reverts changes from PR https://github.com/camunda/camunda/pull/20541/commits  I tried to revert the merge commit  but [read that it might cause issues](https://stackoverflow.com/a/7100005/2165134) when we want to apply the same commits again.  > Source: https://mirrors.edge.kernel.org/pub/software/scm/git/docs/git-revert.html > Reverting a merge commit declares that you will never want the tree changes brought in by the merge. As a result  later merges will only bring in tree changes introduced by commits that are not ancestors of the previously reverted merge. This may or may not be what you want.  So I revert each commit separately (excluding merge commits).  ```sh git revert 966db932161ca6cd17b1e726008eff24cb665b39 c3855ef0facc526e66ea3bbb5788bdd13c0ca4b3 5e00a6200dfa5924e5273d10f71a07cc0591e636 62c7d21419153df062dd4095091946cd8cc897ce 51ffe8081f8f15ab7fb80d882c62ecf0979a3051 a58d6f40c04d5b967801fbb9543156a4a2cc2525 11315f771228c3e4600e0a71822fffb58baedde0 d51bde18db31220990bcd07fca8f39af36dbe949 e037f9896ff3a1f90caf57f1158163b636d6f7b1 48c1702f7e10daf2b1d5180bf402a74413ccce24 d26265e35e9f29fc3a0767ce428c15f1347ebdd7 ed356d95543c14883212d6c0f32744996cb2dc86 f02fc3fb8346fcbf25f4222f4a175afe66c860b0 ``` <!-- Describe the goal and purpose of this PR. -->
camunda,camunda,2af4f3d8c411f09c28d7d6bfd2cbd5d197535af1,https://github.com/camunda/camunda/commit/2af4f3d8c411f09c28d7d6bfd2cbd5d197535af1,feat: improve import performance for treePathCache (#20623)  ## Description  * Cache `DatabaseType` in `DatabaseInfo` * Avoid double lookup for treePath * This PR does not contain `treePathCache` per partition. That raises test issues  therefore this comes in another PR.  ## Checklist  <!--- Please delete options that are not relevant. Boxes should be checked by reviewer. --> - [ ] for CI changes: - [ ] structural/foundational changes signed off by [CI DRI](https://github.com/cmur2) - [ ] [ci.yml](https://github.com/camunda/camunda/blob/main/.github/workflows/ci.yml) modifications comply with ["Unified CI" requirements](https://github.com/camunda/camunda/wiki/CI-&-Automation#workflow-inclusion-criteria)  ## Related issues  Related to #20031   #20027   #20076
camunda,camunda,977e257f0536f600f050042c8360953541aa516e,https://github.com/camunda/camunda/commit/977e257f0536f600f050042c8360953541aa516e,feat: perform license check on startup (#20541)  ## Description  <!-- Describe the goal and purpose of this PR. --> <!-- --> <!-- For structural or foundational CI changes request review from @cmur2 --> On startup  perform a license check on the C8 license passed in via environment variable. Log the status of the license  and store the result of `validate()` in a persistent variable. That variable will available to the `ManagementService` bean  The license checker library handles the logging when `validate()` is called  ## Related issues  closes https://github.com/camunda/camunda/issues/18970
camunda,camunda,349e98254a936c4db6b7fdd79d571eb205e0d927,https://github.com/camunda/camunda/commit/349e98254a936c4db6b7fdd79d571eb205e0d927,refactor: remove superficial Eithers from REST API (#20638)  ## Description  * Removes Either types that don't serve any function to make the code easier to understand. * Extracts error messages into the dedicated class for consistency and builds error responses consistently for search requests as well. * Removes unused methods from the REST mappers for cleaner code.  ## Related issues  n/a
camunda,camunda,757ab6adc2e303e2d1d3e4224a1a01ae0d6fbc27,https://github.com/camunda/camunda/commit/757ab6adc2e303e2d1d3e4224a1a01ae0d6fbc27,refactor: remove superficial Eithers from REST API  * Removes Either types that don't serve any function to make the code easier to understand. * Extracts error messages into the dedicated class for consistency and builds error responses consistently for search requests as well. * Removes unused methods from the REST mappers for cleaner code.
camunda,camunda,11ad4184ab8a5c456e21412780f5eb60a7e4fd67,https://github.com/camunda/camunda/commit/11ad4184ab8a5c456e21412780f5eb60a7e4fd67,refactor: simplify the logic for grpc interceptors  Simplify the logic of the InterceptorRepository class by introducing a LinkedHashMap to ensure correct interceptor ordering.  Also add a comment to reflect any changes in this class to the similar FilterRepository class as it performs the same logic for REST API filters.
camunda,camunda,ea8d032131836220c3ae10bcbfdb9d3697c398f4,https://github.com/camunda/camunda/commit/ea8d032131836220c3ae10bcbfdb9d3697c398f4,Test for snapshot corruption when restoring from backup. (#20415)  ## Description  When restoring from a backup the snapshot should not be corrupted. To this end a file checksum verification is performed.  The snapshot store for restoring did not have a checksum provider while all internal backups are taken with a RocksDB checksum provider causing a combined checksum mismatch. #19988 fixes this however customers that do not have this patch fix will encounter this issue when trying to restore from any snapshots that are created with the help of RocksDB full file checksums.  This PR removes the ability to create a `FileBasedSnapshotStore` without specifying a checksum provider as all stores should aim not to fully read all files when possible for checksums.  `RestoreAcceptance.java` now also restores from snapshots which will then catch the particular case described in paragraph 2. To match the state of the code base to users encountering problems and make the tests fail: 1. `FileBasedSnapshotStore:213` change this line to compare combined values. `if (expectedChecksum.getCombinedValue() != actualChecksum.getCombinedValue()` 2. `PartitionRestoreService:182` Change this snapshot store to just a noop checksum provider `new FileBasedSnapshotStore( brokerId  partition.id().id()  partition.dataDirectory().toPath()  snapshotPath -> Map.of());` so it will calculate the checksum by fully reading all files.  ## Related issues  closes #19984
camunda,camunda,4aefde01b582bebcab50619571d90e78667384ac,https://github.com/camunda/camunda/commit/4aefde01b582bebcab50619571d90e78667384ac,feat: implement building calling element path  For now  the calling element path is constructed using call activity element Ids  This is not performant or memory efficient since element ids have infinite length  Working on a way to encode it to optimize performance and memory usage is going to follow
camunda,camunda,8bc849a735404a61846156ee464c3e7baa89f5d7,https://github.com/camunda/camunda/commit/8bc849a735404a61846156ee464c3e7baa89f5d7,fix: fix concurrency mode performance (#19909)  ## Description  Perform concurrency mode check once per batch instead of once per every imported record.  ## Related issues  closes #19840
camunda,camunda,3b8f8983c1b2bf5cab936c9c46ef41995a8f7fbc,https://github.com/camunda/camunda/commit/3b8f8983c1b2bf5cab936c9c46ef41995a8f7fbc,fix: only fetch credentials once at a time  By removing the synchronized previously  we broke the test case: `OAuthCredentialsProviderTest.shouldCallOauthServerOnlyOnceInMultithreadMode`  This says that we should only make a single Oauth request at a time. Previously  this was controlled by using synchronized on all cache methods. But we only need a synchronized on the `computeIfMissingOrInvalid` method.  This works  because it calls `OAuthCredentialsProvider.fetchCredentials` as the `zeebeClientCredentialsConsumer`. By blocking on the entire cache  we prevent the credentials to be fetched multiple times.  This is a crude solution  as different requests to fetch credentials are likely still needed when different endpoint are used  but the entire method is locked regardless of the endpoint.  As it's unclear what the performance gain would be from locking per endpoint  and this previously hasn't been any issue  I'll revert back to using synchronized on the `computeIfMissingOrInvalid` method.  (cherry picked from commit af15bf2a300356fb024c67b44ab9a31223a948a0)
camunda,camunda,8090103fb7d8d8e5148fbd5482d502cbe5d28ecd,https://github.com/camunda/camunda/commit/8090103fb7d8d8e5148fbd5482d502cbe5d28ecd,perf: remove redundant synchronized  In the previous commit we made this class threadsafe by using an atomic reference and replacing its reference locklessly.  (cherry picked from commit bcc0ee46c017d37437aafe1035c4c2f734c27d19)
camunda,camunda,914835c261f64388f56c441cd5ca630db478b052,https://github.com/camunda/camunda/commit/914835c261f64388f56c441cd5ca630db478b052,refactor: remove unnecessary constructors and parameters  In the Java client  the "useRest" parameter is already contained in the configuration. Thus  we don't need to pass it on separately to constructors and can thus remove superficial constructors as well.
camunda,camunda,0eafbaa16e0309c697ccfb5fea0e590da3c72e6f,https://github.com/camunda/camunda/commit/0eafbaa16e0309c697ccfb5fea0e590da3c72e6f,feat: implement user task search transformers (#19273)  ## Description  Create the User Task Filter/Query on Camunda Service  Unit tests available on this Pull Request: https://github.com/camunda/camunda/pull/19434  Note: This filter will only contemplate User Task  to filter user task variables  this logic will be handled on the endpoint business logic for the performance reasons  only we will have the variable service  and we can divide the search by the tasks that are running vs. tasks that are completed.  The variable service is available on this Pull Request: https://github.com/camunda/camunda/pull/19385  ## How to test?  Example of Java Application ``` public class TestApplication {  public static void main(final String[] args) { final var serverUrl = "http://localhost:9200"; final var restClient = RestClient.builder(HttpHost.create(serverUrl)).build(); final var transport = new RestClientTransport(restClient  new JacksonJsonpMapper()); final var esClient = new ElasticsearchClient(transport);  final var dataStoreClient = new ElasticsearchSearchClient(esClient); final var camundaServices = new CamundaServices(dataStoreClient);  final SearchQueryResult<UserTaskEntity> result = camundaServices .userTaskServices() .search( (q) -> q.page((p) -> p.size(100)) .filter((p) -> p.taskStates("COMPLETED")) .sort((s) -> s.startDate().desc()));  result.items().stream().forEach(System.out::println);  System.exit(-1); } } ```  ## Related issues  closes #https://github.com/camunda/camunda/issues/19210
camunda,camunda,5790f7502ff8122032d5b88ff2aeec17ce80ea1f,https://github.com/camunda/camunda/commit/5790f7502ff8122032d5b88ff2aeec17ce80ea1f,Ensure that stream processor can be closed while writes are rejected (#19278)  ## Description  This fixes a subtle bug that only recently occurred for the first time because we had a [performance regression](https://github.com/camunda/camunda/pull/19233) that made it more likely that writes were rejected  sometimes permanently.  What would happen is that the async scheduled task actor would try to write results from a task but get rejected by the sequencer. The retry strategy used for the _async_ variant had no proper abort condition because we assumed that we could always just close the actor. This is not correct because the retry strategy constantly submits jobs to the fast lane  starving the queue of externally submitted jobs such as the job initiating the actor closing.  So in effect  the stream processor would fail to close properly because it waits indefinitely on the closing of the async scheduled task actor. This would show up as partition transitions not making progress and Zeebe partitions no longer matching their raft role.  ## Related issues  closes #19219
camunda,camunda,6b72c094887e199bb3784eec44cdc700843d44bc,https://github.com/camunda/camunda/commit/6b72c094887e199bb3784eec44cdc700843d44bc,fix: don't adjust append limit when request limit is exhausted (#19233)  Whenever a user request is rejected because the request limit is exhausted  we want to revert the claimed spot in the append limit and not signal that the append was dropped.  This most likely caused weird performance issues where every rejected user command would also lower the append limit  eventually leading to too many rejections and increased workload due to timeouts and retries.
camunda,camunda,abf37203d2b229280fda9b6b817afce3fb5232ed,https://github.com/camunda/camunda/commit/abf37203d2b229280fda9b6b817afce3fb5232ed,fix: fix failing upgrade performance tests
camunda,camunda,791f9a8a38db804070074d6808c1b0bde945a5bc,https://github.com/camunda/camunda/commit/791f9a8a38db804070074d6808c1b0bde945a5bc,feat: Return failed/successful operations count together with batch operation (#18171)  ## Description Added feature to return the failed and successfully completed count of operations for batch operations to display them in the UI.  ## Notes for reviewer I decided to fetch the additional data from the Operation index individually instead of for all currently active operations (as suggested in the issue)  since the batch operations are loaded progressively while scrolling. Performance could probably be improved by requesting information for all the operations that are being loaded in one scroll at once. I'd like to hear opinions on that (and on whether it should be done before we merge the feature or if we can improve that later).  ## Related issues  closes #https://github.com/camunda/operate/issues/6294
camunda,camunda,4f3df7ff2e8c206d69dc1ddff3f7a5d6c90b6c84,https://github.com/camunda/camunda/commit/4f3df7ff2e8c206d69dc1ddff3f7a5d6c90b6c84,feat: improved request performance and refactored batch operations count  -Moved data transformation/aggregation to new class DataTransformer -Changed search requests and interfaces to use nested aggregations rather than multiple ID requests to elasticsearch/opensearch  # Conflicts: #	operate/webapp/src/main/java/io/camunda/operate/webapp/opensearch/reader/OpensearchOperationReader.java
FasterXML,jackson-databind,707c3c19a3adb7eebfc6eeee4e7fce7b0c235518,https://github.com/FasterXML/jackson-databind/commit/707c3c19a3adb7eebfc6eeee4e7fce7b0c235518,Fix #4533  add `MapperFeature.REQUIRE_HANDLERS_FOR_JAVA8_TIMES` (#5105)
FasterXML,jackson-databind,6743f4cea3018ddc336f01cceaa0c8a8381b2c1b,https://github.com/FasterXML/jackson-databind/commit/6743f4cea3018ddc336f01cceaa0c8a8381b2c1b,Fix #5006: add `MapperFeature.REQUIRE_HANDLERS_FOR_JAVA8_OPTIONALS` (#5023)
FasterXML,jackson-databind,cd20d1e497aecb0f58a7983bd1e61a84b238ae6c,https://github.com/FasterXML/jackson-databind/commit/cd20d1e497aecb0f58a7983bd1e61a84b238ae6c,Revert accidental change to `MapperFeature.DEFAULT_VIEW_INCLUSION` in 2.19
FasterXML,jackson-databind,5e4000415d5ffda4ff2c7155a978ae3b8e8722f4,https://github.com/FasterXML/jackson-databind/commit/5e4000415d5ffda4ff2c7155a978ae3b8e8722f4,Tweak working of MapperFeature.SORT_CREATOR_PROPERTIES_BY_DECLARATION_ORDER
FasterXML,jackson-databind,3ddbc97b3503cd741606c6e63182edd588583dff,https://github.com/FasterXML/jackson-databind/commit/3ddbc97b3503cd741606c6e63182edd588583dff,Fix #4580: add `MapperFeature.SORT_CREATOR_PROPERTIES_BY_DECLARATION_ORDER` (#4587)
yacy,yacy_search_server,71a6074cc570df0f9df95bcd24b4325ea01aedf1,https://github.com/yacy/yacy_search_server/commit/71a6074cc570df0f9df95bcd24b4325ea01aedf1,added setting of cache configuration for solr according to recommendation from https://community.searchlab.eu/t/yacy-support-gpt-chatgpt-assistant/1622 However it is not clear if this configuration actually works (has an effect at all) or is the solution for performance issues.
dtinit,data-transfer-project,835e0c51781e78c21fefca11c484ffbd7bbde1aa,https://github.com/dtinit/data-transfer-project/commit/835e0c51781e78c21fefca11c484ffbd7bbde1aa,fix: classify microsoft `quotaLimitReached` code as dest. full (#1447)  unfortunately this isn't perfectly correct; `quotaLimitReached` can mean storage quota but _appears_ it also can be used to mean rate-limiting. Unclear  for now.
dtinit,data-transfer-project,ef14eb62a8b946dce67cd6247add58fdb4c8ca13,https://github.com/dtinit/data-transfer-project/commit/ef14eb62a8b946dce67cd6247add58fdb4c8ca13,noop(prefactor) new `MicrosoftApiResponse` encapsulaing error handling prev. sprinkled around adapter (#1399)  * noop(go/prefactor) define API-translation prev. copy/pasted imperfectly  * noop(prefactor) extracts extant error handling sprinkled around microsoft adapter  * simplify recovery analysis for callers  include example usage in javadoc to show this (want to show we cannot get into more scenarios like we have today with unchecked else-blocks  Ultimately most of the complexity of unwrapping the error--the complexity inherent in the example usage shown in my javadoc--is _itself_ being hidden away by a single private method on the caller (see upcoming PRs). But this at least makes that single helper much much easier to write.
alibaba,spring-ai-alibaba,5ef7ae165c0c617651ba2ef8acfd634255caa311,https://github.com/alibaba/spring-ai-alibaba/commit/5ef7ae165c0c617651ba2ef8acfd634255caa311,Refactor browser actions to use Playwright instead of Selenium  - Updated ClickByElementAction to utilize Playwright's Page and ElementHandle for clicking elements. - Refactored CloseTabAction to close the current tab using Playwright. - Modified ExecuteJsAction to execute JavaScript using Playwright's evaluate method. - Changed GetElementPositionByNameAction to find elements using Playwright's querySelectorAll and handle iframes. - Updated GetHtmlAction to retrieve HTML content using Playwright. - Refactored GetTextAction to get text content using Playwright's textContent method. - Modified InputTextAction to input text using Playwright's ElementHandle and simulate human typing. - Updated KeyEnterAction to simulate pressing the Enter key using Playwright. - Refactored MoveToAndClickAction to use Playwright's mouse API for moving and clicking. - Changed NavigateAction to navigate using Playwright's Page navigate method. - Updated NewTabAction to open a new tab and navigate using Playwright. - Refactored RefreshAction to refresh the page using Playwright's reload method. - Modified ScreenShotAction to capture screenshots using Playwright. - Updated ScrollAction to perform scrolling using Playwright's evaluate method. - Refactored SwitchTabAction to switch tabs using Playwright's context and pages.
alibaba,spring-ai-alibaba,fe659afb3c7459c4df8bcd2ccd262d81b550f0ab,https://github.com/alibaba/spring-ai-alibaba/commit/fe659afb3c7459c4df8bcd2ccd262d81b550f0ab,feat: Introduce new code execution and search functionalities  - Added ToolCallbackContext to manage tool execution context. - Implemented CodeExecutionResult and ExecuteCommandResult for handling code execution results. - Created CodeUtils for code extraction and execution with support for Python and shell scripts. - Developed PythonExecute class to execute Python code and handle outputs and errors. - Added ToolExecuteResult to encapsulate execution results and interruptions. - Implemented GoogleSearch tool to perform web searches using SerpApi  including response parsing. - Introduced SerpApiProperties and SerpApiService for managing API interactions and configurations. - Enhanced logging and error handling across new functionalities.
alibaba,spring-ai-alibaba,2d7405b00e2fdb5a485bdd3434af4386d9a1c6be,https://github.com/alibaba/spring-ai-alibaba/commit/2d7405b00e2fdb5a485bdd3434af4386d9a1c6be,feat: Implement ChromeDriverService for managing ChromeDriver instances  - Added ChromeDriverService to handle the lifecycle of ChromeDriver instances. - Implemented methods for creating  retrieving  and closing drivers based on planId. - Integrated OS detection to set the correct ChromeDriver path. - Added anti-detection scripts to enhance browser automation. - Introduced cleanup mechanisms for resource management on JVM shutdown.  docs: Create howToCreateNewTool documentation  - Documented strategies for managing resource conflicts and lifecycle in tool creation. - Outlined the importance of using services for potential resource conflicts.  feat: Introduce PlanBasedLifecycleService interface  - Created PlanBasedLifecycleService interface for managing lifecycle operations of tools. - Defined cleanup method for resource management.  feat: Develop FileState class for file operation tracking  - Implemented FileState class to track current file path and last operation result. - Added synchronization mechanisms for thread safety.  feat: Create TextFileOperator for text file operations  - Developed TextFileOperator to perform various actions on text files (open  replace  save  etc.). - Integrated with TextFileService for file state management. - Implemented error handling and logging for file operations.  feat: Implement TextFileService for managing text file states  - Created TextFileService to handle file operations and maintain state across plans. - Added methods for validating file types  managing file states  and ensuring security. - Implemented cleanup logic to clear resources on application shutdown.
alibaba,spring-ai-alibaba,16b4b257c5de7e03966b22f96426abc990a47379,https://github.com/alibaba/spring-ai-alibaba/commit/16b4b257c5de7e03966b22f96426abc990a47379,Merge pull request #465 from qnnn/performance  reduce redundant getProperty calls to improve performance
alibaba,spring-ai-alibaba,648f8d3009bf0282ce4155268cd52e967efdd8a2,https://github.com/alibaba/spring-ai-alibaba/commit/648f8d3009bf0282ce4155268cd52e967efdd8a2,reduce redundant getProperty calls to improve performance.
tchiotludo,akhq,f4ae29dfb198555f7392d90edc8c1b384700d3e6,https://github.com/tchiotludo/akhq/commit/f4ae29dfb198555f7392d90edc8c1b384700d3e6,fix(masking): Various performance  functional  and readability fixes for JSON-based masking
apache,logging-log4j2,3709962553ddc27774163eb77845b0a47a7b9684,https://github.com/apache/logging-log4j2/commit/3709962553ddc27774163eb77845b0a47a7b9684,Improve performance and avoid memory consumption if logging primitive arrays as parameters (#3645)  Current implementation: Method ParameterFormatter.appendArray() delegats to java.util.Arrays.toString() which then allocates a new StringBuilder to return a String which is then added to the existing StringBuilder.  Improved implementation: For all primitive types  a method like ParameterFormatter.appendArray(int[]  StringBuilder) has been added which is called by ParameterFormatter.appendArray() and avoids the unnecessary object creation.  * review comments
apache,logging-log4j2,18a1debd11a92d2e898a8e816db810a1777dfa39,https://github.com/apache/logging-log4j2/commit/18a1debd11a92d2e898a8e816db810a1777dfa39,Hardens `PropertiesUtil` against recursive property sources (#3263)  As showed in #3252  Spring's `JndiPropertySource` not only can throw exceptions  but can also perform logging calls. Such a call causes a recursive call to `PropertiesUtil.getProperty("log4j2.flowMessageFactory"`) and a `StackOverflowException` in the best scenario. The worst scenario includes a deadlock.  This PR:  - Moves the creation of the default `MessageFactory` and `FlowMessageFactory` to the static initializer of `LoggerContext`. This should be close enough to the pre-2.23.0 location in `AbstractLogger`. The `LoggerContext` class is usually initialized  before Spring Boot adds its property sources to `PropertiesUtil`. - Adds a check to `PropertiesUtil` to ignore recursive calls.  Closes #3252.
apache,logging-log4j2,70f058daaa64379cb62e4bf473ed1fd3b66c31db,https://github.com/apache/logging-log4j2/commit/70f058daaa64379cb62e4bf473ed1fd3b66c31db,Fix extended stack trace (i.e.  `%xEx`) rendering performance regression (#3123)  Co-authored-by: Volkan Yazıcı <volkan@yazi.ci>
apache,logging-log4j2,78af3169351a2ba059cc7d98d21042c6497027e6,https://github.com/apache/logging-log4j2/commit/78af3169351a2ba059cc7d98d21042c6497027e6,Remove caching from `PropertiesUtil` (+#2849 review)  The `PropertiesUtil` class preemptively caches lookups for all known keys from enumerable property sources. On my Debian system a JVM has around 60 among environment variables and system properties  which causes `PropertiesUtil` to perform around 60 lookups for **each** of the 3 standard property sources. Most of these properties have nothing to do with Log4j.  On the other hand all Log4j artifacts use no more than 100 configuration properties and the results of most of those calls are cache in static fields.  This PR removes the property value caches used by `PropertiesUtil`. The change should have no noticeable impact on the loading time of Log4j Core  while at the same time simplifies the system.
apache,logging-log4j2,151f7a6a902eef57a30cac6822387daf7498d6b1,https://github.com/apache/logging-log4j2/commit/151f7a6a902eef57a30cac6822387daf7498d6b1,Revamp the `Lookups` page  We revamp the `Lookups` page by:  * Adding information on different evaluation contexts supported by lookups. * Adding a cheatsheet on which lookup is available in which evaluation context. E.g.  using `$${sys:something}` on a per-event level is a major performance waster. * Removing the description of the broken `jvmrunargs` lookup (see #2726). * Improving the description of the `main` lookup by providing a code example that does not fail if the user switches logging implementations. * Shortening and improving the description of other lookups.
apache,logging-log4j2,91a8178568301c3ed5ab66c7f3603e995cc69fa9,https://github.com/apache/logging-log4j2/commit/91a8178568301c3ed5ab66c7f3603e995cc69fa9,New ScopedContextMap implementation for better performance
Creators-of-Create,Create,a8a160c666a002d10ce30f678ceec7e60726f9bb,https://github.com/Creators-of-Create/Create/commit/a8a160c666a002d10ce30f678ceec7e60726f9bb,Frame perfect  - Re-enable AO on fluid pipes to restore smooth lighting on them - Boost sounds in stock keeper screen - Tweak item frame transforms of creates components
Creators-of-Create,Create,3e10dbe4bcc3a8e2c447010915f9acaadce77771,https://github.com/Creators-of-Create/Create/commit/3e10dbe4bcc3a8e2c447010915f9acaadce77771,Perfect soup II
Creators-of-Create,Create,20491b3caa061b690fb249110819f36548ced19d,https://github.com/Creators-of-Create/Create/commit/20491b3caa061b690fb249110819f36548ced19d,Perfect soup
Creators-of-Create,Create,c67b0536bcfc6da26ce13d841c2f6499cd318e01,https://github.com/Creators-of-Create/Create/commit/c67b0536bcfc6da26ce13d841c2f6499cd318e01,Formatted perfectly  - Format code with some basic inspections
Creators-of-Create,Create,4c5688a055b4a4c8dde051ba9a5c77b1a41bbed3,https://github.com/Creators-of-Create/Create/commit/4c5688a055b4a4c8dde051ba9a5c77b1a41bbed3,Cheap perf  - Replace SmartBlockEntity#behaviours with a Reference2ObjectArrayMap for faster iteration and because BehaviorTypes are unique by reference - SmartBlockEntityTicker#tick improvements - Client: 9% of all -> 7% of all - Server: 12% of all -> 10% of all
Creators-of-Create,Create,46ad8ebbd59ae16e38996947f137dcb8312cfac0,https://github.com/Creators-of-Create/Create/commit/46ad8ebbd59ae16e38996947f137dcb8312cfac0,Conveyable performance  - Make chain conveyor visual render packages and guards - Chain rendering is already pretty efficient - Tick box visuals in the ChainConveyorVisual if flywheel is enabled
Creators-of-Create,Create,1038b76d38f2bb4534eb86a7355306aed0b77d4c,https://github.com/Creators-of-Create/Create/commit/1038b76d38f2bb4534eb86a7355306aed0b77d4c,Performance improvements in several tick methods (#6697)  * Performance improvements in several tick methods  Avoid capturing lambdas  streams  and Set#removeAll  * Update ServerSchematicLoader to not modify activeUploads while iterating in tick  * Replace iterator with enhanced for loop
Creators-of-Create,Create,9dfbe061def61b99b03b5bbf66a349323709fd91,https://github.com/Creators-of-Create/Create/commit/9dfbe061def61b99b03b5bbf66a349323709fd91,Preserving BakedModel and switching to ItemRenderer.render instead of ItemRenderer.renderStatic for performance
LinShunKang,MyPerf4J,89bb29fbeb5af7d28904f4194013aae7d195f447,https://github.com/LinShunKang/MyPerf4J/commit/89bb29fbeb5af7d28904f4194013aae7d195f447,Merge pull request #106 from LinShunKang/feature/support_springboot3  [MyPerf4J-92]: Support SpringBoot 3.x
AxonFramework,AxonFramework,a902cdc63b00c039ec881da823435164d3504010,https://github.com/AxonFramework/AxonFramework/commit/a902cdc63b00c039ec881da823435164d3504010,Fixes subscription query update permits issue  The permits were based on the configuration of Query permits  rather than the updateBufferSize which can be provided by clients based on their expectations and performance characteristics.  This commit ensures that the updateBufferSize is respected.
AxonFramework,AxonFramework,3f5475e62352c80563d7215b75c994b988d96483,https://github.com/AxonFramework/AxonFramework/commit/3f5475e62352c80563d7215b75c994b988d96483,feat: perform schema compatibility checks  add configuration options
AxonFramework,AxonFramework,be29adf40afe5d3ffac5c57a28d0395a1cfa8361,https://github.com/AxonFramework/AxonFramework/commit/be29adf40afe5d3ffac5c57a28d0395a1cfa8361,Merge pull request #3163 from AxonFramework/enhancement/add-reentrant-lock  Wrap `SinksManyWrapper#performWithBusyWaitSpin` in `ReentrantLock` to improve performance of Subscription Query Updates
AxonFramework,AxonFramework,4ecdc091427e16c0dca103e720e087f9b7b63c83,https://github.com/AxonFramework/AxonFramework/commit/4ecdc091427e16c0dca103e720e087f9b7b63c83,Adjust the performWithBusyWaitSpin to use sleep() i.o. yield()  Adjust the performWithBusyWaitSpin to use sleep() i.o. yield(). This is done as a test to verify if the yield() operation is ignored in certain environments. Yield does not guarantee that anything occurs  as it's a simple hint  while sleep is a specific command. Drawbacks are the necessity to catch the InterruptedException inside the loop  of course.  #enhancement/replace-yield-for-short-sleep
AxonFramework,AxonFramework,c68358e9c4f41e09a8bc5c8908d618767fc9dcb1,https://github.com/AxonFramework/AxonFramework/commit/c68358e9c4f41e09a8bc5c8908d618767fc9dcb1,Code optimizations  Implemented things which were mentioned in the pull request: * deactive the unit test which tests the performance * add debug log if an accessor method returns void * updated the copyright from 2010-2018 to 2010-2024 in one test
springdoc,springdoc-openapi,189151fae2d05c2f023c8f99d014b9ed8832acfa,https://github.com/springdoc/springdoc-openapi/commit/189151fae2d05c2f023c8f99d014b9ed8832acfa,Merge branch 'ML-Marco-performance-controller-advice'
springdoc,springdoc-openapi,9ea87b0dcbb566f126262b34de0c8fa5471b97b6,https://github.com/springdoc/springdoc-openapi/commit/9ea87b0dcbb566f126262b34de0c8fa5471b97b6,Merge branch 'ML-Marco-performance-controller-advice'
springdoc,springdoc-openapi,520f514fe149e29312c3a388574ef506d1c92ef4,https://github.com/springdoc/springdoc-openapi/commit/520f514fe149e29312c3a388574ef506d1c92ef4,Merge branch 'performance-controller-advice' of https://github.com/ML-Marco/springdoc-openapi into ML-Marco-performance-controller-advice
springdoc,springdoc-openapi,93af2e60bab7f701013a8571fce58382729383c8,https://github.com/springdoc/springdoc-openapi/commit/93af2e60bab7f701013a8571fce58382729383c8,Merge branch 'performance-controller-advice' of https://github.com/ML-Marco/springdoc-openapi into ML-Marco-performance-controller-advice
springdoc,springdoc-openapi,02ed05f9ba62275334f4d3609f22ad4acc67bc4a,https://github.com/springdoc/springdoc-openapi/commit/02ed05f9ba62275334f4d3609f22ad4acc67bc4a,Improve performance of getGenericMapResponse
springdoc,springdoc-openapi,47ca1e3679260b06ddb08893bd7853e890802e77,https://github.com/springdoc/springdoc-openapi/commit/47ca1e3679260b06ddb08893bd7853e890802e77,Improve performance of getGenericMapResponse
Netflix,concurrency-limits,53eee348ab55bf1f0c2b6a8a7782642f741a1f0e,https://github.com/Netflix/concurrency-limits/commit/53eee348ab55bf1f0c2b6a8a7782642f741a1f0e,perf: WindowedLimit allow non-updating threads to proceed when updating (#205)
TNG,ArchUnit,4b536129e584203378218cf149bd8ca6a4c39f53,https://github.com/TNG/ArchUnit/commit/4b536129e584203378218cf149bd8ca6a4c39f53,Improve documentation of slices rules (#1454)  [The user guide's slices documentation](https://www.archunit.org/userguide/html/000_Index.html#_slices) had a comment that didn't perfectly match the code: ```java // checks all subpackages of 'myapp' for cycles SlicesRuleDefinition.slices().matching("..myapp.(**)").should().notDependOnEachOther() ```  In addition  the involved methods could use some more JavaDoc: * [`matching(String)`](https://javadoc.io/static/com.tngtech.archunit/archunit/1.4.0/com/tngtech/archunit/library/dependencies/SlicesRuleDefinition.Creator.html#matching(java.lang.String)) * [`notDependOnEachOther()`](https://javadoc.io/static/com.tngtech.archunit/archunit/1.4.0/com/tngtech/archunit/library/dependencies/syntax/SlicesShould.html#notDependOnEachOther())
TNG,ArchUnit,b5e080a407dafb5eadda1c1a4914835c3e56f7d3,https://github.com/TNG/ArchUnit/commit/b5e080a407dafb5eadda1c1a4914835c3e56f7d3,Improve performance for transitive dependency checks (#1381)  `TransitiveDependencyCondition` internally calls `contains()` recursively on the collection of all objects to be tested. If this collection is a large list and there are enough recursive calls to `getDirectDependencyTargetsOutsideOfAnalyzedClasses()` this results in a heavy performance impact. On a reasonable large project a single test using that condition may take minutes to complete.  Here is a 30 seconds FlameGraph taken while an transitive check was running for > 2 minutes:   ![FlameGraph_30s](https://github.com/user-attachments/assets/fe4096b9-c6f7-4d2f-a448-1bf17f5802b5)  Based on the samples  the CPU hangs in [this filter lamdba](https://github.com/TNG/ArchUnit/blob/main/archunit/src/main/java/com/tngtech/archunit/lang/conditions/TransitiveDependencyCondition.java#L91) for > 86% of the time:  - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (47 247 323 samples  0.03%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (44 447 761 samples  0.03%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (10 837 882 731 samples  7.32%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (13 262 127 759 samples  8.96%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (12 668 650 362 samples  8.56%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (15 368 403 186 samples  10.38%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (46 224 364 samples  0.03%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (47 314 101 samples  0.03%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (18 048 208 277 samples  12.19%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (13 405 921 387 samples  9.06%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (93 152 524 samples  0.06%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (3 244 023 882 samples  2.19%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (919 865 902 samples  0.62%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (6 438 577 874 samples  4.35%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (6 760 263 856 samples  4.57%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (7 031 313 250 samples  4.75%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (7 723 048 585 samples  5.22%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (11 918 050 716 samples  8.05%) - TransitiveDependencyCondition$$Lambda$553.0x000000080121ca88:::test (46 026 822 samples  0.03%)  So  converting the given list to a Set with much better `contains()` performance fixes this issue.
TNG,ArchUnit,9c6adcb00d25ae8a2a23ed40edbff27ef163a9d9,https://github.com/TNG/ArchUnit/commit/9c6adcb00d25ae8a2a23ed40edbff27ef163a9d9,Improve performance for transitive dependency checks  `TransitiveDependencyCondition` internally calls `contains()` recursively on the collection of all objects to be tested. If this collection is a large list and there are enough recursive calls to `getDirectDependencyTargetsOutsideOfAnalyzedClasses()` this results in a heavy performance impact. On a reasonable large project a single test using that condition may take minutes to complete. Converting the given list to a Set with much better `contains()` performance fixes this issue.  on-behalf-of: @e-solutions-GmbH <info@esolutions.de> Signed-off-by: To6i <To6i@users.noreply.github.com>
eclipse-openj9,openj9,eed4ec4e8d76f4fa0dc25cd263582a2786849bcd,https://github.com/eclipse-openj9/openj9/commit/eed4ec4e8d76f4fa0dc25cd263582a2786849bcd,Update doPrivilegedWithCombinerHelper function  When we try to invoke doPrivilegedWithCombiner function to perform a privileged action under an existing context environment  we are used to construct a new context but ignore the parent context.  We should take consideration of a combination of the current and parent context  rather than just choose either the current or the parent.  This patch solves a failed case in issue #19499.  Issue: #19499  Signed-off-by: Jinhang Zhang <Jinhang.Zhang@ibm.com>
eclipse-openj9,openj9,6f9e4d2b94c2b52262f4e40ede329c756169aedb,https://github.com/eclipse-openj9/openj9/commit/6f9e4d2b94c2b52262f4e40ede329c756169aedb,Update ClassLoader.findNative to support JEP 472  A new version of ClassLoader.findNative has been introduced  which accepts the Java method name and the Java class where the native method is declared as input parameters. It performs native access checks to comply with JEP 472 (Prepare to Restrict the Use of JNI).  Related: #19680 Related: #20354  Signed-off-by: Babneet Singh <sbabneet@ca.ibm.com>
eclipse-openj9,openj9,c0c04fc18302811c3330f4ea2ef0ce5f2a825b95,https://github.com/eclipse-openj9/openj9/commit/c0c04fc18302811c3330f4ea2ef0ce5f2a825b95,Remove synchronized blocks in Thread isAlive and isDead methods  Made threadRef volatile so that the most up-to-date value is seen by all the threads.  threadRef = NO_REF assignment will happen once  and similarly  the started field is also set once during initialization.  A user of these methods will probably invoke them multiple times until the desired result is achieved. A delay in getting the most up-to-date value should not hinder the functionality of these methods.  To sum up  removing synchronization in these methods improves perf by reducing lock contention without hindering the functionality.  Reference to PR 1FJMO7Q has been removed and a comment related to ThreadGroup has been updated since the code seems to have evolved since the comments were written.  Related: #20414  Signed-off-by: Babneet Singh <sbabneet@ca.ibm.com>
eclipse-openj9,openj9,7789c85a4006c1291e37ed012ebf3a3e45338a8d,https://github.com/eclipse-openj9/openj9/commit/7789c85a4006c1291e37ed012ebf3a3e45338a8d,Disable use of method handle for core reflection in JDK21  In JEP 416  core reflection was re-implemented with Method Handles with the purpose of simplifying adding new language features. While analyzing performance of workload based on large enterprise based application  I observed that this was causing visible performance degradation. Disabling the use of direct method handles for reflection calls while we are working on improving the performance with direct method handles.  Signed-off-by: Rahil Shah <rahil@ca.ibm.com>
apache,fury,50d3cb64b6473d33f4050c9bae5337d29dc14a7c,https://github.com/apache/fury/commit/50d3cb64b6473d33f4050c9bae5337d29dc14a7c,feat(java): row encoder supports custom types and collections (#2243)  ## What does this PR do?  Extend Java Row Format to allow registering custom datatypes (e.g. UUID as Int128) and collection factories (e.g. `SortedSet<UUID>` as `new TreeSet<UUID>(customComparator)` ) Additionally supports arrays of custom types e.g. `UUID[]`  Since the type inference is in `fury-core` but I wanted to keep new features scoped to `fury-format`  I had to add a small plugin interface to core so that format can add types dynamically without affecting existing core behavior.  ## Related issues  https://github.com/apache/fury/issues/2208  ## Does this PR introduce any user-facing change?  - [x] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  The `Encoders` class has new `registerCustomCodec` and `registerCustomCollectionFactory` methods. All custom types are written with the existing protocol as embedded memory buffers just like any other field  but with a custom byte representation  so there should be no wire compatibility concerns.  ## Benchmark  There should be no change to performance in existing use cases. The code is carefully written to have no runtime impact if not used. Custom types are invoked via static methods or instance method on static final fields  which should be easily inlined by jit for minimum overhead.  Here is example generated code to help show this:  https://gist.github.com/stevenschlansker/ed7dae863e78d3c87e30bdea39fa8dea
apache,fury,6613de0fb387de7446c1e6cd52d62b9b776be821,https://github.com/apache/fury/commit/6613de0fb387de7446c1e6cd52d62b9b776be821,fix(java): use serialization binding (#2241)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,d4380ea1db5e1beb0beb868c4d03f65cb8711e05,https://github.com/apache/fury/commit/d4380ea1db5e1beb0beb868c4d03f65cb8711e05,feat(java): Support furyField nullable in codeGen pattern (#2191)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues #2169 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,ea979a1b850a825e0171efe70e58d04f8a736497,https://github.com/apache/fury/commit/ea979a1b850a825e0171efe70e58d04f8a736497,feat(java): add DescriptorBuilder for easy build and copying Descriptor (#2229)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues #2222 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,16459abba585850a0399be4d82987da7f2acb2e6,https://github.com/apache/fury/commit/16459abba585850a0399be4d82987da7f2acb2e6,feat(java): support trackingRef in furyField (#2168)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues #2167 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,b9b22b2aa7d6f6b85341e89d46251cba70e3b180,https://github.com/apache/fury/commit/b9b22b2aa7d6f6b85341e89d46251cba70e3b180,feat(java): type meta encoding for xlang in java (#2197)  ## What does this PR do?  This PR implements type meta encoding for xlang in java and refined the tyoe meta for java based new spec in #2216  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,541f11e3afa58dd956fc679a6ef266d6aa57fd30,https://github.com/apache/fury/commit/541f11e3afa58dd956fc679a6ef266d6aa57fd30,fix(java): fix field super class missing in compatible mode (#2214)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #2210  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,77896a044f3dcb9c5842174a5c23e6c7016979c3,https://github.com/apache/fury/commit/77896a044f3dcb9c5842174a5c23e6c7016979c3,feat(java): add protobuf serializer for message and byte string (#2213)  ## What does this PR do?  add protobuf serializer for message and byte string  ## Related issues  #1945  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a2dab1d9411aafda2bb28a5eb8fb1bbb77271083,https://github.com/apache/fury/commit/a2dab1d9411aafda2bb28a5eb8fb1bbb77271083,fix(java): ensure FuryObjectInputStream.read never returns 0 when length>0 #2204 (#2205)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  Ensure FuryObjectInputStream.read never returns 0 when length>0  so that when outer logic attempts to read in a while loop  it won't get stuck or throw exception.  ## Related Issues #2204  ## Does this PR introduce any user-facing change?  No.  ## Benchmark  No measurable performance impact is expected.  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,dd3edef18cf986d225600e0669d1cebd2fb0c8d7,https://github.com/apache/fury/commit/dd3edef18cf986d225600e0669d1cebd2fb0c8d7,feat: add Dart to Language enums across all implementations (#2187)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on Fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of PR titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md). - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  This PR adds Dart as a recognized language across all Fury language implementations. It modifies the Language enums/constants in Java  Python  Go  JavaScript  Rust  and Dart codebases to include Dart as a peer language.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach benchmark data here. -->
apache,fury,7c2eac1e99362df555e69c8d4382037423b7b3ca,https://github.com/apache/fury/commit/7c2eac1e99362df555e69c8d4382037423b7b3ca,fix(java): ensure readVarUint36Small reads full bits regardless of remaining buffer size (#2179)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of PR titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  This PR fixes the inconsistent behavior of `readVarUint36Small()` when reading from buffers of different remaining sizes. - Aligns the fast path and slow path so both read the full 36 bits. - Updates the bit‐extraction in the slow path (`readVarUint36Slow()`) to match the fast path’s handling of the high‐order bits.  ## Related issues  - #2110  ## Does this PR introduce any user‐facing change?  No user‐facing changes are introduced. - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  No measurable performance impact is expected  as the fix only adjusts bit extraction logic without altering code paths. Benchmark
apache,fury,3a8d478f21291e6abd15bc8b67d3e8275c359850,https://github.com/apache/fury/commit/3a8d478f21291e6abd15bc8b67d3e8275c359850,fix(java): fix nested map chunk serialization codegen (#2172)  ## What does this PR do?  fix nested map chunk serialization codegen ## Related issues  Closes #2170  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,6376e5275524a0254772d52e305b3a1556aef2ca,https://github.com/apache/fury/commit/6376e5275524a0254772d52e305b3a1556aef2ca,feat(java): FuryField annotation hints for struct serialization (#2036)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues #1956 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,a3da498344cfb85ab79b8cd518671406e093b857,https://github.com/apache/fury/commit/a3da498344cfb85ab79b8cd518671406e093b857,feat(java): support enum/time/array final types in xlang serialization (#2164)  ## What does this PR do?  support more final types in xlang serialization: - enum - timestamp/date - primitive array  ## Related issues  Closes #2163  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,2a75704f7dd9f4d9547d5dbcc624b099313fee63,https://github.com/apache/fury/commit/2a75704f7dd9f4d9547d5dbcc624b099313fee63,fix(java): fix xlang container field deserialization type error (#2161)  ## What does this PR do?  fix xlang container field deserialization type error  ## Related issues  Closes #2105  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,b3196e39a46c26b1e49f144283b1f22a398e2b7c,https://github.com/apache/fury/commit/b3196e39a46c26b1e49f144283b1f22a398e2b7c,feat(java): unify java and xlang object serialization (#2146)  ## What does this PR do?  This PR unify the java and xlang object serialization in java: - Remove StructSerializer - Unify struct hash compute between java and python - Align fields sort between java and python - unify the java and xlang object serialization in java  ## Related issues    ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,8ce3953f99ceea71b5b21ad9992e00422d72d810,https://github.com/apache/fury/commit/8ce3953f99ceea71b5b21ad9992e00422d72d810,fix(java): fix fury logger log exception (#2153)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #2152  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,30935d019bf7221677f0e39a7733903a774d9330,https://github.com/apache/fury/commit/30935d019bf7221677f0e39a7733903a774d9330,chore(java): Update the content that needs to be corrected when reading the code. (#2143)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Signed-off-by: Liangliang Sui <coolsui.coding@gmail.com>
apache,fury,3907c0651304efcacb633cdadc7227f86e14881f,https://github.com/apache/fury/commit/3907c0651304efcacb633cdadc7227f86e14881f,chore(java): use the SHA256_HASH field value directly. (#2144)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. --> 1. Directly use the `DisallowedList#DISALLOWED_LIST_TXT_PATH` value in `DisallowedListTest#testCalculateSHA256`. 2. Directly use the `DisallowedList#SHA256_HASH` value in `DisallowedListTest#testCalculateSHA256`.  Reduce manual maintenance costs :smile:  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Signed-off-by: Liangliang Sui <coolsui.coding@gmail.com>
apache,fury,c5ab2a2f1d934312bebb253c8b5b698aacb94f25,https://github.com/apache/fury/commit/c5ab2a2f1d934312bebb253c8b5b698aacb94f25,fix(java): fix DisallowedList calculate hash in Windows (#2142)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do? It appears that [PR #2128](https://github.com/apache/fury/pull/2128) did not truly resolve this issue  as it only modified the test case code without addressing the actual runtime logic. This PR re-fixes the problem. <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,c19bafd4c95fb57896da56c12367a8104819f31e,https://github.com/apache/fury/commit/c19bafd4c95fb57896da56c12367a8104819f31e,refactor(java): move methods from object serializer to abstract object serializer (#2140)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,13cfe6cedc9cfb41f86e11d3fe2790678fdeb5d2,https://github.com/apache/fury/commit/13cfe6cedc9cfb41f86e11d3fe2790678fdeb5d2,refactor(java): refactor object serializer for unifying xlang/java serialization in java (#2139)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,e2ca88e5f4e078758c89334efaee41b2b87a0252,https://github.com/apache/fury/commit/e2ca88e5f4e078758c89334efaee41b2b87a0252,fix(java): fix nested chunk map serialization error when generics exists (#2136)  ## What does this PR do?  fix nested chunk map serialization error when generics exists ## Related issues  Closes #2135  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a37cc5513dd60c1995faa0998ac8c185d8337ce6,https://github.com/apache/fury/commit/a37cc5513dd60c1995faa0998ac8c185d8337ce6,feat: xlang homogeneous collection serialization between java/python (#2130)  ## What does this PR do?  This PR implements homogeneous xlang collection serialization between java/python. Changes include:  - Xlang homogeneous  collection serialization in cython/python - Xlang chunk map serialization in pure python for debug - Use  homogeneous  collection serialization in java - homogeneous  collection serialization between java and python  ## Related issues  Closes #2131 Closes #2132  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,1236559d508e9cd0b4028582c29a5ff71f8611b1,https://github.com/apache/fury/commit/1236559d508e9cd0b4028582c29a5ff71f8611b1,fix(java): fix disallowed.txt check in windows (#2128)  ## What does this PR do? - add windows ci for java21 - Fix sha256 check error on windows:  ![Image](https://github.com/user-attachments/assets/b082fc7a-c929-42fc-a910-707f604251f4) ## Related issues  Closes #2100  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,681a308831e928227e04dcca86b295121f6674ec,https://github.com/apache/fury/commit/681a308831e928227e04dcca86b295121f6674ec,feat: xlang map chunk serialization between java/python (#2127)  ## What does this PR do?  This PR supports xlang map chunk serialization between java/python  ## Related issues  Closes #2125 Closes #2126  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,3aeccf0aed8d382deb767fb96984b40680ac3520,https://github.com/apache/fury/commit/3aeccf0aed8d382deb767fb96984b40680ac3520,feat(java): support inconsistent registration by name/id (#2120)  ## What does this PR do?   ## Related issues Closes #2119 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,d3691fcde21e412642062c2c3895899273d5aa51,https://github.com/apache/fury/commit/d3691fcde21e412642062c2c3895899273d5aa51,feat(java): support nested bean in array/collection/map for row format (#2116)  ## What does this PR do?  support nested bean in array/collection/map for row format  ## Related issues  Closes #2106  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,fcdf77875395dad17b5328c7f721e4adf1384e99,https://github.com/apache/fury/commit/fcdf77875395dad17b5328c7f721e4adf1384e99,fix(java): use registered id to sort fields (#2115)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #2093  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,22020323dfa1cff8ae29ca236c90ff7e78b2b667,https://github.com/apache/fury/commit/22020323dfa1cff8ae29ca236c90ff7e78b2b667,fix(java): fix not null value flag (#2114)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #2089  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,8df1c7f4d1f8ddb413f2f0a8d6c62996550a2950,https://github.com/apache/fury/commit/8df1c7f4d1f8ddb413f2f0a8d6c62996550a2950,feat(java): support passed tracking ref meta when building serializers (#2113)  ## What does this PR do?  This pr supports passed tracking ref meta when building serializers. The meta can be pssed by Type Annotation in #2036 or by classdef encoded into binary.  ## Related issues   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,594278ef23c46c50063e124033fce563a46ea099,https://github.com/apache/fury/commit/594278ef23c46c50063e124033fce563a46ea099,fix(java): fix serialization npe of collection with all null elems (#2111)  ## What does this PR do?  ## Related issues  Closes #2109  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,bc6a0b586aa2ab99e3a0059ee645c7b9901e92a3,https://github.com/apache/fury/commit/bc6a0b586aa2ab99e3a0059ee645c7b9901e92a3,perf(java): Refactor field sorting in StructSerializer to cache transformed field names and avoid redundant computation (#2091)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  This PR refactors the field sorting in `StructSerializer.java` by caching transformed field names to avoid redundant computations during sorting. The `lowerCamelToLowerUnderscore` transformation is now applied once per field instead of multiple times.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,fcdbebad94c7dd76dd66cd1888c06fe29f26fd53,https://github.com/apache/fury/commit/fcdbebad94c7dd76dd66cd1888c06fe29f26fd53,feat(java): use sha256 to check disallowed.txt tamper (#2102)  ## What does this PR do?  use sha256 to check disallowed.txt tamper ## Related issues  Closes #2100  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,676c5742b94ec30137cf282b7ebfd69186d9f8b6,https://github.com/apache/fury/commit/676c5742b94ec30137cf282b7ebfd69186d9f8b6,perf(java): Improve performance by using System.arraycopy to copy between byte arrays (#2101)  ## What does this PR do?  Try to use `System.arraycopy` when copying between byte arrays. This significantly increases performance  in our use case serialization is twice as fast (serializing/deserializing 1M complex strucutures using (de)serializeJavaObject in parallel). When using Platform.copyMemory  the JDK is doing a lot of type checking to see if it's a byte array  causing a visible slowdown. `Unsafe.memoryCopy` implementation was changed in Java 9 from a native method to a method calling the internal unsafe method containing the reflection checks.  There are a few places where System.arraycopy is already used  referring to it being faster on certain JDK implementations. Tested our use case on a macbook with arm64 cpu against zulu 21 and open-jdk 23.   ![image](https://github.com/user-attachments/assets/ce4a2c9d-e7b7-49ae-867d-68eadbe8a140)  ## Does this PR introduce any user-facing change?  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark I tried to reproduce the performance impact through one on existing benchmarks `UserTypeSerializeSuite.fury_serialize`  however I didn't see a difference here.  Signed-off-by: Seppe Volkaerts <seppevolkaerts@hotmail.com>
apache,fury,8ab006c3ea02dc2fd9dd2e57867f6c073a81bff2,https://github.com/apache/fury/commit/8ab006c3ea02dc2fd9dd2e57867f6c073a81bff2,perf(java): Refactor ThreadPoolFury to improve performance (#2092)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. --> After testing  it was observed that ​ThreadPoolFury​ experiences prolonged blocking during cold starts under high-concurrency scenarios. Analysis revealed that improper usage of locks in ​ClassLoaderFuryPooled​ was the root cause. This PR refactors the implementation of ​ClassLoaderFuryPooled​ by significantly reducing the granularity of locks  thereby drastically minimizing blocking time during cold starts.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 --> [](url)https://github.com/apache/fury/issues/2087 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. --> CPU:9950X Class:org.apache.fury.benchmark.ThreadPoolFurySuite.java old: Percentiles  ms/op: p(0.0000) =      0.001 ms/op p(50.0000) =      0.001 ms/op p(90.0000) =   1587.388 ms/op p(95.0000) =   1587.388 ms/op p(99.0000) =   1587.388 ms/op p(99.9000) =   1587.388 ms/op p(99.9900) =   1587.388 ms/op p(99.9990) =   1587.388 ms/op p(99.9999) =   1587.388 ms/op p(100.0000) =   1587.388 ms/op  new: Percentiles  ms/op: p(0.0000) =      0.001 ms/op p(50.0000) =      0.001 ms/op p(90.0000) =     62.746 ms/op p(95.0000) =     62.746 ms/op p(99.0000) =     62.746 ms/op p(99.9000) =     62.746 ms/op p(99.9900) =     62.746 ms/op p(99.9990) =     62.746 ms/op p(99.9999) =     62.746 ms/op p(100.0000) =     62.746 ms/op
apache,fury,ecc3347066c099438d8d5cd5abbe4f3687c3f830,https://github.com/apache/fury/commit/ecc3347066c099438d8d5cd5abbe4f3687c3f830,fix(java): Modify some mistake (#2086)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,6e5179230496f90cf75c1f6cfb728168cac405b7,https://github.com/apache/fury/commit/6e5179230496f90cf75c1f6cfb728168cac405b7,fix(java): fix ImmutableCollections$SubList duplicate registration (#2074)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do? fix ImmutableCollections$SubList duplicate registration  java.util.ImmutableCollections$SubList has been registered twice  resulting in different numbers of classIdGenerator across different JDK versions  which ultimately leads to serialization failures when crossing JDK versions. <!-- Describe the purpose of this PR. -->  ## Related issues Closes #2070 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: 吴怡帆 <wuyifan3@xiaomi.com>
apache,fury,670588f8564d425ab98f5eaf7bdf46374f01e945,https://github.com/apache/fury/commit/670588f8564d425ab98f5eaf7bdf46374f01e945,fix(java): java.util.Date and its subclasses are mutable (#2076)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  java.util.Date and its subclasses are mutable but in version <= 0.10.0 all TimeSerializers extends from ImmutableSerializer. This pr reorganized the inheritance relationships of TimeSerializers.Moved and added test case to adapte new version.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 --> [issus:](url)https://github.com/apache/fury/issues/2071  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,c531d84b5b6303c0b602b56ade7cc947a94d0ea2,https://github.com/apache/fury/commit/c531d84b5b6303c0b602b56ade7cc947a94d0ea2,perf(java): Optimize Computational Efficiency of MetaStringEncoder::encodeGeneric (#2072)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  In this update  the bit manipulation logic within the MetaStringEncoder::encodeGeneric method has been optimized for better performance. The original implementation processed the bits one at a time in each loop iteration  updating only a single bit per cycle. The new approach improves efficiency by processing multiple bits (up to one byte) per loop iteration  reducing the number of iterations required and minimizing the overhead of bit-level operations.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  | Test # | encodeGeneric (ms) | new encodeGeneric (ms) | |--------|--------------------|---------------------| | 1      | 102                | 27                  | | 2      | 85                 | 29                  | | 3      | 86                 | 29                  | | 4      | 81                 | 26                  | | 5      | 94                 | 33                  | | **Average** | **89.6**             | **28.8**             |   here is the simple testing code: ```java package org.example;  public class Main { private static int charToValueLowerSpecial(char c) { return 127; }  private static int charToValueLowerUpperDigitSpecial(char c) { return 127; }  static private byte[] encodeGeneric2(char[] chars  int bitsPerChar) { int totalBits = chars.length * bitsPerChar + 1; int byteLength = (totalBits + 7) / 8; byte[] bytes = new byte[byteLength]; int byteInd = 0; int bitInd = 1;  // Start from the second bit (the first is reserved for the flag) int charInd = 0; int charBitRemain = bitsPerChar;  // Remaining bits to process for the current character int mask; while (charInd < chars.length) { int charVal = (bitsPerChar == 5) ? charToValueLowerSpecial(chars[charInd]) : charToValueLowerUpperDigitSpecial(chars[charInd]); // Calculate how many bits are remaining in the current byte int nowByteRemain = 8 - bitInd; if (nowByteRemain >= charBitRemain) { // If the remaining bits in the current byte can fit the whole character value mask = (1 << charBitRemain) - 1;  // Create a mask for the bits of the character bytes[byteInd] |= (byte) ((charVal & mask) << (nowByteRemain - charBitRemain));  // Place the character bits into the byte bitInd += charBitRemain; if (bitInd == 8) { // Move to the next byte if the current byte is filled ++byteInd; bitInd = 0; } // Character has been fully placed in the current byte  move to the next character ++charInd; charBitRemain = bitsPerChar;  // Reset the remaining bits for the next character } else { // If the remaining bits in the current byte are not enough to hold the whole character mask = (1 << nowByteRemain) - 1;  // Create a mask for the current available bits in the byte bytes[byteInd] |= (byte) ((charVal >> (charBitRemain - nowByteRemain)) & mask);  // Place part of the character bits into the byte ++byteInd;  // Move to the next byte bitInd = 0;  // Reset bit index for the new byte charBitRemain -= nowByteRemain;  // Decrease the remaining bits for the character } }  boolean stripLastChar = bytes.length * 8 >= totalBits + bitsPerChar; if (stripLastChar) { // Mark the first byte as indicating a stripped character bytes[0] = (byte) (bytes[0] | 0x80); } return bytes; }  static private byte[] encodeGeneric(char[] chars  int bitsPerChar) { int totalBits = chars.length * bitsPerChar + 1; int byteLength = (totalBits + 7) / 8; // Calculate number of needed bytes byte[] bytes = new byte[byteLength]; int currentBit = 1; for (char c : chars) { int value = (bitsPerChar == 5) ? charToValueLowerSpecial(c) : charToValueLowerUpperDigitSpecial(c); // Encode the value in bitsPerChar bits for (int i = bitsPerChar - 1; i >= 0; i--) { if ((value & (1 << i)) != 0) { // Set the bit in the byte array int bytePos = currentBit / 8; int bitPos = currentBit % 8; bytes[bytePos] |= (byte) (1 << (7 - bitPos)); } currentBit++; } } boolean stripLastChar = bytes.length * 8 >= totalBits + bitsPerChar; if (stripLastChar) { bytes[0] = (byte) (bytes[0] | 0x80); } return bytes; }  static private boolean bytesEqual(byte[] bytes1  byte[] bytes2) { if (bytes1.length != bytes2.length) { return false; } for (int i = 0; i < bytes1.length; ++i) { if (bytes1[i] != bytes2[i]) { return false; } } return true; }  public static void main(String[] args) { test(); }  // test performance comparison encodeGeneric2 vs encodeGeneric public static void test() { char[] chars = new char[10000000]; for (int i = 0; i < chars.length; i++) { // random chars[i] = (char) (Math.random() * 127); } long start = System.currentTimeMillis(); byte[] bytes = encodeGeneric(chars  5); System.out.println("encodeGeneric: " + (System.currentTimeMillis() - start) + "ms"); start = System.currentTimeMillis(); byte[] bytes2 = encodeGeneric2(chars  5); System.out.println("encodeGeneric2: " + (System.currentTimeMillis() - start) + "ms"); assert bytes.length == bytes2.length && bytesEqual(bytes  bytes2); } } ```
apache,fury,23299daa7eb52a4fe6c5bcebcc2705673ce4d5e6,https://github.com/apache/fury/commit/23299daa7eb52a4fe6c5bcebcc2705673ce4d5e6,fix(java): fix read primitives error on fill buffer bound (#2064)  ## What does this PR do? fix read primitives error on fill buffer bound <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #2060 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,e33a1f71d32d767fb62e5b7da576ccada702116b,https://github.com/apache/fury/commit/e33a1f71d32d767fb62e5b7da576ccada702116b,fix(java): Fix error with `MemoryBuffer::readBytesAsInt64` when not in LITTLE_ENDIAN mode #2068 (#2069)    ## What does this PR do?  Fix the issue with `MemoryBuffer::readBytesAsInt64` when the system is not in LITTLE_ENDIAN mode.  ## Related issues - Fix #2068  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,bda04fe7a5ff763f000b7c3db1ca0e3cdd6dc833,https://github.com/apache/fury/commit/bda04fe7a5ff763f000b7c3db1ca0e3cdd6dc833,fix(java): fix read null chunk out of bound (#2065)  ## What does this PR do?  fix read null chunk out of bound  ## Related issues  Closes #2062  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,f23f71781fb7b83b20265118eadf6c7c91835c0a,https://github.com/apache/fury/commit/f23f71781fb7b83b20265118eadf6c7c91835c0a,feat(java): Add fastpath for collection/map serialize and deserialize (#2050)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  Add fastpath serialize and deserialize for hashmap and arraylist type  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,9e8316eb106444f04c1f9b11b61bc88466ff1ccb,https://github.com/apache/fury/commit/9e8316eb106444f04c1f9b11b61bc88466ff1ccb,feat(spec): remove polymorphic from type id (#2054)  ## What does this PR do?  polymorphic info is only used when serializing fields of a struct. This is not a generic type information  we should not include it in type id.    ## Related issues   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,e412cd46640e935715bcf196ff4ff1048837d845,https://github.com/apache/fury/commit/e412cd46640e935715bcf196ff4ff1048837d845,feat(java): support register type by name in java (#2053)  ## What does this PR do?  support register type by name in java   ## Related issues  Closes https://github.com/apache/fury/discussions/1969 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a80140aca047d6c3ced2b7f0ff090ed154429f68,https://github.com/apache/fury/commit/a80140aca047d6c3ced2b7f0ff090ed154429f68,feat(java): zstd meta compressor (#2042)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do? create zstd metacompressor as an option. let zstd access the src arr instead of copy to new array.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,b288a23e44d8d84dcf2eb9f884cdc679a552a8af,https://github.com/apache/fury/commit/b288a23e44d8d84dcf2eb9f884cdc679a552a8af,fix(java): fix duplicate entry write at max chunk size bound (#2040)  ## What does this PR do?  fix duplicate entry write at max chunk size bound ## Related issues  #2025 #2027  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,888920373463877bab4f4114fc90a780fbc175fe,https://github.com/apache/fury/commit/888920373463877bab4f4114fc90a780fbc175fe,feat(java): deserialize one pojo into another type (#2012)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do? replace class def if target class is different type with the actual serialized one  so it can be deserialized to another type #1998 <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: chaokunyang <shawn.ck.yang@gmail.com>
apache,fury,e952b63a2dadd4c903019ecca1deabac63a14a6e,https://github.com/apache/fury/commit/e952b63a2dadd4c903019ecca1deabac63a14a6e,feat(java): jit support for chunk based map serialization (#2027)  ## What does this PR do? This PR added jit support for chunk based map serialization  it supports all kinds of map serializaiton by generated code: -  final map key and value field type - polymorphic map key and value field type - nested map key and value type  This PR also removed the old map serialization protocol code.   The new chunk based protocol improve serialized size by **2.3X** at most.  data: ``` stringMap: {"k1": "v1"  "k2": "v2  ...  "k10": "v10" } intMap: {1:2  2:4  3: 6  ...  10: 20} ```  new protocol: ``` stringMapBytes 68 stringKVStructBytes 69 intMapBytes 28 intKVStructBytes 29 ```  old protocol: ``` stringMapBytes 104 stringKVStructBytes 87 intMapBytes 64 intKVStructBytes 47 ```  And improve performance by 20%   ## Related issues  Closes #925  #2025  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark   [chunk-jmh-result.csv](https://github.com/user-attachments/files/18575900/chunk-jmh-result.csv)  [nochunk-jmh-result.csv](https://github.com/user-attachments/files/18575901/nochunk-jmh-result.csv)   ![image](https://github.com/user-attachments/assets/754f8e48-b45e-489b-adf5-cca1c5d03f1e)
apache,fury,a907a9a3c70123de5ced40bc2e841c8ba346df8e,https://github.com/apache/fury/commit/a907a9a3c70123de5ced40bc2e841c8ba346df8e,fix(java): chunk map serialize an error (#2030)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  fix some error  new implement of map chunk can not pass all of the unit test in the org.apache.fury.serializer.collection.MapSerializersTest   ![image](https://github.com/user-attachments/assets/2af5978d-2ebc-4ad9-9ac2-f90511ed5c59)  ![image](https://github.com/user-attachments/assets/edc08cc4-495f-4cbe-9f48-0b8e5bcef10e) I found an error in line 298 of the AbstractMapSequencer file ## Related issues  <!-- Is there any related issue? Please attach here. none - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change? none <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark none <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Co-authored-by: hening <ninghe.hn@alibaba-inc.com>
apache,fury,2faede237f3362e5c9221c5a272e0cd927bd8f46,https://github.com/apache/fury/commit/2faede237f3362e5c9221c5a272e0cd927bd8f46,feat(java): support streaming encode/decode to/from buffer for row format (#2024)  ## What does this PR do?  support streaming encode/decode to/from buffer for row format  ## Related issues  Closes #2019  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,7fd582a587b6ffbe79629b944f300057884b216c,https://github.com/apache/fury/commit/7fd582a587b6ffbe79629b944f300057884b216c,feat(java): Chunk by chunk predictive map serialization protocol (#1722)  ## What does this PR do?  Implement chunk based map serialization in #925. This pr doesn't provide JIT support  it will be implemented in later PR.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #925  -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: hening <ninghe.hn@alibaba-inc.com> Co-authored-by: chaokunyang <shawn.ck.yang@gmail.com>
apache,fury,b952bf1e067bdd0821a9c3c88a8f04924c486186,https://github.com/apache/fury/commit/b952bf1e067bdd0821a9c3c88a8f04924c486186,feat(java): make 4 bytes utf16 size header optional for utf8 encoding (#2010)  ## What does this PR do?  Currently fury serialize utf8 string in java will write num bytes of utf16 first  so that the deserializaiton can save one copy. But C++ and golang does not need this information. This PR makes the 4 bytes utf16 size header optional for utf8 encoding  so theat the xlang serialiation can use the standard fury string serialization spec  and align to other languages.  For performance consideration  this PR introduce `writeNumUtf16BytesForUtf8Encoding` which can perserve current behaviour.  ## Related issues  #1890  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  This PR will introduce an extra copy for deserialization since we can't know the size of utf16 in advance before decoding utf8 string.
apache,fury,880c6e50e93889971eb9426515c19a68f78e9324,https://github.com/apache/fury/commit/880c6e50e93889971eb9426515c19a68f78e9324,feat(python): support latin1/utf16 string encoding in python (#1997)  ## What does this PR do?  Support support latin1/utf16 string encoding in python. For utf16  since python doesn't use surrogate pairs  this pr also added a vectorized surrogate pairs check function.  Note: - Python UCS-2 doesn't contains surrogate pairs  we must check utf16 first before contruct string from the binary. This is different from java/nodejs  ## Related issues  Closes #1967  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,d7ddd904f8805948c8dd25d8b469c9f720d7cc0c,https://github.com/apache/fury/commit/d7ddd904f8805948c8dd25d8b469c9f720d7cc0c,fix(java): Compatible mode on de/serialize api failed to deserialize (#1996)  ## What does this PR do? Read and write class data on COMPATIBLE mode for de/serializeJavaObject api.  When COMPATIBLE mode is on and need to serialize and deserialize different POJO  users are required to register classes those are going to be serialized or deserialized.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,3c1df17a1edd8b64196475c8c6a7db5acf2e502a,https://github.com/apache/fury/commit/3c1df17a1edd8b64196475c8c6a7db5acf2e502a,fix(java): Fix the issue caused by not using readCompressedBytesString during deserialization when string compression is enabled. (#1991)  Fix the issue caused by not using readCompressedBytesString during deserialization when string compression is enabled.  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  fix issue [#1984 ](https://github.com/apache/fury/issues/1984) <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,98efd72c2c740167edafd23d9d9c8c157095b780,https://github.com/apache/fury/commit/98efd72c2c740167edafd23d9d9c8c157095b780,feat(java/python): new xlang type system spec implementation (#1690)  ## What does this PR do?  This PR implements a new [type system](https://fury.apache.org/docs/specification/fury_xlang_serialization_spec/#type-systems) for xlang serialization between java and python.  The changes includes: - Refine type system spec: added new types: - named_enum: an enum whose value will be serialized as the registered name. - struct: a morphic(final) type serialized by Fury Struct serializer. - polymorphic_struct: a type which is not morphic(not final). i.e. it don't have subclasses. Suppose we're deserializing `List<SomeClass>`  we can save dynamic serializer dispatch if `SomeClass` is morphic(final). - compatible_struct: a morphic(final) type serialized by Fury compatible Struct serializer. - polymorphic_compatible_struct: a non-morphic(non-final) type serialized by Fury compatible Struct serializer. - named_struct: a `struct` whose type mapping will be encoded as a name. - named_polymorphic_struct: a `polymorphic_struct` whose type mapping will be encoded as a name. - named_compatible_struct: a `compatible_struct` whose type mapping will be encoded as a name. - named_polymorphic_compatible_struct: a `polymorphic_compatible_struct` whose type mapping will be encoded as a name. - ext: a type which will be serialized by a customized serializer. - polymorphic_ext: an `ext` type which is not morphic(not final). - named_ext: an `ext` type whose type mapping will be encoded as a name. - named_polymorphic_ext: an `polymorphic_ext` type whose type mapping will be encoded as a name. - Added a new XtypeResolver in java to resolve xlang types - Support register class mapping by id. Before this PR  we only support register class by name  which is more expensive at space/performance cost. - Support pass type into to resolve type ambiguation such as `ArrayList/Object[]` in java. Users can `serialize(List.of(1  2   3))` and deserialize it into array by `deserialize(bytes  Integer[].class)` - Refactor pyfury serialization by moving type resolver into python code from cython  this will make debug more easy and reduce code duplciation  it also speed serialization performance. - golang xtype serialization test are disabled  it will be reenabled after new type system is implemented in golang  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,1515f94c0013276f6cbb2f6b704b525fca036c2d,https://github.com/apache/fury/commit/1515f94c0013276f6cbb2f6b704b525fca036c2d,fix(java): only print warn message if scopedMetaShareEnabled is true … (#1985)  …and not in CompatibleMode  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  - https://github.com/quarkiverse/quarkus-fury/issues/51  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,b3f531cee934256c4b1f4fb929548d013b57e2e2,https://github.com/apache/fury/commit/b3f531cee934256c4b1f4fb929548d013b57e2e2,feat(java): configurable buffer size limit (#1963)  ## What does this PR do?  This PR introduces a new configuration option `bufferSizeLimitBytes` that replaces the hard-coded default of 128kb.  ## Related issues  #1950  ## Does this PR introduce any user-facing change?  The PR introduces a new configuration option `bufferSizeLimitBytes`.  - [x] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Discussion  This PR solves my problem  but I'm not sure if it is the right way to move forward. This is quite a low-level configuration option  but a potentially very important one. Every user whose average payload size is >=128kb  will need to increase this value for maximum performance. Maybe the default limit should be increased to something less conservative like 1MB  so fewer users will need to adjust this setting?
apache,fury,54b62fb6ab5d7e557131efe07c7402c885f6e7c4,https://github.com/apache/fury/commit/54b62fb6ab5d7e557131efe07c7402c885f6e7c4,feat(java): use varint for jdk compatible serializers (#1960)  ## What does this PR do?  use varint for jdk compatible serializers to reduce serialized size  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a3a22381a67451327f1b000cad780f730db951d7,https://github.com/apache/fury/commit/a3a22381a67451327f1b000cad780f730db951d7,fix(java): fix find constructor error in generated serializer class caused by duplicated class classloading for Fury (#1948)  ## What does this PR do?  fix duplicate classloading in parent classloader.  Some classloader such as flink classloader can load class from children classloader. If fury is located in children classloader  but we are serializing a class in such parent class  the parent class will load Fury class again  which caused two Fury clases loaded.  ## Related issues Closes #1947  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,8add13c7967735f2fbe45ad879f4e071db0faa55,https://github.com/apache/fury/commit/8add13c7967735f2fbe45ad879f4e071db0faa55,fix(java): ClassLoaderFuryPooled#setFactoryCallback cannot effect old Fury (#1946)    ## What does this PR do?  ClassLoaderFuryPooled#setFactoryCallback cannot effect old fury. so if `org.apache.fury.pool.FuryPooledObjectFactory#classLoaderFuryPooledCache` expired  new classLoaderFuryPooled can not effected by custom factoryCallback  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Co-authored-by: shuchang.li <shuchang.li@qunar.com>
apache,fury,5b22ccd035ea80f453a7483b407ced6a5401a738,https://github.com/apache/fury/commit/5b22ccd035ea80f453a7483b407ced6a5401a738,fix(java): Move schema caching to unsafe trait to avoid issues when using non-inferred schema. (#1944)    ## What does this PR do?  This PR removes the java specific `ExtField` class from the schema and moves the extData mechanism to the internal UnsafeTrait class. This is necessary because `ExtField` is only created internally from `inferSchema` method used by java  and we would potentially import or derive schemas from other sources (e.g XLANG  handwritten schema  arrow-native source) -- those schemas will be incompatible when we attempt to retrieve the cached schema. Removing schema-caching will fix the issue  but create allocations  so after some discussion  we decided to move the mechanism to the internal UnsafeTrait class.  This implementation makes changes internal API: - Derived classes from `UnsafeTrait` need to initialize the `extData` cache and define the number of extData slots needed. - The internal `getStruct` method needs to define which slot we use to retrieve `extData`.  Other:  - REVERTED: pom.xml for fury-format will automatically run tests with appropriate --add-opens flag for arrow  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change? N/A  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,2ed6adc2492d99e437d804474c1ffa3cb6c4ab59,https://github.com/apache/fury/commit/2ed6adc2492d99e437d804474c1ffa3cb6c4ab59,feat(java): ReplaceResolveSerializer deep copy (#1925)    ## What does this PR do? Adjusting the deep copy of ReplaceResolveSerializer.  obj -> writeReplace -> copy -> readResolve -> newObj  <!-- Describe the purpose of this PR. -->  ## Related issues https://github.com/apache/fury/issues/1849 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,785572daf60d166d7f9c884ec57c79b11bb4cad0,https://github.com/apache/fury/commit/785572daf60d166d7f9c884ec57c79b11bb4cad0,feat(scala): support scala native image build (#1922)  ## What does this PR do?  This PR supports scala native image build and fix quarkus graalvm build for java in https://github.com/quarkiverse/quarkus-fury/issues/7:  ``` Error: Class initialization of org.apache.fury.type.ScalaTypes failed. Use the option  '--initialize-at-run-time=org.apache.fury.type.ScalaTypes'  to explicitly request initialization of this class at run time. com.oracle.svm.core.util.UserError$UserException: Class initialization of org.apache.fury.type.ScalaTypes failed. Use the option  '--initialize-at-run-time=org.apache.fury.type.ScalaTypes'  to explicitly request initialization of this class at run time. at org.graalvm.nativeimage.builder/com.oracle.svm.core.util.UserError.abort(UserError.java:85) at org.graalvm.nativeimage.builder/com.oracle.svm.hosted.classinitialization.ClassInitializationSupport.ensureClassInitialized(ClassInitializationSupport.java:195) at .................. org.graalvm.nativeimage.builder/com.oracle.svm.hosted.classinitialization.ClassInitializationSupport.ensureClassInitialized(ClassInitializationSupport.java:177) ... 43 more Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: scala.collection.Iterable at org.apache.fury.reflect.ReflectionUtils.loadClass(ReflectionUtils.java:649) at org.apache.fury.type.ScalaTypes.<clinit>(ScalaTypes.java:40) ... 46 more Caused by: java.lang.ClassNotFoundException: scala.collection.Iterable at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526) at org.graalvm.nativeimage.builder/com.oracle.svm.hosted.NativeImageClassLoader.loadClass(NativeImageClassLoader.java:637) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526) at org.apache.fury.reflect.ReflectionUtils.loadClass(ReflectionUtils.java:646) ... 47 more ```  ## Related issues  Closes https://github.com/quarkiverse/quarkus-fury/issues/7  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,0201ade88eefc0660ddce813d66b46ecffe4d402,https://github.com/apache/fury/commit/0201ade88eefc0660ddce813d66b46ecffe4d402,feat(java): Improve error message on architecture not using little-endian format (#1918)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  previously there was this kind of error stacktrace: ``` Caused by: java.lang.IllegalArgumentException: false at org.apache.fury.util.Preconditions.checkArgument(Preconditions.java:52) at org.apache.fury.Fury.deserialize(Fury.java:765) at org.apache.fury.Fury.deserialize(Fury.java:815) at org.apache.fury.Fury.deserialize(Fury.java:808) at org.apache.camel.component.fury.FuryDataFormat.unmarshal(FuryDataFormat.java:79) ```  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Signed-off-by: Aurélien Pupier <apupier@redhat.com>
apache,fury,e08748177a1217faf8f9e84957ad7bef67817857,https://github.com/apache/fury/commit/e08748177a1217faf8f9e84957ad7bef67817857,fix(java): Fix incorrect results of utf16 to utf8 conversion for latin1 but not ascii characters (#1914)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  Fix incorrect results of utf16 to utf8 conversion for latin1 but not ascii characters  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,b222660278d75e4ef0779d96667692d8c55de032,https://github.com/apache/fury/commit/b222660278d75e4ef0779d96667692d8c55de032,fix(java): child container deep copy (#1911)    ## What does this PR do? fix child container bug   <!-- Describe the purpose of this PR. -->  ## Related issues https://github.com/apache/fury/issues/1855 <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a45886efe81bf223f6c33574c69e7769c8ab0aba,https://github.com/apache/fury/commit/a45886efe81bf223f6c33574c69e7769c8ab0aba,fix(java): ThreadLocalFury and ThreadPoolFury prioritize using the user classloader (#1907)    ## What does this PR do? ThreadLocalFury and ThreadPoolFury prioritize using the user-specified ClassLoader   <!-- Describe the purpose of this PR. -->  ## Related issues [1884](https://github.com/apache/fury/issues/1884) [1878](https://github.com/apache/fury/issues/1878) <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,39b919e04c5646d470e67681b31272368e8be91d,https://github.com/apache/fury/commit/39b919e04c5646d470e67681b31272368e8be91d,fix(java): NonExistentEnum on mode serializeEnumByName (#1904)    ## What does this PR do? Handle NonExistentEnum on mode serializeEnumByName by returning UNKNOWN. since there are no relevancy anymore by using enum ordinal.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,5bd3de5fb0e87274879e19a795bebc1148949af8,https://github.com/apache/fury/commit/5bd3de5fb0e87274879e19a795bebc1148949af8,feat(java): add option to treat enum as string (#1892)  ## What does this PR do? ### Current implementation For now fury read/write enum with ordinal approach by default. So when serializer serialize as 1  deserializer will return the 2nd enum (index 1)  #### For example : non updated library on serializer: ``` enum SearchMode { CHEAPEST  EARLIEST } ```  updated library on deserializer:  ``` enum SearchMode { RECOMMENDED  CHEAPEST  EARLIEST } ``` - if serializer serialize CHEAPEST  deserializer will return RECOMMENDED.  ___ ### New Option This PR will allow fury to treat enum as string so it can have more flexibility like JSON serializer. #### For example: non updated library on serializer: ``` enum SearchMode { CHEAPEST  EARLIEST } ``` updated library on deserializer: ``` enum SearchMode { RECOMMENDED  CHEAPEST  EARLIEST } ``` updated library with different class on deserializer: ``` enum SearchModeV2 { RECOMMENDED  CHEAPEST  EARLIEST } ``` - if serializer serialize CHEAPEST  deserializer will return CHEAPEST. - if serializer serialize CHEAPEST  deserializer for different class will still return CHEAPEST.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [x] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,d6698b0bab04288349626a4d4098edeb90992868,https://github.com/apache/fury/commit/d6698b0bab04288349626a4d4098edeb90992868,feat(scala): add scala range serializer (#1899)  ## What does this PR do?  add scala range serializer  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,14bad4225a45182895df711c721cd1c52db72f32,https://github.com/apache/fury/commit/14bad4225a45182895df711c721cd1c52db72f32,feat(java): support thread safe register callback for scala kotlin (#1895)  ## What does this PR do? support thread safe register callback for scala kotlin <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #1894  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,09abde8afe115f0691062998ad37fef5721ff14b,https://github.com/apache/fury/commit/09abde8afe115f0691062998ad37fef5721ff14b,feat(java): Refactor String serialization and deserialization (#1890)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #1868 Closes #1754  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: chaokunyang <shawn.ck.yang@gmail.com>
apache,fury,750a511d38df604c858f17b274b7843d5121a1ff,https://github.com/apache/fury/commit/750a511d38df604c858f17b274b7843d5121a1ff,feat(kotlin): Add unsigned array support and tests for arrays and strings (#1891)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  This PR adds tests for serializing/deserializing: - Strings (same as Java) - Primitive arrays in Kotlin (same as Java) - Array<T> in kotlin (same as Java) - Unsigned arrays in kotlin - `UByteArray`  `UIntArray`  `UShortArray`  `ULongArray`  Unsigned arrays in kotlin are currently marked experimental  and are subject to API changes (hence the annotations needed to suppress those warnings).  These types are implemented as a view over the signed arrays e.g. UByteArray is a view over ByteArray with contents reinterpreted as UByte  so serializers. The current implementation delegate to existing serializers for corresponding signed types.  The xlang type id is set to LIST for unsigned types.  ## Related issues  #683  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  Yes. Unsigned primitives no longer need to be registered for `fury-kotlin`.  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark N/A
apache,fury,027ddaadf9fe6160d9f01deb304afc7f6817bd41,https://github.com/apache/fury/commit/027ddaadf9fe6160d9f01deb304afc7f6817bd41,fix(java): fix add fury thread safety issue (#1889)  ## What does this PR do? fix add fury thread safety issue <!-- Describe the purpose of this PR. -->  ## Related issues #1840  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,d44e27dc0277da8350b6d3c03cbfb689673844ec,https://github.com/apache/fury/commit/d44e27dc0277da8350b6d3c03cbfb689673844ec,feat(kotlin): Add Unsigned Primitive Support (#1886)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  This PR adds unsigned primitive support to Kotlin Fury. It also adds tests for the standard kotlin primitives(supported by fury Java)  nullable primitive tests  boundary tests for unsigned serializers.  ## Related issues #683  ## Does this PR introduce any user-facing change?  Yes it adds Unsigned support for Kotlin. There's documentation new issue (should add something to document Kotlin!)  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark N/A  ---------  Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,c8698b65f976987edc189a537ca66bf42b9cfcdc,https://github.com/apache/fury/commit/c8698b65f976987edc189a537ca66bf42b9cfcdc,fix(java): fix async compilation switch for non-public nested class (#1883)  ## What does this PR do?  - fix async compilation switch for non-public nested class - fix install sbt for scala - fix install python  ## Related issues  Closes #1879  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,da57b79c2fa6fea93046dfda591b36ce579ae10b,https://github.com/apache/fury/commit/da57b79c2fa6fea93046dfda591b36ce579ae10b,feat(java): use SubListViewSerializer only when tracking ref (#1858)  ## What does this PR do?  use SubListViewSerializer only when tracking ref  ## Related issues Closes #1198  #1856  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,5c82798d252a1e23e2efbc92c8b0f993a5a64bf9,https://github.com/apache/fury/commit/5c82798d252a1e23e2efbc92c8b0f993a5a64bf9,feat(java): implement sublist serializers (#1856)  ## What does this PR do?  Since so many users serialize sublist  but we don't allow this type for serialization  this introduce some confustion. So I added sublist serializers in this PR.  ## Related issues Closes https://github.com/apache/fury/issues/281  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,12a6c83ad3705289a41c35767dbd523fff81278e,https://github.com/apache/fury/commit/12a6c83ad3705289a41c35767dbd523fff81278e,fix(java): fix jdk proxy serialization when proxy writePlace method (#1857)  ## What does this PR do?  fix jdk proxy serialization when proxy writePlace method ## Related issues  Closes #1854  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,3d559292233ae6734fef103703e6972eb39b7840,https://github.com/apache/fury/commit/3d559292233ae6734fef103703e6972eb39b7840,feat(scala): optimize scala class serialization (#1853)  ## What does this PR do?  - Optimize scala Iterable type serialization - Reduce type name writing for msot scala collection and factory types - Add serializers for ToFactory type ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,df5bd78b39149f4ee4ed0a7ee8f56a8342dd0b91,https://github.com/apache/fury/commit/df5bd78b39149f4ee4ed0a7ee8f56a8342dd0b91,perf(java): optimize read classdef perf (#1852)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,902594715659345cb85e92855506e6dabba6724d,https://github.com/apache/fury/commit/902594715659345cb85e92855506e6dabba6724d,perf(java): inline same element invoke in jit (#1851)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,89a8d012901c10981e9fe0b29ba06bfad6321716,https://github.com/apache/fury/commit/89a8d012901c10981e9fe0b29ba06bfad6321716,chore(java): simplify generated codec name (#1850)  ## What does this PR do?  simplify generated codec name  before:  ![image](https://github.com/user-attachments/assets/bbb51fe7-23d1-405a-89c4-e4d0c4c8ab60)  after:  ![image](https://github.com/user-attachments/assets/f3540884-89e7-4a81-9d04-d3dff66ff379)  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,1f1528fd832f9a346e3573c9cf7bed2fbd2be09a,https://github.com/apache/fury/commit/1f1528fd832f9a346e3573c9cf7bed2fbd2be09a,feat(java): support graalvm 17/21/22 (#1845)  ## What does this PR do?  - Support  graalvm 17/21/22 - Add ci for Fury graalvm 17/22 support  ## Related issues  https://github.com/apache/fury/pull/1813   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,8c45d959d1d11fca0b4534f8497c98cc1902fb7b,https://github.com/apache/fury/commit/8c45d959d1d11fca0b4534f8497c98cc1902fb7b,fix(java): Fix for maximum size of java arrays (#1843)  ## What does this PR do?  Fixes the maximum size of Java arrays using Integer.MAX_VALUE when it should be Integer.MAX_VALUE - 8. See this https://github.com/openjdk/jdk14u/blob/84917a040a81af2863fddc6eace3dda3e31bf4b5/src/java.base/share/classes/jdk/internal/util/ArraysSupport.java#L577 or https://www.baeldung.com/java-arrays-max-size  ## Related issues   - #1842  ## Does this PR introduce any user-facing change?  No  - [ ] Does this PR introduce any public API change? No - [ ] Does this PR introduce any binary protocol compatibility change? No  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  Co-authored-by: Arthur Finkelstein <arthur.finkelstein@instant-system.com>
apache,fury,8bbd35effac7e864bb7aa8fc7d61fc35699db2dd,https://github.com/apache/fury/commit/8bbd35effac7e864bb7aa8fc7d61fc35699db2dd,fix(java): fix serializer factory getSerializerClass (#1836)  ## What does this PR do? fix serializer factory getSerializerClass <!-- Describe the purpose of this PR. -->  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,bcc01d7a0f2d8b14b5a6018491613369c18d0a3b,https://github.com/apache/fury/commit/bcc01d7a0f2d8b14b5a6018491613369c18d0a3b,fix(java): fix long type name meta string encoding (#1837)  ## What does this PR do? fix long type name meta string encoding <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #1835 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,d648c2840fd414e74ea597874ea61c0b32a6497e,https://github.com/apache/fury/commit/d648c2840fd414e74ea597874ea61c0b32a6497e,fix(java): fix collection view serialization (#1833)  ## What does this PR do? fix collection view serialization <!-- Describe the purpose of this PR. -->  ## Related issues Closes #1831 Closes #1832  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,3cef53c340efc30dd8e8499e29128eb81212f144,https://github.com/apache/fury/commit/3cef53c340efc30dd8e8499e29128eb81212f144,fix(java): fix nested map field value serialization by private map serializer (#1820)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues Closes #1816  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,2f64ade0944193d108fda7fee6fe23a7fe308968,https://github.com/apache/fury/commit/2f64ade0944193d108fda7fee6fe23a7fe308968,fix(java): fix reserved keyword conflict (#1819)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues Closes #1818 ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,8d5f8f3da1e8217edcdc448ab83ff7b7efe6b073,https://github.com/apache/fury/commit/8d5f8f3da1e8217edcdc448ab83ff7b7efe6b073,fix(java): Fix replace resolver serializaiton (#1812)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  ## Related issues  Closes #1805 Closes #1804   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?  ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,fd4ba2e2cbb7da1d0c6752de20752290b9594cee,https://github.com/apache/fury/commit/fd4ba2e2cbb7da1d0c6752de20752290b9594cee,fix(scala): fix nested type serialization in scala object type (#1809)  ## What does this PR do?  <!-- Describe the purpose of this PR. -->   ## Related issues  Closes #1801   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,9a39cb3fc7a92e2c9546b094d60af62fc6d2e603,https://github.com/apache/fury/commit/9a39cb3fc7a92e2c9546b094d60af62fc6d2e603,feat(java): Support copy capabilities for some classes without no-argument constructors (#1794)  ## What does this PR do? Some classes with no-argument constructors will report an error when calling `copy()`.  This pr: - implement the copy method for the no-argument constructor serializer - add test case   ## Related issues https://github.com/apache/fury/issues/1777 https://github.com/apache/fury/issues/1679  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,497fe0af970c4a40d5b20e4323e9f9736171fdb4,https://github.com/apache/fury/commit/497fe0af970c4a40d5b20e4323e9f9736171fdb4,fix(java): fix classloader get npe (#1792)  ## What does this PR do?  fix classloader get npe  ## Related issues  Closes #1763   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,6aa768665ebf06d5b374f64f7097a890bbf27f1a,https://github.com/apache/fury/commit/6aa768665ebf06d5b374f64f7097a890bbf27f1a,chore(java): Disallow writing meta classdef when obj is null (#1686)    ## What does this PR do?  <!-- Describe the purpose of this PR. --> When obj is null  if shareMeta is enabled  no need to write meta classdef  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Signed-off-by: LiangliangSui <coolsui.coding@gmail.com> Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,407b65820fa3c1395b9643557faee30e39451f58,https://github.com/apache/fury/commit/407b65820fa3c1395b9643557faee30e39451f58,feat(java): ThreadSafeFury add getClassResolver method (#1780)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. --> Because using ThreadSafeFury cannot get ClassResolver  so I submitted this pr to increase some checklistener and other behavior  ## Related issues    ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,fd3976076a27f6f906c79bedb7c0c23a34a3abca,https://github.com/apache/fury/commit/fd3976076a27f6f906c79bedb7c0c23a34a3abca,feat(java): support deep ref copy (#1771)  ## What does this PR do?  support deep ref copy  ## Related issues  Closes #1747 https://github.com/apache/fury/issues/1679   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,f4865f097d1c21777c12fb326c139d48beb65855,https://github.com/apache/fury/commit/f4865f097d1c21777c12fb326c139d48beb65855,perf(java): optimize object array copy perf (#1770)  ## What does this PR do?  optimize object array copy perf  ## Related issues  Closes #1749   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  ``` Benchmark                             (bufferType)  (references)   Mode  Cnt        Score         Error  Units CopyBenchmark.fury_copy_object_array         array         false  thrpt    3  3738130.731 ± 2558439.249  ops/s CopyBenchmark.kryo_copy_object_array         array         false  thrpt    3  1840856.001 ± 3577628.201  ops/s  Benchmark                             (bufferType)  (references)   Mode  Cnt        Score         Error  Units CopyBenchmark.fury_copy_object_array         array         false  thrpt    3  8025674.789 ± 7659990.718  ops/s CopyBenchmark.kryo_copy_object_array         array         false  thrpt    3  1167037.754 ± 1372076.679  ops/s ```
apache,fury,4cdb9a2ad68d718d57c0dcabb187b2d18a61ae38,https://github.com/apache/fury/commit/4cdb9a2ad68d718d57c0dcabb187b2d18a61ae38,perf(java): optimize list copy perf (#1769)  ## What does this PR do?  optimize list copy perf  ## Related issues Closes #1743   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  ```  Benchmark                     (bufferType)  (references)   Mode  Cnt        Score         Error  Units CopyBenchmark.fury_copy_list         array         false  thrpt    3  3942934.726 ± 2361062.022  ops/s CopyBenchmark.kryo_copy_list         array         false  thrpt    3   910135.076 ±  914811.092  ops/s  Benchmark                     (bufferType)  (references)   Mode  Cnt        Score         Error  Units CopyBenchmark.fury_copy_list         array         false  thrpt    3  5797916.088 ± 3054882.337  ops/s CopyBenchmark.kryo_copy_list         array         false  thrpt    3   942358.419 ±  942454.986  ops/s ```
apache,fury,33eef02cfefc2a824418230df034b697e440e63f,https://github.com/apache/fury/commit/33eef02cfefc2a824418230df034b697e440e63f,perf(java): optimize map copy perf (#1767)  ## What does this PR do?  optimize map copy perf and add benchmark  ## Related issues  Closes #1744   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  ``` Benchmark                           (bufferType)  (references)   Mode  Cnt        Score         Error  Units CopyBenchmark.fury_copy_int_map            array         false  thrpt    3  1779502.276 ± 2305262.034  ops/s CopyBenchmark.fury_copy_string_map         array         false  thrpt    3  1845474.980 ± 2072340.777  ops/s CopyBenchmark.kryo_copy_int_map            array         false  thrpt    3   625471.456 ±  289827.215  ops/s CopyBenchmark.kryo_copy_string_map         array         false  thrpt    3   649970.300 ±  404700.716  ops/s  Benchmark                           (bufferType)  (references)   Mode  Cnt        Score         Error  Units CopyBenchmark.fury_copy_int_map            array         false  thrpt    3  2023724.754 ± 2067096.856  ops/s CopyBenchmark.fury_copy_string_map         array         false  thrpt    3  1979796.679 ±  593560.983  ops/s CopyBenchmark.kryo_copy_int_map            array         false  thrpt    3   569146.293 ±  461667.597  ops/s CopyBenchmark.kryo_copy_string_map         array         false  thrpt    3   591048.755 ±  669110.894  ops/s ```
apache,fury,40697ed5855830542d9c259cd91af90ff529c5a3,https://github.com/apache/fury/commit/40697ed5855830542d9c259cd91af90ff529c5a3,fix(java): fix enum abstract field serialization (#1765)  ## What does this PR do?  fix enum abstract field serialization  ## Related issues  Closes #1764  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,4f24bd4d196080cf8b85b3042cdd001b723a87d7,https://github.com/apache/fury/commit/4f24bd4d196080cf8b85b3042cdd001b723a87d7,fix(java): fix fury logger npe (#1762)  ## What does this PR do?   ## Related issues Closes #1761   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,6e4d8a0a746a8d471895afbdbda51caa4ea0e633,https://github.com/apache/fury/commit/6e4d8a0a746a8d471895afbdbda51caa4ea0e633,fix(java): fix big buffer streaming MetaShared read offset (#1760)  ## What does this PR do?  fix big buffer streaming MetaShared read by using relative offset  ## Related issues  Closes #1759  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,7b6e9ed5b5472f37bfcaf92ac517ade81c6d7a82,https://github.com/apache/fury/commit/7b6e9ed5b5472f37bfcaf92ac517ade81c6d7a82,chore(java): rename copyTrackingRef to copyRef (#1748)  ## What does this PR do?  rename copyTrackingRef to copyRef  ## Related issues #1679 #1747   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,1e2a52843ad2540eb0bd444d970f8cfa3cc8cee3,https://github.com/apache/fury/commit/1e2a52843ad2540eb0bd444d970f8cfa3cc8cee3,fix(java): fix streaming classdef read (#1758)  ## What does this PR do?  fix streaming classdef read  ## Related issues  Closes #1757  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,1a5c35788fdfcffbb518b44c77eb6c7cf7a3533d,https://github.com/apache/fury/commit/1a5c35788fdfcffbb518b44c77eb6c7cf7a3533d,feat(java): support Ignore inconsistent types deserialize (#1737)    ## What does this PR do?  <!-- Describe the purpose of this PR. -->   ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [x] Does this PR introduce any public API change? - [x] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: weijiang.wj <weijiang.wj@alibaba-inc.com> Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,eee65280990b10a4e875b6a17da8112cddea9c9e,https://github.com/apache/fury/commit/eee65280990b10a4e875b6a17da8112cddea9c9e,feat(java): support jdk17+ record copy (#1741)  ## What does this PR do?  This PR supports jdk17+ record copy  ## Related issues  #1739 #1701  Closes #1740  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,5e0b8a9c535c954132baad1cd0bf580824f7226d,https://github.com/apache/fury/commit/5e0b8a9c535c954132baad1cd0bf580824f7226d,perf(java): optimize pojo copy performance (#1739)  ## What does this PR do? optimize pojo copy performance by 1~4X and add copy benchmark  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  Before: ```java Benchmark                (bufferType)   (objectType)  (references)   Mode  Cnt        Score         Error  Units CopyBenchmark.fury_copy         array  MEDIA_CONTENT         false  thrpt    3  1294614.644 ± 2103796.392  ops/s CopyBenchmark.fury_copy         array         SAMPLE         false  thrpt    3  1909071.799 ± 2343118.356  ops/s CopyBenchmark.fury_copy         array         STRUCT         false  thrpt    3  1220680.635 ± 1019806.837  ops/s CopyBenchmark.fury_copy         array        STRUCT2         false  thrpt    3   584429.541 ±  111229.502  ops/s CopyBenchmark.kryo_copy         array  MEDIA_CONTENT         false  thrpt    3  1008490.635 ±  309047.316  ops/s CopyBenchmark.kryo_copy         array         SAMPLE         false  thrpt    3   921863.274 ± 1082442.180  ops/s CopyBenchmark.kryo_copy         array         STRUCT         false  thrpt    3  1336939.990 ±  795836.830  ops/s CopyBenchmark.kryo_copy         array        STRUCT2         false  thrpt    3   168367.000 ±  236966.711  ops/s ```  Java ```java Benchmark                (bufferType)   (objectType)  (references)   Mode  Cnt        Score         Error  Units CopyBenchmark.fury_copy         array  MEDIA_CONTENT         false  thrpt    3  2201830.808 ± 4640532.805  ops/s CopyBenchmark.fury_copy         array         SAMPLE         false  thrpt    3  4945272.027 ± 5429361.187  ops/s CopyBenchmark.fury_copy         array         STRUCT         false  thrpt    3  4809373.970 ± 6803285.896  ops/s CopyBenchmark.fury_copy         array        STRUCT2         false  thrpt    3  2577391.052 ± 6682601.210  ops/s CopyBenchmark.kryo_copy         array  MEDIA_CONTENT         false  thrpt    3   830059.189 ± 2509547.599  ops/s CopyBenchmark.kryo_copy         array         SAMPLE         false  thrpt    3   696901.072 ±  525070.309  ops/s CopyBenchmark.kryo_copy         array         STRUCT         false  thrpt    3   980635.311 ± 2495689.418  ops/s CopyBenchmark.kryo_copy         array        STRUCT2         false  thrpt    3   141996.627 ±  343339.930  ops/s ```
apache,fury,a8a140b41c744467239461d0d9742254ee035233,https://github.com/apache/fury/commit/a8a140b41c744467239461d0d9742254ee035233,perf(java): add struct benchmark with pb (#1736)  ## What does this PR do?  add struct benchmark with pb:  Perf: ``` Benchmark                       Mode  Cnt      Score      Error  Units fury_deserialize                thrpt   30  49667.900 ± 3004.061  ops/s fury_kv_compatible_deserialize  thrpt   30  33014.595 ± 3716.199  ops/s fury_kv_compatible_serialize    thrpt   30  23915.260 ± 3968.119  ops/s fury_serialize                  thrpt   30  63146.826 ± 2930.505  ops/s protobuf_deserialize            thrpt   30  14156.610 ±  685.272  ops/s protobuf_serialize              thrpt   30  10060.293 ±  706.064  ops/s ```   Lib | Size -- | -- fury | 8077 furystrict | 8009 furykv | 48028 protobuf | 18000  <br class="Apple-interchange-newline">    ![image](https://github.com/user-attachments/assets/f46a7e66-ae50-44ca-972c-4b176b38146c)  ![image](https://github.com/user-attachments/assets/74b59d30-028d-47a2-8499-0962ab44e20c)   ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a5fc14246a3d15d4742f2e5bcb920b2106d8d639,https://github.com/apache/fury/commit/a5fc14246a3d15d4742f2e5bcb920b2106d8d639,perf(java): optimize scoped meta share mode perf (#1734)  ## What does this PR do?  This PR optimizes scoped meta share mode writing perf by about 30%: - Replace ArrayList by ObjectArray  which can save `clear` cost - Speed up copy performance when writing classdefs  ## Related issues  #1733  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,c9705d1bd976f33a8e5f49431dc796853b069668,https://github.com/apache/fury/commit/c9705d1bd976f33a8e5f49431dc796853b069668,feat(java): enable scoped meta share for compatible mode by default (#1733)  ## What does this PR do?  This PR enable scoped meta share for compatible mode by default: - Enable scoped meta share for compatible mode by default - Extensive tests for scoped meta share mode - Support for graalvm  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,46d48c312b8f414498a6b9dd4b395d7017da5fc1,https://github.com/apache/fury/commit/46d48c312b8f414498a6b9dd4b395d7017da5fc1,chore(java): merge reflect.Types into TypeRef (#1731)  ## What does this PR do?  This PR moves `org.apache.fury.reflect.Types` into TypeRef. Types is used in Fury type system  by merge `org.apache.fury.reflect.Types`  we can reduce ambiguation in fury type system  ## Related issues #1553 #1690  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,0c9dcc2056b332e73584d2f7cddf3b412d0699c7,https://github.com/apache/fury/commit/0c9dcc2056b332e73584d2f7cddf3b412d0699c7,fix(java): fix fastjson object serialization (#1717)  ## What does this PR do?  Closes #1716  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a7c45f344bf65bea73e2f0cb9aeb17a3698bec1f,https://github.com/apache/fury/commit/a7c45f344bf65bea73e2f0cb9aeb17a3698bec1f,fix(java): fix nested map serialization codegen (#1713)  ## What does this PR do?  Closes #1700  ## Related issues     ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,09fda94ab7f476da4dc3b6752825b7249bda6ac2,https://github.com/apache/fury/commit/09fda94ab7f476da4dc3b6752825b7249bda6ac2,refactor(java): move latin language checker method from string serializer to string util (#1708)    ## What does this PR do?  <!-- Describe the purpose of this PR. --> This PR decouples and moves the `isLatin([])` method from `StringSerializer` class to `StringUtils`.   ## Related issues  <!-- Is there any related issue? Please attach here.  - #1703 - #xxxx1 - #xxxx2 --> #1703   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Co-authored-by: Shawn Yang <chaokunyang@apache.org>
apache,fury,f1651d6ed8fa389dd19ad766021e74869bf59d04,https://github.com/apache/fury/commit/f1651d6ed8fa389dd19ad766021e74869bf59d04,fix(java): return fury to pooled which get from (#1697)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. -->  incase of pooledCache expired from cache  and fury will return to another pooledCache. so we save pooledCache instead of get from cache.  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,31d37f9cf2021899f3dc2f1b06d5b45e79099251,https://github.com/apache/fury/commit/31d37f9cf2021899f3dc2f1b06d5b45e79099251,perf(java): Add ClassInfo ClassBytes generation conditions. (#1667)  <!-- **Thanks for contributing to Fury.**  **If this is your first time opening a PR on fury  you can refer to [CONTRIBUTING.md](https://github.com/apache/incubator-fury/blob/main/CONTRIBUTING.md).**  Contribution Checklist  - The **Apache Fury (incubating)** community has restrictions on the naming of pr titles. You can also find instructions in [CONTRIBUTING.md](https://github.com/apache/incubator-fury/blob/main/CONTRIBUTING.md).  - Fury has a strong focus on performance. If the PR you submit will have an impact on performance  please benchmark it first and provide the benchmark result here. -->  ## What does this PR do?  <!-- Describe the purpose of this PR. --> `ClassInfo#classNameBytes` and `ClassInfo#packageNameBytes` are only used when `classInfo.classId == NO_CLASS_ID && !metaContextShareEnabled`  ```java public void writeClass(MemoryBuffer buffer  ClassInfo classInfo) { if (classInfo.classId == NO_CLASS_ID) { // no class id provided. // use classname if (metaContextShareEnabled) { buffer.writeByte(USE_CLASS_VALUE_FLAG); // FIXME(chaokunyang) Register class but not register serializer can't be used with //  meta share mode  because no class def are sent to peer. writeClassWithMetaShare(buffer  classInfo); } else { // if it's null  it's a bug. assert classInfo.packageNameBytes != null; metaStringResolver.writeMetaStringBytesWithFlag(buffer  classInfo.packageNameBytes); assert classInfo.classNameBytes != null; metaStringResolver.writeMetaStringBytes(buffer  classInfo.classNameBytes); } } else { // use classId buffer.writeVarUint32(classInfo.classId << 1); } } ```   ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/incubator-fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Signed-off-by: LiangliangSui <coolsui.coding@gmail.com>
apache,fury,a2515a936b439129d93eb22acc5c63a23285f23b,https://github.com/apache/fury/commit/a2515a936b439129d93eb22acc5c63a23285f23b,fix(java): Fix header offset issue in MetaStringBytes hashcode (#1668)    ## What does this PR do?  <!-- Describe the purpose of this PR. --> MetaStringBytes `hashcode & 0xff`  that is  header  represents the encoding  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/incubator-fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->  ---------  Signed-off-by: LiangliangSui <coolsui.coding@gmail.com>
apache,fury,da5f8473818bc2768b4f3b5a42c39a2c7fff8120,https://github.com/apache/fury/commit/da5f8473818bc2768b4f3b5a42c39a2c7fff8120,feat(java): support meta compression by Deflater (#1663)  ## What does this PR do?  This PR support meta compression and add Deflater as default compressor.  In our test  it can compress meta by reduce size of **243** without introducing any performance cost:  ``` before: Fury | STRUCT | false | array | 1227 |  after STRUCT | false | array | 984 |  ```  ## Related issues #1660   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/incubator-fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,a0a6d7b7f494f167ac46592dfc8b2a8c4706c5f2,https://github.com/apache/fury/commit/a0a6d7b7f494f167ac46592dfc8b2a8c4706c5f2,feat(java): scoped meta share mode for type forward/backward compaibility (#1660)  ## What does this PR do?  This PR implements scoped meta share mode for type forward/backward compaibility  ## Related issues  #202   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/incubator-fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark Perf increased from `1900102.586` to `2430410.064` ``` Before:  Benchmark                                                       (bufferType)   (objectType)  (references)   Mode  Cnt        Score        Error  Units fury_deserialize                              array  MEDIA_CONTENT         false  thrpt   10  2734151.212 ± 253921.628  ops/s fury_deserialize_compatible                   array  MEDIA_CONTENT         false  thrpt   10  1900102.586 ±  62176.872  ops/s furymetashared_deserialize_compatible         array  MEDIA_CONTENT         false  thrpt   10  3011439.327 ± 260518.752  ops/s  After:  Benchmark                                                       (bufferType)   (objectType)  (references)   Mode  Cnt        Score        Error  Units fury_deserialize                              array  MEDIA_CONTENT         false  thrpt   10  2661186.814 ± 279377.198  ops/s fury_deserialize_compatible                   array  MEDIA_CONTENT         false  thrpt   10  2430410.064 ± 164165.865  ops/s furymetashared_deserialize_compatible         array  MEDIA_CONTENT         false  thrpt   10  3098083.064 ± 259391.053  ops/s ```  Size decreased from **732 to 577**: ``` Before 2024-05-30 01:00:49 INFO  FuryState:157 [fury_deserialize_compatible-jmh-worker-1] - ======> Fury | MEDIA_CONTENT | false | array | 732 |  After 2024-05-30 12:57:00 INFO  FuryState:157 [fury_deserialize_compatible-jmh-worker-1] - ======> Fury | MEDIA_CONTENT | false | array | 577 | ```  The
apache,fury,3f5cf31461cfed1f2c1d0f5b620529fe4abb6f40,https://github.com/apache/fury/commit/3f5cf31461cfed1f2c1d0f5b620529fe4abb6f40,fix(java): fix scala object type codegen (#1659)  ## What does this PR do?  fix scala object type codegen  ## Related issues  Closes #1658  ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/incubator-fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
apache,fury,3a0e410cb83756d3b139a9fbed01a9a64dbb2970,https://github.com/apache/fury/commit/3a0e410cb83756d3b139a9fbed01a9a64dbb2970,feat(java): support nonexistent class deserialization in meta share mode (#1646)  ## What does this PR do?  support nonexistent class deserialization in meta share mode  ## Related issues  <!-- Is there any related issue? Please attach here.  - #xxxx0 - #xxxx1 - #xxxx2 -->   ## Does this PR introduce any user-facing change?  <!-- If any user-facing interface changes  please [open an issue](https://github.com/apache/incubator-fury/issues/new/choose) describing the need to do so and update the document if necessary. -->  - [ ] Does this PR introduce any public API change? - [ ] Does this PR introduce any binary protocol compatibility change?   ## Benchmark  <!-- When the PR has an impact on performance (if you don't know whether the PR will have an impact on performance  you can submit the PR first  and if it will have impact on performance  the code reviewer will explain it)  be sure to attach a benchmark data here. -->
EngineHub,WorldEdit,9ea136acc3d4acab6a77773980791c3af68e4ffc,https://github.com/EngineHub/WorldEdit/commit/9ea136acc3d4acab6a77773980791c3af68e4ffc,Merge pull request #2745 from EngineHub/ot/perf/clipboard-optimizations  Use a one-dimensional array for BlockArrayClipboard
EngineHub,WorldEdit,e54a68325c4977c69d19784d28397c8cd8f5ac47,https://github.com/EngineHub/WorldEdit/commit/e54a68325c4977c69d19784d28397c8cd8f5ac47,Fix SideEffectSet inconsistencies  improve perf  The `apply` term was inconsistent in this API  but should have always been the same. This makes the "set" actually store the state of the given effect  even if it's the default  and therefore consistent.
EngineHub,WorldEdit,6e37730bad9d963ea3e6d41e7ccd761e8b8a91dc,https://github.com/EngineHub/WorldEdit/commit/6e37730bad9d963ea3e6d41e7ccd761e8b8a91dc,Fix incorrect paste orientation with `//perf -h update off` (#2601)  Fixes #2575
aeron-io,simple-binary-encoding,ec42a79fab4eadc5fc248c06fef453a39258573d,https://github.com/aeron-io/simple-binary-encoding/commit/ec42a79fab4eadc5fc248c06fef453a39258573d,Std span (#1038)  * [C++] Integrate std::span support for flyweight API  The impetus was a bug that we ran into when writing a string-literal to a fixed-width char field:  ```c++ flyweight.putFixedChar("hello"); ```  This is unsafe: - If the field size is less than 6  we overrun the buffer and corrupt it. - If the field size is more than 6  we don't zero pad the rest of it.  Instead  we build on support for the std::string_view getters and setters  which do length checking. std::span generalizes this to fixed-width fields of all types. Notably  if the size of the std::span is knowable at compile time  we pay no runtime cost for the length checking  and we should get similar performance to the existing API which takes a raw pointer.  Further  we add a sbetool option to disable accepting arrays by raw pointer  which should prevent memcpy operation without bounds checking. This is off by default to avoid a breaking change.  * [C++] hide USE_SPAN behind ENABLE_SPAN  * [C++] add macro guards  ---------  Co-authored-by: Matt Stern <stern@pdtpartners.com>
aeron-io,simple-binary-encoding,801b5bada7b387340cd88778d777daeeb4e1b2bb,https://github.com/aeron-io/simple-binary-encoding/commit/801b5bada7b387340cd88778d777daeeb4e1b2bb,Merge pull request #957 from ZachBray/feature/dtos  [C# C++ Java] Generate DTOs for non-perf-sensitive usecases.
aeron-io,simple-binary-encoding,96b70326c33c1f1756aca7e5f5746022d8aee72c,https://github.com/aeron-io/simple-binary-encoding/commit/96b70326c33c1f1756aca7e5f5746022d8aee72c,[C++] Generate DTOs for non-perf-sensitive usecases.  In some applications performance is not cricital. Some users would like to use SBE across their whole "estate"  but don't want the "sharp edges" associated with using flyweight codecs  e.g.  accidental escape.  In this commit  I've added a first cut of DTO generation for C++ and a simple test based on the Car Example.  The DTOs support encoding and decoding via the generated codecs using `DtoT::encode(CodecT& codec  const DtoT& dto)` and `DtoT::decode(CodecT& codec  Dto& dto)` methods.  Generation can be enabled specifying the target code generator class  `uk.co.real_logic.sbe.generation.cpp.CppDtos`  or by passing a system property `-Dsbe.cpp.generate.dtos=true`.
aeron-io,simple-binary-encoding,5389910a15df61c0c0f1f27810d2c4cfb47d0a21,https://github.com/aeron-io/simple-binary-encoding/commit/5389910a15df61c0c0f1f27810d2c4cfb47d0a21,[C#] Generate DTOs from SBE IR for non-perf-sensitive usecases.  In some applications performance is not cricital. Some users would like to use SBE across their whole "estate"  but don't want the "sharp edges" associated with using flyweight codecs  e.g.  accidental escape.  In this commit  I've added a first cut of DTO generation for C# and a simple test based on the Car Example.  The DTOs support encoding and decoding via the generated codecs using `EncodeInto(CodecT codec)` and `DecodeFrom(CodecT codec)` methods.  Currently there is no support for equality/comparison or read-only views over the data; although  these have been requested.  Here are some points that we may or may not wish to change in the future:  1. Non-present (due to the encoded version) string/array data and repeating groups are represented as `null` rather than empty.  2. Non-present primitive values are represented as their associated null value rather than using nullable types.  3. Non-present bitsets are represented as `0`.  4. DTOs are generated via a separate `CodeGenerator` rather than a flag to the existing C# `CodeGenerator`.
portfolio-performance,portfolio,46f36c346b2529b119f9b583a1a59ad35c5c2995,https://github.com/portfolio-performance/portfolio/commit/46f36c346b2529b119f9b583a1a59ad35c5c2995,Modify Swissquote PDF-Importer to support new trasnactions (#4751)  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/52
portfolio-performance,portfolio,9231f9dd6d540bbf0a922b7757f0bd0739775766,https://github.com/portfolio-performance/portfolio/commit/9231f9dd6d540bbf0a922b7757f0bd0739775766,Modify KBC Group NV PDF-Importer to support new transaction (#4750)  https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/34 https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/35 https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/36
portfolio-performance,portfolio,6245079836a1075304ff42bf0520cff76e5661aa,https://github.com/portfolio-performance/portfolio/commit/6245079836a1075304ff42bf0520cff76e5661aa,Modify Saxo Bank PDF-Importer to support new transactions (#4745)  Closes #4711 Closes #4712 Closes #4713 Closes #4714  https://forum.portfolio-performance.info/t/pdf-import-from-saxo-bank/21481/33
portfolio-performance,portfolio,ec2fe497c88011008617f7d905589f677c77d1aa,https://github.com/portfolio-performance/portfolio/commit/ec2fe497c88011008617f7d905589f677c77d1aa,Modify Baader Bank PDF-Importe to support new transaction (#4738)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/447
portfolio-performance,portfolio,f1d75fbc98f6f3b8f72351af824317289456e7af,https://github.com/portfolio-performance/portfolio/commit/f1d75fbc98f6f3b8f72351af824317289456e7af,Fixed calculation of moving average in trades for currencies other than EUR  Issue: https://forum.portfolio-performance.info/t/error-in-trades-view-after-adding-column-profit-loss-ma-and-entry-value-ma/33132
portfolio-performance,portfolio,cee2fc02b44649e44c0d336998e4687df396f7de,https://github.com/portfolio-performance/portfolio/commit/cee2fc02b44649e44c0d336998e4687df396f7de,Modify Scalable Capital PDF-Importer to support new transaction (#4727)  https://forum.portfolio-performance.info/t/pdf-import-von-scalable-capital/33088/17
portfolio-performance,portfolio,f0df65976425abfedd12742c35bbe517a253696b,https://github.com/portfolio-performance/portfolio/commit/f0df65976425abfedd12742c35bbe517a253696b,Modify Scalable Capital PDF-Importer to support new transaction (#4725)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/436 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/441 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/442
portfolio-performance,portfolio,4e70bd5182429d3fb5566202223db007c18f7ecc,https://github.com/portfolio-performance/portfolio/commit/4e70bd5182429d3fb5566202223db007c18f7ecc,Modify N26 PDF-Importer to support new transaction (#4723)  https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/16
portfolio-performance,portfolio,c1607a5549656cafc40dbbc829e18dc58b513177,https://github.com/portfolio-performance/portfolio/commit/c1607a5549656cafc40dbbc829e18dc58b513177,Modify Scalable Capital PDF-Importer to support new trasnaction (#4722)  https://forum.portfolio-performance.info/t/pdf-import-von-scalable-capital/33088
portfolio-performance,portfolio,5c37029f6d30cdea6e0759b31da4db62bcfa00b0,https://github.com/portfolio-performance/portfolio/commit/5c37029f6d30cdea6e0759b31da4db62bcfa00b0,Modify Saxo Bank PDF-Importer to support new transaction (#4684)  https://forum.portfolio-performance.info/t/pdf-import-from-saxo-bank/21481/23
portfolio-performance,portfolio,bcaf07c55a98e8ddc25b0ce137e365795a23eb7d,https://github.com/portfolio-performance/portfolio/commit/bcaf07c55a98e8ddc25b0ce137e365795a23eb7d,Troubleshooting and menu improvement  Closes #3591  * The entries in the help menu are sorted by intuitive. * Some URLs are incorrect  incomplete  or link to the wrong website. * Add the menu manual * Add the menu Report an issue * Add in bundle.properties command.openbrowser.{...}.url * Revision in bundle.properties command.openbrowser.{...}.name * Revision in bundle.properties command.openbrowser.{...}.tooltip * Moved OpenBrowserHandler from name.abuchen.portfolio.ui.handlers to name.abuchen.portfolio.bootstrap.handlers  Some command.openbrowser.{...}.tooltips were removed  because they are either identical with command.openbrowser.{...}.name or not necessary. There would therefore be duplicate translations  only that the parameter is different.  As an example: %command.openbrowser.changelog.name is equal to %command.openbrowser.changelog.tooltip ...  therefore tooltip is removed and %command.openbrowser.changelog.name is used as tooltip.  Portfolio Performance main menu:  New & noteworthy: Link not correct (e.g. ...tooltip in Polish) Default: "https://forum.portfolio-performance.info/t/new-noteworthy/17945/last/" German: "https://forum.portfolio-performance.info/t/sunny-neues-nennenswertes/23/last/"  Manual: Newly added Default: "https://help.portfolio-performance.info/en/" German: "https://help.portfolio-performance.info/de/"  Report an issue: Newly added Default: "https://forum.portfolio-performance.info/t/what-should-i-bear-in-mind-when-reporting-a-problem-or-error/32558/" German: "https://forum.portfolio-performance.info/t/was-sollte-ich-beim-melden-eines-problems-oder-fehlers-beachten/463/"  Forum: Default: "https://forum.portfolio-performance.info/c/english/" German: "https://forum.portfolio-performance.info/c/deutsch/"  GitHub: a. o. Portuguese is linked incorrectly Default: "https://github.com/portfolio-performance/portfolio/"  How-Tos: The link "https://forum.portfolio-performans.info/c/how-to" does not exist. Links assigned incorrectly. Default: "https://forum.portfolio-performance.info/c/english/how-to/18/" German: "https://forum.portfolio-performance.info/c/deutsch/how-to/5/" Rename: in bundle rename "how-tos" to "howTo"  FAQ: The FAQ link (https://forum.portfolio-performance.info/c/faq) redirects to the How-Tos section  therefore it is duplicated by the How-Tos. Removed: FAQ menu link  Changelog: The links are wrong  incorrect and mixed up - https://github.com/buchen/portfolio/releases/ - https://github.com/portfolio-performance/portfolio/releases  Default: "https://github.com/portfolio-performance/portfolio/releases/" German: "https://www.portfolio-performance.info/portfolio/versions.html"  ---  Welcome page: Removed: FAQ removed. The same as in the main menu. Added: Portfolio Performance Manual  Issue: #4646
portfolio-performance,portfolio,94b36a83ba7f5768b5fcbc7e040f80d0e329cd69,https://github.com/portfolio-performance/portfolio/commit/94b36a83ba7f5768b5fcbc7e040f80d0e329cd69,Improve PortfolioPerformanceSearchProvider  Convert the security type using the SecuritySearchProvider instance Remove OpenFIGI.java  Issue: #4672 Signed-off-by: Alexander Ott <webmaster@nirus-online.de> [added missing type to SecuritySearchProvider; keep original capitalization if type is unknown] Signed-off-by: Andreas Buchen <andreas.buchen@gmail.com>
portfolio-performance,portfolio,2575926221e49f0ae52157b28dbefa2647b5de65,https://github.com/portfolio-performance/portfolio/commit/2575926221e49f0ae52157b28dbefa2647b5de65,Modify Commerzbank PDF-Importer to support new transaction (#4682)  https://github.com/portfolio-performance/portfolio/pull/4651#issuecomment-2832306299
portfolio-performance,portfolio,9f48cca1bcefcdb67329dfa0f5f3a04b35805b04,https://github.com/portfolio-performance/portfolio/commit/9f48cca1bcefcdb67329dfa0f5f3a04b35805b04,Modify KBC Group NV PDF-Importer to support new transaction (#4679)  https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/28 https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/30
portfolio-performance,portfolio,1c0c56727a834c872561f9f17cd62581cc4c52e5,https://github.com/portfolio-performance/portfolio/commit/1c0c56727a834c872561f9f17cd62581cc4c52e5,Modify Trade Republic PDF-Importer to support new transaction (#4678)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/766 Closes #4642
portfolio-performance,portfolio,5902183ecda7f879dc9b878ffb0dd238b87cddf4,https://github.com/portfolio-performance/portfolio/commit/5902183ecda7f879dc9b878ffb0dd238b87cddf4,Refactor tree map view: option to color by performance  Additionally adds the option to show a heading for the categories to increase readability (but could decrease the comparability of the rectangle sizes).
portfolio-performance,portfolio,36a4791e06392ea4a5bddfb342bc2c35edefbc14,https://github.com/portfolio-performance/portfolio/commit/36a4791e06392ea4a5bddfb342bc2c35edefbc14,Modify Consorbank PDF-Importer to support new transaction (#4677)  https://forum.portfolio-performance.info/t/pdf-import-von-consorsbank/2697/313
portfolio-performance,portfolio,a5d061da48db05875fcec889438e3bae70859808,https://github.com/portfolio-performance/portfolio/commit/a5d061da48db05875fcec889438e3bae70859808,Modify Modena Estonia PDF-Importer to support new transaction (#4675)  https://forum.portfolio-performance.info/t/pdf-import-von-modena-estonia-ou/32378/8
portfolio-performance,portfolio,125abb2f43a4d0bf8dc63ebf7831a9e31de5e7a9,https://github.com/portfolio-performance/portfolio/commit/125abb2f43a4d0bf8dc63ebf7831a9e31de5e7a9,Update edit dialog for Portfolio Performance feed
portfolio-performance,portfolio,5459519ea20fccfbdd57336d56db9716eb19683c,https://github.com/portfolio-performance/portfolio/commit/5459519ea20fccfbdd57336d56db9716eb19683c,Modify Commerzbank PDF-Importer (#4659)  Merge #4651 Closes #4621 https://forum.portfolio-performance.info/t/pdf-import-von-commerzbank/9865/46 https://forum.portfolio-performance.info/t/pdf-import-von-commerzbank/9865/52 https://forum.portfolio-performance.info/t/pdf-import-von-commerzbank/9865/58
portfolio-performance,portfolio,3403b59ab06d2bde618c180e678052850abd782c,https://github.com/portfolio-performance/portfolio/commit/3403b59ab06d2bde618c180e678052850abd782c,Modify ComDirect PDF-Importer to support new transaction (#4656)  https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/413 https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/418 https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/419
portfolio-performance,portfolio,b076195087c2ea7dce0e00adc74dbe0e006c355a,https://github.com/portfolio-performance/portfolio/commit/b076195087c2ea7dce0e00adc74dbe0e006c355a,Use lazy security performance record in SecuritiesChart
portfolio-performance,portfolio,5467825377c92643acd958b86b35017d5a4c61b0,https://github.com/portfolio-performance/portfolio/commit/5467825377c92643acd958b86b35017d5a4c61b0,Modify Saxo Bank PDF-Importer to support new transaction (#4644)  https://forum.portfolio-performance.info/t/pdf-importer-fur-saxo-bank/30548/31
portfolio-performance,portfolio,5dd6ca0552458d6db4322af0ba7c96843a0b0bd4,https://github.com/portfolio-performance/portfolio/commit/5dd6ca0552458d6db4322af0ba7c96843a0b0bd4,Modfiy BSDEX PDF-Importer to support new transaction (#4643)  https://forum.portfolio-performance.info/t/pdf-import-von-bsdex-borse-stuttgart-digital-exchange/20155/17
portfolio-performance,portfolio,4337463f1d44ffa9d6d7ed6844eb156178951c31,https://github.com/portfolio-performance/portfolio/commit/4337463f1d44ffa9d6d7ed6844eb156178951c31,Fix NumberFormatException with moving average gains and outbound delivery of zero  Issue: https://forum.portfolio-performance.info/t/v-0-75-0-bugs-neue-wertpapier-suchmaske/32532/7?u=andreasb Issue: https://forum.portfolio-performance.info/t/0-75-0-interner-zinsfuss-fehlermeldung/32545
portfolio-performance,portfolio,0bea413de223c73dbbcae3e76939a12860f5d901,https://github.com/portfolio-performance/portfolio/commit/0bea413de223c73dbbcae3e76939a12860f5d901,Modify Bison PDF-Importer to support new transaction (#4633)  https://forum.portfolio-performance.info/t/pdf-import-von-bison/18929/11
portfolio-performance,portfolio,a73d34ba23e4bf1eb921217052bab255665f7e6f,https://github.com/portfolio-performance/portfolio/commit/a73d34ba23e4bf1eb921217052bab255665f7e6f,Add test case to Baader Bank PDF-Importer (#4632)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/428
portfolio-performance,portfolio,2c0b42fcac8de55757cfc8154d2bb1b1b5982e6b,https://github.com/portfolio-performance/portfolio/commit/2c0b42fcac8de55757cfc8154d2bb1b1b5982e6b,Modify Estateguru PDF-Importer to support new transactions (#4631)  https://forum.portfolio-performance.info/t/pdf-import-von-estateguru/21304/9
portfolio-performance,portfolio,17b645cd1f8e0b97cbc91be8c960c33c8d04822d,https://github.com/portfolio-performance/portfolio/commit/17b645cd1f8e0b97cbc91be8c960c33c8d04822d,Modify Swissquote PDF-Importer to support new transaction (#4626)  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/47
portfolio-performance,portfolio,690f070f62524b0289a1780d806762fe39460e80,https://github.com/portfolio-performance/portfolio/commit/690f070f62524b0289a1780d806762fe39460e80,Modify Trade Republic PDF-Importer to support new transaction (#4625)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/104
portfolio-performance,portfolio,4ca9fdbc61f599fec2c0cda42f6c201cccb9313a,https://github.com/portfolio-performance/portfolio/commit/4ca9fdbc61f599fec2c0cda42f6c201cccb9313a,Modify FlatEx PDF-Importer and implement getTickerSymbolForCrypto  https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/277 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/278 ------------------ Enhancement: Implement getTickerSymbolForCrypto Method Description  This enhancement introduces the getTickerSymbolForCrypto method  which maps cryptocurrency names to their respective ticker symbols. The method accepts a cryptocurrency name as input. It normalizes the input (trims whitespace and converts it to uppercase). A predefined mapping (cryptoTickerMap) returns the corresponding ticker symbol. If the name is missing or not found  an empty string ("") is returned.
portfolio-performance,portfolio,8e58e17a2e0cb1f66bd766359930d7f2a5d0a1d9,https://github.com/portfolio-performance/portfolio/commit/8e58e17a2e0cb1f66bd766359930d7f2a5d0a1d9,Modify Arkéa Direct Bank PDF-Importer to support new transaction (#4619)  Close #4614 https://forum.portfolio-performance.info/t/pdf-import-from-arkea-direct-bank-fortuneo-banque-france/28131/24
portfolio-performance,portfolio,b7bf6d26388b8b5c303ab1eb1fcff3e98e09d591,https://github.com/portfolio-performance/portfolio/commit/b7bf6d26388b8b5c303ab1eb1fcff3e98e09d591,Add new Bourse Direct PDF-Importer (#4609)  https://forum.portfolio-performance.info/t/pdf-import-from-boursedirect/31291
portfolio-performance,portfolio,d70103a845f2011b07f273bc5c44f6e0df214465,https://github.com/portfolio-performance/portfolio/commit/d70103a845f2011b07f273bc5c44f6e0df214465,Modify BaaderBank PDF-Importer to support new transactions (#4608)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/418 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/424
portfolio-performance,portfolio,52e30af9093df9da482d5d6e6c7ca1fe48132f3b,https://github.com/portfolio-performance/portfolio/commit/52e30af9093df9da482d5d6e6c7ca1fe48132f3b,Add new Modena Estonia OÜ PDF-Importer (#4607)  https://forum.portfolio-performance.info/t/pdf-import-von-modena-estonia-ou/32378
portfolio-performance,portfolio,a4a0c6f9d06bbf4a645e80811ca50d456599c925,https://github.com/portfolio-performance/portfolio/commit/a4a0c6f9d06bbf4a645e80811ca50d456599c925,Add new Crédit Mutuel Alliance Fédérale PDF-Importer (#4606)  https://forum.portfolio-performance.info/t/pdf-import-from-credit-mutuel-alliance-federale-suravenir/31859
portfolio-performance,portfolio,e7d575a8ff629066d91c9438c2b79a65f14a5e77,https://github.com/portfolio-performance/portfolio/commit/e7d575a8ff629066d91c9438c2b79a65f14a5e77,Add test case to Comdirect PDF-Importer (#4603)  https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/395
portfolio-performance,portfolio,fca87b06377db0efc082737ebe12715c51507ee5,https://github.com/portfolio-performance/portfolio/commit/fca87b06377db0efc082737ebe12715c51507ee5,Modify Easybank PDF-Importer to support new transaction (#4601)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/75
portfolio-performance,portfolio,963139350a8caa81c258b4253673627a73c3a243,https://github.com/portfolio-performance/portfolio/commit/963139350a8caa81c258b4253673627a73c3a243,Fix Swissquote PDF-Importer (#4594)  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/39
portfolio-performance,portfolio,04b011078e50a8735b65def10ed59ac49b1c3192,https://github.com/portfolio-performance/portfolio/commit/04b011078e50a8735b65def10ed59ac49b1c3192,Modify Postbank PDF-Importer to support new transaction (#4592)  https://forum.portfolio-performance.info/t/pdf-import-von-postbank/7234/74
portfolio-performance,portfolio,0a8934769f014261fffd84314ff838a99002a201,https://github.com/portfolio-performance/portfolio/commit/0a8934769f014261fffd84314ff838a99002a201,Modify ING Diba PDF-Importer to support new transaction (#4591)  https://forum.portfolio-performance.info/t/pdf-import-von-ing/7114/176
portfolio-performance,portfolio,acacde0da63ebbc21621b4b43e57b3738365a793,https://github.com/portfolio-performance/portfolio/commit/acacde0da63ebbc21621b4b43e57b3738365a793,Calculate capital gains based costs using the moving average method  * Added CapitalGainsCalculationMovingAverage class to calculate gains * Calculate forex gains based on average exchange rate * Extend performance view  security performance view  dashboard widgets with an option to pick the cost calculation method  Issue: #4546 Signed-off-by: mierin12 <mattmailspoubelle+GIT@gmail.com> [squashed commits; updated calculation of forex gains; added test case; renamed variables for consistency; rebased to master] Signed-off-by: Andreas Buchen <andreas.buchen@gmail.com>
portfolio-performance,portfolio,81c13917161a6c59913362cbfb2b2a912b14dd0c,https://github.com/portfolio-performance/portfolio/commit/81c13917161a6c59913362cbfb2b2a912b14dd0c,Modify Trade Republic PDF-Importer to support new transactions (#4587)  Closes #4584 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/757 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/760
portfolio-performance,portfolio,48c27d00c4f86bf9830d5fe0150f13dd19e18f5a,https://github.com/portfolio-performance/portfolio/commit/48c27d00c4f86bf9830d5fe0150f13dd19e18f5a,Separate filter drop down in SecuritiesPerformanceView
portfolio-performance,portfolio,c5afe6ea9eeb1ca60207c41e38488176ba7e8348,https://github.com/portfolio-performance/portfolio/commit/c5afe6ea9eeb1ca60207c41e38488176ba7e8348,Fix NullPointerException in TextUtil.compare  https://forum.portfolio-performance.info/t/fehlermeldung-internal-error/32110
portfolio-performance,portfolio,2bbb8bc46522cea4a5ece7182f0098778f0b70d8,https://github.com/portfolio-performance/portfolio/commit/2bbb8bc46522cea4a5ece7182f0098778f0b70d8,Modify Swissquote PDF-Importer to support new transaction (#4579)  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/37
portfolio-performance,portfolio,5e7db84e30e8c219b38ca050fc3f116f92f90ef2,https://github.com/portfolio-performance/portfolio/commit/5e7db84e30e8c219b38ca050fc3f116f92f90ef2,Modify Trade Republic PDF.Importer to support new transaction (#4573)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/753
portfolio-performance,portfolio,124794fbd724d78415eb4ff50f7a2ea37638affd,https://github.com/portfolio-performance/portfolio/commit/124794fbd724d78415eb4ff50f7a2ea37638affd,Modify Trade Republic PDF-Importer to support new transaction (#4567)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/745 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/746 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/747 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/749 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/751
portfolio-performance,portfolio,d28c9d46940c01104c79d229734ad7255a3af67f,https://github.com/portfolio-performance/portfolio/commit/d28c9d46940c01104c79d229734ad7255a3af67f,Modify N26 PDF-Importer to support new transaction (#4564)  https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/14
portfolio-performance,portfolio,a067e0910e404adf8e4b7090e116e75da9bafe9f,https://github.com/portfolio-performance/portfolio/commit/a067e0910e404adf8e4b7090e116e75da9bafe9f,Modify Trade Republic PDF-Importer to support new transaction (#4556)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/99
portfolio-performance,portfolio,d3d7cd3ed983740b5f3544a70c57a16b39df9881,https://github.com/portfolio-performance/portfolio/commit/d3d7cd3ed983740b5f3544a70c57a16b39df9881,Modify Saxo PDF-Importer to support new transaction (#4553)  https://forum.portfolio-performance.info/t/pdf-importer-fur-saxo-bank/30548/24
portfolio-performance,portfolio,8760a90034f87b91f7b3d2a7f12fddf46a13811f,https://github.com/portfolio-performance/portfolio/commit/8760a90034f87b91f7b3d2a7f12fddf46a13811f,Modify Trade Republic PDF-Importer to support new transaction (#4552)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/96
portfolio-performance,portfolio,86cb5e23340d7a228fa6d188de0fb9581e2790e3,https://github.com/portfolio-performance/portfolio/commit/86cb5e23340d7a228fa6d188de0fb9581e2790e3,Modify Tradegate AG PDF-Importer to support new transactions (#4550)  https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/13 https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/14 https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/15
portfolio-performance,portfolio,964dfbec93980e35d8584eb4309c8d80a7c02c2d,https://github.com/portfolio-performance/portfolio/commit/964dfbec93980e35d8584eb4309c8d80a7c02c2d,Modify ING DiBa PDF-Importer to support new transaction (#4549)  https://forum.portfolio-performance.info/t/pdf-import-von-ing/7114/174
portfolio-performance,portfolio,0bc6e6e8cda5296415563a9067b3bb6b8694c067,https://github.com/portfolio-performance/portfolio/commit/0bc6e6e8cda5296415563a9067b3bb6b8694c067,Fixed tax withholding calculation in the Targo Bank PDF importer (#4547)  https://forum.portfolio-performance.info/t/pdf-import-von-targobank/5537/60  1.) No tax burden: The losses offset the income so that no withholding tax is payable. 2.) Offsetting only with tax liability: Withholding tax can only be offset if capital gains tax is levied in Germany. 3.) Loss offsetting: The offsetting reduces the tax assessment basis to zero  which also eliminates offsetting. 4.) Savers' lump sum/exemption order: If used  this could also reduce the tax burden to zero.  See test case of taxes treatment transaction ...Dividende05 vs. ...Dividende08
portfolio-performance,portfolio,6acc517477caf125653b25fed50166b1511befa9,https://github.com/portfolio-performance/portfolio/commit/6acc517477caf125653b25fed50166b1511befa9,Enhancement of Arkea Direct Bank pdf import to support more cases (#4534) (#4535)  * Enhancement of Arkea Direct Bank pdf import to support more cases (#4534)  - Allows duplicate ')' at the end of ISIN - Quantity and date can be on the same line - Get security currency from amount instead of earlier from a data which is not always present  Closes #4534  * Improve Arkéa Direct Bank PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-from-arkea-direct-bank-fortuneo-banque-france/28131/13  ---------  Co-authored-by: Alexander Ott <webmaster@nirus-online.de>
portfolio-performance,portfolio,7a8bc4306bc567e9269b93e1a3c876a573be47f6,https://github.com/portfolio-performance/portfolio/commit/7a8bc4306bc567e9269b93e1a3c876a573be47f6,Modify Targo Bank PDF-Importer to support new transaction (#4537)  https://forum.portfolio-performance.info/t/pdf-import-von-targobank/5537/47
portfolio-performance,portfolio,6b3000757488c8855e9efa64cf1e0c6c6cde97dd,https://github.com/portfolio-performance/portfolio/commit/6b3000757488c8855e9efa64cf1e0c6c6cde97dd, Modify FlatEx PDF-Importer to support new transaction (#4533)  https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/265
portfolio-performance,portfolio,aa15fd64eb2b84bd8ae1715f333968f0c2dbb1cd,https://github.com/portfolio-performance/portfolio/commit/aa15fd64eb2b84bd8ae1715f333968f0c2dbb1cd, Modify C24 Bank GmbH PDF-Importer to support new transaction (#4532)  https://forum.portfolio-performance.info/t/pdf-import-von-c24-bank-gmbh/28636/7
portfolio-performance,portfolio,b0e65bc1155cd9f5a6a6810a3bab55751834195e,https://github.com/portfolio-performance/portfolio/commit/b0e65bc1155cd9f5a6a6810a3bab55751834195e,Modify Baader Bank PDF-Importer to support new transaction (#4531)  https://forum.portfolio-performance.info/t/pdf-import-von-traders-place/25739/8 https://forum.portfolio-performance.info/t/pdf-import-von-traders-place/25739/13 https://forum.portfolio-performance.info/t/pdf-import-von-traders-place/25739/14
portfolio-performance,portfolio,224e5c3c411fb83835b24fe30b59d0649ef298c0,https://github.com/portfolio-performance/portfolio/commit/224e5c3c411fb83835b24fe30b59d0649ef298c0,Modify FlatEx PDF-Importer to support new transaction (#4530)  https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/261 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/267 https://forum.portfolio-performance.info/t/import-von-flatex/31261
portfolio-performance,portfolio,df36da793b0cbc6ea948803eddb2f1c04c43fdcb,https://github.com/portfolio-performance/portfolio/commit/df36da793b0cbc6ea948803eddb2f1c04c43fdcb,Modify Trade Republic PDF-Importer to support new transactions (#4524)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/83 https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/88 https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/90
portfolio-performance,portfolio,a4c37c28506b2f72fab04716cc4323f051a3c6a3,https://github.com/portfolio-performance/portfolio/commit/a4c37c28506b2f72fab04716cc4323f051a3c6a3,Modify Scalable Capital  PDF-Importer to support new transaction (#4523)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/416
portfolio-performance,portfolio,1d6930cd7be435410bf33a080d3ddee2600c3dd9,https://github.com/portfolio-performance/portfolio/commit/1d6930cd7be435410bf33a080d3ddee2600c3dd9,Modify DKB PDF-Importer to support new transaction (#4522)  https://forum.portfolio-performance.info/t/pdf-import-von-dkb/4449/125
portfolio-performance,portfolio,b3a39ff07af5f913556a5730b06ada2da572a6cc,https://github.com/portfolio-performance/portfolio/commit/b3a39ff07af5f913556a5730b06ada2da572a6cc,Modify Solarisbank AG PDF-Importer to support new transaction (#4485)  https://forum.portfolio-performance.info/t/pdf-import-von-solarisbank-ag/22410/2
portfolio-performance,portfolio,c1cceacde12b3f6fe5270dd3a762255eef264bf2,https://github.com/portfolio-performance/portfolio/commit/c1cceacde12b3f6fe5270dd3a762255eef264bf2,Modify Comdirect PDF-Importer to support new transaction (#4484)  https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/392
portfolio-performance,portfolio,e96d3f4d675a79f3d46b9c00c5485604227428bf,https://github.com/portfolio-performance/portfolio/commit/e96d3f4d675a79f3d46b9c00c5485604227428bf,Modify BoursoBank PDF-Importer to support new transaction (#4483)  https://forum.portfolio-performance.info/t/pdf-import-from-boursobank-ex-boursorama-banque/28876/7
portfolio-performance,portfolio,4c21d8b954bb19cf83a3831ff63061d1716b748a,https://github.com/portfolio-performance/portfolio/commit/4c21d8b954bb19cf83a3831ff63061d1716b748a,Modify Easybank PDF-Importer to support new transaction (#4482)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/71
portfolio-performance,portfolio,aea6e0899314dd7194058ea66f6cff69306b0080,https://github.com/portfolio-performance/portfolio/commit/aea6e0899314dd7194058ea66f6cff69306b0080,fix NullPointerException  https://forum.portfolio-performance.info/t/tried-to-read-6-bytes-but-only-got-1/29723/18
portfolio-performance,portfolio,6d209b2c266ba5a3321241cad2958f76825eaefd,https://github.com/portfolio-performance/portfolio/commit/6d209b2c266ba5a3321241cad2958f76825eaefd,Fix NullPointerException for DekaBank PDF-Importer (#4444)  If the transactions for the latest ISIN in a quarterly report span several PDF pages  they have not been processed correctly. Issue: https://forum.portfolio-performance.info/t/pdf-import-von-dekabank/18048/61 Co-authored-by: Christian <c.klossek@group-apo.com>
portfolio-performance,portfolio,3202aa6cd74128e2e0987481fb4822afa71d2015,https://github.com/portfolio-performance/portfolio/commit/3202aa6cd74128e2e0987481fb4822afa71d2015,Modify Saxo PDF-Importer to support new format (#4442)  https://forum.portfolio-performance.info/t/pdf-importer-fur-saxo-bank/30548/18
portfolio-performance,portfolio,5e49a736887309eb26a87ae25af0bda21348fe0f,https://github.com/portfolio-performance/portfolio/commit/5e49a736887309eb26a87ae25af0bda21348fe0f,Modify Trade Republic PDF-Importer to support new transaction (#4440)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/78
portfolio-performance,portfolio,4e7e746c3ea82255926d4643b7ab5b695455612c,https://github.com/portfolio-performance/portfolio/commit/4e7e746c3ea82255926d4643b7ab5b695455612c,Modify Easybank PDF-Importer to support new transaction (#4439)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/69
portfolio-performance,portfolio,68bff777018ce53bcc817ebe91b6b452f2b2c129,https://github.com/portfolio-performance/portfolio/commit/68bff777018ce53bcc817ebe91b6b452f2b2c129,Modify Scalable Capita PDF-Importer to support new transaction (#4438)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/414
portfolio-performance,portfolio,6025f87c0f2d73d11587dd5e8b1c1e835cb283d3,https://github.com/portfolio-performance/portfolio/commit/6025f87c0f2d73d11587dd5e8b1c1e835cb283d3,Rework Targo Bank PDF-Importer (#4428)  Complete rework of Targo Bank PDF-Importer https://forum.portfolio-performance.info/t/pdf-import-von-targobank/5537/38 https://forum.portfolio-performance.info/t/pdf-import-von-targobank/5537/39 https://forum.portfolio-performance.info/t/pdf-import-von-targobank/5537/44
portfolio-performance,portfolio,924037cb40cf3fe476cf6f226b86b96f4c8f9d04,https://github.com/portfolio-performance/portfolio/commit/924037cb40cf3fe476cf6f226b86b96f4c8f9d04,Modify Hypothekarbank PDF-Importer to support new transaction (#4426)  https://forum.portfolio-performance.info/t/pdf-import-von-hypothekarbank-lenzburg-ag/27716/26
portfolio-performance,portfolio,0143cc4b5636e014b1044b1bf95b6e7caca4d1bd,https://github.com/portfolio-performance/portfolio/commit/0143cc4b5636e014b1044b1bf95b6e7caca4d1bd,Modify PostFinance PDF-Importer to support new transaction (#4422)  https://forum.portfolio-performance.info/t/pdf-import-von-postfinance-ag/19186/16
portfolio-performance,portfolio,49d08f6455d5eebc3938378c7fdf363b4666e8c7,https://github.com/portfolio-performance/portfolio/commit/49d08f6455d5eebc3938378c7fdf363b4666e8c7,Modify Baader Bank PDF-Importer to support new transaction (#4417)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/411
portfolio-performance,portfolio,870fab98164de17e1fa2096fa65b8e5b239570f8,https://github.com/portfolio-performance/portfolio/commit/870fab98164de17e1fa2096fa65b8e5b239570f8,Modify Baader Bank PDF-Importer to support new transaction (#4416)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/407 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/409
portfolio-performance,portfolio,cd20daf536ef15f93e12b47f2e38ad26ae3528ac,https://github.com/portfolio-performance/portfolio/commit/cd20daf536ef15f93e12b47f2e38ad26ae3528ac,Modify Commerzbank PDF-Importer to support new transaction (#4413)  https://forum.portfolio-performance.info/t/pdf-import-von-commerzbank/9865/38 https://forum.portfolio-performance.info/t/pdf-import-von-commerzbank/9865/39
portfolio-performance,portfolio,f541d2544fe2d18290cd3a541618e01e090b60ae,https://github.com/portfolio-performance/portfolio/commit/f541d2544fe2d18290cd3a541618e01e090b60ae,Added purchase price with taxes and fees in assets and performance views  Closes #792 Issue: #4310 Signed-off-by: mierin12 <mattmailspoubelle+GIT@gmail.com> [renamed to column.heading; flipped label to 'taxes and fees'] Signed-off-by: Andreas Buchen <andreas.buchen@gmail.com>
portfolio-performance,portfolio,0ad66baf39084d6f6e86c03eff1bf4570b3c3321,https://github.com/portfolio-performance/portfolio/commit/0ad66baf39084d6f6e86c03eff1bf4570b3c3321,Fix range out of bounds in ComDirect PDF-Importer (#4409)  https://forum.portfolio-performance.info/t/fehlermeldung-bei-comdirect-import-von-gemeinschaftsdepot/30910
portfolio-performance,portfolio,821e29e5274db520a165833363b0fe1be313574c,https://github.com/portfolio-performance/portfolio/commit/821e29e5274db520a165833363b0fe1be313574c,Modify Saxo Bank PDF-Importer to support new transactions (#4402)  https://forum.portfolio-performance.info/t/pdf-importer-fur-saxo-bank/30548/12
portfolio-performance,portfolio,2dd534e15b03547a508432803ebb6b7d1110724a,https://github.com/portfolio-performance/portfolio/commit/2dd534e15b03547a508432803ebb6b7d1110724a,Modify Consorbank PDF-Importer to support new transaction (#4386)  https://forum.portfolio-performance.info/t/consors-kauf-eurex-pdf/30661/3
portfolio-performance,portfolio,e773fdcd69dfd0bbdcb86bbb5b82f9888a53d921,https://github.com/portfolio-performance/portfolio/commit/e773fdcd69dfd0bbdcb86bbb5b82f9888a53d921,Modify Baader Bank PDF-Importer to support new transaction (#4378)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/395
portfolio-performance,portfolio,b9083e55df9f82de4866855d463fc0ed1b00656f,https://github.com/portfolio-performance/portfolio/commit/b9083e55df9f82de4866855d463fc0ed1b00656f,Modify DADAT PDF-Importer to support new transaction (#4377)  https://forum.portfolio-performance.info/t/pdf-import-von-dadat/15684/63
portfolio-performance,portfolio,93051e1b1f277b607966b6a069d3a979572fd679,https://github.com/portfolio-performance/portfolio/commit/93051e1b1f277b607966b6a069d3a979572fd679,Fix FlatEx PDF-Importer with Steuerkorrektur transaction (#4375)  https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/259
portfolio-performance,portfolio,8d6222cabf6a864c798a01fef839e6e5a6ceae4d,https://github.com/portfolio-performance/portfolio/commit/8d6222cabf6a864c798a01fef839e6e5a6ceae4d,Modify Deutsche Bank PDF-Importer to support new transaction (#4373)  https://forum.portfolio-performance.info/t/pdf-import-von-deutsche-bank/2973/61
portfolio-performance,portfolio,12fb0e837db824ef5bfdd5606d4a8b81963599e5,https://github.com/portfolio-performance/portfolio/commit/12fb0e837db824ef5bfdd5606d4a8b81963599e5,fix italic and small font for "alt hint" in chart tooltip  Fix https://github.com/portfolio-performance/portfolio/issues/4318
portfolio-performance,portfolio,26c0aba3ded0c31e22c5c6d7c2434183d67873c3,https://github.com/portfolio-performance/portfolio/commit/26c0aba3ded0c31e22c5c6d7c2434183d67873c3,Add new Saxo bank PDF-Importer (#4370)  https://forum.portfolio-performance.info/t/pdf-importer-fur-saxo-bank/30548
portfolio-performance,portfolio,a55d72f917661ea5c1eaeb3ec760cdcb61e0e2e3,https://github.com/portfolio-performance/portfolio/commit/a55d72f917661ea5c1eaeb3ec760cdcb61e0e2e3,Add test case to Baader Bank PDF-Importer (#4368)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/385
portfolio-performance,portfolio,62df9543e240feefd435e4b8198228e9c2b66473,https://github.com/portfolio-performance/portfolio/commit/62df9543e240feefd435e4b8198228e9c2b66473,Modify Quirin Privatbank AG PDF-Importer to support new transaction (#4363)  https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/74 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/75 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/77 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/79 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/81
portfolio-performance,portfolio,c4804772822812439702f0a01bcd09722c48934a,https://github.com/portfolio-performance/portfolio/commit/c4804772822812439702f0a01bcd09722c48934a,Modify 1822direkt PDF-Importer to support new transaction (#4360)  https://forum.portfolio-performance.info/t/pdf-import-von-1822direkt/984/44
portfolio-performance,portfolio,e98bef3d19da7c19f2297414445d5e1cc3eef79b,https://github.com/portfolio-performance/portfolio/commit/e98bef3d19da7c19f2297414445d5e1cc3eef79b,Modify Raiffeisenbank PDF-Importer to support new transaction (#4354)  https://forum.portfolio-performance.info/t/pdf-import-von-raiffeisen-schweiz/30442
portfolio-performance,portfolio,6972c65bd5fe000bf82d0ecd0b6b1288915fe05b,https://github.com/portfolio-performance/portfolio/commit/6972c65bd5fe000bf82d0ecd0b6b1288915fe05b,Modify Easybank PDF-Importer to support taxes lost adjustment (#4353)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/63 https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/64
portfolio-performance,portfolio,e723fd4f71edaadb171f9aee5dfc894b0daeb8be,https://github.com/portfolio-performance/portfolio/commit/e723fd4f71edaadb171f9aee5dfc894b0daeb8be,Modify Trade Republic PDF-Importer to support new transaction (#4352)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/77
portfolio-performance,portfolio,545d1c277f501d1421c98a19f2c4071fd7a38fa8,https://github.com/portfolio-performance/portfolio/commit/545d1c277f501d1421c98a19f2c4071fd7a38fa8,Modify Easybank PDF-Importer to support new transaction (#4350)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/61
portfolio-performance,portfolio,3a230232b9f2d7cdb7e11ae2945a121a87ab7058,https://github.com/portfolio-performance/portfolio/commit/3a230232b9f2d7cdb7e11ae2945a121a87ab7058,Modify Trade Republic PDF-Importer to support new transaction (#4349)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/697 https://forum.portfolio-performance.info/t/trade-republic-import/30322/2
portfolio-performance,portfolio,ef02b13e1fc714e009179df6446a45d66700fe4a,https://github.com/portfolio-performance/portfolio/commit/ef02b13e1fc714e009179df6446a45d66700fe4a,Make column config dialog in CSV import bigger and resizable  Issue: https://forum.portfolio-performance.info/t/csv-import-improvements/29924
portfolio-performance,portfolio,5558b94b836b99d9a53b221df7aeed2bf67bc457,https://github.com/portfolio-performance/portfolio/commit/5558b94b836b99d9a53b221df7aeed2bf67bc457,sanitize account and portfolio names for bulk CSV export  Closes https://github.com/portfolio-performance/portfolio/issues/4231
portfolio-performance,portfolio,b6e46c39490c86e17456709df8989d0600d2eb6c,https://github.com/portfolio-performance/portfolio/commit/b6e46c39490c86e17456709df8989d0600d2eb6c,Modify 1822direct PDF-Importer to support new transaction (#4337)  https://forum.portfolio-performance.info/t/pdf-import-von-1822direkt/984/42
portfolio-performance,portfolio,f10dbc7da65c1de741eb29653e91e0a4094937d7,https://github.com/portfolio-performance/portfolio/commit/f10dbc7da65c1de741eb29653e91e0a4094937d7,Modify Audi Bank PDF-Importer to support new transaction (#4334)  https://forum.portfolio-performance.info/t/pdf-import-von-audi-bank/30187/10
portfolio-performance,portfolio,0f6aa4f47c84b5ba6001e5e5dcee907d9352f4ef,https://github.com/portfolio-performance/portfolio/commit/0f6aa4f47c84b5ba6001e5e5dcee907d9352f4ef,Modify Audi Bank PDF-Importer to support new transaction (#4333)  https://forum.portfolio-performance.info/t/pdf-import-von-audi-bank/30187/7
portfolio-performance,portfolio,9859649512714c52eca27cdec0c2e1b9ec5ba7a4,https://github.com/portfolio-performance/portfolio/commit/9859649512714c52eca27cdec0c2e1b9ec5ba7a4, Modify Trade Republic PDF-Importer to support new transaction (#4332)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/690
portfolio-performance,portfolio,ccd521eb8a6f5fd9ea3e8ecf2ebba819414d0705,https://github.com/portfolio-performance/portfolio/commit/ccd521eb8a6f5fd9ea3e8ecf2ebba819414d0705,Add new Audi Bank PDF-Importer (#4324)  https://forum.portfolio-performance.info/t/pdf-import-von-audi-bank/30187/4
portfolio-performance,portfolio,f807a885c2105896e1dc5dd37711c591985e17a9,https://github.com/portfolio-performance/portfolio/commit/f807a885c2105896e1dc5dd37711c591985e17a9,Modify Oldenburgische Landesbank AG PDF-Importer to support new transaction (#4317)  https://forum.portfolio-performance.info/t/pdf-import-fuer-ebase/7204/94 https://forum.portfolio-performance.info/t/pdf-import-fuer-ebase/7204/95
portfolio-performance,portfolio,f06ce514c5bc95e718ea0f52096dccfb11e3f2b5,https://github.com/portfolio-performance/portfolio/commit/f06ce514c5bc95e718ea0f52096dccfb11e3f2b5,Modify FNZ Bank AG PDF-Importer to support new transaction (#4316)  https://forum.portfolio-performance.info/t/pdf-import-von-fnz-bank-vorher-ebase/25223/10 https://forum.portfolio-performance.info/t/pdf-import-von-fnz-bank-vorher-ebase/25223/14
portfolio-performance,portfolio,93f220f58e592207af6dc23cc6f1b0331b3a56e4,https://github.com/portfolio-performance/portfolio/commit/93f220f58e592207af6dc23cc6f1b0331b3a56e4,Modify Trade Republic PDF-Importer to support new transaction (#4315)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/681
portfolio-performance,portfolio,c595ebbd7f52c784aee0a43cdacf0b3cf103a23e,https://github.com/portfolio-performance/portfolio/commit/c595ebbd7f52c784aee0a43cdacf0b3cf103a23e,Modify Hypothekarbank Lenzburg AG PDF-Importer to support new transaction (#4307)  https://forum.portfolio-performance.info/t/pdf-import-von-hypothekarbank-lenzburg-ag/27716/21
portfolio-performance,portfolio,71b3c10f049618451a5eb9a633e5b6a1be6a0478,https://github.com/portfolio-performance/portfolio/commit/71b3c10f049618451a5eb9a633e5b6a1be6a0478,Modify PostFinance AG PDF-Importer to support new transaction (#4305)  https://forum.portfolio-performance.info/t/pdf-import-von-postfinance-ag/19186/14
portfolio-performance,portfolio,faae7a5b466349809933438f437f6a8238acd8a1,https://github.com/portfolio-performance/portfolio/commit/faae7a5b466349809933438f437f6a8238acd8a1,Modify kbc group nv pdf importer to support (#4294)  https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/10 https://forum.portfolio-performance.info/t/pdf-import-from-kbc-bank-nv/22455/11
portfolio-performance,portfolio,d86f22e8a41e1cf61a9174efd65bb2bf14307956,https://github.com/portfolio-performance/portfolio/commit/d86f22e8a41e1cf61a9174efd65bb2bf14307956,Modify Hypothekarbank Lenzburg AG PDF-Importer to support new transaction (#4292)  https://forum.portfolio-performance.info/t/pdf-import-von-hypothekarbank-lenzburg-ag/27716/19
portfolio-performance,portfolio,498b95f99d6d2c05e37c5d869ea23c536ea4f7ef,https://github.com/portfolio-performance/portfolio/commit/498b95f99d6d2c05e37c5d869ea23c536ea4f7ef,Add test case to sBroker PDF-Importer (#4289)  https://forum.portfolio-performance.info/t/pdf-import-von-s-broker-sparkasse/5195/130
portfolio-performance,portfolio,fdc5013666344f15b563216bc58f7bfbce15c6b4,https://github.com/portfolio-performance/portfolio/commit/fdc5013666344f15b563216bc58f7bfbce15c6b4,Modify sBroker PDF-Importer to support new transaction (#4288)  https://forum.portfolio-performance.info/t/pdf-import-von-s-broker-sparkasse/5195/128
portfolio-performance,portfolio,f2e3834ffed50e5e2d5ac9dd7f0147346ad8f718,https://github.com/portfolio-performance/portfolio/commit/f2e3834ffed50e5e2d5ac9dd7f0147346ad8f718,Modify Trade Republic PDF-Importer to support new transactions (#4286)  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/71 https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/72
portfolio-performance,portfolio,deb12732001b71186083e7af2280f4a58ad86051,https://github.com/portfolio-performance/portfolio/commit/deb12732001b71186083e7af2280f4a58ad86051,Modify Trade Republic PDF-Importer to support new transaction (#4285)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/676
portfolio-performance,portfolio,89ba6a62d880a09fb924573c519e254c74da659b,https://github.com/portfolio-performance/portfolio/commit/89ba6a62d880a09fb924573c519e254c74da659b,Add test case to OnVista PDF-Importer (#4282)  https://forum.portfolio-performance.info/t/pdf-import-von-onvista-bank/2076/154
portfolio-performance,portfolio,2a0ab43a4acb2e034500dc4380718f60afb3584c,https://github.com/portfolio-performance/portfolio/commit/2a0ab43a4acb2e034500dc4380718f60afb3584c,Modify FlaxEx PDF-Importer to support new transaction (#4275)  https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/254 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/253 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/244 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/239 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/237 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/231 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/195 https://forum.portfolio-performance.info/t/pdf-import-von-flatex/1752/193
portfolio-performance,portfolio,ce8fb86d30c0ccdadca4bc4e07089ba6c2297789,https://github.com/portfolio-performance/portfolio/commit/ce8fb86d30c0ccdadca4bc4e07089ba6c2297789,Modify EstateGuru PDF-Importer to support new transaction (#4273)  https://forum.portfolio-performance.info/t/pdf-import-von-estateguru/21304/7
portfolio-performance,portfolio,fee2e032ac480b8ba78eb523b7c767a64958597d,https://github.com/portfolio-performance/portfolio/commit/fee2e032ac480b8ba78eb523b7c767a64958597d,Fix WitheBox PDF-Importer (#4272)  https://forum.portfolio-performance.info/t/pdf-import-von-whitebox-gmbh/29756/6 Recalculation for fees after TeamViewer  Meeting
portfolio-performance,portfolio,531a5e79838e41c85e17f4f41fa73c3ee1c75acb,https://github.com/portfolio-performance/portfolio/commit/531a5e79838e41c85e17f4f41fa73c3ee1c75acb,Modify Estateguru PDF-Importer to support new transactions (#4271)  https://forum.portfolio-performance.info/t/pdf-import-von-estateguru/21304/5
portfolio-performance,portfolio,d3d268322eab86c9b6fb05d5d40bc296e8db39e9,https://github.com/portfolio-performance/portfolio/commit/d3d268322eab86c9b6fb05d5d40bc296e8db39e9,Modify ING Diba PDF-Importer to support new transaction (#4269)  https://forum.portfolio-performance.info/t/pdf-import-von-ing/7114/166
portfolio-performance,portfolio,953560993eb55d78da63cdc433352799d9c5883e,https://github.com/portfolio-performance/portfolio/commit/953560993eb55d78da63cdc433352799d9c5883e,Modify Barclay PDF-Importer to support new transaction (#4267)  https://forum.portfolio-performance.info/t/pdf-import-von-barclays-bank/27762/3?u=nirus
portfolio-performance,portfolio,d186edf6828b3b6c3a3fe3ea1f92463d05d772f3,https://github.com/portfolio-performance/portfolio/commit/d186edf6828b3b6c3a3fe3ea1f92463d05d772f3,Fixed exception while collecting header information from request  Issue: https://forum.portfolio-performance.info/t/kurse-von-ariva-werden-nicht-aktualisiert/29744/54?u=andreasb
portfolio-performance,portfolio,cad4b38461b7a9ec5954f92e0c127e1acf25eb16,https://github.com/portfolio-performance/portfolio/commit/cad4b38461b7a9ec5954f92e0c127e1acf25eb16,Modify Trade Republic PDF-Importer to support new transaction (#4266)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/657
portfolio-performance,portfolio,efb50e6bcd61c1b614958e91a0f3ba81d4423e39,https://github.com/portfolio-performance/portfolio/commit/efb50e6bcd61c1b614958e91a0f3ba81d4423e39,Change "absolute deviation" in the "delta percentage indicator"  `node.getRoot()` leaded to an "absolute deviation" calculated in relation to the total value of *all* (filtered) assets. The filtering worked  but the "root" TaxonomyNode includes the "Without Classification" class  which we don't want.  Issue: https://forum.portfolio-performance.info/t/ohne-klassifizierungen-nicht-auswerten-filter-vergessen-ihre-einstellungen/655/9 Issue: #4260
portfolio-performance,portfolio,23ea30327d4a6ddca6c864742a1a91d127780fcd,https://github.com/portfolio-performance/portfolio/commit/23ea30327d4a6ddca6c864742a1a91d127780fcd,Modify Baader Bank PDF-Importer to support new transaction (#4262)  https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/360 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/363 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/370 https://forum.portfolio-performance.info/t/pdf-import-von-baader-bank-scalable-capital-smartbroker-plus/2057/373
portfolio-performance,portfolio,712eea7f4e19a5075dd90a65f92cb437b85e1f76,https://github.com/portfolio-performance/portfolio/commit/712eea7f4e19a5075dd90a65f92cb437b85e1f76,Add new Estateguru PDF-Importer (#4259)  https://forum.portfolio-performance.info/t/pdf-import-von-estateguru/21304/3
portfolio-performance,portfolio,0bbb11c4e1b8f6d8953c16ca87cbe84d9d49e5da,https://github.com/portfolio-performance/portfolio/commit/0bbb11c4e1b8f6d8953c16ca87cbe84d9d49e5da,Add new Alpac Capital PDF-Importer (#4257)  https://forum.portfolio-performance.info/t/pdf-import-from-alpac-capital/29082 https://forum.portfolio-performance.info/t/pdf-import-from-alpac-capital/29082/3
portfolio-performance,portfolio,e13cfc95bb27fe32d8930d56f2978f63394839ca,https://github.com/portfolio-performance/portfolio/commit/e13cfc95bb27fe32d8930d56f2978f63394839ca,Modify Tradegate AG PDF-Importer to support new transaction (#4255)  https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/9
portfolio-performance,portfolio,047a0f7b8441060133f0460577ab2f670899e470,https://github.com/portfolio-performance/portfolio/commit/047a0f7b8441060133f0460577ab2f670899e470,Modify Trade Republic PDF-Importer to support new transaction (#4254)  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/651
portfolio-performance,portfolio,15c1ebcb6b2c93833322aa6202a04f2f6090ff6e,https://github.com/portfolio-performance/portfolio/commit/15c1ebcb6b2c93833322aa6202a04f2f6090ff6e,Modify Easybank PDF-Importer to support new transaction (#4253)  https://forum.portfolio-performance.info/t/pdf-import-von-easybank/21323/58
portfolio-performance,portfolio,ddc569f751a949e8220981e2495bd161c91289c8,https://github.com/portfolio-performance/portfolio/commit/ddc569f751a949e8220981e2495bd161c91289c8,Modify Akf Bank PDF-Importer to support new transaction (#4251)  https://forum.portfolio-performance.info/t/pdf-import-von-akf-bank/27325/6
portfolio-performance,portfolio,641cb331e3f03412f72d4e8343ee89739d1df5b1,https://github.com/portfolio-performance/portfolio/commit/641cb331e3f03412f72d4e8343ee89739d1df5b1,Add new WitheBox GmbH PDF-Importer (#4247)  https://forum.portfolio-performance.info/t/pdf-import-von-whitebox-gmbh/29756
portfolio-performance,portfolio,65882becb775334356b3490b60b9169e1f409b58,https://github.com/portfolio-performance/portfolio/commit/65882becb775334356b3490b60b9169e1f409b58,Modify Tiger Broker PDF-Importer to support new transaction (#4246)  https://forum.portfolio-performance.info/t/pdf-import-from-tiger-brokers/20484/26 https://forum.portfolio-performance.info/t/pdf-import-from-tiger-brokers/20484/34
portfolio-performance,portfolio,a6e386a428a9306960fb7b96813726c0e4b0c8df,https://github.com/portfolio-performance/portfolio/commit/a6e386a428a9306960fb7b96813726c0e4b0c8df,Modify KBC Bank NV PDF-Importer to support new transactions (#4244)  https://forum.portfolio-performance.info/t/pdf-import-from-bolero/29582
portfolio-performance,portfolio,15ab2a2ca6a8de20ecd36b2036cc7d2e1f9dc6ba,https://github.com/portfolio-performance/portfolio/commit/15ab2a2ca6a8de20ecd36b2036cc7d2e1f9dc6ba,Fixed 'widget diposed' error when remove the active sort column  Issue: https://forum.portfolio-performance.info/t/spalte-div-jahr-wird-nur-einmal-berechnet-danach-ist-die-spalte-leer/29706
portfolio-performance,portfolio,1fb005be8f9ecfbaa5193f5934254e056ced5f93,https://github.com/portfolio-performance/portfolio/commit/1fb005be8f9ecfbaa5193f5934254e056ced5f93,Fixed lazy dividend calculation requiring the moving averages from cost calculation  Issue: https://forum.portfolio-performance.info/t/spalte-div-jahr-wird-nur-einmal-berechnet-danach-ist-die-spalte-leer/29706
portfolio-performance,portfolio,49197f2afaf772c1e3727133fe52d534787fb2a1,https://github.com/portfolio-performance/portfolio/commit/49197f2afaf772c1e3727133fe52d534787fb2a1,Labels summary row consistently with 'Sum'  The view 'statement of assets'  'payments' and 'security performance' now use all the same label for the summary row.
portfolio-performance,portfolio,27b936fc4722f240a0ad6b62db451acd8d7e048f,https://github.com/portfolio-performance/portfolio/commit/27b936fc4722f240a0ad6b62db451acd8d7e048f,Modify AKF Bank PDF-Importer to support new transaction (#4242)  https://forum.portfolio-performance.info/t/pdf-import-von-akf-bank/27325/3
portfolio-performance,portfolio,60d92eb6451789deca9598460966afe8ac67d310,https://github.com/portfolio-performance/portfolio/commit/60d92eb6451789deca9598460966afe8ac67d310,Modify OnVista PDF-Importer to support new transactions (#4241)  https://forum.portfolio-performance.info/t/pdf-import-von-onvista-bank/2076/149 Merge parts of #4147  Co-authored-by: ZfT2 <16590801+zft2@users.noreply.github.com>
portfolio-performance,portfolio,f7a2f397c4498a833d3f08c6aa91da87c01e1001,https://github.com/portfolio-performance/portfolio/commit/f7a2f397c4498a833d3f08c6aa91da87c01e1001,Modify N26 PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/7
portfolio-performance,portfolio,ce6b4fbe436ea21c11efa3df31f6b6d048fdf086,https://github.com/portfolio-performance/portfolio/commit/ce6b4fbe436ea21c11efa3df31f6b6d048fdf086,ModModify Hypothekarbank Lenzburg AG PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-hypothekarbank-lenzburg-ag/27716/17
portfolio-performance,portfolio,04e0bea2379664141a4d572217004c44f3f14fee,https://github.com/portfolio-performance/portfolio/commit/04e0bea2379664141a4d572217004c44f3f14fee,Added aggregate rows to the security performance view
portfolio-performance,portfolio,314d6b3f2923d9bdf8ab05a8d30df80ab75424aa,https://github.com/portfolio-performance/portfolio/commit/314d6b3f2923d9bdf8ab05a8d30df80ab75424aa,SecuritySelection: Allow to track selection of multiple securities  Some views in the app allow to select multiple securities  so allow to perform operations on all of them. Without such support  behavior can be confusing  for example  currently a user can select multiple securities in "Securities" view  and choose "Update quotes (selected)" via main menu  but only one will be actually updated.
portfolio-performance,portfolio,876ea7315e4d1093655a779524bb873ec71eb181,https://github.com/portfolio-performance/portfolio/commit/876ea7315e4d1093655a779524bb873ec71eb181,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/61 https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/66 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/634 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/638
portfolio-performance,portfolio,2ad60fbbb58f1e6a2864792eda905ecd636f3cfe,https://github.com/portfolio-performance/portfolio/commit/2ad60fbbb58f1e6a2864792eda905ecd636f3cfe,SecurityEvent: Move "source" field to the parent SecurityEvent class  Previously  "source" field was available only in the DividendEvent subclass. But it makes sense to have it on the parent class  to streamline integration with external tools to manage events. Thus  "source" is a generic machine-readable field to record source and/or "ownership" of the event information. A typical workflow would be:  1. An external tool TOOL1 creates an event and sets its source attribute to "TOOL1" to designate this event is "owned" by the tool  which it's free to modify or even delete later. 2. This attribute eventually gets rendered as <source> element within <event> element in PortfolioPerformance XML. 3. PP preserves the value of the source attributes across load/save cycle of the portfolio. 4. On the next run  TOOL1 can identify events originally create by itself  and update them as needed. This is important  because event information provided by external source if often tentative and subject to change.  As mentioned above  the "source" field is intended to be machine-readable  and as such  it's currently now shown anywhere in the PP UI. It might be useful to address that at later time  if the need for that is found.  WARNING: This change doesn't include steps which would be required to migrate Protobuf binary format. It's expected that developers familiar with this format would step in to help with the migration.
portfolio-performance,portfolio,7c4b3fa2da55f948378cf5d838026969e93253cb,https://github.com/portfolio-performance/portfolio/commit/7c4b3fa2da55f948378cf5d838026969e93253cb,Modify FFB PDF-Importer to support new transactions  https://forum.portfolio-performance.info/t/pdf-import-von-frankfurter-fondsbank-ffb/4751/79
portfolio-performance,portfolio,11a976084d919bab5ce0feded5e7d7e20aff1d9a,https://github.com/portfolio-performance/portfolio/commit/11a976084d919bab5ce0feded5e7d7e20aff1d9a,Modify C24 Bank GmbH PDF-Importer to support new transaction (#4186)  https://forum.portfolio-performance.info/t/pdf-import-von-c24-bank-gmbh/28636/3
portfolio-performance,portfolio,a10214344ec3b98b024d1c56584f76e84d99f604,https://github.com/portfolio-performance/portfolio/commit/a10214344ec3b98b024d1c56584f76e84d99f604,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/57
portfolio-performance,portfolio,09d57e1522421feb473c44081b3ce81cc19b13c5,https://github.com/portfolio-performance/portfolio/commit/09d57e1522421feb473c44081b3ce81cc19b13c5,Removed immutable type setting for latest security price  For whatever reasons  there are actually some referenced latest security prices in files out there in the wild.  Issue: #4117 Issue: https://forum.portfolio-performance.info/t/fehlermeldungen-nach-update-auf-version-70/29289 Issue: https://forum.portfolio-performance.info/t/fehler-beim-offnen-xml-kann-nicht-geparst-werden/21859/18 Issue: https://www.wertpapier-forum.de/topic/38306-portfolio-performance-mein-neues-programm/?do=findComment&comment=1728577
portfolio-performance,portfolio,22c93ba43eb30034abf51be069ea7042e95d2cea,https://github.com/portfolio-performance/portfolio/commit/22c93ba43eb30034abf51be069ea7042e95d2cea,Added filtered columns to the security performance view
portfolio-performance,portfolio,18dd67ed4640e54998aa19a1c73f7119b92ec1ff,https://github.com/portfolio-performance/portfolio/commit/18dd67ed4640e54998aa19a1c73f7119b92ec1ff,Modify Trade Repbulic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/613 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/617 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/625 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/629
portfolio-performance,portfolio,21f8fabcc1561660c544f80236331ad158e5f759,https://github.com/portfolio-performance/portfolio/commit/21f8fabcc1561660c544f80236331ad158e5f759,ClientFactory: Autodetect reference format of plain XML files  When opening XML file  do detection whether it uses XPath for references (older PortfolioPerformance way) or "id" attributes. Sadly  the serialization library  XStream  doesn't do such detection on its own. We do detection by reading a few initial bytes of file and seeing if top-level <client> tag has "id" attribute (just using string matching). This is done only on plain-text XML files (not compressed  not encrypted). This should be adequate trade-off  as the whole idea of switching to "id" encoding is to simplify interoperability with 3rd-party tools  which requires plain-text XML anyway.  Issue: #4117
portfolio-performance,portfolio,10cf095424c137063643fe8871727d55940ae06a,https://github.com/portfolio-performance/portfolio/commit/10cf095424c137063643fe8871727d55940ae06a,Allow '.' from numpad as French decimal separator  When entering a number of a French keyboard  the dot from the numpad is usually automatically converted into the French decimal separator ' '.  We do not have to change the NumberVerifyListener. It is not triggered because the KeyListener is directly inserting the decimal. However  that is not a problem because we know it is a valid character for numbers.  Issue: #4143 Issue: #3780 Issue: https://forum.portfolio-performance.info/t/decimal-separator-on-french-keyboard/28939 Signed-off-by: mierin12 <mattmailspoubelle+GIT@gmail.com> [squashed commits; refactored code into utility class; rebased to master] Signed-off-by: Andreas Buchen <andreas.buchen@gmail.com>
portfolio-performance,portfolio,878494f6967c28488d3cff02a8dbedc87c301d85,https://github.com/portfolio-performance/portfolio/commit/878494f6967c28488d3cff02a8dbedc87c301d85,Add new VZ Depotbank AG PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-vz-depotbank-ag/29123/6
portfolio-performance,portfolio,5aa37c5b6715ca3065fa4ecfc4b480f79c240dd3,https://github.com/portfolio-performance/portfolio/commit/5aa37c5b6715ca3065fa4ecfc4b480f79c240dd3,Modify KeyTrade PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-keytrade/19238/4 https://forum.portfolio-performance.info/t/pdf-import-from-keytrade/19238/5 https://forum.portfolio-performance.info/t/pdf-import-from-keytrade/19238/6 https://forum.portfolio-performance.info/t/pdf-import-from-keytrade/19238/8
portfolio-performance,portfolio,17f303a8b4a5acd385abee4545b5bdb3316a2d18,https://github.com/portfolio-performance/portfolio/commit/17f303a8b4a5acd385abee4545b5bdb3316a2d18,Fixed NPE when linking securities to portfolio report after creation from PDF  Issue: https://forum.portfolio-performance.info/t/wertpapieranlage-aus-pdf-import-fuhrt-zu-fehler/29130
portfolio-performance,portfolio,a3bdaaac40504e3173b7a6b6ee9bbedd2969c266,https://github.com/portfolio-performance/portfolio/commit/a3bdaaac40504e3173b7a6b6ee9bbedd2969c266,Modify Deutsche Bank PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-deutsche-bank/2973/58
portfolio-performance,portfolio,be536a5743f602a82c639f1fd0cf15a046bca84b,https://github.com/portfolio-performance/portfolio/commit/be536a5743f602a82c639f1fd0cf15a046bca84b,Fix N26 PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/5
portfolio-performance,portfolio,f4ea0c295038694f572456d7b5d327df8599d7f9,https://github.com/portfolio-performance/portfolio/commit/f4ea0c295038694f572456d7b5d327df8599d7f9,Modify Direkt1822 PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-1822direkt/984/40
portfolio-performance,portfolio,1bd7ecfeeb49cc967b0f96d3326de3416f79bcec,https://github.com/portfolio-performance/portfolio/commit/1bd7ecfeeb49cc967b0f96d3326de3416f79bcec,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/55
portfolio-performance,portfolio,4e15abcca6744839fe4aee05edea147db193a6db,https://github.com/portfolio-performance/portfolio/commit/4e15abcca6744839fe4aee05edea147db193a6db,Add new N26 PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/2 https://forum.portfolio-performance.info/t/pdf-import-von-n26-bank-ag/28945/3
portfolio-performance,portfolio,e09822256fba990f8ca96141f5bb8e91106e33d9,https://github.com/portfolio-performance/portfolio/commit/e09822256fba990f8ca96141f5bb8e91106e33d9,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/52
portfolio-performance,portfolio,2b2869441bf72ec23faa843eed6a77f9d20ee147,https://github.com/portfolio-performance/portfolio/commit/2b2869441bf72ec23faa843eed6a77f9d20ee147,Modify Consorbank PDF-Importer to support new transaction (#4151)  https://forum.portfolio-performance.info/t/pdf-import-von-consorsbank/2697/292
portfolio-performance,portfolio,32490aab0b138c0c6367820585595bb9d79884f7,https://github.com/portfolio-performance/portfolio/commit/32490aab0b138c0c6367820585595bb9d79884f7,Modify Trade Republic PDF-Importer to support new transaction (#4150)  Closes #4137 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/571 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/586 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/587 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/588 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/597
portfolio-performance,portfolio,4335a52a43ba4c0847a1ed1fc6aeb7bb6820d9c4,https://github.com/portfolio-performance/portfolio/commit/4335a52a43ba4c0847a1ed1fc6aeb7bb6820d9c4,Modify DKB PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-dkb/4449/121
portfolio-performance,portfolio,3ded76152a8524617b1a4e8a80e38d2f479f7337,https://github.com/portfolio-performance/portfolio/commit/3ded76152a8524617b1a4e8a80e38d2f479f7337,Sortability of the status when checking the extracted elements  https://forum.portfolio-performance.info/t/sortierung-nach-status-bei-buchung-aus-pdf-importieren/28819
portfolio-performance,portfolio,7ee0694402fd2de9d46b936e4d2b1d4258e26ffc,https://github.com/portfolio-performance/portfolio/commit/7ee0694402fd2de9d46b936e4d2b1d4258e26ffc,Use lazy security performance record for security performance view
portfolio-performance,portfolio,d018e1c4cded8c1d5ea9cff10c9575a5fc1c30b6,https://github.com/portfolio-performance/portfolio/commit/d018e1c4cded8c1d5ea9cff10c9575a5fc1c30b6,Added a lazy security performance record that computes values on demand
portfolio-performance,portfolio,07de44c5629bc1ffd005201fc29684b92c5c0425,https://github.com/portfolio-performance/portfolio/commit/07de44c5629bc1ffd005201fc29684b92c5c0425,Refactored SecurityPerformanceRecord in order to modularize computations
portfolio-performance,portfolio,f89f0c70a3f5689560f7f7c2fb0b4ddd25139e3c,https://github.com/portfolio-performance/portfolio/commit/f89f0c70a3f5689560f7f7c2fb0b4ddd25139e3c,Performance improvement on TTWROR calculation: incrementally apply transactions
portfolio-performance,portfolio,611ee3e0cc8fbde82d36182bc69f53cc3c277a9e,https://github.com/portfolio-performance/portfolio/commit/611ee3e0cc8fbde82d36182bc69f53cc3c277a9e,Modify Raisin Bank AG PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-raisin-bank-ag/28544/12
portfolio-performance,portfolio,957ba5d9b2f9d28b03dc5974fdad7282345390a8,https://github.com/portfolio-performance/portfolio/commit/957ba5d9b2f9d28b03dc5974fdad7282345390a8,Add new Bourso Bank PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-from-boursobank-ex-boursorama-banque/28876
portfolio-performance,portfolio,5c2b1af0ba691f2887f0656561889798c70568f2,https://github.com/portfolio-performance/portfolio/commit/5c2b1af0ba691f2887f0656561889798c70568f2,Modify Quirin PDF-Importer to support new transaction (#4128)  https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/65 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/67 https://forum.portfolio-performance.info/t/pdf-import-von-quirin-privatbank-ag/21748/68
portfolio-performance,portfolio,a01b34a49b738b5e510e8fb058409e7c384b4a6d,https://github.com/portfolio-performance/portfolio/commit/a01b34a49b738b5e510e8fb058409e7c384b4a6d,Modify Raisin Bank AG PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-raisin-bank-ag/28544/10
portfolio-performance,portfolio,dd97c0ebd6b93c6a9671d8beef467d625289f890,https://github.com/portfolio-performance/portfolio/commit/dd97c0ebd6b93c6a9671d8beef467d625289f890,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/567
portfolio-performance,portfolio,2412a7fb7c4bfbd963b93baf2b3d6cbd62e95b45,https://github.com/portfolio-performance/portfolio/commit/2412a7fb7c4bfbd963b93baf2b3d6cbd62e95b45,Modify ING DiBa PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-ing-bank-nv-espana/28716 https://forum.portfolio-performance.info/t/pdf-import-from-ing-bank-nv-espana/28716/3
portfolio-performance,portfolio,afc3db74aebe638f602235772cfe282329d29dd1,https://github.com/portfolio-performance/portfolio/commit/afc3db74aebe638f602235772cfe282329d29dd1,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/44
portfolio-performance,portfolio,c79e42c9bda21c0f37aeb219dd1bb6f12229d617,https://github.com/portfolio-performance/portfolio/commit/c79e42c9bda21c0f37aeb219dd1bb6f12229d617,Modify JustTrade PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-justtrade/10853/52
portfolio-performance,portfolio,5c11edde034a0d1e621fd76c30541526a5c05ddb,https://github.com/portfolio-performance/portfolio/commit/5c11edde034a0d1e621fd76c30541526a5c05ddb,Modify Tradegate PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/4
portfolio-performance,portfolio,c17a9927f791d441a9cc9e7cebcf5edfaa8ee107,https://github.com/portfolio-performance/portfolio/commit/c17a9927f791d441a9cc9e7cebcf5edfaa8ee107,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/561
portfolio-performance,portfolio,82a4644caf6fcfcc45c9f265322a4156f805fcc5,https://github.com/portfolio-performance/portfolio/commit/82a4644caf6fcfcc45c9f265322a4156f805fcc5,Modify Raisin Bank AG PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-raisin-bank-ag/28544/6
portfolio-performance,portfolio,2719f201ef8b413b93a61b20e0f9747af7c229f9,https://github.com/portfolio-performance/portfolio/commit/2719f201ef8b413b93a61b20e0f9747af7c229f9,Modify FFB PDF-Importer to support new transactions (#4101)  https://forum.portfolio-performance.info/t/pdf-import-von-frankfurter-fondsbank-ffb/4751/77
portfolio-performance,portfolio,0518dbcd6e6355eadb090d63e389f00c9d79bba0,https://github.com/portfolio-performance/portfolio/commit/0518dbcd6e6355eadb090d63e389f00c9d79bba0,Modify Arkea Direct Bank PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-arkea-direct-bank-fortuneo-banque-france/28131/8  Fixing multiple transactions in one document Renaming of files and tests by language
portfolio-performance,portfolio,8dad1d391c876de7187ccc7d79041ccba93473e9,https://github.com/portfolio-performance/portfolio/commit/8dad1d391c876de7187ccc7d79041ccba93473e9,Modify sBroker PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-s-broker-sparkasse/5195/118
portfolio-performance,portfolio,2ca4664998d2872a4ddfc8c0a4e4c5b89a6ee6ee,https://github.com/portfolio-performance/portfolio/commit/2ca4664998d2872a4ddfc8c0a4e4c5b89a6ee6ee,Modify ING DIBa PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-ing/7114/159
portfolio-performance,portfolio,3681558b96038dccab8ddeaad58e00ad9b37cc18,https://github.com/portfolio-performance/portfolio/commit/3681558b96038dccab8ddeaad58e00ad9b37cc18,Modify Trade Republic PDF-Importer to support new transaction  Closes #4083 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/540 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/543 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/544 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/545 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/546 https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/547
portfolio-performance,portfolio,459c8606a8f1b0edc96dd8830eff67b927606f94,https://github.com/portfolio-performance/portfolio/commit/459c8606a8f1b0edc96dd8830eff67b927606f94,Modify Swissquote PDF-Importer to support new transaction (#4080)  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/27  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/28
portfolio-performance,portfolio,fc4bd5b956e69dbace5863db62771b8aed225f7b,https://github.com/portfolio-performance/portfolio/commit/fc4bd5b956e69dbace5863db62771b8aed225f7b,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-from-trade-republic/24425/43
portfolio-performance,portfolio,68a49aad4c784c425abcf6c8eee16b422c792620,https://github.com/portfolio-performance/portfolio/commit/68a49aad4c784c425abcf6c8eee16b422c792620,Modify Swissquote PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/25
portfolio-performance,portfolio,f45aab26d0160bd2446c74af49c638887abac65a,https://github.com/portfolio-performance/portfolio/commit/f45aab26d0160bd2446c74af49c638887abac65a,Fix Disappearing filter in Transactions Panes  Fixes https://github.com/portfolio-performance/portfolio/pull/3985#issuecomment-2129064338 Due to keeping Filter in Pane's property transactionFilter its widget was not null stoping DropDown::fill method from exectuion
portfolio-performance,portfolio,0bf1d193b1cc767f1fff757e93529d296c7610d9,https://github.com/portfolio-performance/portfolio/commit/0bf1d193b1cc767f1fff757e93529d296c7610d9,Modify Swissquote PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-swissquote-bank/15835/21
portfolio-performance,portfolio,818b9d2332445310eef560eba7a6b5930992251d,https://github.com/portfolio-performance/portfolio/commit/818b9d2332445310eef560eba7a6b5930992251d,Modify Comsorbank PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-consorsbank/2697/289
portfolio-performance,portfolio,d34b3833d39eb8a7ec7e6efb5de52e73a302d748,https://github.com/portfolio-performance/portfolio/commit/d34b3833d39eb8a7ec7e6efb5de52e73a302d748,Add new Tradegate AG PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606 https://forum.portfolio-performance.info/t/pdf-import-von-tradegate-ag/28606/2
portfolio-performance,portfolio,b147d6ba527befebeadaecbdfb3147e88e31d257,https://github.com/portfolio-performance/portfolio/commit/b147d6ba527befebeadaecbdfb3147e88e31d257,Add new Raisin Bank AG PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-weltsparen-raisin-bank/28544
portfolio-performance,portfolio,b302023ed829ca924266b07b6e9fdfeb32c3402d,https://github.com/portfolio-performance/portfolio/commit/b302023ed829ca924266b07b6e9fdfeb32c3402d,Modify Trade Republic PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-trade-republic/5107/521
portfolio-performance,portfolio,de401b9dc5ff7f0e43476ed69f643a91136760f5,https://github.com/portfolio-performance/portfolio/commit/de401b9dc5ff7f0e43476ed69f643a91136760f5,Add new Firstrade Securities Inc. PDF-Importer  https://forum.portfolio-performance.info/t/pdf-import-von-firstrade/28276/4
portfolio-performance,portfolio,6f9997bb8dedec3204e0e7973428fe6aacdc83a8,https://github.com/portfolio-performance/portfolio/commit/6f9997bb8dedec3204e0e7973428fe6aacdc83a8,Modify Comdirect PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-comdirect/1647/380  Rename VerkaufMitSteuerbehandlung08 in Verkauf04 which is without tax treatment
portfolio-performance,portfolio,cd3dc367bfeaf05f83e827d701c83e28441af6d5,https://github.com/portfolio-performance/portfolio/commit/cd3dc367bfeaf05f83e827d701c83e28441af6d5, Modify Raiffeisenbank PDF-Importer to support new transactions (#4036)  https://forum.portfolio-performance.info/t/pdf-import-von-raiffeisenbank-bankgruppe/12535/135
portfolio-performance,portfolio,a0079a75b95410f5882f1b3a4222fcbbbde1fda6,https://github.com/portfolio-performance/portfolio/commit/a0079a75b95410f5882f1b3a4222fcbbbde1fda6, Modify ING DIBa PDF-Importer to support new transaction  https://forum.portfolio-performance.info/t/pdf-import-von-ing/7114/157
portfolio-performance,portfolio,3276a30fc815f0ce9fc8329f8d552455f43d2df3,https://github.com/portfolio-performance/portfolio/commit/3276a30fc815f0ce9fc8329f8d552455f43d2df3,fix Tooltips of ActualValueStackedChart in Discreet mode  Closes https://github.com/portfolio-performance/portfolio/issues/4020
Helium314,HeliBoard,eec197c32cfc809a82fa88a524fdc317807d8ddc,https://github.com/Helium314/HeliBoard/commit/eec197c32cfc809a82fa88a524fdc317807d8ddc,cache subtype display names for improved performance
Helium314,HeliBoard,d3bd97a1043c98a4504ef8a6ea7753f6ee8d3d1f,https://github.com/Helium314/HeliBoard/commit/d3bd97a1043c98a4504ef8a6ea7753f6ee8d3d1f,reload text on selection updates even if selection is as expected  if composing region was changed e.g. KDE Connect removes composing region after entering a letter  and we should be able to deal with it this is not really a good solution  as it will reload the suggestions  which flashes the underline and has noticeable performance impact  fixes #1141
Helium314,HeliBoard,6e50d6e20820fb50e8ab1fcf4c7d250d790853b3,https://github.com/Helium314/HeliBoard/commit/6e50d6e20820fb50e8ab1fcf4c7d250d790853b3,perform recapitalization on shift only in alphabet mode  fixes #1256
Helium314,HeliBoard,5b1f40f0f6c415a17e554df217a75df98598da3a,https://github.com/Helium314/HeliBoard/commit/5b1f40f0f6c415a17e554df217a75df98598da3a,do a sanity check when setting composing text on text fields that have suggestions disabled fixes #225 could cause unnecessary text / suggestion reloads or performance regressions  though not found in testing
prometheus,jmx_exporter,0a3437b2e3c5f4ccac1042bab0abf842c3bc7fd2,https://github.com/prometheus/jmx_exporter/commit/0a3437b2e3c5f4ccac1042bab0abf842c3bc7fd2,Redesign cache for better performance (#1163)  Signed-off-by: Rafał Sumisławski <rafal.sumislawski@coralogix.com>
freeplane,freeplane,46d42dbeb7725ecc3973f1a30801ab87c9044105,https://github.com/freeplane/freeplane/commit/46d42dbeb7725ecc3973f1a30801ab87c9044105,Issue #2041 Cannot perform consecutive rename operations on categorized tag
freeplane,freeplane,6fdc8e4fd70988479fdff72770831c5e09bf48a8,https://github.com/freeplane/freeplane/commit/6fdc8e4fd70988479fdff72770831c5e09bf48a8,Fix java.lang.ArrayIndexOutOfBoundsException:  at java.base/java.util.Vector.elementAt(Vector.java:466) at java.desktop/javax.swing.table.DefaultTableColumnModel.getColumn(DefaultTableColumnModel.java:298) at org.freeplane.view.swing.features.time.mindmapmode.nodelist.NodeList.initializeUI(NodeList.java:712) at org.freeplane.view.swing.features.time.mindmapmode.nodelist.NodeList.startup(NodeList.java:500) at org.freeplane.view.swing.features.time.mindmapmode.nodelist.AllMapsNodeListAction.actionPerformed(AllMapsNodeListAction.java:48) at java.desktop/javax.swing.SwingUtilities.notifyAction(SwingUtilities.java:1810)
Guardsquare,proguard,03d7effdd2be72db980814a44b745f99bbff4d2d,https://github.com/Guardsquare/proguard/commit/03d7effdd2be72db980814a44b745f99bbff4d2d,Improve DictionaryNameFactory performance
spring-projects,spring-data-jpa,16448d88a634ad72e558d3349446f6f4040f60fa,https://github.com/spring-projects/spring-data-jpa/commit/16448d88a634ad72e558d3349446f6f4040f60fa,Add HQL rendering tests.  Extend tests for parsing/rendering hql functions and expressions. Assert count query creation works fine for when nested function calls are present in select. Fix minor rendering issue that added superfluous leading whitespace to expressions nested within a function. Bring newly added reserved keywords into order.  Original Pull Request: #3691
spring-projects,spring-data-jpa,8c690555fc68a3652fd08a0e02296af9151ae686,https://github.com/spring-projects/spring-data-jpa/commit/8c690555fc68a3652fd08a0e02296af9151ae686,Move Benchmarks from `performance` module into `spring-data-jpa`.  Closes #3655
spring-projects,spring-data-jpa,58fe95f1d63477a1c4a00b186d08ace56689f863,https://github.com/spring-projects/spring-data-jpa/commit/58fe95f1d63477a1c4a00b186d08ace56689f863,Optimize entity deletion in SimpleJpaRepository.  This change improves the performance of the delete method by first checking if the entity is already managed by the EntityManager. If so  it removes the entity directly without additional database queries. This optimization can reduce unnecessary database lookups in certain scenarios.  Closes #3564
spring-projects,spring-data-jpa,b8319a07b9e3abf2c914ff09b66decc83430ca09,https://github.com/spring-projects/spring-data-jpa/commit/b8319a07b9e3abf2c914ff09b66decc83430ca09,Add performance module.  Add new module using JMH to benchmark certain aspects of query parsing & rendering.  See: #3309
wildfly,wildfly,de908e0298acff4ab0570447eec5866d1cf84ed1,https://github.com/wildfly/wildfly/commit/de908e0298acff4ab0570447eec5866d1cf84ed1,[WFLY-20521] Revert "[WFLY-19393] Persistence container bytecode enhancement must be enabled by default to ensure better performance"  This reverts commit 2caa7d617b148be19a5f2c70830fc28dc592c3c2.
wildfly,wildfly,2caa7d617b148be19a5f2c70830fc28dc592c3c2,https://github.com/wildfly/wildfly/commit/2caa7d617b148be19a5f2c70830fc28dc592c3c2,[WFLY-19393] Persistence container bytecode enhancement must be enabled by default to ensure better performance
wildfly,wildfly,6fc93af1c284fb2227cc1575ada47f889e439fdf,https://github.com/wildfly/wildfly/commit/6fc93af1c284fb2227cc1575ada47f889e439fdf,Merge pull request #18112 from pferraro/WFLY-19613  WFLY-19613 Immutability performance optimizations
apache,maven-mvnd,0bdeee9dcb0b8a8f9e648648c9ba037f8ef91315,https://github.com/apache/maven-mvnd/commit/0bdeee9dcb0b8a8f9e648648c9ba037f8ef91315,[MMG-8600] Adopt to Maven changes (#1277)  Adopt to https://github.com/apache/maven/pull/2134  Goal is to be able to perform required customizations to Terminal  but also "raw streams" (as Maven acts on false  while mvnd on true).
DependencyTrack,dependency-track,25d63180e857225519e0fe9177f93ce9882e5191,https://github.com/DependencyTrack/dependency-track/commit/25d63180e857225519e0fe9177f93ce9882e5191,Reduce database round-trips during BOM processing  In the previous implementation  a `SELECT` query was issued for every single component and service in a BOM  in order to find existing components that match their identity.  In retrospect  this causes a lot of unnecessary database round-trips and puts the database under unnecessary stress  in particular for new projects where no components and services exist yet.  Now  we query all existing components and services of the project once in bulk.  A situation where this approach can perform worse  is when a BOM is uploaded to an existing project  and the content differs wildly between BOM and project. We would then load many components into memory  only to delete them shortly after. However  this scenario should be less common. Usually  projects are either empty  or have significant overlap with the uploaded BOM.  Backports https://github.com/DependencyTrack/hyades-apiserver/pull/1006  Signed-off-by: nscuro <nscuro@protonmail.com>
DependencyTrack,dependency-track,04f5ccc1ede03dd26af2cda3e6a9994bd7cf8901,https://github.com/DependencyTrack/dependency-track/commit/04f5ccc1ede03dd26af2cda3e6a9994bd7cf8901,Merge pull request #3869 from nscuro/issue-3811  Improve performance of findings retrieval
DependencyTrack,dependency-track,ba17eb211f421f804d97b9312a358ca0403e6c24,https://github.com/DependencyTrack/dependency-track/commit/ba17eb211f421f804d97b9312a358ca0403e6c24,Improve performance of findings retrieval  The `/v1/finding/{projectUuid}` endpoint has historically been slow to respond (#3811). While the "main" query behind it is somewhat optimized SQL already  it still suffered from various performance killers:  * Filtering of suppressed findings was done in-memory  and required fetching of individual `Analysis` records *for every single finding*. * `Clob` fields were not mapped directly from the SQL query result  but instead by re-fetching `Component` and `Vulnerability` records *for every single finding*  such that the ORM would provide properly `String`-ified field values. * Aliases were fetched *for every single finding* individually. * Latest component versions were fetched *for every single finding* individually.  Performance was improved via the following changes:  1. Filtering of suppressed findings is moved to the main SQL query  voiding the need to fetch individual `Analysis` records later. This also reduces the overall result set that needs to be transferred and mapped. 2. Mapping of `Clob` fields is done within the `Finding` constructor  voiding the need to re-fetch `Vulnerability` records in order to retrieve `String` values for them. 3. Aliases are loaded in bulk  and in a way that avoids redundant queries if the same `Vulnerability` appears multiple times within a list of `Finding`s. 4. Latest component versions are loaded in bulk  and in a way that avoids redundant queries if the same `Component` appears multiple times within a list of `Finding`s.  Because the modified functionality is re-used across the code base  multiple features benefit from this enhancement:  * `/v1/finding/{projectUuid}` endpoint * Corresponds to the *Audit Vulnerabilities* tab in the UI * `/v1/project/{projectUuid}/export` endpoint * CycloneDX exports for *Inventory with Vulnerabilities*  *VDR*  and *VEX* * Fortify  Kenna  and DefectDojo integrations  Signed-off-by: nscuro <nscuro@protonmail.com>
DependencyTrack,dependency-track,27e8a5c35148fe9c9971441f5330880cf7456d0e,https://github.com/DependencyTrack/dependency-track/commit/27e8a5c35148fe9c9971441f5330880cf7456d0e,Fix licenses not being resolved by name  The name was already considered for resolution  but the matching was only performed on license IDs.  Fixes #3781  Signed-off-by: nscuro <nscuro@protonmail.com>
apache,nutch,b02340dfecb26d0599f57358e18edfb12030ff34,https://github.com/apache/nutch/commit/b02340dfecb26d0599f57358e18edfb12030ff34,Merge pull request #827 from sebastian-nagel/NUTCH-3067  NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved
apache,nutch,633fa10d821870e7c96022836285072a4cbb5997,https://github.com/apache/nutch/commit/633fa10d821870e7c96022836285072a4cbb5997,NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved  - fix typo in name of variable
apache,nutch,63da6267f82e49778abae3549a9a2d5c5b2e5cd8,https://github.com/apache/nutch/commit/63da6267f82e49778abae3549a9a2d5c5b2e5cd8,NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved  - more verbose logging when reaching the Fetcher throughput threshold  when emptying fetch queues and when aborting with hung threads - add note that fetcher.throughput.threshold.retries should not exceed the timeout defined by mapreduce.task.timeout and fetcher.threads.timeout.divisor
apache,nutch,bd2fce6ff391ef7f6291cae86e295472d9eb45ac,https://github.com/apache/nutch/commit/bd2fce6ff391ef7f6291cae86e295472d9eb45ac,NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved  - skip empty fetch queues which hold exception counts after the time configured in fetcher.exceptions.per.queue.clear.after has passed in addition to the delay defined by the exponential backoff
apache,nutch,0b06b1bc783ef9a3971f9dd8b7b5162fabd19b89,https://github.com/apache/nutch/commit/0b06b1bc783ef9a3971f9dd8b7b5162fabd19b89,NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved  - purge and block queues which are delayed because of exceptions in case the next fetch would happen after the fetcher timelimit
apache,nutch,e053ed078412c17ff9f0fa2a23848511ca4b052e,https://github.com/apache/nutch/commit/e053ed078412c17ff9f0fa2a23848511ca4b052e,NUTCH-3067 Improve performance of FetchItemQueues if error state is preserved  - reduce memory footprint of FetchItemQueue
aeron-io,agrona,c5ad4a1f7dcb2fbdae27d2df1a94411d07ce15d4,https://github.com/aeron-io/agrona/commit/c5ad4a1f7dcb2fbdae27d2df1a94411d07ce15d4,AtomicBuffer opaque operations (#313)  Added opaque operations to AtomicBuffer. An opaque operation provides atomicity and visibility  but it doesn't provide any ordering guarantees beyond coherence. So it doesn't order loads/stores to different addresses  only to its own address.  Opaque operations are great for performance counters or progress indicators and provide the least amount of overhead on the CPU (all modern CPUs are coherent); especially CPUs with a weak memory model like ARM or RISC-V.
aeron-io,agrona,98f5ebc957e5abd585a334a6d8c5d53d82990376,https://github.com/aeron-io/agrona/commit/98f5ebc957e5abd585a334a6d8c5d53d82990376,Added MarkFile.timestampRelease (#318)  This method is a replacement for the timestampOrdered. The ordered methods use an old naming schema and the release methods use the new naming schema.  There is a slight performance penalty because the timestampOrdered calls the timestampRelease method.
aeron-io,agrona,92e7bc4c9ecd59b23ccc52444ef869e5a60ee9f5,https://github.com/aeron-io/agrona/commit/92e7bc4c9ecd59b23ccc52444ef869e5a60ee9f5,AtomicBuffer acquire/release methods (#314)  * AtomicBuffer acquire/release operations.  The AtomicBuffer has release method for every ordered method. So for e.g. a AtomicBUffer.putLongOrdered  there is a putLongRelease.  The ordered method will call the release version  so there is a sligth performance penalty.  Also acquire get methods have been added that have slightly weaker memory ordering semantics compared to a volatile read. Which could provide more oppertunity for the JIT to do its magic and in theory could give better performance on ISAs with weaker memory models.  * Added JCStressTests for release/acquire  * Added @since  * Fixed checkstyle issues in UnsafeBufferTest
aeron-io,agrona,a49ea6713a0551ba61acb6f8358017b848fd3bd8,https://github.com/aeron-io/agrona/commit/a49ea6713a0551ba61acb6f8358017b848fd3bd8,Improve performance of IntHashSet via avoiding the fill operation by using 0 as MISSING_VALUE.
apache,tika,469cd40c058c5de4269cd5f8ea8d88adb9008922,https://github.com/apache/tika/commit/469cd40c058c5de4269cd5f8ea8d88adb9008922,TIKA-4290 Replaced string concatenation in loop - for every loop previously a StringBuilder was implicitly created. It's performance related (#1900)
apache,lucene,b5e79a345fbe7116983d6cbad1c325b0c097eacf,https://github.com/apache/lucene/commit/b5e79a345fbe7116983d6cbad1c325b0c097eacf,Refactor main top-n bulk scorers to evaluate hits in a more term-at-a-time fashion. (#14701)  `MaxScoreBulkScorer` and `BlockMaxConjunctionBulkScorer` currently evaluate hits in a doc-at-a-time (DAAT) fashion  meaning that they they look at all their clauses to find the next doc and so forth until all docs from the window are evaluated. This changes evaluation to run in a more term-at-a-time fashion (TAAT) within scoring windows  meaning that each clause is fully evaluated within the window before moving on to the next clause.  Note that this isn't completely new  `BooleanScorer` has been doing this to exhaustively evaluate disjunctive queries  by loading their matches into a bit set  one clause at a time. Also note that this is a bit different from traditional TAAT as this is scoped to small-ish windows of doc IDs  not the entire doc ID space.  This in-turn allows these scorers to take advantage of the new `Scorer#nextDocsAndScores` API  and provides a good speedup. A downside is that we may need to perform more memory copying in some cases  and evaluate a bit more documents  but the change still looks like a win in general.
apache,lucene,92e0eb8f71e2eaad8c2d18890ed3e506bbfdc5b8,https://github.com/apache/lucene/commit/92e0eb8f71e2eaad8c2d18890ed3e506bbfdc5b8,Don't perform additional KNN querying after timeout  fixes #14639 (#14640)
apache,lucene,22279df7471748b43fd0a93af68206ff739a0d49,https://github.com/apache/lucene/commit/22279df7471748b43fd0a93af68206ff739a0d49,Adding benchmark for histogram collector over point range query (#14622)  * Adding benchmark for histogram collector over point range query  * Fixing perf issue when subtree doesnt overlap query for histogram collection
apache,lucene,d05d6fe173f8544ca06af7fc274122ad8848c7c4,https://github.com/apache/lucene/commit/d05d6fe173f8544ca06af7fc274122ad8848c7c4,Provide better impacts for fields indexed with IndexOptions.DOCS (#14511)  Postings always return impacts with freq=Integer.MAX_VALUE and norm=1 when frequencies are not indexed (IndexOptions.DOCS). This significantly overestimates the score upper bound of term queries  since the similarity scorer is effectively called with freq=1 all the time in this case (and either norm=1 if norms are not indexed  or the number of terms in the field otherwise).  This updates postings to always return impacts with freq=1 and norm=1 when frequencies are not indexed  which helps compute better score upper bounds  and in-turn makes dynamic pruning perform better.  Closes #14445
apache,lucene,d72021a1a7baf7ae1b11f6a1aa9e96bd90cb1858,https://github.com/apache/lucene/commit/d72021a1a7baf7ae1b11f6a1aa9e96bd90cb1858,Fix leadCost calculation in BooleanScorerSupplier.requiredBulkScorer (#14543)  Fixes #14542 by setting leadCost in BooleanScorerSupplier.requiredBulkScorer to the minimum of both MUST and FILTER clauses' costs. This bug caused performance regressions in some queries. More details are in the original issue.
apache,lucene,686a5b8def51f52c06fc9b5c714262685ac52a45,https://github.com/apache/lucene/commit/686a5b8def51f52c06fc9b5c714262685ac52a45,Add support for determining off-heap memory requirements for KnnVectorsReader (#14426)  This PR adds support to KnnVectorsReader in order to determine the off-heap memory requirements.  The motivation here is to give better insight into the size of off-heap memory that will be needed  so that deployments can be better scaled so that vector search workloads fit in memory  in order to provide best execution performance.
apache,lucene,46917196dc3e23e2ad703e42b7486fbd6dc8c265,https://github.com/apache/lucene/commit/46917196dc3e23e2ad703e42b7486fbd6dc8c265,knn search - add tests to perform exact search when filtering does not return enough results (#14274)  When doing approximate knn search  it's possible that the approximate search returns less than k results. In case there is a filter  we know the filter cost so we can check if there are actually more than k results that match the filter.  In that case  we can switch to exact search to ensure all possible results are returned  as the approximate search was not able to find all results that matched the filter.  This was already done by #14160 - this PR adds tests to check this specific case.
apache,lucene,fefa4e7076fb33eed33fa1af88b5559c007fbaae,https://github.com/apache/lucene/commit/fefa4e7076fb33eed33fa1af88b5559c007fbaae,RegExp: add CASE_INSENSITIVE_RANGE support (#14381)  Add optional flag to support case-insensitive ranges. A minimal DFA is always created. This works with Unicode but may have a performance cost.  Each codepoint in the range must be iterated  and any alternatives added to a set. This can be large if the range spans much of Unicode.  CPU and memory costs are contained within a single function enabled by the optional flag. For example when matching a caseless /[a-z]/  54 codepoints will be accumulated into an int[]  which is then compressed to 4 ranges before adding to the parse tree.  Closes #14378
apache,lucene,f28a8ae29c22aa1ebce51ced5f18525b444312c7,https://github.com/apache/lucene/commit/f28a8ae29c22aa1ebce51ced5f18525b444312c7,Bump floor segment size to 16MB. (#14189)  This bumps the floor segment size from 2MB (`TieredMergePolicy`) / 1.6MB (`LogByteSizeMergePolicy`) to 16MB.  My motivation is that such small segment sizes don't make index structures actually helpful vs. linear scans  so we should avoid them. Furthermore  there has been progress on merging rules for segments below the floor size  in particular merge policies no longer perform quadratic merging (#900) so this change will not make indexing/merging absurdly slow if an application flushes tiny segments.
apache,lucene,88d7709de7e4ac8a7f8fbe97cb2f71e32f611652,https://github.com/apache/lucene/commit/88d7709de7e4ac8a7f8fbe97cb2f71e32f611652,Fix optimization to help inline calls to live docs. (#14294)  While doing benchmarks on indexes with deletions  I found a bug in `ScorerUtil`  which optimizes live docs for the wrong class: `FixedBitSet` instead of `FixedBit`. Another performance bug is that `FixedBits` did not override `Bits#applyMask` with a more efficient implementation.
apache,lucene,2d422afe31025a35c182fceab08303d4d9bcf784,https://github.com/apache/lucene/commit/2d422afe31025a35c182fceab08303d4d9bcf784,Add histogram facet capabilities. (#14204)  This is inspired from a paper by Tencent where the authors describe how they speed up so-called "histogram queries" by sorting the index by timestamp translating ranges of values corresponding to each histogram bucket to ranges of doc IDs. This way  at collection time  they no longer need to look up values and can compute the histogram purely by looking at collected doc IDs.  YU  Muzhi  LIN  Zhaoxiang  SUN  Jinan  et al. TencentCLS: the cloud log service with high query performances. Proceedings of the VLDB Endowment  2022  vol. 15  no 12  p. 3472-3482.  Instead of binary-searching the doc ID space to translate histogram buckets into ranges of doc IDs  the new collector manager uses recently introduced support for sparse indexing. When playing with the geonames dataset  computing a histogram of the elevation field runs ~2-3x faster with this optimization than with the naive implementation.
apache,lucene,f315f53acab338e92a14b073efcadcb5ce8d5435,https://github.com/apache/lucene/commit/f315f53acab338e92a14b073efcadcb5ce8d5435,Optimize ContextQuery with big number of contexts (#14169)  When there are big number of contexts  ContextQuery may take a lot of time because of how context automata are constructed. Instead of the currentt appraoch of repeatedly concatenating and unioning context automata  this PR first constucts all individual context automata and then does one single union at the end.  Thus for the added test with 1000 contexts  the performance improved from 4000 ms to 18 ms.
apache,lucene,e4b85cab57602ddcd9c7e2e6647be9988621ebbe,https://github.com/apache/lucene/commit/e4b85cab57602ddcd9c7e2e6647be9988621ebbe,Implement IntersectVisitor#visit(IntsRef) whenever it makes sense (#14138)  Implement IntersectVisitor#visit(IntsRef) in many of the current implementations and add BulkAdder#add(IntsRef) method. They should provide better performance due to less virtual method calls and more efficient bulk processing.
apache,lucene,34f0453283a45acac366539d08d47a5e8939204a,https://github.com/apache/lucene/commit/34f0453283a45acac366539d08d47a5e8939204a,Add two new "Seeded" Knn queries for seeded vector search (#14084)  ### Description  In some vector search cases  users may already know some documents that are likely related to a query. Let's support seeding HNSW's scoring stage with these documents  rather than using HNSW's hierarchical stage.  An example use case is hybrid search  where both a traditional and vector search are performed. The top results from the traditional search are likely reasonable seeds for the vector search. Even when not performing hybrid search  traditional matching can often be faster than traversing the hierarchy  which can be used to speed up the vector search process (up to 2x faster for the same effectiveness)  as was demonstrated in [this article](https://arxiv.org/abs/2307.16779) (full disclosure: seanmacavaney is an author of the article).  The main changes are: - A new "seeded" focused knn collector and collector manager - Two new basic knn queries that expose using these specialized collectors for seeded entrypoint - `HnswGraphSearcher`  which bypasses the `findBestEntryPoint` step if seeds are provided.   //cc @seanmacavaney  Co-authored-by: Sean MacAvaney <smacavaney@bloomberg.com> Co-authored-by: Sean MacAvaney <sean.macavaney@gmail.com> Co-authored-by: Christine Poerschke <cpoerschke@apache.org>
apache,lucene,e34e0824fdbe200af72add419d43471938d56e5d,https://github.com/apache/lucene/commit/e34e0824fdbe200af72add419d43471938d56e5d,Reduce specialization in `ForUtil` and `ForDeltaUtil`. (#14048)  These classes specialize all bits per value up to 24. But performance of high numbers of bits per value is not very important  because they are used by short postings lists  which are fast to iterate anyway. So this PR only specializes up to 16 bits per value.  For instance  if a postings list uses blocks of 17 bits per value  it means that one can find gaps of 65 536 consecutive doc IDs that do not contain the term. Such rare terms do not drive query performance.
apache,lucene,c88f9334e5c99abbeb4f233f9606873e5037c118,https://github.com/apache/lucene/commit/c88f9334e5c99abbeb4f233f9606873e5037c118,Introduce a BulkScorer for DisjunctionMaxQuery. (#14040)  This introduces a bulk scorer for `DisjunctionMaxQuery` that delegates to the bulk scorers of the query clauses. This helps make the performance of top-level `DisjunctionMaxQuery` better  especially when its clauses have optimized bulk scorers themselves (e.g. disjunctions).
apache,lucene,6c48b404cd4c5a48435350007e1f5f41a0f3d01c,https://github.com/apache/lucene/commit/6c48b404cd4c5a48435350007e1f5f41a0f3d01c,Combine all postings enum impls of the default codec into a single class (#14033)  Recent speedups by making call sites bimorphic made me want to play with combining all postings enums and impacts enums of the default codec into a single class  in order to reduce polymorphism. Unfortunately  it does not yield a speedup since the major polymorphic call sites we have that hurt performance (DefaultBulkScorer  ConjunctionDISI) are still 3-polymorphic or more.  Yet  reduced polymorphism at little performance impact is a good trade-off as it would help make call sites bimorphic for users who don't have as much query diversity as nightly benchmarks  or in the future when we remove other causes of polymorphism.
apache,lucene,07955ff88d6ef900ee471426408ad02e42312309,https://github.com/apache/lucene/commit/07955ff88d6ef900ee471426408ad02e42312309,Avoid performance regression by constructing lazily the PointTree in NumericComparator (#13498) (#13877)
apache,lucene,e37eaea11a523fd31ed6fa320b8b3479c18a7841,https://github.com/apache/lucene/commit/e37eaea11a523fd31ed6fa320b8b3479c18a7841,Revert "Disjunction as CompetitiveIterator for numeric dynamic pruning (#13221)" (#13857) (#13971)  This reverts commit 1ee4f8a1115d1de623f242014681032d87ed2c1e.  We have observed performance regressions that can be linked to #13221. We will need to revise the logic that such change introduced in main and branch_10x. While we do so  I propose that we bake it out of branch_10_0 and we release Lucene 10 without it.  Closes #13856
apache,lucene,0bbef32cf500b4c4e630b05c744d8cfc44460c37,https://github.com/apache/lucene/commit/0bbef32cf500b4c4e630b05c744d8cfc44460c37,Check ahead if we can get the count (#13899)  Currently  we traverse the BKD tree or perform a binary search using DocValues first  and then check whether the count can be obtained in the count() method of IndexSortSortedNumericDocValuesRangeQuery.  we should consider providing a mechanism to perform this check beforehand  avoid unnecessary processing when dealing with a sparseRange
apache,lucene,a779a64d7b1c6f63f489d5c6a11693169b9e4362,https://github.com/apache/lucene/commit/a779a64d7b1c6f63f489d5c6a11693169b9e4362,Move BooleanScorer to work on top of Scorers rather than BulkScorers. (#13931)  I was looking at some queries where Lucene performs significantly worse than Tantivy at https://tantivy-search.github.io/bench/  and found out that we get quite some overhead from implementing `BooleanScorer` on top of `BulkScorer` (effectively implemented by `DefaultBulkScorer` since it only runs term queries as boolean clauses) rather than `Scorer` directly.  The `CountOrHighHigh` and `CountOrHighMed` tasks are a bit noisy on my machine  so I did 3 runs on wikibigall  and all of them had speedups for these two tasks  often with a very low p-value.  In theory  this change could make things slower when the inner query has a specialized bulk scorer  such as `MatchAllDocsQuery` or a conjunction. It does feel right to optimize for term queries though.
apache,lucene,6d987e1ce1c3f3215633a979ce048829fe1bb6ed,https://github.com/apache/lucene/commit/6d987e1ce1c3f3215633a979ce048829fe1bb6ed,Disable intra-merge parallelism for all structures but kNN vectors (#13799)  After adjusting tests that truly exercise intra-merge parallelism  more issues have arisen. See: https://github.com/apache/lucene/issues/13798  To be risk adverse & due to the soon to be released/freezed Lucene 10 & 9.12  I am reverting all intra-merge parallelism  except for the parallelism when merging HNSW graphs.  Merging other structures was never really enabled in a release (we disabled it in a bugfix for Lucene 9.11). While this is frustrating as it seems like we leaving lots of perf on the floor  I am err'ing on the side of safety here.  In Lucene 10  we can work on incrementally reenabling intra-merge parallelism.  closes: https://github.com/apache/lucene/issues/13798
apache,lucene,74e3c44063a7e643bcc1c63594d5eb956e050209,https://github.com/apache/lucene/commit/74e3c44063a7e643bcc1c63594d5eb956e050209,Unify how missing field entries are handle in knn formats (#13641)  During segment merge we must verify that a given field has vectors and exists. The typical knn format checks assume the per-field format is used and thus only check for `null`.  But we should check for field existence in the field info and verify it has dense vectors  Additionally  this commit unifies how the knn formats work and they will throw if a non-existing field is queried. Except for PerField format  which will return null (like the other per field formats)
apache,lucene,fafd6af004e0c39582043b797555d6eeb9aa7638,https://github.com/apache/lucene/commit/fafd6af004e0c39582043b797555d6eeb9aa7638,Add support for intra-segment search concurrency (#13542)  This commit introduces support for optionally creating slices that target leaf reader context partitions  which allow them to be searched concurrently. This is good to maximize resource usage when searching force-merged indices  or indices with rather big segments  by parallelizig search execution across subsets of segments being searched.  Note: this commit does not affect default generation of slices. Segments can be partitioned by overriding the `IndexSearcher#slices(List<LeafReaderContext>)` method to plug in ad-hoc slices creation. Moreover  the existing  `IndexSearcher#slices` static method now creates segment partitions when the additional `allowSegmentsPartitions` argument is set to `true`.  The overall design of this change is based on the existing search concurrency support that is based on `LeafSlice` and `CollectorManager`. A new `LeafReaderContextPartition` abstraction is introduced  that holds a reference to a `LeafReaderContext` and the range of doc ids it targets. A `LeafSlice` noew targets segment partitions  each identified by a `LeafReaderContext` instance and a range of doc ids. It is possible for a partition to target a whole segment  and for partitions of different segments to be combined into the same leaf slices freely  hence searched by the same thread. It is not possible for multiple partitions of the same segment to be added to the same leaf slice.  Segment partitions are searched concurrently leveraging the existing `BulkScorer#score(LeafCollector collector  Bits acceptDocs  int min  int max)` method  that allows to score a specific subset of documents for a provided `LeafCollector`  in place of the `BulkScorer#score(LeafCollector collector  Bits acceptDocs)` that would instead score all documents.  ## Changes that require migration  The migrate guide has the following new clarifying items around the contract and breaking changes required to support intra-segment concurrency: - `Collector#getLeafCollector` may be called multiple times for the same leaf across distinct `Collector` instances created by a `CollectorManager`. Logic that relies on `getLeafCollector` being called once per leaf per search needs updating. - a `Scorer`  `ScorerSupplier` or `BulkScorer` may be requested multiple times for the same leaf - `IndexSearcher#searchLeaf` change of signature to accept the range of doc ids - `BulkScorer#score(LeafCollector  BitSet)` is removed in favour of `BulkScorer#score(LeafCollector  BitSet  int  int)` -  static `IndexSearcher#slices` method changed to take a last boolean argument that optionally enables the creation of segment partitions - `TotalHitCountCollectorManager` now requires that an array of `LeafSlice`s  retrieved via `IndexSearcher#getSlices`  is provided to its constructor   Note: `DrillSideways` is the only component that does not support intra-segment concurrency and needs considerable work to do so  due to its requirement that the entire set of docs in a segment gets scored in one go.  The default searcher slicing is not affected by this PR  but `LuceneTestCase` now randomly leverages intra-segment concurrency. An additional `newSearcher` method is added that takes a `Concurrency` enum as the last argument in place of the `useThreads` boolean flag. This is important to disable intra-segment concurrency for `DrillSideways` related tests that do support inter-segment concurrency but not intra-segment concurrency.  ## Next step  While this change introduces support for intra-segment concurrency  it only sets up the foundations of it. There is still a performance penalty for queries that require segment-level computation ahead of time  such as points/range queries. This is an implementation limitation that we expect to improve in future releases  see #13745.  Additionally  we will need to decide what to do about the lack of support for intra-segment concurrency in `DrillSideways` before we can enable intra-segment slicing by default. See #13753 .  Closes #9721
apache,lucene,79fd9fee97bf9d74e8cfda0d403c4ce8f50a75b3,https://github.com/apache/lucene/commit/79fd9fee97bf9d74e8cfda0d403c4ce8f50a75b3,Speed up advancing within a block. (#13692)  Advancing within a block consists of finding the first index within an array of 128 values whose value is greater than or equal a target. Given the small size  it's not obvious whether it's better to perform a linear search  a binary search or something else... It is surprisingly hard to beat the linear search that we are using today.  Experiments suggested that the following approach works in practice: - First check if the next item in the array is greater than or equal to the target. - Then find the first 4-values interval that contains our target. - Then perform a branchless binary search within this interval of 4 values.  This approach still biases heavily towards the case when the target is very close to the current index  only a bit less than a linear search.
apache,lucene,b4a8810b7aea2fa6143aa0323f924f85c5cb3329,https://github.com/apache/lucene/commit/b4a8810b7aea2fa6143aa0323f924f85c5cb3329,Inline skip data into postings lists (#13585)  This updates the postings format in order to inline skip data into postings. This format is generally similar to the current `Lucene99PostingsFormat`  e.g. it shares the same block encoding logic  but it has a few differences: - Skip data is inlined into postings to make the access pattern more sequential. - There are only 2 levels of skip data: on every block (128 docs) and every 32 blocks (4 096 docs).  In general  I found that the fact that skip data is inlined may slow down a bit queries that don't need skip data at all (e.g. `CountOrXXX` tasks that never advance of consult impacts) and speed up a bit queries that advance by small intervals. The fact that the greatest level only allows skipping 4096 docs at once means that we're slower at advancing by large intervals  but data suggests that it doesn't significantly hurt performance.
apache,lucene,8d4f7a6e99d2da802b7019247b0f8f305d71c024,https://github.com/apache/lucene/commit/8d4f7a6e99d2da802b7019247b0f8f305d71c024,Bump the window size of disjunction from 2 048 to 4 096. (#13605)  It's been pointed multiple times that a difference between Tantivy and Lucene is the fact that Tantivy uses windows of 4 096 docs when Lucene has a 2x smaller window size of 2 048 docs and that this might explain part of the performance difference. luceneutil suggests that bumping the window size to 4 096 does indeed improve performance for counting queries  but not for top-k queries. I'm still suggesting to bump the window size across the board to keep our disjunction scorer consistent.
apache,lucene,392ddc154f229e141ddbf32dde945b0fefe99b0e,https://github.com/apache/lucene/commit/392ddc154f229e141ddbf32dde945b0fefe99b0e,Introduce TestLucene90DocValuesFormatVariableSkipInterval for testing docvalues skipper index (#13550)  this commit makes possible to configure dynamically the interval size for doc values skipperfor testing  and add a new test suite that changes the interval size randomly.
apache,lucene,3304b60c9cc2fd23531263ea9d95dc3e92c0b2ce,https://github.com/apache/lucene/commit/3304b60c9cc2fd23531263ea9d95dc3e92c0b2ce,Improve VectorUtil::xorBitCount perf on ARM (#13545)  This commit improves the performance of VectorUtil::xorBitCount on ARM by ~4x.  This change is effectively a workaround for the lack of vectorization of Long::bitCount on ARM.  On x64 there is no issue  the long variant of xorBitCount outperforms the int variant by ~15%.
apache,lucene,2a8d328ab22261d22616370b51da088aa005223f,https://github.com/apache/lucene/commit/2a8d328ab22261d22616370b51da088aa005223f,Replace AtomicLong with LongAdder in HitsThresholdChecker (#13546)  The value for the global count is incremented a lot more than it is read  the space overhead of LongAdder seems irrelevant => lets use LongAdder. The performance gain from using it is the higher the more threads you use  but at 4 threads already very visible in benchmarks.
apache,lucene,512ff4ac9241751d448d75d32ad987ee1a5a91ad,https://github.com/apache/lucene/commit/512ff4ac9241751d448d75d32ad987ee1a5a91ad,MultiTermQuery return null for ScoreSupplier (#13454)  MultiTermQuery return null for ScoreSupplier if there are no terms in an index that match query terms.  With the introduction of PR #12156 we saw degradation in performance of bool queries where one of the mandatory clauses is a TermInSetQuery with query terms not present in the field. Before for such cases TermsInSetQuery returned null for ScoreSupplier which would shortcut the whole bool query.  This PR adds ability for MultiTermQuery to return null for ScoreSupplier if a field doesn't contain any query terms.  Relates to PR #12156
apache,lucene,846aa2f8c3f1e6a691f0448d90023b1bbb5b5ada,https://github.com/apache/lucene/commit/846aa2f8c3f1e6a691f0448d90023b1bbb5b5ada,Use `ReadAdvice#NORMAL` on files that have a forward-only access pattern. (#13450)  This applies to files where performing readahead could help: - Doc values data (`.dvd`) - Norms data (`.nvd`) - Docs and freqs in postings lists (`.doc`) - Points data (`.kdd`)  Other files (KNN vectors  stored fields  term vectors) keep using a `RANDOM` advice.
apache,lucene,ddf538d43e94a814f783fcc40728ee45038a03a1,https://github.com/apache/lucene/commit/ddf538d43e94a814f783fcc40728ee45038a03a1,Move bulkScorer() from Weight to ScorerSupplier (#13408)  This relates to #13359: we want to take advantage of the `Weight#scorerSupplier` call to start scheduling some I/O in the background in parallel across clauses. For this to work properly with top-level disjunctions  we need to move `#bulkScorer()` from `Weight` to `ScorerSupplier` as well  so that the disjunctive `BooleanQuery` first performs a call to `Weight#scorerSupplier()` on all inner clauses  and then `ScorerSupplier#bulkScorer` on all inner clauses.  `ScorerSupplier#get` and `ScorerSupplier#bulkScorer` only support being called once. This forced me to fix some inefficiencies in `bulkScorer()` implementations when we would pull scorers and then throw it away when realizing that the strategy we were planning on using was not optimal. This is why e.g. `ReqExclBulkScorer` now also supports prohibited clauses that produce a two-phase iterator.
internetarchive,heritrix3,4a49557b8b28a7423867e2b0aea7e2b099585897,https://github.com/internetarchive/heritrix3/commit/4a49557b8b28a7423867e2b0aea7e2b099585897,Force strict if URL matching regex.  Adds a list of regular expressions that URLs being processed by the ConfigurableExtractorJS are evaluted against. If they match the extraction is performed in strict mode  even if strict mode is not set.  This requires a minor modification to ExtractorJS so that the CrawlURI is passed to the shouldAddUri method that ConfigurableExtractorJS overrides.
soot-oss,soot,8d201bb94d0d23513d82806137489a4cbd0edcdf,https://github.com/soot-oss/soot/commit/8d201bb94d0d23513d82806137489a4cbd0edcdf,Merge pull request #2138 from MarcMil/mdev  Fix the ConstantValueToInitializerTransformer & Performance optimizations for type resolver
soot-oss,soot,4047aae55472e871d7c4c100639e7e6abce4e424,https://github.com/soot-oss/soot/commit/4047aae55472e871d7c4c100639e7e6abce4e424,Improve type assigner performance  Parallelize minimize typing check and reuse local defs/uses.
soot-oss,soot,21d341fb3e28ef4c3c890ff3154120f873e47ceb,https://github.com/soot-oss/soot/commit/21d341fb3e28ef4c3c890ff3154120f873e47ceb,Merge pull request #2116 from MarcMil/fixes  Improve performance of dex typer & introduce Array Type Cache
soot-oss,soot,8154e49c35e92a3e21ea7734c8914c7f00bbe331,https://github.com/soot-oss/soot/commit/8154e49c35e92a3e21ea7734c8914c7f00bbe331,Separate constant typings for performance reasons
soot-oss,soot,6df988477e5d77d8e4bee3753efd044c11f193bb,https://github.com/soot-oss/soot/commit/6df988477e5d77d8e4bee3753efd044c11f193bb,Improve performance of typer
soot-oss,soot,0069dc36c45498a35758560f24bc4c0cd3e1d5dc,https://github.com/soot-oss/soot/commit/0069dc36c45498a35758560f24bc4c0cd3e1d5dc,Merge pull request #2115 from MarcMil/fixes  Fix a bug with explicit cassts in dex and improve performance of type assigner
soot-oss,soot,987955d1ac6d224fcb29749b316021a558427190,https://github.com/soot-oss/soot/commit/987955d1ac6d224fcb29749b316021a558427190,Performance
soot-oss,soot,b1b2022506a7f73c7f64e536f1c2525bb497a26f,https://github.com/soot-oss/soot/commit/b1b2022506a7f73c7f64e536f1c2525bb497a26f,Fix a bug with explicit cassts in dex and improve performance of typer slightly
igniterealtime,Openfire,14832dacf84faf3cef2f6e29ee0efda52dd9ea20,https://github.com/igniterealtime/Openfire/commit/14832dacf84faf3cef2f6e29ee0efda52dd9ea20,OF-3048: Improve ClientSession comparison performance (clustering)  Comparing ClientSessions typically involves evaluating the 'is anonymous' property of a session.  In a cluster  the property value of remote sessions is retrieved under guard of a distributed lock. As comparison of larger sets frequently evaluates the property value  that lock is obtained very frequently. This dramatically reduces performance.  As the property will never change after authentication  it's safe to cache the value. This improves performance considerably.  Note that RemoteClientSession instances are typically short-lived (as they're typically discarded after use)  which reduces the effectiveness of this change. Still  even without re-use  the performance improvement is considerable: in a cluster with 10 000 sessions  the responsiveness of the session summary page is improved from many minutes to 20 to 30 seconds in my test environment.
igniterealtime,Openfire,6d0de547f5feb4ed36d3d6e1cee2c0e8b76de6a6,https://github.com/igniterealtime/Openfire/commit/6d0de547f5feb4ed36d3d6e1cee2c0e8b76de6a6,OF-3028: Netty threads from 'child' EventLoop should use Netty-default settings  As we're overriding the thread factories used for Netty EventLoops to change the names of threads  we've also changed the default configuration of threads used by Netty.  Netty's default configuration can be expected to be optimized for performance. Openfire should use a similar configuration.  In this commit  the configuration is reverted back to the default configuration that's used by Netty (based on `io.netty.util.concurrent.DefaultThreadFactory`).
igniterealtime,Openfire,741fc5560869663a0fe46bcb1bb565ee92f5c0dd,https://github.com/igniterealtime/Openfire/commit/741fc5560869663a0fe46bcb1bb565ee92f5c0dd,Chore: when debug logging  replace string concat of message  Instead of concatenation  use the `{}` construct will prevent building the string until it's actually logged.  Although it's probably a performance improvement  it's mainly a readability improvement.
igniterealtime,Openfire,ad0c69b5d3b61c8defdd0dfbd4fe778756f62da6,https://github.com/igniterealtime/Openfire/commit/ad0c69b5d3b61c8defdd0dfbd4fe778756f62da6,Using the more generic org.eclipse.jetty.server.Handler in place of the more specific org.eclipse.jetty.server.handler.ContextHandler implementation. According to the [Migration Guide](https://jetty.org/docs/jetty/12/programming-guide/migration/11-to-12.html#api-changes-handler-sequence)  Handler.Sequence replaced HandlerCollection and HandlerList. ContextHandlerCollection is retained for its efficient child ContextHandler selection. Unless performance considerations or the need for a more restrictive implementation justify it  a more generic implementation reduces code complexity and provides greater functional possibilities for plugins. (#2609)  Co-authored-by: “Huy <huy.vu@surevine.com> Co-authored-by: Guus der Kinderen <guus.der.kinderen@gmail.com>
igniterealtime,Openfire,67108eddd2346099eab7cd4081286548c8bf3d70,https://github.com/igniterealtime/Openfire/commit/67108eddd2346099eab7cd4081286548c8bf3d70,OF-2824: Manipulation of client-related caches in RoutingTable using the same lock  RoutingTableImpl has three closely related caches  that are used to represent the state of client session routes: - `usersSessionsCache` - `anonymousUsersCache` - `usersCache`  Each value in the first cache is expected to correspond to a value in one of the other two caches.  Under OF-2824  a bug is described where `usersCache` contains values that are _not_ in `usersSessionsCache`. That shouldn't be possible.  Prior to this commit  manipulation of these caches is performed under a lock obtained from each of the caches. This means that the overall operation of adding an entry to `usersSessionsCache` and one of the other two caches is _not_ guarded by one singular lock (instead  two locks are used  each guarding the operation pertaining to that particular cache). This leaves room for a race-condition.  This commit addresses the race condition by using one singular lock to guard manipulations in all of these caches.  As all caches use a JID (in either bare or full form) as their key value  the singular lock introduced by this commit is based on the bare JID of the key that's being manipulated. This lock is obtained from the `usersSessionsCache`.  This change can lead to more lock contention (as more operations are guarded by the same lock). Simultaneously  less acquiring of locks will take place (as many operations previously required two locks to be acquired  while now  only one is needed. What the effects are on performance is as of yet undetermined.  This commit also introduces some related  minor changes: - Logged messages are made more consistent - Some operations have been moved outside of the protection of a (potentially cluster-wide) lock  to improve performance - Where methods expect to be called with a full JID  exceptions are thrown when a bare JID is used. This fail-fast behavior is intended to uncover any existing or future bugs.
igniterealtime,Openfire,3c7a0c684939ca5738868689360d22cb3583904a,https://github.com/igniterealtime/Openfire/commit/3c7a0c684939ca5738868689360d22cb3583904a,OF-2824: Remove cluster-cache optimization  It is suspected that  under race conditions  the optimization removed by this commit introduces data inconsistency.  The optimization prevents modification of a second cache  as through the output of manipulation of the first cache it can be deduced that the second cache should already be in the expected state. By skipping manipulation of that second cache  a cluster-wide operation is prevented  which improves performance.  The presence-based override for the optimization - one that I frankly do not understand - becomes obsolete by this change  and is also removed.  The change introduced by this commit trades performance for more reliable data consistency. As an added benefit  the code becomes less complex  reducing maintenance costs.  This commit changes the public signature of the `addClientRoute` method of `RoutingTable`: it no longer returns a value. The return value is currently not used by Openfire's own code. It was used for only two days  back in 2007: it was introduced in commit a940eeff4f72e4e9da70fcd0b4a1db1b3c40cd8d where it was used to update a statistic. This statistic got removed two days later  in commit 67f9ab65c36b8a38f5ab7c480415897839da21f0. Given the nature of the code (RoutingTable being _very_ low level)  it is not expected that third-party code uses this method. It contract change should therefor be reasonably safe to do.
emanuele-f,PCAPdroid,1d89de51dc4363e726535c1d6fc1325772dc8c69,https://github.com/emanuele-f/PCAPdroid/commit/1d89de51dc4363e726535c1d6fc1325772dc8c69,Add ability to decrypt PCAP/Pcapng files  A new "Decrypt PCAP file" entry has been added to the main menu  which allows loading a PCAP+keylog or a Pcapng with secrets and show the decrypted data in PCAPdroud.  The decryption itself is performed by Wireshark  which is built as the standalone shared library libushark.so  thanks to ushark.  The shared library is loaded via dlopen to allow proper re-initialization of the static variables in Wireshark. This also provides the benefit to avoud unnecessary overhead and possible inteferences when not used.  HTTP/2 reassembly is properly supported (implemented in ushark) and content decoding works as expected.  See #351
apache,pdfbox,ed6def6fe9f5c63dedac9f3958f5b45edc5c8dea,https://github.com/apache/pdfbox/commit/ed6def6fe9f5c63dedac9f3958f5b45edc5c8dea,PDFBOX-5847: Improve performance of FileSystemFontProvider.scanFonts() by introducing an "only headers" mode for the font parsers where each table reads as little information as possible  by Mykola Bohdiuk  git-svn-id: https://svn.apache.org/repos/asf/pdfbox/trunk@1918773 13f79535-47bb-0310-9956-ffa450edef68
konsoletyper,teavm,7d865565f8bf52c24869ebee1e8b1fbdd0b48a4d,https://github.com/konsoletyper/teavm/commit/7d865565f8bf52c24869ebee1e8b1fbdd0b48a4d,wasm gc: improve disassembler performance
konsoletyper,teavm,64ceaf3958d45e958da9ae5dc7dd6ebe0fcabd82,https://github.com/konsoletyper/teavm/commit/64ceaf3958d45e958da9ae5dc7dd6ebe0fcabd82,wasm gc: improve deobfuscator performance
konsoletyper,teavm,39cbcfce66862090cb18d2def4fed7d5067b3a0c,https://github.com/konsoletyper/teavm/commit/39cbcfce66862090cb18d2def4fed7d5067b3a0c,wasm gc: improve performance when copying JS typed arrays to/from Java arrays
konsoletyper,teavm,baf61b1f4eb5d9b544e2779a28e5aa030ba0ff38,https://github.com/konsoletyper/teavm/commit/baf61b1f4eb5d9b544e2779a28e5aa030ba0ff38,classlib: improve performance of time zone compilation
konsoletyper,teavm,ab8fb13415f94b49c7e1e92c3971a44817f4f223,https://github.com/konsoletyper/teavm/commit/ab8fb13415f94b49c7e1e92c3971a44817f4f223,Refactor JUnit test runner and improve performance of running in  browser
konsoletyper,teavm,e14993f50992c2daf1f539b9ab225bc7be812cdc,https://github.com/konsoletyper/teavm/commit/e14993f50992c2daf1f539b9ab225bc7be812cdc,Improve performance of test running by running all tests from single class in same iframe without recompilation
konsoletyper,teavm,8cbbb35d9ba4f1a52ed67789ca0c41b4dbd03e36,https://github.com/konsoletyper/teavm/commit/8cbbb35d9ba4f1a52ed67789ca0c41b4dbd03e36,wasm gc: compiler performance optimization
konsoletyper,teavm,753a028fc9111c93ee87181d5c284e0f6be14704,https://github.com/konsoletyper/teavm/commit/753a028fc9111c93ee87181d5c284e0f6be14704,wasm gc: improve performance of JS interop
spring-projects,spring-batch,5a62de9031923aac0e76fc52b972ee2aafdd00c3,https://github.com/spring-projects/spring-batch/commit/5a62de9031923aac0e76fc52b972ee2aafdd00c3,Move ORDER BY in getLastStepExecution from DB to java  This addresses performance issues with large STEP_EXECUTION table on DB2.  Fixes #4657
apache,netbeans,a1c16923a7b53bd7e3831267090255d57885d9e9,https://github.com/apache/netbeans/commit/a1c16923a7b53bd7e3831267090255d57885d9e9,Merge pull request #8481 from mbien/compute-overrides-performance  ComputeOverrides: move ClasspathInfo creation out of inner loops
apache,netbeans,5780a4ce6007994bd32f8ebcbcebffa9ebb3097a,https://github.com/apache/netbeans/commit/5780a4ce6007994bd32f8ebcbcebffa9ebb3097a,Merge pull request #8437 from mbien/unused-pkgprivate-performance  Cache ClassIndex during UnusedDetector search
apache,netbeans,3c852705e11b30b32e9ae43dfe7a1fba4dbc3f4e,https://github.com/apache/netbeans/commit/3c852705e11b30b32e9ae43dfe7a1fba4dbc3f4e,Merge pull request #8423 from mbien/js-embedder-perf-scaling  Fix performance scaling problem in JS-embedder annotation scanner
apache,netbeans,ba7c010f12458eee1e6c4d7e6aab755caa994b33,https://github.com/apache/netbeans/commit/ba7c010f12458eee1e6c4d7e6aab755caa994b33,Fix performance scaling problem in JS-embedder annotation scanner  Reduced scan time from 102s to 13ms for a synthetic test file containing 10k inner classes with Override annotations (no javascript).
apache,netbeans,93c422f3639fa767b33c850fb5b82e331b09126b,https://github.com/apache/netbeans/commit/93c422f3639fa767b33c850fb5b82e331b09126b,Merge pull request #8417 from mbien/javadoc-hint-performance   Remove source level query from javadoc hint.
apache,netbeans,54231a1eecd01530d4f74ff4096b7be60dfd8a06,https://github.com/apache/netbeans/commit/54231a1eecd01530d4f74ff4096b7be60dfd8a06,Versioning: don't wait for indexer before refresh  - indexing and versioning status updates can run concurrently since they don't interact with each other. - git status refresh typically takes < 1s even in large projects. indexing can take much longer.  fix and performance:  - refresh versioning annotations on project open otherwise they might be missing on re-open until the tree is expanded - lock-free CacheIndex#get(file)
apache,netbeans,5118ff96fc41cc94a49b7674fec834b95eef10cb,https://github.com/apache/netbeans/commit/5118ff96fc41cc94a49b7674fec834b95eef10cb,PHP 8.4 Support: #[\Deprecated] Attribute (Part 1)  - https://github.com/apache/netbeans/issues/8035 - https://wiki.php.net/rfc#php_84 - https://wiki.php.net/rfc/deprecated_attribute - Fix/Improve `SemanticAnalysis` - Get deprecated attributes or phpdoc tags from ASTNodes(Attributes and PHPDoc comments) instead of getting deprecated flags from an index. That way  it should also improve performance. - Add the `IncorrectDeprecatedAttributeHintError` because "Deprecated" attribute cannot target type and field - Add unit tests for the navigator  hints  and semantic analysis
apache,netbeans,29306ab5eab265c1e6125be82ebd72b9e40a8b56,https://github.com/apache/netbeans/commit/29306ab5eab265c1e6125be82ebd72b9e40a8b56,Merge pull request #7740 from sdedic/lsp/protocol-tracing  Perform LSP tracing server-side.
apache,netbeans,27ac704c6eb896d2182985a52d3d1ae88effa3e2,https://github.com/apache/netbeans/commit/27ac704c6eb896d2182985a52d3d1ae88effa3e2,Perform LSP tracing server-side.
apache,netbeans,a2d29064140a57f6c913c4397dd640521111d8ea,https://github.com/apache/netbeans/commit/a2d29064140a57f6c913c4397dd640521111d8ea,Enterprise: Fix multiple places where null values could be encountered and are not protected   Observed exception:  java.lang.NullPointerException: Cannot invoke "org.netbeans.api.j2ee.core.Profile.isAtMost(org.netbeans.api.j2ee.core.Profile)" because "profile" is null at org.netbeans.modules.websvc.rest.editor.AsyncConverter.isApplicable(AsyncConverter.java:98) at org.netbeans.modules.websvc.rest.editor.AsyncConverterTask.run(AsyncConverterTask.java:61) at org.netbeans.modules.websvc.rest.editor.AsyncConverterTask.run(AsyncConverterTask.java:52) [catch] at org.netbeans.modules.java.source.JavaSourceAccessor$CancelableTaskWrapper.run(JavaSourceAccessor.java:273) at org.netbeans.modules.parsing.impl.TaskProcessor.callParserResultTask(TaskProcessor.java:561) at org.netbeans.modules.parsing.impl.TaskProcessor$RequestPerformer.run(TaskProcessor.java:786) at org.openide.util.lookup.Lookups.executeWith(Lookups.java:288) at org.netbeans.modules.parsing.impl.TaskProcessor$RequestPerformer.execute(TaskProcessor.java:702) at org.netbeans.modules.parsing.impl.TaskProcessor$CompilationJob.run(TaskProcessor.java:663) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) at org.openide.util.RequestProcessor$Task.run(RequestProcessor.java:1403) at org.netbeans.modules.openide.util.GlobalLookup.execute(GlobalLookup.java:45) at org.openide.util.lookup.Lookups.executeWith(Lookups.java:287) at org.openide.util.RequestProcessor$Processor.run(RequestProcessor.java:2018)
apache,netbeans,0a958b681f868b724684c9f9ebc069834a5cdfa6,https://github.com/apache/netbeans/commit/0a958b681f868b724684c9f9ebc069834a5cdfa6,After 562cfade954ea27308de01738813d050d1b57ce1 ContextAction.Performer must be context aware  With 562cfade954ea27308de01738813d050d1b57ce1 state tracking was introduced to ContextAction. Both ContextAction and Performer are now tied to the lookup and thus a new Performer must be used when createContextAwareInstance is invoked.  To support this instead of a performer  a performer factory is supplied to the ContextAction and the Performer is created in the constructor of the ContextAction.
apache,netbeans,397d507ada822e9b483b26fd045a3526527992f4,https://github.com/apache/netbeans/commit/397d507ada822e9b483b26fd045a3526527992f4,Warmup Maven Embedder to improve first-project-creation UX  performance:  - first embedder initialization can take a while  async-profiler showed that a big chunk of it is spent within google guice - this starts the warmup task early  which solves the problem  since this will happen while the user is looking at the wizard - the warmup task is a no-op if it was already initialized  cleanup:  - small jdk 17 renovation
apache,netbeans,7ad885e30e2f5427d6927aaf8d300cc7ec447409,https://github.com/apache/netbeans/commit/7ad885e30e2f5427d6927aaf8d300cc7ec447409,Performance Improvements  CopyFinderBasedBulkSearch:  - implemented matches method - extracted matcher from loop  JavaFixUtilities:  - fast path for JavaFixUtilities can-safely-remove scanners
apache,netbeans,b4ab366c8541387cac7dabd020a9f6ef0384d1d6,https://github.com/apache/netbeans/commit/b4ab366c8541387cac7dabd020a9f6ef0384d1d6,Run maven embedder with the "normal" NetBeans environment not special context class loader  To parse JSF libraries in an (more or less) isolated environment  the JSF library parsing is done with a special context class loader in place.  However as part of the preparation of the parsing process also maven embedded is used to fetch the JSF reference implementation(s).  For a subset of users this exception was oberserved:  SEVERE [org.openide.util.Exceptions] java.lang.ClassCastException: class org.apache.commons.logging.impl.LogFactoryImpl cannot be cast to class org.apache.commons.logging.LogFactory (org.apache.commons.logging.impl.LogFactoryImpl is in unnamed module of loader org.netbeans.modules.web.jsf.editor.facelets.FaceletsLibrarySupport$3 @473436a0; org.apache.commons.logging.LogFactory is in unnamed module of loader org.netbeans.modules.netbinox.NetbinoxLoader @1f369488) at org.apache.commons.logging.LogFactory.newFactory(LogFactory.java:1445) at org.apache.commons.logging.LogFactory.getFactory(LogFactory.java:818) at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:987) at org.apache.http.conn.ssl.AbstractVerifier.<init>(AbstractVerifier.java:61) at org.apache.http.conn.ssl.AllowAllHostnameVerifier.<init>(AllowAllHostnameVerifier.java:44) at org.apache.http.conn.ssl.AllowAllHostnameVerifier.<clinit>(AllowAllHostnameVerifier.java:46) Caused: java.lang.ExceptionInInitializerError at org.apache.http.conn.ssl.SSLConnectionSocketFactory.<clinit>(SSLConnectionSocketFactory.java:151) at org.eclipse.aether.transport.http.GlobalState.newConnectionManager(GlobalState.java:169) at org.eclipse.aether.transport.http.LocalState.<init>(LocalState.java:57) at org.eclipse.aether.transport.http.HttpTransporter.<init>(HttpTransporter.java:197) at org.eclipse.aether.transport.http.HttpTransporterFactory.newInstance(HttpTransporterFactory.java:95) at org.eclipse.aether.internal.impl.DefaultTransporterProvider.newTransporter(DefaultTransporterProvider.java:94) at org.eclipse.aether.connector.basic.BasicRepositoryConnector.<init>(BasicRepositoryConnector.java:128) at org.eclipse.aether.connector.basic.BasicRepositoryConnectorFactory.newInstance(BasicRepositoryConnectorFactory.java:172) at org.eclipse.aether.internal.impl.DefaultRepositoryConnectorProvider.newRepositoryConnector(DefaultRepositoryConnectorProvider.java:122) at org.eclipse.aether.internal.impl.DefaultArtifactResolver.performDownloads(DefaultArtifactResolver.java:536) at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:449) at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifacts(DefaultArtifactResolver.java:261) at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifact(DefaultArtifactResolver.java:243) at org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveArtifact(DefaultRepositorySystem.java:278) at org.apache.maven.artifact.resolver.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:197) at org.apache.maven.artifact.resolver.DefaultArtifactResolver.resolveAlways(DefaultArtifactResolver.java:152) at org.netbeans.modules.maven.embedder.MavenEmbedder.resolveArtifact(MavenEmbedder.java:424) at org.netbeans.modules.maven.j2ee.MavenJsfReferenceImplementationProvider.artifactPathFor(MavenJsfReferenceImplementationProvider.java:81) at org.netbeans.modules.web.jsf.editor.facelets.FaceletsLibrarySupport.parseLibraries(FaceletsLibrarySupport.java:361) at org.netbeans.modules.web.jsf.editor.facelets.FaceletsLibrarySupport._findLibraries(FaceletsLibrarySupport.java:325)   The theory is  that running DefaultArtifactResolver#resolveAlways causes loading of class LogFactoryImpl. As the context class loader is replaced that context class loader is used for loading the class. That clashes with the LogFactory  that is loaded via NetBinox.  It is expected  that removing the maven embedder call resolves the problem or at least reduces the situation where this problem might occur.
apache,netbeans,e1dd39de3a236df5cb11152371e284769f54c3a5,https://github.com/apache/netbeans/commit/e1dd39de3a236df5cb11152371e284769f54c3a5,General cleanup (unused code/warning) in ContextAction  - Performer#attach is never called -> owner is never set -> remove it - Performer#enabler is never set -> stopListeners and startListeners are no-ops -> remove - weakEnableListener + weakActionListener are never set -> remove usages -> detach becomes noop -> remove - General warning cleanup - Remove unnecessary NONE reference  instDelegate is already null guarded so use that as sentinal
apache,paimon,abe5b4b496581b0a37a11965d4a72398e1695eb4,https://github.com/apache/paimon/commit/abe5b4b496581b0a37a11965d4a72398e1695eb4,[core] Improve performance of PartialUpdateMergeFunction with sequence group (#5481)
apache,paimon,1fc123e18bf1160e41f7afb70763968fc77d0497,https://github.com/apache/paimon/commit/1fc123e18bf1160e41f7afb70763968fc77d0497,[cdc] Fix database sync performance issue of schema evolution (#5382)
apache,paimon,274ed05699439d008094d10ffe6f49ce2ac523a6,https://github.com/apache/paimon/commit/274ed05699439d008094d10ffe6f49ce2ac523a6,[core] optimize the binlog table read performance (#4773)
apache,paimon,cdd4061db4b43393aab6fc5b2ce2c13ed34c69f3,https://github.com/apache/paimon/commit/cdd4061db4b43393aab6fc5b2ce2c13ed34c69f3,[core] Improve the performance of show tables with hive metastore (#4605)
apache,paimon,139b5a75a5f0b89bc9d9c91f8c06dfb68691c9e0,https://github.com/apache/paimon/commit/139b5a75a5f0b89bc9d9c91f8c06dfb68691c9e0,[core] Improve the performance of show tables (#4592)
apache,paimon,c7170e60f41a263770a3f4ba9d08b53ecdf69e9d,https://github.com/apache/paimon/commit/c7170e60f41a263770a3f4ba9d08b53ecdf69e9d,[flink] Make FileStoreLookupFunction.refreshBlacklist nullable to avoid performance regression
apache,paimon,668e673353f7a02aa6e160b3ac2f84e8c6746aad,https://github.com/apache/paimon/commit/668e673353f7a02aa6e160b3ac2f84e8c6746aad,[core] Improve TruncateSimpleColStatsCollector performance (#4338)
apache,paimon,95c3ce17f6b9cf25fe91c3ab5c573671d697afc2,https://github.com/apache/paimon/commit/95c3ce17f6b9cf25fe91c3ab5c573671d697afc2,[arrow] Remove field vector set row count to avoid performance issue (#4012)
apache,paimon,6da33cb6072ce90ad311450e0143aa9bf069d624,https://github.com/apache/paimon/commit/6da33cb6072ce90ad311450e0143aa9bf069d624,[core] Improve performance of dinstinct collect agg. (#3772)
apache,paimon,fcf30bdecb022f808462aa442aef91e0144b286a,https://github.com/apache/paimon/commit/fcf30bdecb022f808462aa442aef91e0144b286a,[cdc]Fix performance issue in CanalRecordParser (#3572)
apache,paimon,5d394d630c1f4e3e1d7644fbe63a0200b60a9414,https://github.com/apache/paimon/commit/5d394d630c1f4e3e1d7644fbe63a0200b60a9414,[flink] Fix performance issue in CdcActionCommonUtils (#3550)
apache,paimon,22c7c6136619563fe026dd854a7a05c10ee30ea8,https://github.com/apache/paimon/commit/22c7c6136619563fe026dd854a7a05c10ee30ea8,[core] Adjust VectoredReadable to better read performance (#3430)
apache,commons-lang,665f047e552ad71c189582af15a0b697133fff0b,https://github.com/apache/commons-lang/commit/665f047e552ad71c189582af15a0b697133fff0b,[StringUtils::indexOfAnyBut] redesign due to inconsistent/faulty behaviour regarding UTF-16 surrogates (#1327)  * [StringUtils::indexOfAnyBut] redesign due to inconsistent/faulty… …behaviour regarding UTF-16 surrogates  Both signatures of StringUtils::indexOfAnyBut currently behave inconsistently in matching UTF-16 supplementary characters and single UTF-16 surrogate characters (i.e. paired and unpaired surrogates)  since they differ unnecessarily in their algorithmic implementations  use their own incomplete and faulty interpretation of UTF-16 and don't take full advantage of the standard library.  The example cases below show that they may yield contradictory results or correct results for the wrong reasons.  This proposal gives a unified algorithmic implementation of both signatures that a) is much easier to grasp due to a clear mathematical set approach and safe iteration and doesn't become entangled in index arithmetic; stresses the set semantics of the 2nd argument b) fully relies on the standard library for defined UTF-16 handling/interpretation; paired surrogates are merged into one codepoint  unpaired surrogates are left as they are c) scales much better with input sizes and result index position d) can benefit from current and future improvements in the standard library and JVM (streams implementation  parallelization  JIT optimization  JEP 218  ???…)  The algorithm boils down to: find index i of first char in cs such that (cs.codePointAt(i) ∈ {x ∈ codepoints(cs) ∣ x ∉ codepoints(searchChars) })  Examples: ---------  <H>: high-surrogate character <L>: low-surrogate character (<H><L>): valid supplementary character signature 1: StringUtils::indexOfAnyBut(final CharSequence seq  final CharSequence searchChars) signature 2: StringUtils::indexOfAnyBut(final CharSequence cs  final char... searchChars)  Case 1: matching of unpaired high-surrogate ---------seq/cs-------searchChars------exp./new-----sig.1-------sig.2---  1.1     <H>aaaa      <H>abcd          !found       !found      !found sig.2: 'a' happens to follow <H> in searchChars; sig.1: 'a' is somewhere in searchChars  1.2     <H>baaa      <H>abcd          !found       !found      0 sig.1: 'b' is somewhere in searchChars  1.3     <H>aaaa      (<H><L>)abcd     0            !found      0 sig.1: 'a' is somewhere in searchChars  1.4     aaaa<H>      (<H><L>)abcd     4            !found      !found sig.1+2 don't interpret suppl. character  Case 2: matching of unpaired low-surrogate ---------seq/cs-------searchChars------exp./new-----sig.1-------sig.2---  2.1     <L>aaaa      (<H><L>)abcd     0            !found      !found sig.1+2 don't interpret suppl. character  2.2     aaaa<L>      (<H><L>)abcd     4            !found      !found sig.1+2 don't interpret suppl. character  Case 3: matching of supplementary character ---------seq/cs-------------searchChars-----exp./new----sig.1-----sig.2-  3.1     (<H><L>)aaaa       <L>ab<H>cd      0           !found    0 sig.1: <L> is somewhere in searchChars  3.2     (<H><L>)aaaa       abcd            0           1         0 sig.1 always points to low-surrogate of (fully) unmatched suppl. character  3.3     (<H><L>)aaaa       abcd<H>         0           0         1 3.4     (<H><L>)aaaa       abcd<L>         0           !found    0 sig.1: <H> skipped by algorithm  * [StringUtils::indexOfAnyBut] further reduction of algorithm  by simplifying set consideration: find index i of first char in seq such that (seq.codePointAt(i) ∉ { x ∈ codepoints(searchChars) })  * [StringUtils::indexOfAnyBut] simplify input-sequence iteration  by transforming ListIterator loop into index-based loop  advancing by Character.charCount(codepoint); enabling short-circuit processing  avoiding full in-advance processing of input-sequence  * [StringUtils:indexOfAnyBut] parameterization of test functions  providing a single source-of-truth (arguments stream) for the two function variants  * [StringUtils:indexOfAnyBut] remove comment  Set::contains of immutable Set has unclear desastrous performance issues when searching for large values (here: >0xffff) in a set of smaller values (including JDK 23)  ---------  Co-authored-by: IBue <>
apache,commons-lang,55a70c84f3658fd3ad3de72649b511ff21dc7175#r143834333,https://github.com/apache/commons-lang/commit/55a70c84f3658fd3ad3de72649b511ff21dc7175#r143834333,Reimplement RandomStringUtils on top of SecureRandom#getInstanceStrong() (#1235)  * Reimplement RandomStringUtils on top of SecureRandom#getInstanceStrong()  The previous implementation used  ThreadLocalRandom#current()  * Performance optimizations for RandomStringUtils  This commit improves the performance of RandomStringUtils:  * Reduces the number of random bytes generated and the number of calls to the random number generator  by using a cache system `AmortizedRandomBits`. * Optimizes the case of alphanumerical strings  reducing the number of rejections in the rejection sampling.  See comments in code for details.  * Code style and comment improvements  * Fix 2 checkstyle errors. * Apply suggestions from review for garydgregory/commons-lang#2 * Improve comments in new (non-public) class `AmortizedRandomBits` to match comments in other classes.  * Make class final  - Rename package-private class - Whitespace - Add null check - Add serialVersionUID - Remove redunant type cast - Throw IllegalStateException  not RuntimeException - nextBytes() should throw NullPointerException per contract - Javadoc: Use longer lines  * Apply comments by aherbert in
apache,commons-lang,f382d61a03778ccf838c6c051bd8692e4834dec2,https://github.com/apache/commons-lang/commit/f382d61a03778ccf838c6c051bd8692e4834dec2,Rename package-private class - Whitespace - Add null check - Add serialVersionUID - Remove redunant type cast - Throw IllegalStateException  not RuntimeException - nextBytes() should throw NullPointerException per contract - Javadoc: Use longer lines  * Apply comments by aherbert in  ---------  Co-authored-by: Gary Gregory <gardgregory@gmail.com> Co-authored-by: Fabrice Benhamouda <yfabrice@amazon.com>
Minestom,Minestom,461c56e7495b472d6c6a5f3d0c7b5abe4866adc9,https://github.com/Minestom/Minestom/commit/461c56e7495b472d6c6a5f3d0c7b5abe4866adc9,Revert "Improve collision performance (#2321)" (#2322)  This reverts commit f917ba1b9f3d7ff1991844b118f1f9a084b60072.
Minestom,Minestom,f917ba1b9f3d7ff1991844b118f1f9a084b60072,https://github.com/Minestom/Minestom/commit/f917ba1b9f3d7ff1991844b118f1f9a084b60072,Improve collision performance (#2321)  * Improve fastPhysics perfomance  * Re-add final to Vec
openrewrite,rewrite,3a8aab51759c4248860b7210ff110fd42fc985c4,https://github.com/openrewrite/rewrite/commit/3a8aab51759c4248860b7210ff110fd42fc985c4,Fix performance issue with `UpgradeDependencyVersion` (#5479)
openrewrite,rewrite,96e17a833e2e300e213de93563ebeb7ec86577f5,https://github.com/openrewrite/rewrite/commit/96e17a833e2e300e213de93563ebeb7ec86577f5,Improve performance of `MethodMatcher#matches(JavaType.Method)`  Won't make a big difference...
openrewrite,rewrite,15ba7737ab0e8a1b6047e4f0b4b5053617bda9db,https://github.com/openrewrite/rewrite/commit/15ba7737ab0e8a1b6047e4f0b4b5053617bda9db,PropertyPlaceholderHelper Not Handling Lack of Separation Between Placeholders (#5398)  * Adding test that is failing for `PropertyPlaceholderHelper` (no space between placeholders)  * Adding a number of other tests for other functionality present in `PropertyPlaceholderHelper`  including nested placeholders  value separators  etc. Changing calculation for the new start index after performing a replace (was off by the size of the placeholder suffix).
openrewrite,rewrite,174d5da9495373a425cf600bf006072a5c30f18a,https://github.com/openrewrite/rewrite/commit/174d5da9495373a425cf600bf006072a5c30f18a,Improve performance of FindAnnotations precondition (#5315)
openrewrite,rewrite,b288a97641b4fc51f7568e6fd464717bf1ef3314,https://github.com/openrewrite/rewrite/commit/b288a97641b4fc51f7568e6fd464717bf1ef3314,Remove metrics from `TreeVisitor#visit()`  As of OpenRewrite 8.14.5 we have the `RecipeRunStats` data table  which should provide enough actionable metrics when investigating performance issues.
openrewrite,rewrite,37dee365b67fe33f546438bf9aec7afd98340405,https://github.com/openrewrite/rewrite/commit/37dee365b67fe33f546438bf9aec7afd98340405,Improve `TypeMatcher` performance  No more `NoViableAltException` get instantiated.
openrewrite,rewrite,d5a44f7b545256f40d810b9b824c5be3da33147b,https://github.com/openrewrite/rewrite/commit/d5a44f7b545256f40d810b9b824c5be3da33147b,Performance improvement for `TypeTable.Reader` (#5093)  * Performance improvement for `TypeTable.Reader`  Only match row GAV against artifact name patterns when different from previous row's GAV.  * Performance improvement for `TypeTable.Reader`  Only match row GAV against artifact name patterns when different from previous row's GAV.  Also  don't accumulate classes over artifact boundaries.
openrewrite,rewrite,777d6e151554d77bbc8d00c7fdd05ad523d7787e,https://github.com/openrewrite/rewrite/commit/777d6e151554d77bbc8d00c7fdd05ad523d7787e,Remove Snappy dependency (#5092)  The `JavaTypeCache` now internally uses the `AdaptiveRadixTree` instead of a `HashMap` with Snappy-compressed keys.  Overall this results in a lower memory footprint:  ``` Retained AdaptiveRadixTree size:    7316248 bytes Retained Snappy size:               8575104 bytes Retained HashMap size:              9105560 bytes ```  Here are the JMH benchmark measurements showing an overall increase in performance (the tests with the `AddOpens` suffix started the JVM using `--add-opens java.base/java.lang=ALL-UNNAMED` to skip some `byte[]` allocations for increased performance):  ``` Benchmark                                                              Mode  Cnt         Score   Error   Units JavaTypeCacheBenchmark.readAdaptiveRadix                              thrpt    2       794.740           ops/s JavaTypeCacheBenchmark.readAdaptiveRadix:gc.alloc.rate                thrpt    2      2239.494          MB/sec JavaTypeCacheBenchmark.readAdaptiveRadix:gc.alloc.rate.norm           thrpt    2   2956386.085            B/op JavaTypeCacheBenchmark.readAdaptiveRadix:gc.count                     thrpt    2       104.000          counts JavaTypeCacheBenchmark.readAdaptiveRadix:gc.time                      thrpt    2        37.000              ms JavaTypeCacheBenchmark.readAdaptiveRadixAddOpens                      thrpt    2       886.947           ops/s JavaTypeCacheBenchmark.readAdaptiveRadixAddOpens:gc.alloc.rate        thrpt    2         0.002          MB/sec JavaTypeCacheBenchmark.readAdaptiveRadixAddOpens:gc.alloc.rate.norm   thrpt    2         1.842            B/op JavaTypeCacheBenchmark.readAdaptiveRadixAddOpens:gc.count             thrpt    2           ≈ 0          counts JavaTypeCacheBenchmark.readSnappy                                     thrpt    2       344.614           ops/s JavaTypeCacheBenchmark.readSnappy:gc.alloc.rate                       thrpt    2      3134.230          MB/sec JavaTypeCacheBenchmark.readSnappy:gc.alloc.rate.norm                  thrpt    2   9544228.824            B/op JavaTypeCacheBenchmark.readSnappy:gc.count                            thrpt    2       144.000          counts JavaTypeCacheBenchmark.readSnappy:gc.time                             thrpt    2        61.000              ms JavaTypeCacheBenchmark.writeAdaptiveRadix                             thrpt    2       650.472           ops/s JavaTypeCacheBenchmark.writeAdaptiveRadix:gc.alloc.rate               thrpt    2      4941.345          MB/sec JavaTypeCacheBenchmark.writeAdaptiveRadix:gc.alloc.rate.norm          thrpt    2   7969122.563            B/op JavaTypeCacheBenchmark.writeAdaptiveRadix:gc.count                    thrpt    2       112.000          counts JavaTypeCacheBenchmark.writeAdaptiveRadix:gc.time                     thrpt    2       101.000              ms JavaTypeCacheBenchmark.writeAdaptiveRadixAddOpens                     thrpt    2       754.388           ops/s JavaTypeCacheBenchmark.writeAdaptiveRadixAddOpens:gc.alloc.rate       thrpt    2      3604.748          MB/sec JavaTypeCacheBenchmark.writeAdaptiveRadixAddOpens:gc.alloc.rate.norm  thrpt    2   5012698.210            B/op JavaTypeCacheBenchmark.writeAdaptiveRadixAddOpens:gc.count            thrpt    2       100.000          counts JavaTypeCacheBenchmark.writeAdaptiveRadixAddOpens:gc.time             thrpt    2        81.000              ms JavaTypeCacheBenchmark.writeSnappy                                    thrpt    2       324.280           ops/s JavaTypeCacheBenchmark.writeSnappy:gc.alloc.rate                      thrpt    2      3355.699          MB/sec JavaTypeCacheBenchmark.writeSnappy:gc.alloc.rate.norm                 thrpt    2  10859397.103            B/op JavaTypeCacheBenchmark.writeSnappy:gc.count                           thrpt    2        77.000          counts JavaTypeCacheBenchmark.writeSnappy:gc.time                            thrpt    2       102.000              ms ```
openrewrite,rewrite,81ba6ec4029d8ebc6a4baa3d108f3ad1a1b907ca,https://github.com/openrewrite/rewrite/commit/81ba6ec4029d8ebc6a4baa3d108f3ad1a1b907ca,Improve performance by caching the handful of new LST elements this recipe needs to generate rather than re-parsing many similar variations.  Shouldn't make any difference if you're using only one copy of the recipe  but in a situation where it is invoked many times in a recipe run the advantages should be quite noticeable.
openrewrite,rewrite,ef2e4f39590cd8bea7456a9b41ec8773c6c45526,https://github.com/openrewrite/rewrite/commit/ef2e4f39590cd8bea7456a9b41ec8773c6c45526,ChangeDependencyConfiguration should only perform updates when the or… (#4941)  * ChangeDependencyConfiguration should only perform updates when the original configuration matches  * Polish
openrewrite,rewrite,2a8bf2d7d3bad4f90b668e0304f10c68b159bbd6,https://github.com/openrewrite/rewrite/commit/2a8bf2d7d3bad4f90b668e0304f10c68b159bbd6,Add lombok support for java-11 (#4769)  * Add lombok support for java-11  * Handle erroneous nodes in open rewrite (#4412)  * Handle erroneous nodes in a tree  * Add visitErroneous to all java parser visitors  * Override the visitVariable to handle erroneous identifier names set by JavacParser  * retain name and suffix for erroneous varDecl  * override the visitVariable to handle error identifiers in all java parser visitors  * Remove sysout  * Update rewrite-java-test/src/test/java/org/openrewrite/java/JavaParserTest.java  Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>  * Update rewrite-java-test/src/test/java/org/openrewrite/java/JavaParserTest.java  Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>  * handle errors in method params  variable declarations  fix tests  * Add missing license headers  * fix compilation error  * fix compilation error in Java8ParserVisitor  * Apply code suggestions from bot  * fix cases for statementDelim  * fix block statement template generator to handle adding semicolon  * fix ChangeStaticFieldToMethod recipe  * Record compiler errors from erroneous LST nodes  * Adjustments for comments  * Java 17 parser adjustment alos in 8  11 and 21  * Add `FindCompileErrorsTest` & move away from deprecated `print()`  ---------  Co-authored-by: Jonathan Schnéider <jkschneider@gmail.com> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: Tim te Beek <tim@moderne.io> Co-authored-by: aboyko <aboyko@vmware.com>  * Make Groovy Parser correctly handle nested parenthesis (#4801)  * WIP  * Format  * Format  * Move grabbing of whitespace and resetting cursor to where it is actually required  * Extra check is not required  * Use toString  * Add `emptyListLiteralWithParentheses` test  * Add `insideFourParenthesesAndEnters` test  * Move list tests all to ListTest  * Add `emptyMapLiteralWithParentheses`  * Review feedback and fix new testcases  * Add `attributeWithParentheses`  * Improve AttributeTest  * Improve AttributeTest  * Improve AttributeTest  * Improve AttributeTest  * Improve AttributeTest  * Review fix new testcases  * Revert edit to testcase  * Add and fix testcase with newline  * Add JavaDoc and move logic regarding whitespace and resetting cursor  ---------  Co-authored-by: lingenj <jacob.van.lingen@moderne.io>  * suppress javax.json (#4804)  * suppress javax.json  * Update suppressions.xml  * Refactor SpringReference (#4805)  * Separating and clearer naming  * Add license header  * Review feedback  * refactor: Update Gradle wrapper (#4808)  Use this link to re-run the recipe: https://app.moderne.io/recipes/org.openrewrite.gradle.UpdateGradleWrapper?organizationId=T3BlblJld3JpdGU%3D#defaults=W3sibmFtZSI6ImFkZElmTWlzc2luZyIsInZhbHVlIjoiRmFsc2UifV0=  Co-authored-by: Moderne <team@moderne.io>  * Add recipe to remove Gradle Enterprise and Develocity (#4809)  * Add recipe to remove Gradle Enterprise and Develocity  * Remove left over java plugin  * Add a UsesType precondition to ReplaceConstant  * Allow file scheme in `RemoteArchive` to simplify testing (#4791)  * Allow file scheme in `RemoteArchive` to simplify testing  While it might look a bit controversial  the file scheme can also point to a remote (for instance a mounted network share) file. By allowing the `file://` scheme we can use `RemoteArchive` for those files.  As a useful side effect  this makes testing RemoteArchive handling a lot easier.  * fix test  * Update rewrite-core/src/test/java/org/openrewrite/remote/RemoteArchiveTest.java  Co-authored-by: Sam Snyder <sam@moderne.io>  ---------  Co-authored-by: Sam Snyder <sam@moderne.io>  * Try alternative way of determining parenthesis level for `BinaryExpression` when AST doesn't provide `_INSIDE_PARENTHESES_LEVEL` flag (#4807)  * Add a `isClassAvailable` method to the ReflectionUtils (#4810)  * Add a `isClassAvailable` method to the ReflectionUtils  * Add a `isClassAvailable` method to the ReflectionUtils  * Add a `isClassAvailable` method to the ReflectionUtils  * Update rewrite.yml to enforce CompareEnumsWithEqualityOperator  * Correctly map generic return and parameter types in `JavaReflectionTypeMapping` (#4812)  * Polish formatting  * Add more scenarios to JavaTypeGoat for simply typed fields and methods that return exceptions.  * Support mapping of generic thrown exception types (#4813)  * refactor: Enum values should be compared with "==" (#4811)  Use this link to re-run the recipe: https://app.moderne.io/recipes/org.openrewrite.staticanalysis.CompareEnumsWithEqualityOperator?organizationId=T3BlblJld3JpdGU%3D  Co-authored-by: Moderne <team@moderne.io>  * Keep the names of generic type variables defined by methods. (#4814)  * Make the same performance improvement to parameter names allocations that we previously made to Java 17/21 in #3345.  * Fix Java reflection mapping of generic typed fields. (#4815)  * Revert parenthesis changes (#4818)  * Revert "Try alternative way of determining parenthesis level for `BinaryExpression` when AST doesn't provide `_INSIDE_PARENTHESES_LEVEL` flag (#4807)"  This reverts commit e59e48b3a6e6be18ecb779ac329a243ed025da58.  * Revert "Make Groovy Parser correctly handle nested parenthesis (#4801)"  This reverts commit 91a031a3d517be1fe78656eb6b841141b336c085.  * JavaTemplate bug when inserting `final var` into for-each (#4806)  * JavaTemplate bug when inserting `final var` into for-each  * Split variable declarations when they contain stop comment  * Reduce accidental changes between Java 11 and 17 parsers  * Add missing import  ---------  Co-authored-by: Udayani Vaka <79973862+vudayani@users.noreply.github.com> Co-authored-by: Jonathan Schnéider <jkschneider@gmail.com> Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com> Co-authored-by: Tim te Beek <tim@moderne.io> Co-authored-by: aboyko <aboyko@vmware.com> Co-authored-by: Laurens Westerlaken <laurens.westerlaken@jdriven.com> Co-authored-by: lingenj <jacob.van.lingen@moderne.io> Co-authored-by: Peter Streef <peter@moderne.io> Co-authored-by: Shannon Pamperl <shanman190@gmail.com> Co-authored-by: Moderne <team@moderne.io> Co-authored-by: Sam Snyder <sam@moderne.io>
openrewrite,rewrite,6dc9d5064071df4dac16c06a0c5a292a5dbcfa72,https://github.com/openrewrite/rewrite/commit/6dc9d5064071df4dac16c06a0c5a292a5dbcfa72,Keep the names of generic type variables defined by methods. (#4814)  * Make the same performance improvement to parameter names allocations that we previously made to Java 17/21 in #3345.
openrewrite,rewrite,8cfcf0c8c26f3a4abb361456ba83c8bf37e51ca5,https://github.com/openrewrite/rewrite/commit/8cfcf0c8c26f3a4abb361456ba83c8bf37e51ca5,Remove `Find#findAllNewLineIndexes()`  Minor performance optimization.
openrewrite,rewrite,d789bcb8418c985982b4ed5ef9a1a4b010ca8bf0,https://github.com/openrewrite/rewrite/commit/d789bcb8418c985982b4ed5ef9a1a4b010ca8bf0,Performance improvements of `Find` recipe (#4758)  * Implement some performance improvements on Find recipe  * Add extra tests  * Modify last test  * Restore linked list  * More performance gains  ---------  Co-authored-by: Tim te Beek <tim@moderne.io>
openrewrite,rewrite,01813e39fdd155a621f3e0113664d4f121a29f16,https://github.com/openrewrite/rewrite/commit/01813e39fdd155a621f3e0113664d4f121a29f16,`TypeUtils#isAssignable()` improvements (#4696)  * Add `TypeUtils` tests for primitives  * Correct `TypeUtils#isAssignableTo()` for primitive array types  * New `TypeUtils#isAssignableTo()` API  We need to consider the "position" when performing the assignability check. I.e. check if a parameterized type compared against a generic type variable is used in a context where the generic type parameter is bound by the parameterized type or not.  * Implement new API  * Remove one more `stream()` call  * Update Javadoc visitors to use `TypeUtils#isAssignableTo()` correctly  * Correction  * Corrections  * Corrections  * More Javadoc fixes  * Add missing `@MinimumJava11` annotation
openrewrite,rewrite,0cf8d79456287f2141c8f8e0428534e2b60e4c17,https://github.com/openrewrite/rewrite/commit/0cf8d79456287f2141c8f8e0428534e2b60e4c17,Yet another YAML `JsonPatchMatcher` performance tweak
openrewrite,rewrite,f30cd55a8de23ff9823dc1cf2a40343ac4b80edc,https://github.com/openrewrite/rewrite/commit/f30cd55a8de23ff9823dc1cf2a40343ac4b80edc,Yet another YAML `JsonPatchMatcher` performance tweak
openrewrite,rewrite,ae9083973ca333639ffc4b2231822bbdf7da4e6e,https://github.com/openrewrite/rewrite/commit/ae9083973ca333639ffc4b2231822bbdf7da4e6e,Improve performance of `JsonPathMatcher`s by only parsing once
openrewrite,rewrite,f71dc0d429184ba7094715a0cf6dd51e17873e2f,https://github.com/openrewrite/rewrite/commit/f71dc0d429184ba7094715a0cf6dd51e17873e2f,Improve performance of YAML `JsonPathMatcher`  Only apply `ReplaceAliasWithAnchorValueVisitor` to `Yaml.Document` elements.  See: #4650
openrewrite,rewrite,0f76203bede5f77c0df557f42c7bd5627f9b8c4b,https://github.com/openrewrite/rewrite/commit/0f76203bede5f77c0df557f42c7bd5627f9b8c4b,Slightly improve performance of `JsonPathMatcher`s  This is especially important for the YAML implementation  which applies the `ReplaceAliasWithAnchorValueVisitor` visitor to all elements.
openrewrite,rewrite,c63ab562c3347b341893d045e991d639bd12debb,https://github.com/openrewrite/rewrite/commit/c63ab562c3347b341893d045e991d639bd12debb,Skip UTF-8 BOM mark in `EncodingDetectingInputStream` and default to UTF-8 in `RewriteTest` (#4546)  * Skip UTF-8 BOM mark in `EncodingDetectingInputStream`  As the `EncodingDetectingInputStream` is only used as input for the parsers  we typically don't want to see any UTF-8 BOM marker. Additionally  platforms like .NET remove the BOM mark as well  so this change brings better compatibility.  * Add test for BOM skipping  * Improve performance of `EncodingDetectingInputStream` by a lot  * Fix `JavadocPrinterTest`  * Use UTF-8 as default encoding in `RewriteTest`  * Fix bug when reading from single byte stream  * Add missing test  * Adjust `CompilationUnitTest` whitespace  * Fix handling of empty files  * Polish one more test case
openrewrite,rewrite,4569dae252b61c66c942860e1365c7b28f8d2e73,https://github.com/openrewrite/rewrite/commit/4569dae252b61c66c942860e1365c7b28f8d2e73,Added Profile dependency migration (#4311)  * Added profile dependency migration into dependency  dependencyManagement and plugin section  * Apply formatter & remove superfluous tests  * Remove unused import  ---------  Co-authored-by: marcel-gepardec <marcel.reiter@gepardec.at>
cucumber,cucumber-jvm,20ec82a9a6edd35641c246463ecd76fa709990e9,https://github.com/cucumber/cucumber-jvm/commit/20ec82a9a6edd35641c246463ecd76fa709990e9,[Core] Improve caching glue performance (#2971)  The `CachingGlue` performance has been improved by caching the following elements:  - parameter types - data table types - docString types - step definitions  The cache is invalidated when a parameter type or a step definition is added/removed  or when the language changes. The cache is not used when scenario-scoped glue is present (e.g. cucumber-java8).   This improves the `CachingGlue` performance.  On a personal project with about 250 step definitions and 1000 test scenarios  the performance is the following:  | Version | Test duration | |---------|----------------| |7.20.1 (original) | 8.2 seconds | |7.21.0-SNAPSHOT (this PR) | 3.4 seconds (2.4x faster) |   ---------  Co-authored-by: M.P. Korstanje <rien.korstanje@gmail.com> Co-authored-by: Julien Kronegg <julien@kronegg.ch>
apache,incubator-hugegraph,f6f37081971f1bdecd726f685961977d53e98269,https://github.com/apache/incubator-hugegraph/commit/f6f37081971f1bdecd726f685961977d53e98269,BREAKING CHANGE(server): support "parent & child" EdgeLabel type (#2662)  HugeGraph supports the parent-child edge feature  meaning that an Edgelabel can have a subordinate type. Using the bank transfer graph as an example  transfers may include person-to-person transfers (person-to-person)  person-to-company transfers (person-to-company)  and company-to-company transfers (company-to-company). These three different types of transfers share a common operation  transfer.  In actual business scenarios  it is often necessary to retrieve all transfer edges with a single query. Currently  competitors can only manually split the transfer edge types  perform multiple queries  and then aggregate the results. With HugeGraph's parent-child edge feature  it is possible to query the corresponding person-to-person transfers  person-to-company transfers  and other sub-edge types  and also conveniently and efficiently retrieve all transfer-related edges at once.  PS: The parent-child edge feature for the cassandra and scylladb backends has been temporarily disabled through the store feature.  > Code formatting will be done separately in the next PR.  Related to: - https://github.com/apache/incubator-hugegraph/issues/745 - https://github.com/apache/incubator-hugegraph/issues/447
termux,termux-api,c986fb09787ecf18e0058e14f0a01615851d64ba,https://github.com/termux/termux-api/commit/c986fb09787ecf18e0058e14f0a01615851d64ba,Fixed(NfcAPI): Fix `termux-nfc -t x` resulting in error notification being shown as method was not returned from after sending error resulting in `ResultReturner.returnData()` being called twice  ``` Error in ResultReturner: java.io.IOException: Connection refused at android.net.LocalSocketImpl.connectLocal(Native Method) at android.net.LocalSocketImpl.connect(LocalSocketImpl.java:259) at android.net.LocalSocket.connect(LocalSocket.java:162) at com.termux.api.util.ResultReturner.lambda$returnData$0(SourceFile:250) at com.termux.api.util.ResultReturner.$r8$lambda$RFR2zSHu5FsJH7JvuCx4CPnUmMY(SourceFile:0) at com.termux.api.util.ResultReturner$$ExternalSyntheticLambda0.run(SourceFile:0) at java.lang.Thread.run(Thread.java:1119) Suppressed: java.lang.Exception: Called by: at com.termux.api.util.ResultReturner.returnData(SourceFile:239) at com.termux.api.apis.NfcAPI$NfcActivity.errorNfc(SourceFile:48) at com.termux.api.apis.NfcAPI$NfcActivity.onCreate(SourceFile:87) at android.app.Activity.performCreate(Activity.java:9155) at android.app.Activity.performCreate(Activity.java:9133) at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1521) at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:4262) at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:4467) at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:222) at android.app.servertransaction.TransactionExecutor.executeNonLifecycleItem(TransactionExecutor.java:133) at android.app.servertransaction.TransactionExecutor.executeTransactionItems(TransactionExecutor.java:103) at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:80) at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2823) at android.os.Handler.dispatchMessage(Handler.java:110) at android.os.Looper.loopOnce(Looper.java:248) at android.os.Looper.loop(Looper.java:338) at android.app.ActivityThread.main(ActivityThread.java:9067) at java.lang.reflect.Method.invoke(Native Method) at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:593) at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:932)  ```
assertj,assertj,aed17a70ed3998b90f15bf278ea14651350669f4,https://github.com/assertj/assertj/commit/aed17a70ed3998b90f15bf278ea14651350669f4,perf: improve performance on map key assertion - fix #3744
MeteorDevelopment,meteor-client,12d4b1f1a02b315a94d37f00cd6d1cc97610f3d4,https://github.com/MeteorDevelopment/meteor-client/commit/12d4b1f1a02b315a94d37f00cd6d1cc97610f3d4,Band-aid fix to various issues caused by player head textures being loaded on the wrong thread.  It wiped saved accounts and sometimes caused crashes. Not a perfect fix because the textures now load corrupted  but can be improved later.
mongodb,mongo-java-driver,01aff5a0789f71b9d0b56190d4996e8ac5436827,https://github.com/mongodb/mongo-java-driver/commit/01aff5a0789f71b9d0b56190d4996e8ac5436827,Implement Client Side Operation Timeout (CSOT) feature (#1215)  Introduce a unified Client Side Operation Timeout (CSOT) as an Alpha feature to simplify the multitude of existing timeout settings. This consolidation merges various complex timeout settings into a single  overarching operation timeout option  enhancing usability and predictability. Existing timeout options will continue to be honored if the operation timeout is not set. The options `serverSelectionTimeoutMS` and `connectTimeoutMS` are still honored even if the operation timeout is set.  Key Changes:  - Implement CSOT across various layers of the driver  applying the new operation timeout configuration to govern execution time in areas such as Authentication  Connection Monitoring and Pooling  Server Discovery and Monitoring  CRUD  GridFS  CSFLE  Transactions  Handshake  and Server Selection.  - Annotate CSOT API as Alpha  indicating that this public API is in the early stages of development and may undergo incompatible changes.  - Centralize the setting of maxTimeMS just before sending commands to the server. This optimization enhances performance and ensures precise enforcement of timeouts.  - Introduce TimeoutContext to centralize timeout-related logic  enhancing the clarity and ease of maintenance for timeout management within the driver.  - Enable a new retry mode when CSOT is active  allowing operations to retry until the timeout is reached. This improves resiliency compared to the previous behavior of retrying only once.  - Enhance MongoClient  MongoDatabase  MongoCollection  and ClientSession to support setting and inheriting the operation timeout.  - Introduce MongoCluster  a new conceptual entity representing server deployments  configurable with specific writeConcern  timeout  and read preference.  - Account for minimum Round Trip Time (RTT) for socket reads and writes when CSOT is enabled to ensure timeouts are adjusted for network latency  avoiding connection churn.  - Standardize CSOT error handling by transforming timeout errors into the new `MongoOperationTimeoutException`.  ---------  Co-authored-by: Ross Lawley <ross@mongodb.com> Co-authored-by: Maxim Katcharov <maxim.katcharov@mongodb.com> Co-authored-by: Jeff Yemin <jeff.yemin@mongodb.com> Co-authored-by: Viacheslav Babanin <slav.babanin@mongodb.com> Co-authored-by: Valentin Kovalenko <valentin.kovalenko@mongodb.com>
FabricMC,fabric,6a293bdb89e26d68b3174e7d2916979de01b1718,https://github.com/FabricMC/fabric/commit/6a293bdb89e26d68b3174e7d2916979de01b1718,Update Model Loading API to 1.21.4 (#4243)  * Update Model Loading API to 1.21.4  - Split model modifier events and callbacks - one set for static models and one set for block models - This is necessary because static models use UnbakedModel and are baked with settings while block models use GroupableModel and are not baked with settings - This cleans up the Identifier/ModelIdentifier getters - OnLoad for block models was not added because the unbaked block model map is not a cache and block models cannot inherit from other models - Make DelegatingUnbakedModel a record to allow accessing the delegate ID - Remove BuiltinItemRenderer  BuiltinItemRendererRegistry  and BuiltinItemRendererRegistryImpl as they were replaced by a TAW to SpecialModelTypes.ID_MAPPER  * Add fabric_ prefix to methods in BakedModelsHooks and fix checkstyle  * Remove ModelResolver and BlockStateResolver  - The functionality of ModelResolver could be perfectly replicated with ModelModifier.OnLoad with OVERRIDE_PHASE - The functionality of BlockStateResolver could be perfectly replicated with ModelModifier.BeforeBakeBlock with OVERRIDE_PHASE - Fix log warning caused by half_red_sand.json not defining particle sprite  * Re-add BlockStateResolver and ModelModifier.OnLoadBlock  - BeforeBakeBlock runs too late to allow modifying how models are grouped  so OnLoadBlock is necessary to allow that - OnLoadBlock only runs for models which were actually loaded from blockstate files  so BlockStateResolver is necessary to allow adding models for blocks without a corresponding blockstate file - Add UnwrappableBakedModel - Moved and renamed from FRAPI's WrapperBakedModel (original will be deleted in separate PR) - Implement it and interface inject it on vanilla's WrapperBakedModel - Add new static UnwrappableBakedModel#unwrap method which accepts a Predicate saying when to stop unwrapping - Add WrapperUnbakedModel which simply delegates all method calls to a different UnbakedModel  * Remove ModelModifier.*Bake*  - Remove BeforeBake  AfterBake  BeforeBakeBlock  AfterBakeBlock - Remove DelegatingUnbakedModel - Add WrapperGroupableModel - Add documentation and extra constructor to WrapperUnbakedModel  * Clarify OnLoad doc about meaning of null model
FabricMC,fabric,1e12ea3cc3245dcb6ddc5ebbc9ae9bd07130fc5b,https://github.com/FabricMC/fabric/commit/1e12ea3cc3245dcb6ddc5ebbc9ae9bd07130fc5b,Add more `c` fluid tags (#4134)  * Add more `c` fluid tags  * fixed a javadoc  * Cleaned up javadoc  * checkstyle  * removed space  * Adjust experience rate to more reasonable amount  * explain perfect extractions  * fixed javadoc  (cherry picked from commit dde8f6bb9cae32b7a667d5bcd3bee1d1fec020cd)
itwanger,paicoding,283f04d99451a96963da86380742cf45ed50554e,https://github.com/itwanger/paicoding/commit/283f04d99451a96963da86380742cf45ed50554e,Merge pull request #78 from wznanfang/main  perf: 补充实现方法缺失的注解
itwanger,paicoding,46dbaf6cd97e3cb7570c1c1e56dc648a71365990,https://github.com/itwanger/paicoding/commit/46dbaf6cd97e3cb7570c1c1e56dc648a71365990,perf: 补充实现方法缺失的注解
jobrunr,jobrunr,679181dfbe62f197390058ddb86b82aabd6461e0,https://github.com/jobrunr/jobrunr/commit/679181dfbe62f197390058ddb86b82aabd6461e0,improve readability+perfomance in comparison between instants (#1078)
risesoft-y9,Digital-Infrastructure,51e553cb4a70a23dda706d9083fdd6fe686bd33c,https://github.com/risesoft-y9/Digital-Infrastructure/commit/51e553cb4a70a23dda706d9083fdd6fe686bd33c,perf: 优化授权表查询
risesoft-y9,Digital-Infrastructure,af994d552bda8b05b46279a5cfd682cec0827458,https://github.com/risesoft-y9/Digital-Infrastructure/commit/af994d552bda8b05b46279a5cfd682cec0827458,perf: 使用 HashSet 代替 List
opensolon,solon,22d0cdbf295e2781053c98918bad7567c2f47d03,https://github.com/opensolon/solon/commit/22d0cdbf295e2781053c98918bad7567c2f47d03,perf: 添加 AssistantMessage:getResultContent 获取没有推理标签的内容
opensolon,solon,edca91a62f2e93bb78edd6d8f3abc68cda55fa89,https://github.com/opensolon/solon/commit/edca91a62f2e93bb78edd6d8f3abc68cda55fa89,perf: 添加 AssistantMessage:getResultContent 获取没有推理标签的内容
opensolon,solon,f2a3d718d33b09df0c1d201ff436ee05de660229,https://github.com/opensolon/solon/commit/f2a3d718d33b09df0c1d201ff436ee05de660229,perf: 添加 AssistantMessage:getResultContent 获取没有推理标签的内容
guardianproject,orbot-android,dc4288a3eb829459da848b095a1b17c50913711c,https://github.com/guardianproject/orbot-android/commit/dc4288a3eb829459da848b095a1b17c50913711c,superficial tv linting
guardianproject,orbot-android,ac3cc7f6293346be2ead67d810144d4cfff61d45,https://github.com/guardianproject/orbot-android/commit/ac3cc7f6293346be2ead67d810144d4cfff61d45,superficial tv linting
eclipse-collections,eclipse-collections,c94f63a119a61f1f825add792581c35d767ef2ba,https://github.com/eclipse-collections/eclipse-collections/commit/c94f63a119a61f1f825add792581c35d767ef2ba,Merge pull request #1621 from motlin/fix_perfomance_problem_in_mutableList_sublist
eclipse-collections,eclipse-collections,08af2e983cc9ce298289f9bd210fad076c166409,https://github.com/eclipse-collections/eclipse-collections/commit/08af2e983cc9ce298289f9bd210fad076c166409,Fix performance problem in MutableList.subList() and implement similar optimization as ArrayList.subList()
casbin,jcasbin,677c1061743235a0903514a06b665b9b52b89ee4,https://github.com/casbin/jcasbin/commit/677c1061743235a0903514a06b665b9b52b89ee4,feat: optimize convertInSyntax's performance (#450)  Optimise convertInSyntax method by compiling Pattern only once and creating StringBuffer only when needed.
Netflix,archaius,41cd26db871070363c5774dc6491c8dbfcff9be3,https://github.com/Netflix/archaius/commit/41cd26db871070363c5774dc6491c8dbfcff9be3,Merge pull request #736 from Netflix/type-error-handling  Small performance and memory usage tweaks.  Refactor PropertyImpl#get() to avoid an allocation on each call and to avoid locking. The new implementation should be marginally faster and removes a few dozen bytes from each Property instance.
Netflix,archaius,3cfa68100b263eccc8f7c111926ca7064b48d1c2,https://github.com/Netflix/archaius/commit/3cfa68100b263eccc8f7c111926ca7064b48d1c2,Small performance and memory usage tweaks  Refactor PropertyImpl#get() to avoid an allocation on each call. The new implementation should be marginally faster.  Removed a few unnecessary objects from each instance of Property objects returned from the DefaultPropertyFactory. This will remove a few dozen bytes from each Property instance.
Azure,azure-sdk-for-java,c44458e904ab416cd05c6975831821e498d208ce,https://github.com/Azure/azure-sdk-for-java/commit/c44458e904ab416cd05c6975831821e498d208ce,Perform Per-Partition Automatic Failover. (#44099)  * Adding bogus change.  * Initial changes.  * Refactoring.  * Refactoring.  * Reacting to review comments.  * React to 403/3.  * Refactoring.  * Fix merge issues.  * Add 503 handling for PPAF.  * Add 408 and HttpRequestException handling for PPAF.  * Sample code for reproducing change feed + binary encoding issue with array of array of nos.  * Revert.  * Added a way to opt into PPAF at the client-level.  * Enforce preferred regions when PPAF is enabled.  * Adding PPAF tests.  * Adding tests.  * Stabilizing CI pipeline.  * Stabilizing CI pipeline.  * Adding PPAF tests.  * Bug fixes  * Bug fixes  * Fixing PPAF tests.  * Adding PPAF tests for write failover behavior.  * Added read failover test scenario.  * Refactoring  * Refactoring  * Refactoring  * Fixing compilation errors.  * Refactoring.  * Adding PPAF unit tests.  * Adding PPAF unit tests.  * Adding PPAF unit tests.  * Adding serialization skeleton in ThinClientStoreModel  * Fix locationCache region resolution bug for writes and PPAF enabled.  * Adding tests for 408 handling with PPAF.  * Add PPAF failover info to diagnostics.  * Add PPAF failover info to diagnostics.  * Add PPAF failover info to diagnostics.  * Code refactoring.  * Fixing CI pipeline.  * Adding PPAF enablement flag in CosmosDiagnostics.  * Fixing CI issues.  * Add 408:20008 integration.  * Revert "Add 408:20008 integration."  This reverts commit 672e703bd766631fb9223f0ed56c84df1d1a74ce.  * Modify diagnostic string.  * Rectify next endpoint to choose for PPAF.  * Update javadoc.  * Fixing live tests pipeline.  * Added false progress compatible merge of session tokens.  * Added false progress compatible merge of session tokens.  * Fixing live tests pipeline.  * Fixing CI pipeline.  * Mark partition as down if hit with 408/503 from the Gateway.  * Add relevant system environment variables.  * Adding initial PPAF + other HA integrations.  * Fixing CI pipeline.  * Fixing CI pipeline.  * Adding PPAF + Exclude Region tests.  * Adding PPCB + Exclude Region tests where active region set size is to be kept > 1.  * Adding (PPCB + Exclude Region + PPAF) tests with empty preferred regions special casing.  * Adding integration tests with PPCB and single-write accounts  * Adding integration tests with PPCB and single-write accounts  * Fixing CI pipeline.  * Adding PPAF + GW mock tests.  * Fixing CI pipeline.  * Adding PPAF + GW mode tests.  * Adding PPAF + GW mode tests.  * Adding PPAF + GW mode tests.  * Refactoring.  * Add RegionalEndpoints  * fix variable name in resolveServiceEndpoint  * Handling empty preferred region + all region excluded scenario.  * Refactoring.  * Refactoring.  * Refactoring.  * Fixing CI pipeline.  * Fixing CI pipeline.  * Fixing CI pipeline.  * Fixing CI pipeline.  * Fixing CI pipeline.  * Fixing CI pipeline.  * Fixing CI pipeline.  * Addressing review comments.  * Addressing review comments.  * Modify `ResourceThrottleRetryPolicy`.  * Pinning relevant GitHub issues.  * Addressing review comments.  * Addressing review comments.  * Addressing review comments.  * Addressing review comments.  * Addressing RegionalRoutingContext.  * Pulling in `RegionalRoutingContext` changes.  * Pulling in `RegionalRoutingContext` changes.  * Fixing CI pipeline.  * Add retries to GW errors when PPAF has kicked in.  * Add license header.  * Fixing CI pipeline.  * Addressing review comments.  * Fixing CI pipeline.  * Pushing diagnostic changes.  * Fixing system property bug.  * Pin 404:1002 cross region retry handling to PPAF-override for reads (single-write account).  * Pin 404:1002 cross region retry handling to PPAF-override for reads (single-write account).  * Fix failback scheduler daemon issue.  * [PPCB]: Make failback flow best effort.  * [PPCB]: Make failback flow best effort.  * [PPCB]: Fix failover scenario for E2E timeout + GW Read Timeout.  * [PPCB]: Fix failover scenario for E2E timeout + GW Read Timeout.  * [PPCB]: Fix failover scenario for E2E timeout + GW Read Timeout.  * [PPCB]: Fix failover scenario for E2E timeout + GW Read Timeout.  * [PPCB]: Fix failover scenario for E2E timeout + GW Read Timeout.  * [PPCB]: Fix failover scenario for E2E timeout + GW Read Timeout.  * [PPCB]: Fix failover scenario for E2E timeout + GW Read Timeout.  * [PPCB]: Fix failover scenario for E2E timeout + GW Read Timeout.  * [PPCB]: Fix failover scenario for E2E timeout + GW Read Timeout.  * Fixing CI pipeline.  * Opt in into PPCB when PPAF is enabled.  * Opt in into PPCB when PPAF is enabled.  * Push initial e2e timeout tracking.  * Push initial e2e timeout tracking.  * Push initial e2e timeout tracking.  * Push initial e2e timeout tracking.  * Fixing CI pipeline.  * Update CI pipeline threshold to 120 minutes.  * Update CI pipeline threshold to 120 minutes.  * [PPCB]: Fixing fail-back flow.  * [PPCB]: Fixing fail-back flow.  * Reverting `warn` logs to `debug` logs.  * [PPCB + PPAF]: Apply customer override for PPCB when PPAF is opted into.  * [PPCB + PPAF]: Apply customer override for PPCB when PPAF is opted into.  * [PPCB + PPAF]: Apply customer override for PPCB when PPAF is opted into.  * Add PPAF opt-in setting for benchmark.  * [HA]: Always have PPAF and PPCB as opted into for benchmark purposes.  * Add managed identity support for account to which benchmark specific workload hits.  * Add managed identity support for account to which benchmark specific workload hits.  * Add managed identity support for account to which benchmark specific workload hits.  * Add managed identity support for account to which benchmark specific workload hits.  * Add managed identity support for account to which benchmark specific workload hits.  * Add managed identity support for account to which benchmark specific workload hits.  * [HA]: Always have PPAF and PPCB as opted into for benchmark purposes.  * Only create containers / databases when key-based authentication is used.  * Only create containers / databases when key-based authentication is used.  * Correctly handle close() of result uploader client.  * Fixed merge assertions for RegionScopedSessionContainerTest.  * Refactoring and test fixes.  * Refactoring.  * Refactoring.  * Refactoring.  * Update CHANGELOG.md.  * Resolving conflicts in `LocationCache`.  ---------  Co-authored-by: Neha Rao <nehrao@microsoft.com>
Azure,azure-sdk-for-java,1f9f7c084ab9d5d4f789f95268f06499198c39de,https://github.com/Azure/azure-sdk-for-java/commit/1f9f7c084ab9d5d4f789f95268f06499198c39de,Internal: Allows configuring PendingAcquireMaxCount (#44512)  Internal: Allows configuring PendingAcquireMaxCount for HTTP connection pool in benchmark (to simulate low connection pool size with high concurrency perf workload)
Azure,azure-sdk-for-java,670b4458ed662037652f747940e1b9a2ee62e00d,https://github.com/Azure/azure-sdk-for-java/commit/670b4458ed662037652f747940e1b9a2ee62e00d,Adding readMany() support for findAllByIds() to improve performance. (#43759)  * Adding readMany() support for findAllByIds() to improve performance.  * Updating the changelog  * Update sdk/spring/azure-spring-data-cosmos/CHANGELOG.md  Co-authored-by: Kushagra Thapar <kushuthapar@gmail.com>  ---------  Co-authored-by: Kushagra Thapar <kushuthapar@gmail.com>
Azure,azure-sdk-for-java,c1810590c8e8120c9d28df2dfce312eaa88bad2e,https://github.com/Azure/azure-sdk-for-java/commit/c1810590c8e8120c9d28df2dfce312eaa88bad2e,Fixed small perf overhead due to NPE for readItem returning 404 (#44008)  * Addressing NPE for readItem returning 404  * Update CHANGELOG.md  * Update SparkE2EGatewayChangeFeedITest.scala
Azure,azure-sdk-for-java,0028d6603d6b81ce5b53d90f64fbde9e17c5bc8c,https://github.com/Azure/azure-sdk-for-java/commit/0028d6603d6b81ce5b53d90f64fbde9e17c5bc8c,Perform cross-region retry of Document Read request when enclosing address request hits a 408:10002. (#43937)  * Adding changes to do cross-region of Document request when enclosing address refresh request times out.  * Modified tests.  * Modified tests.  * Modified tests.  * Fixing live tests pipeline.  * Fixing live tests pipeline.  * Addressing review comments.
Azure,azure-sdk-for-java,5195bd659ffad3f4571fb9f8289ca555860f70fe,https://github.com/Azure/azure-sdk-for-java/commit/5195bd659ffad3f4571fb9f8289ca555860f70fe,Delete Track 1 Event Hubs and Service Bus perf tests (#42704)
Azure,azure-sdk-for-java,6e0db58a4c89b2253241b59fa1aac31f83252ec8,https://github.com/Azure/azure-sdk-for-java/commit/6e0db58a4c89b2253241b59fa1aac31f83252ec8,Update how get-properties perf test runs (#42435)  Update how get-properties perf test runs
Azure,azure-sdk-for-java,959ac8f02226ef2be19e2756e9c6bcc4a2b7abfc,https://github.com/Azure/azure-sdk-for-java/commit/959ac8f02226ef2be19e2756e9c6bcc4a2b7abfc,Setup clientcore perf pipeline (#41841)
Azure,azure-sdk-for-java,28a65f7df30a3032bf74e8d010e0389dbacf304e,https://github.com/Azure/azure-sdk-for-java/commit/28a65f7df30a3032bf74e8d010e0389dbacf304e,GetProperties Blob Performance Test (#42300)
Azure,azure-sdk-for-java,93c25d89399d7790d6e5d92ffbdfc123675c6308,https://github.com/Azure/azure-sdk-for-java/commit/93c25d89399d7790d6e5d92ffbdfc123675c6308,Enable Spotless for azure-data-appconfiguration and azure-data-appconfiguration-perf (#41950)
Azure,azure-sdk-for-java,f72e89d25a76ad0482c36e96200f728d416af9c5,https://github.com/Azure/azure-sdk-for-java/commit/f72e89d25a76ad0482c36e96200f728d416af9c5,Perf investigations for async clientcore (#41494)
Azure,azure-sdk-for-java,a7cb0c5b6a92e98e69ee87d2480a388442f0f895,https://github.com/Azure/azure-sdk-for-java/commit/a7cb0c5b6a92e98e69ee87d2480a388442f0f895,Stop partition recovery flow when a client instance is closed. (#41486)  * Push down collection and partition key range resolution.  * Tweaking threshold behavior.  * Fixing tests.  * Fixing tests.  * Fixing tests.  * Fixing tests.  * Perform collectionLink normalization.  * Fix CI pipeline.  * Reacting to review comments.  * Updated CHANGELOG.md.  * Force circuit breaking to be enabled.  * Increase error thresholds.  * Force circuit breaker for certain tests.  * Scope live test matrix.  * Scope live test matrix.  * Scope live test matrix.  * Close globalPartitionEndpointManagerForCircuitBreaker.  * Close globalPartitionEndpointManagerForCircuitBreaker.  * Use non-static scheduler.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Test multi-region + circuit-breaker job.  * Scrubbing off Java 21 emulator targeting Spring emulator test.  * Refactoring.  * Reacting to review comments.  * Attempt at fixing live tests pipeline.  * Reacting to review comments.  * Reacting to review comments.  * Reacting to review comments.  * Revert some live test pipeline changes.  * Updated CHANGELOG.md.
Azure,azure-sdk-for-java,e40aa59476848e46bcfb1fb59b6e5840189548a6,https://github.com/Azure/azure-sdk-for-java/commit/e40aa59476848e46bcfb1fb59b6e5840189548a6,Push down collection and partition key range resolution for circuit breaker. (#41428)  * Push down collection and partition key range resolution.  * Tweaking threshold behavior.  * Fixing tests.  * Fixing tests.  * Fixing tests.  * Fixing tests.  * Perform collectionLink normalization.  * Fix CI pipeline.  * Reacting to review comments.  * Updated CHANGELOG.md.
http-kit,http-kit,383198243183b3a77f96d58939e69eece97e8a66,https://github.com/http-kit/http-kit/commit/383198243183b3a77f96d58939e69eece97e8a66,[fix] [client] [#568] [#569] Fix performance regression (@bsless)  It seems that a client performance regression was accidentally introduced with #446:  By using string joins at - keepalives.remove(job.addr.toString() + job.host); - return (addr.toString() + host).equals(obj) || key.equals(obj);  We ended up adding non-trivial string allocation for every object checked for equality against a PersistentConn - harming performance especially when many hosts are being used.  This commit attempts to fix the regression by avoiding the string joining.
FCL-Team,FoldCraftLauncher,53be73d39295180fec5c3dd7e6b31d400f1ce490,https://github.com/FCL-Team/FoldCraftLauncher/commit/53be73d39295180fec5c3dd7e6b31d400f1ce490,Feat: Sustained performance mode
bcgit,bc-java,0f613faaad1ee64639709a38fa6fa91b33d6b5ab,https://github.com/bcgit/bc-java/commit/0f613faaad1ee64639709a38fa6fa91b33d6b5ab,Enable AsconXof to perform multiple outputs
bcgit,bc-java,8a50bb77cdb54c1c85a9fa489b31803b973cfd60,https://github.com/bcgit/bc-java/commit/8a50bb77cdb54c1c85a9fa489b31803b973cfd60,Xoodyak perf. opts.
bcgit,bc-java,1b32536cf79c2e0af0f54746c3bc4b9b26cff49f,https://github.com/bcgit/bc-java/commit/1b32536cf79c2e0af0f54746c3bc4b9b26cff49f,PhotonBeetle perf. opts.  - still very slow
tlaplus,tlaplus,96056d942b70656e7f88349e2f1fa38514c48783,https://github.com/tlaplus/tlaplus/commit/96056d942b70656e7f88349e2f1fa38514c48783,Report distinct variable values observed in TLC coverage statistics.  These statistics show the number of unique values each variable takes during model checking. An unusually high number of values for a particular variable may suggest that the model is not properly constrained  potentially leading to state space explosion during exhaustive analysis.  Note I: The data structure used to estimate these counts is probabilistic—specifically  HyperLogLog—which helps minimize memory usage. As a result  the reported counts may have a small margin of error. Additionally  the use of this structure introduces contention among workers  which can negatively affect performance and scalability. However  empirical measurements (see https://github.com/tlaplus/tlaplus/pull/1183#issuecomment-2870039111) have shown that the performance overhead of variable statistics collection on top of action and ordinary coverage is negligible.  Note II: The `TLC!TLCGet("spec")` named register equals the same data and serves as a more appropriate and structured input for extracting and parsing these numeric values during subsequent processing stages:  ```tla ---- MODULE Spec ---- EXTENDS TLC  Json  ... MyStats == PrintT( ToJson(  \* Alternatively  see CSV!CSVWrite operator. { [name |-> v.name  count |-> v.coverage.distinct] : v \in TLCGet("spec").variables } ) ) ==== ---- CONFIG Spec ---- ... _PERIODIC MyStats POSTCONDITION MyStats ==== ```  Variable statistics can be enabled independently of the `-coverage someTime` option by setting the Java system property `-Dtlc2.TLCGlobals.coverage=2` when running TLC. To activate both action and variable statistics  use `-Dtlc2.TLCGlobals.coverage=3`.  [Feature][TLC]  Signed-off-by: Markus Alexander Kuppe <github.com@lemmster.de>
tlaplus,tlaplus,06d00391770f0f7edfc9f2a8463dfeada2331b4d,https://github.com/tlaplus/tlaplus/commit/06d00391770f0f7edfc9f2a8463dfeada2331b4d,Strengthen assertions of OffHeapDiskFPSet both indexer.  (Also remove superfluous RemoteException from API)  Part of Github issue #1112 "Warning: DiskFPSet.mergeNewEntries: xxx is already on disk with hangup following." https://github.com/tlaplus/tlaplus/issues/1112  [Refactor][TLC]  Signed-off-by: Markus Alexander Kuppe <github.com@lemmster.de>
tlaplus,tlaplus,c61bad9a00b5be5f18e121c19ab885e90d0e6c01,https://github.com/tlaplus/tlaplus/commit/c61bad9a00b5be5f18e121c19ab885e90d0e6c01,Do not override an fp's done state when recording an new <<fp  tidx>> GraphNode (where tidx is some tableau index).  Let s -> t be an action that is inserted into the behavior graph (BG). Liveness checking marks s as done/explored by adding it to a seen set S.  It is important to note that S must be distinct from the BFS' fingerprint set (FPSet) because FPSet is inconsistent with respect to liveness checking due to the concurrent addition of new states by other BFS workers (in practice  the elements of both sets are not states but fingerprints).  After adding s to S  liveness checking processes s->t by creating the cross product of {t} and the set of consistent nodes in the tableau graph. The resulting GraphNodes are inserted into BG. If and only if t is not in S  liveness checking will recursively process all of t's (state graph) successor states that are likewise not in S.  The implementation had the bug where inserting a GraphNode <<t  ...>> into the BG would remove t from S as a side effect  violating a fundamental invariant of the algorithm: [][t \in S => [](t \in S)]_S  This bug was unlikely to occur because state graph nodes are typically explored in BFS order.  This fix has minimal performance implications  except for the addition of an extra method parameter to `TableauNodePtrTable#put`. Unless inlining kicks in  recording a node in the behavior graph will require one extra stack frame.  Fixes Github issue #971 https://github.com/tlaplus/tlaplus/issues/971  [Bug][TLC]  Signed-off-by: Markus Alexander Kuppe <github.com@lemmster.de>
mc1arke,sonarqube-community-branch-plugin,c28e16e0b5ef47c6abace93fe4af41b9e895538d,https://github.com/mc1arke/sonarqube-community-branch-plugin/commit/c28e16e0b5ef47c6abace93fe4af41b9e895538d,Remove old Gitlab and Azure Devops summary comments on new decoration  The plugin historically left old comments in place but resolved conversations where comments had become outdated or the underlying issue had been resolved. However  in Gitlab  the summary comments always remained visible even when resolved as they were the first comment in the thread so were not minimised by the Gitlab UI. For a merge request being scanned multiple times as issues are being fixed  other review comments responded to  and rebasing activities performed  this can lead to a number of summary comments being added where the last comment is typically only the one that developers are about.  As editing comments is not good practice since it's unclear what any resulting comments in the thread are referring to and Gitlab does not send emails to notify that comments have changed  the summary comment is continuing to be posted as a new comment  but the old summary comments are now being deleted. Where a thread has spawned from an old summary comment  that comment will not be deleted  but a note added to notify the users that the summary comment is outdated and the thread can be resolved once the discussion reaches a conclusion.
hellokaton,30-seconds-of-java8,18055589e4de0744c4404f8b5ca782dbb62bc633,https://github.com/hellokaton/30-seconds-of-java8/commit/18055589e4de0744c4404f8b5ca782dbb62bc633,chunk and performance test
spring-io,start.spring.io,2401e9539cc7ea460a08e6c60f690a590474ea15,https://github.com/spring-io/start.spring.io/commit/2401e9539cc7ea460a08e6c60f690a590474ea15,Remove superfluous Gradle DSL classes for GraalVM  Gradle and Kotlin DSL did the same thing  this condenses it down to only one class.
openjdk,jmh,7c6da9526c6544acbe9f19f2cf075793f240b120,https://github.com/openjdk/jmh/commit/7c6da9526c6544acbe9f19f2cf075793f240b120,7903740: JMH: Perf event validation not working with skid options  Reviewed-by: shade
openjdk,jmh,c2931c90fd4079271b69a8a393dfe277634f3e52,https://github.com/openjdk/jmh/commit/c2931c90fd4079271b69a8a393dfe277634f3e52,7903722: JMH: Add xctrace-based perfnorm profiler for macOS  Reviewed-by: shade
openjdk,jmh,fedf7639176ea2b3e521950777808701e2c5cc1a,https://github.com/openjdk/jmh/commit/fedf7639176ea2b3e521950777808701e2c5cc1a,7903739: JMH: perf stat command not constructed right for event selection  Reviewed-by: shade
aws,aws-sdk-java-v2,cd0172d77c3cf9ffd7ee777887a6d186bea6bffa,https://github.com/aws/aws-sdk-java-v2/commit/cd0172d77c3cf9ffd7ee777887a6d186bea6bffa,Remove def 16MB read chunk size (#5941)  * Remove def 16MB read chunk size  This commit removes the hardcoded 16MiB read chunk size for uploadFile() in the TransferManager. This was originally a workaround to improve performance when using CRT; however this is no longer necessary because file uploads are done entirely in CRT without the SDK/Java layer doing any reads.  Without this default size  the default read chunk size reverts back to the default from `FileAsyncRequestBody` which is 16KiB.  * Fix test
aws,aws-sdk-java-v2,9a313431cb1930cb4158a13004b9331115d06041,https://github.com/aws/aws-sdk-java-v2/commit/9a313431cb1930cb4158a13004b9331115d06041,Feature: EmfMetricLoggingPublisher (#5792)  * EmfMetricPublisher class added (#5752)  * EmfMetricPublisher class with basic unit test added  * Edge cases handled  * javadoc for class added  * Java doc for methods added  * minor unit test changes  * Change from using jacksonCore directly to using jsonWriter  * EmfMetricConfiguration class added  * MetricEmfConverter class added  * new module checklist done  * checkStyle fixed  * minor build problem fixed  * internal package added  * fixed the issue passing raw objects  * added java clock to unit test  * added unit test for publish method  * minor changes  * config class to builder pattern  * config class to builder pattern  * minor change  * End to end test passed  * added valid case in WARN_LOG allowlist  * unit tests adjusted  * Change logGroupName to required field  * move MetricValueNormalizer class to utils package  * converter implementation changed  * schemaConformTest added  * SchemaConformTest adjusted  * minor change  * emf metric publisher performance test (#5775)  * Benchmark test for emfMetricPublisher added  * input name changed  * add emfBenchmarkTest to BenchmarkRunner  * enabled METRIC_BENCHMARKS  * checkStyle fixed  * Changed the classname to EmfMetricLoggingPublisher (#5783)  * change class name and artifactId  * test-coverage pom changed  * javadoc fixed  * Cloudwatch and Logging benchmark test added / Changed logGroupName config (#5790)  * more benchmark test added/changed logGroupName config  * teardown method overrided  * logGroupName access method changed  * Snapshot version changed  * LogGroupName in lambda unit test added  * suppression added for system  * changelog added  * changeLog modified  * minor fix  * snapshot version fixed  * Snapshot version changed
aws,aws-sdk-java-v2,ca6c59cc03d07311e242b98fef2aa7f7d476da20,https://github.com/aws/aws-sdk-java-v2/commit/ca6c59cc03d07311e242b98fef2aa7f7d476da20,Smithy RPCv2 CBOR merge (#5621)  * Add RPCv2 module  * Add RPCv2 module (#5445)  * Comment out empty rpcv2 dependency  * Sync version to 2.26.31-SNAPSHOT  * Sugmanue/add byte support (#5477)  * Add support to serialize byte values  * Add tests for byte support  * Address PR comments  * Add rpcv2 protocol core (#5496)  * Add support to serialize byte values  * Add RPCv2 protocol core marshalling/unmarshalling  * Address PR comments  * Address PR comments 2  * Address PR comments 3  * Support for operation without input defined (#5512)  * Support for operation without input defined  * Fix a checkstyle issue  * Code clean up  * Code clean up 2  * Rewrite the condition to conjunctive normal form  * Add codgen tests (#5517)  * Add codgen tests  * Address PR comments  * Address PR comments 2  * Add missing class rename  * Add missing AWS_JSON protocol facts  * Account for null protocol case  * Add RPCv2 benchmark tests (#5526)  * Add RPCv2 benchmark tests  * Give the constants name a meaningful name  * Avoid parsing numbers when using RPCv2 protocol (#5539)  * Avoid parsing numbers when using RPCv2 protocol  * Refactor to avoid impacting JSON with RPCv2 logic (#5544)  * Refactor to avoid impacting JSON with RPCv2 logic  * Avoid making the unmarshallers depend on timestamp formats  * Avoid streams while unmarshalling  * Fix build failures  * Fix build failures 2  * Avoid growing copies of collections of known size (#5551)  * Add the new Smithy RPCv2 package  * Sugmanue/rpcv2 improve cbor performance 04 (#5564)  * Improve lookup by marshalling type  * Improve trait lookup using TraitType  * Add support for Smithy RPCv2 to the new service scripts (#5613)  * Add changelog for the release  * Fix typo in changelog  * Update to next SNAPSHOT version
aws,aws-sdk-java-v2,7ae6ed3d15f30f13a02c39ffac26f3759868635b,https://github.com/aws/aws-sdk-java-v2/commit/7ae6ed3d15f30f13a02c39ffac26f3759868635b,Fixed EnhancedClient UpdateItem operation to make it work on nested attributes as well (#5593)  * Fixed EnhancedClient UpdateItem operation to make it work on nested attributes as well  * Add Tests for multi-level nesting updates to work correctly  * Fixed PR feedback  * Updated Javadocs  * Addressed Pr feedback  * Removed indendation changes  * Added tests for FlattenedMapper  * fixed indendation  * Fix indendation and remove unintentional changes  * Configured MappingConfiguration object  * Added methods to AttributeMapping interface  * Fixed unintentional indendation changes  * Fixed unintentional indendation changes  * Add changelogs  * Introduce a new method to transform input to be able to perform update operations on nested DynamoDB object attributes.  * Remove unwanted changes  * Indent  * Remove unwanted changes  * Added testing  * Added test to validate updating string to null  * Remove unintentional indentation changes  * Fix checkstyle  * Update test case to add empty nested attribute  * Added a test to verify that updates to non-scalar nested attributes with ignoreNulls set to true  throws DDBException  * Uncomment test assertions  * Ensure correct workings of updating to an emoty map  * Fixed checkstyle  * Addressed pr feedback  * Created modes of update operation and deprecated existing ignoreNulls configuration  * Added more comments around deprecation  * Grouped validation methods  * Added more documentation to define the modes of update operations  * Removed MAPS_ONLY mode and added more documentation  * Added unit test  * Improved documentation to establish tradeoffs between update modes  * Modified error code in documentation  * Updated Documentation around update modes
aws,aws-sdk-java-v2,79394aa920b6d8b94c116eb090d62e9c27e577d8,https://github.com/aws/aws-sdk-java-v2/commit/79394aa920b6d8b94c116eb090d62e9c27e577d8,Fixed EnhancedClient UpdateItem operation to make it work on nested attributes as well (#5380)  * Fixed EnhancedClient UpdateItem operation to make it work on nested attributes as well  * Add Tests for multi-level nesting updates to work correctly  * Fixed PR feedback  * Updated Javadocs  * Addressed Pr feedback  * Removed indendation changes  * Added tests for FlattenedMapper  * fixed indendation  * Fix indendation and remove unintentional changes  * Configured MappingConfiguration object  * Added methods to AttributeMapping interface  * Fixed unintentional indendation changes  * Fixed unintentional indendation changes  * Add changelogs  * Introduce a new method to transform input to be able to perform update operations on nested DynamoDB object attributes.  * Remove unwanted changes  * Indent  * Remove unwanted changes  * Added testing  * Added test to validate updating string to null  * Remove unintentional indentation changes  * Fix checkstyle  * Update test case to add empty nested attribute  * Added a test to verify that updates to non-scalar nested attributes with ignoreNulls set to true  throws DDBException  * Uncomment test assertions  * Ensure correct workings of updating to an emoty map  * Fixed checkstyle  * Addressed pr feedback
aws,aws-sdk-java-v2,ecd78dad90037fd89e6e7819b365763032117a53,https://github.com/aws/aws-sdk-java-v2/commit/ecd78dad90037fd89e6e7819b365763032117a53, Add exception handling for TLS half-close in ApacheHTTPClient #5385  (#5398)  * Check if input is shut down before writing (#5257)  This commit adds a wrapper socket that ensures the read end of the socket is still open before performing a {@code write()}. In TLS 1.3  it is permitted for the connection to be in a half-closed state  which is dangerous for the Apache client because it can get stuck in a state where it continues to write to the socket and potentially end up a blocked state writing to the socket indefinitely.  * Delegate write() methods directly to base  * Added test cases to test TLS half close  * feat: Add exception handling for TLS half-close in ApacheHTTPClient  Implemented a feature to handle TLS half-close scenarios by throwing an exception in the ApacheHTTPClient package. In TLS 1.3  the inbound and outbound close_notify alerts are independent. When the client receives a close_notify alert  it only closes the inbound stream but continues to send data to the server. Previously  the SDK could not detect that the connection was closed on the server side  causing it to get stuck while writing to the socket and eventually timing out. This feature ensures proper detection and handling of closed connections  improving overall reliability by preventing client hangs.  * Handled review comemnts  * Updated test case to consider Jdk 1.8 builds which donot support TLS1.3 half close  ---------  Co-authored-by: Dongie Agnir <261310+dagnir@users.noreply.github.com> Co-authored-by: Dongie Agnir <dongie@amazon.com>
aws,aws-sdk-java-v2,e7a2aabec8d4210b5bce4259b678dab8d4956074,https://github.com/aws/aws-sdk-java-v2/commit/e7a2aabec8d4210b5bce4259b678dab8d4956074,Add exception handling for TLS half-close in ApacheHTTPClient (#5385)  * Check if input is shut down before writing (#5257)  This commit adds a wrapper socket that ensures the read end of the socket is still open before performing a {@code write()}. In TLS 1.3  it is permitted for the connection to be in a half-closed state  which is dangerous for the Apache client because it can get stuck in a state where it continues to write to the socket and potentially end up a blocked state writing to the socket indefinitely.  * Delegate write() methods directly to base  * Added test cases to test TLS half close  * feat: Add exception handling for TLS half-close in ApacheHTTPClient  Implemented a feature to handle TLS half-close scenarios by throwing an exception in the ApacheHTTPClient package. In TLS 1.3  the inbound and outbound close_notify alerts are independent. When the client receives a close_notify alert  it only closes the inbound stream but continues to send data to the server. Previously  the SDK could not detect that the connection was closed on the server side  causing it to get stuck while writing to the socket and eventually timing out. This feature ensures proper detection and handling of closed connections  improving overall reliability by preventing client hangs.  * Handled review comemnts  ---------  Co-authored-by: Dongie Agnir <261310+dagnir@users.noreply.github.com> Co-authored-by: Dongie Agnir <dongie@amazon.com>
firebase,firebase-android-sdk,cad26b95c065db3c0fc4f7bdff532579ccbc0588,https://github.com/firebase/firebase-android-sdk/commit/cad26b95c065db3c0fc4f7bdff532579ccbc0588,[Perf] Cache attribute key validation regex (#6865)  The `validateAttribute` method is static and can be called multiple times  which makes re-compiling the key regex validation every time a waste of resources.  Originally reported as https://github.com/firebase/firebase-android-sdk/issues/6862
firebase,firebase-android-sdk,49a637fea4c49dcc447692b1e3358f9578914a3e,https://github.com/firebase/firebase-android-sdk/commit/49a637fea4c49dcc447692b1e3358f9578914a3e,fix(firebase-perf): Update device cache only if RC value is different from cached value (#6431)  Fixes #6407
firebase,firebase-android-sdk,531f25bb0417370c0da56825e5f2505b3cc49666,https://github.com/firebase/firebase-android-sdk/commit/531f25bb0417370c0da56825e5f2505b3cc49666,Migrate crashlytics tests to JUnit4 (#6402)  Per [b/375055031](https://b.corp.google.com/issues/375055031)   This performs various cleanups on the integration tests for crashlytics. This should help narrow down flakey test behavior  and avoid any issues that may arise from deprecated usages.  Namely  this PR does the following: - Fully migrates all the tests to JUnit4. Some of them were still using JUnit3 behaviors. - Migrates `initMocks` to `openMocks`. There may have been broken behavior from `initMocks` leakage. - Updates some existing `openMocks` to properly close. There may have been leakage into other tests  which could cause false positives. - Migrate `AndroidJunit4` usages to the new import. Some of the tests were still using the old (deprecated) version. - Suppresses the receiver filter warning. This is a false positive as the use-site only specifies two arguments. By suppressing the warning- it should help depollute our logs.
apache,cloudstack,85765c3125fbe409f31b015ca0ef4ce29a07594b,https://github.com/apache/cloudstack/commit/85765c3125fbe409f31b015ca0ef4ce29a07594b,backup: simple NAS backup plugin for KVM (#9451)  This is a simple NAS backup plugin for KVM which may be later expanded for other hypervisors. This backup plugin aims to use shared NAS storage on KVM hosts such as NFS (or CephFS and others in future)  which is used to backup fully cloned VMs for backup & restore operations. This may NOT be as efficient and performant as some of the other B&R providers  but maybe useful for some KVM environments who are okay to only have full-instance backups and limited functionality.  Design & Implementation follows the `networker` B&R plugin  which is simply:  - Implement B&R plugin interfaces - Use cmd-answer pattern to execute backup and restore operations on KVM host when VM is running (or needs to be restored) - instead of a B&R API client  relies on answers from KVM agent which executes the operations - Backups are full VM domain snapshots  copied to a VM-specific folders on a NAS target (NFS) along with a domain XML - Backup uses libvirt feature: https://libvirt.org/kbase/live_full_disk_backup.html orchestrated via virsh/bash script (nasbackup.sh) as the libvirt-java lacks the bindings - Supported instance volume storage for restore operations: NFS & local storage  Refer the doc PR for feature limitations and usage details: https://github.com/apache/cloudstack-documentation/pull/429  Signed-off-by: Rohit Yadav <rohit.yadav@shapeblue.com> Co-authored-by: Pearl Dsilva <pearl1594@gmail.com> Co-authored-by: Abhishek Kumar <abhishek.mrt22@gmail.com> Co-authored-by: Suresh Kumar Anaparti <sureshkumar.anaparti@gmail.com>
apache,cloudstack,c3f0d14d31583fdf2570de2633db5c87a0f08fa4,https://github.com/apache/cloudstack/commit/c3f0d14d31583fdf2570de2633db5c87a0f08fa4,storage/object: Add support for Ceph RGW Object Store (#8389)  This feature adds support for Ceph's RADOS Gateway (RGW) support for the Object Store feature of CloudStack.  The RGW of Ceph is Amazon S3 compliant and is therefor an easy and straigforward implementation of basic S3 features.  Existing Ceph environments can have the RGW added as an additional feature to a cluster already providing RBD (Block Device) to a CloudStack environment.  Introduce the BucketTO to pass to the drivers. This replaces just passing the bucket's name.  Some upcoming drivers require more information then just the bucket name to perform their actions  for example they require the access and secret key which belong to the account of this bucket.  This is leftover code from a long time ago and this validation test has nu influence on the end result on how a URL will be used afterwards.  We should support hosts pointing to an IPv6(-only) address out of the box.  For the code it does not matter if it's IPv4 or IPv6. This is the admin's choice.  Signed-off-by: Rohit Yadav <rohit.yadav@shapeblue.com> Co-authored-by: Rohit Yadav <rohit.yadav@shapeblue.com>
apache,cloudstack,7e085d5e1df9fd58305f0a9504c27b35e077e4f0,https://github.com/apache/cloudstack/commit/7e085d5e1df9fd58305f0a9504c27b35e077e4f0,framework/db: use HikariCP as default and improvements (#9518)  Per docs  if the mysql connector is JDBC2 compliant then it should use the Connection.isValid API to test a connection. (https://docs.oracle.com/javase/8/docs/api/java/sql/Connection.html#isValid-int-)  This would significantly reduce query lags and API throughput  as for every SQL query one or two SELECT 1 are performed everytime a Connection is given to application logic.  This should only be accepted when the driver is JDBC4 complaint.  As per the docs  the connector-j can use /* ping */ before calling SELECT 1 to have light weight application pings to the server: https://dev.mysql.com/doc/connector-j/en/connector-j-usagenotes-j2ee-concepts-connection-pooling.html  Replaces dbcp2 connection pool library with more performant HikariCP. With this unit tests are failing but build is passing.  Signed-off-by: Rohit Yadav <rohit.yadav@shapeblue.com> Signed-off-by: Abhishek Kumar <abhishek.mrt22@gmail.com> Co-authored-by: Rohit Yadav <rohityadav89@gmail.com>
apache,cloudstack,5bf81cf00233efe4478552e7bb13f3e96dc58ed1,https://github.com/apache/cloudstack/commit/5bf81cf00233efe4478552e7bb13f3e96dc58ed1,Merge release branch 4.19 to main  * 4.19: linstor: Improve copyPhysicalDisk performance (#9417)
apache,cloudstack,3d8d4875fe365e4ceaa60cf097fc57b5b71a32d3,https://github.com/apache/cloudstack/commit/3d8d4875fe365e4ceaa60cf097fc57b5b71a32d3,Merge release branch 4.18 to 4.19  * 4.18: linstor: Improve copyPhysicalDisk performance (#9417)
apache,cloudstack,27f23f4f75a4e19e014dcf986c59f2a365d3d716,https://github.com/apache/cloudstack/commit/27f23f4f75a4e19e014dcf986c59f2a365d3d716,linstor: Improve copyPhysicalDisk performance (#9417)  Tell qemu-img that we don't want to use a write cache (we are a block device) and also specify that we have zeroed devices in most cases.
apache,cloudstack,e41add31e72928ebbca13fb043189c70fcbcbf8c,https://github.com/apache/cloudstack/commit/e41add31e72928ebbca13fb043189c70fcbcbf8c,saml: signature check improvements  Adminstrators should ensure that IDP configuration has a signing certificate for the actual signature check to be performed. In addition to this  this change introduces a new global setting saml2.check.signature  with the default value of true  which can deliberately fail a SAML login attempt when the SAML response has a missing signature. Purges the SAML token upon handling the first SAML response.  Authored-by: Rohit Yadav <rohit.yadav@shapeblue.com>  Signed-off-by: Abhishek Kumar <abhishek.mrt22@gmail.com>
apache,cloudstack,f0faa4a6b3023f7e0b24d072c0a6de3a81dedfed,https://github.com/apache/cloudstack/commit/f0faa4a6b3023f7e0b24d072c0a6de3a81dedfed,saml: signature check improvements  Adminstrators should ensure that IDP configuration has a signing certificate for the actual signature check to be performed. In addition to this  this change introduces a new global setting saml2.check.signature  with the default value of true  which can deliberately fail a SAML login attempt when the SAML response has a missing signature. Purges the SAML token upon handling the first SAML response.  Authored-by: Rohit Yadav <rohit.yadav@shapeblue.com>  Signed-off-by: Abhishek Kumar <abhishek.mrt22@gmail.com>
apache,cloudstack,46f672563ebdab08e3b242fd968fc0b21e08f237,https://github.com/apache/cloudstack/commit/46f672563ebdab08e3b242fd968fc0b21e08f237,Improve migration of external VMware VMs into KVM cluster (#8815)  * Create/Export OVA file of the VM on external vCenter host  to temporary conversion location (NFS)  * Fixed ova issue on untar/extract ovf from ova file "tar -xf" cmd on ova fails with "ovf: Not found in archive" while extracting ovf file  * Updated VMware to KVM instance migration using OVA  * Refactoring and cleanup  * test fixes  * Consider zone wide pools in the destination cluster for instance conversion  * Remove local storage pool support as temporary conversion location - OVA export not possible as the pool is not accessible outside host  NFS pools are supported.  * cleanup unused code  * some improvements  and refactoring  * import nic unit tests  * vmware guru unit tests  * Separate clone VM and create template file for VMware migration - Export OVA (of the cloned VM) to the conversion location takes time. - Do any validations with cloned VM before creating the template (and fail early). - Updated unit tests.  * Check conversion support on host before clone vm / create template on vmware (and fail early)  * minor code improvements  * Auto select the host with instance conversion capability  * Skip instance conversion supported response param for non-KVM hosts  * Show supported conversion hosts in the UI  * Skip persistence map update if network doesn't exist  * Added support to export OVA from KVM host  through ovftool (when installed in KVM host)  * Updated importvm api param 'usemsforovaexport' to 'forcemstodownloadvmfiles'  to be generic  * Updated hardcoded UI messages with message labels  * Updated UI to support importvm api param - forcemstodownloadvmfiles  * Improved instance conversion support checks on ubuntu hosts  and for windows guest vms  * Use OVF template (VM disks and spec files) for instance conversion from VMware  instead of OVA file - this would further increase the migration performance (as it reduces the time for OVA preparation / archiving of the VM files into a single file)  * OVF export tool parallel threads code improvements  * Updated 'convert.vmware.instance.to.kvm.timeout' config default value to 3 hrs  * Config values check & code improvements  * Updated import log  with time taken and vm details  * Support for parallel downloads of VMware VM disk files while exporting OVF from MS  and other changes below. - Skip clone for powered off VMs - Fixes to support standalone host (with its default datacenter) - Some code improvements  * rebase fixes  * rebase fixes  * minor improvement  * code improvements - threads configuration  and api parameter changes to import vm files  * typo fix in error msg
apache,cloudstack,2ca0857bd59fbba87ccf3cfec57041ef5bff52a1,https://github.com/apache/cloudstack/commit/2ca0857bd59fbba87ccf3cfec57041ef5bff52a1,api: listVM API improvement followup  change returning of stats detail (#9177)  - Changes behaviour of details param handling via global setting: - listVirtualMachines API: when the details param is not provided  it returns whether stats are returned controlled by a new global setting `list.vm.default.details.stats` - listVirtualMachinesMetrics API: when the details param is not provided  it uses `all` details including `stats` - Users who are affected slow performance of the listVirtualMachines API response time can set `list.vm.default.details.stats` to `false` - Remove ConfigKey vm.stats.increment.metrics.in.memory which was renamed to `vm.stats.increment.metrics` in #5984 and also remove unused/unnecessary global settings via upgrade path - Changes default value of VM stats accumulation setting `vm.stats.increment.metrics` to false until a better solution emerges. Since #5984  this is true and during the execution of listVM APIs the stats are clubbed/calculated which can immensely slow down list VM API calls. Any costly operations such as summing of stats shouldn't be done during the course of a synchronous API  such as the list VM API. - Fix UI that uses listVirtualMachinesMetrics to not call `stats` detail when in list view without metrics selected.  Signed-off-by: Rohit Yadav <rohit.yadav@shapeblue.com>
apache,cloudstack,78ace3a750c0285884b52646d4926052023ebd00,https://github.com/apache/cloudstack/commit/78ace3a750c0285884b52646d4926052023ebd00,saml: introduce saml2.check.signature (#9219)  Adminstrators should ensure that IDP configuration has signing certificate for the actual signature check to be performed. In addition to this  this change introduces a new global setting `saml2.check.signature` which can deliberately fail a SAML login attempt when the SAML response has missing signature.  Signed-off-by: Rohit Yadav <rohit.yadav@shapeblue.com>
google,copybara,98cb0ffde604e4ce41dea56a3073d68f55ccab5f,https://github.com/google/copybara/commit/98cb0ffde604e4ce41dea56a3073d68f55ccab5f,Compute git consistency file hashes from the FS  Previously  to generate the consistency file on git destinations  we would run `git show` on every file and compute the hash from the output. This is quite inefficient due to the overhead of spawning the git command. It is much more efficient to just read the file directly from the local checkout instead.  In my use case that involves ~55k files  this change significantly improves copybara performance  reducing run time from ~14 minutes to ~9 minutes.  Change-Id: Ibd69f243a6b9a52ee872b2ce9da230d3059d54de
google,copybara,5e79da92a01cde059680981739ebc0d5b3cf617f,https://github.com/google/copybara/commit/5e79da92a01cde059680981739ebc0d5b3cf617f,Use git checkout in GitRepository#checkout()  GitRepository#checkout() was previously written in an odd way where it would poorly reinvent `git checkout` by running `git show` on every single file  sequentially. This is extremely inefficient with a large number of files. Change the logic to do the obvious thing  which is to use `git checkout` instead.  In my use case that involves ~55k files with merge import enabled  this change dramatically improves copybara performance  reducing run time from ~26 minutes to ~14 minutes.  Change-Id: Idb3e15a53c7bb78c764a95a5ec5d67e7df048111
google,copybara,4b179773828daa9f2b0446a782d30b3ce99d5770,https://github.com/google/copybara/commit/4b179773828daa9f2b0446a782d30b3ce99d5770,Add interface for URL query params in GitLabApi  Classes that implement this interface will store param values that GitLabApi will then add to the URL string when performing GET requests.  The intent is to use this for specifying params when requesting merge requests and other entities. See the supported attibutes section for an example here: https://docs.gitlab.com/api/merge_requests/#list-merge-requests  BUG=393384198 PiperOrigin-RevId: 734208297 Change-Id: I267cad25e2b62626120ce2eda58fd0d9c243d38c
google,copybara,6394e1a4f2eb9ad7473cbd3c827fb3a6360f92b2,https://github.com/google/copybara/commit/6394e1a4f2eb9ad7473cbd3c827fb3a6360f92b2,Creating logic to perform squash merge in Copybara  BUG=131335631 PiperOrigin-RevId: 693806970 Change-Id: I44a108548fcc1cb4c1bbfefbcb12cd54aabc3499
google,copybara,49cbd69d0e9f858520e78c8946f73835d279149c,https://github.com/google/copybara/commit/49cbd69d0e9f858520e78c8946f73835d279149c,Allow Sequences of fully qualified files as Glob  Globs perform poorly when files are explicitly listed  this is an easier syntax and a performance boost.  BUG=348385876 PiperOrigin-RevId: 644504452 Change-Id: I1946852ce0e773506a90cea57daa2f344c321fdf
opentripplanner,OpenTripPlanner,a447a26aad345daf52679231d28e417d9ae93c0b,https://github.com/opentripplanner/OpenTripPlanner/commit/a447a26aad345daf52679231d28e417d9ae93c0b,Merge pull request #6260 from HSLdevcom/speculative-rental-fanout  Improve performance of speculative rental vehicle use in reverse search
opentripplanner,OpenTripPlanner,49db57e7fb7f4f185335e3df53ae793446aae750,https://github.com/opentripplanner/OpenTripPlanner/commit/49db57e7fb7f4f185335e3df53ae793446aae750,Add a matcher API for filters in the transit service used for datedServiceJourneyQuery (#5713)  * Adds a matcher API for the transit service and makes use of it in the DatedServiceJourneyQuery.  This is the first simple implementation of a filter using the unified matcher API.  * Adds an expression builder for building up a list of matchers simply and in a logically consistent manner.  Also does List.copyOf instead of simple reassignment between TripOnServiceDateRequestBuilder and TripOnServiceDateRequest.  * Adds convenience function for null and empty check for collections and removes get prefix from getters.  * Addresses minor comments in code review.  * Makes ContainsMatcher way more configurable and performant using suggestions from code review.  Also improves documentation generally.  * Addresses comments in PR.
opentripplanner,OpenTripPlanner,c1bcc7ed253cf6bb19b36268e75f05405034e7b8,https://github.com/opentripplanner/OpenTripPlanner/commit/c1bcc7ed253cf6bb19b36268e75f05405034e7b8,refactor: Add CompositeUtil and more unit tests to the ParetoSet  This extract logic to merge a hierarchy of objects containing composites  flatten the structure into one list of elements. There is a small performance optimization  but the important thing is that we will use this later in the design later.
spring-projects,spring-kafka,53149d4e6557420172b410fea8a12b10bdf320ee,https://github.com/spring-projects/spring-kafka/commit/53149d4e6557420172b410fea8a12b10bdf320ee,GH-3764: Replace LinkedList with ArrayList in listener container for records  Fixes: #3764 Issue link: https://github.com/spring-projects/spring-kafka/issues/3764  Acknowledging an index in a batch has quadratic time `N(N+1)/2` ~ `N^2`  Batch consumers operate on a `LinkedList` of records. If the consumer uses `MANUAL_IMMEDIATE` ack mode  and the listener invokes `acknowledgement.acknowledge(index)` where index is relatively big (e.g. when processing batches of `100k`)  performance takes hit because of the linear lookup `records.get(i)` in a loop  Signed-off-by: Janek Lasocki-Biczysko <janek.lb@gmail.com>  [artem.bilan@broadcom.com: improve commit message] **Auto-cherry-pick to `3.3.x`** Signed-off-by: Artem Bilan <artem.bilan@broadcom.com>
confluentinc,schema-registry,4555074376029cb2227bcb331af67dcfb29699f1,https://github.com/confluentinc/schema-registry/commit/4555074376029cb2227bcb331af67dcfb29699f1,DGS-15999 Add performanceMetric to get config metadata API (#3265) (#3311)
confluentinc,schema-registry,ef26d45780aa99f7a713d22574558c42cd4d6f66,https://github.com/confluentinc/schema-registry/commit/ef26d45780aa99f7a713d22574558c42cd4d6f66,DGS-15999 Add performanceMetric to get config metadata API (#3265)
confluentinc,schema-registry,036d99528fc538b10b844ceaae873cde4361b397,https://github.com/confluentinc/schema-registry/commit/036d99528fc538b10b844ceaae873cde4361b397,DGS-15999 Add performanceMetric to list schema refs API (#3264)
confluentinc,schema-registry,e0b2fdfea7edb9b02069c90ffc23aecfe4d65e09,https://github.com/confluentinc/schema-registry/commit/e0b2fdfea7edb9b02069c90ffc23aecfe4d65e09,DGS-15999 Add performanceMetric to global delete config API (#3263)
confluentinc,schema-registry,7b79ae6cc158a35a0677c4cc36c0953ca4b95d29,https://github.com/confluentinc/schema-registry/commit/7b79ae6cc158a35a0677c4cc36c0953ca4b95d29,Merge pull request #3225 from confluentinc/performance-metric-apis  Add performance metric to all SR APIs
confluentinc,schema-registry,593308bcb5d2f5b79c61cb79230e5294f5db2963,https://github.com/confluentinc/schema-registry/commit/593308bcb5d2f5b79c61cb79230e5294f5db2963,add performance metric to all SR APIs (#21)
polymorphicshade,Tubular,48b200868a455722690881f739bcd4cb96e52c74,https://github.com/polymorphicshade/Tubular/commit/48b200868a455722690881f739bcd4cb96e52c74,BF-11894 : Fix the menu disappearing on performing backGesture
polymorphicshade,Tubular,fef40014a0da63aa264448c676a519ffa1d00bbf,https://github.com/polymorphicshade/Tubular/commit/fef40014a0da63aa264448c676a519ffa1d00bbf,Added not null check for thumbnail URL before performing comparison
prometheus,client_java,4ce0d1513b7b211285478e77b11375da2aecd09d,https://github.com/prometheus/client_java/commit/4ce0d1513b7b211285478e77b11375da2aecd09d,additional improve MetricSnapshots.Builder performance (#985)  Signed-off-by: Andrey Burov <burov4j@yandex.ru>
bytedance,scene,1231f8cda0e3778f1c400f87b0c1e94c3069acd8,https://github.com/bytedance/scene/commit/1231f8cda0e3778f1c400f87b0c1e94c3069acd8,perf: use ThreadLocal to improve checkUIThread performance jiangqi 2024/10/9  15:41
bytedance,scene,d2229932ed597aefd034d0d53066fff53752f326,https://github.com/bytedance/scene/commit/d2229932ed597aefd034d0d53066fff53752f326,perf: create NavigationSceneManager#mThrowableHandler instance lazily
bytedance,scene,ba651c84750db5c90151e13b0be2708c459fb351,https://github.com/bytedance/scene/commit/ba651c84750db5c90151e13b0be2708c459fb351,perf: create Android8DefaultSceneAnimatorExecutor instance lazily
bytedance,scene,fe7cceda183e7c41d9c6981ca6402d9db3e60911,https://github.com/bytedance/scene/commit/fe7cceda183e7c41d9c6981ca6402d9db3e60911,refactor: improve ThreadUtility.checkUIThread performance
bytedance,scene,9226f5a9abc81dfcda357a0481a8322dd368e9c2,https://github.com/bytedance/scene/commit/9226f5a9abc81dfcda357a0481a8322dd368e9c2,refactor: NavigationScene add preloadClasses method to improve startup performance
bytedance,scene,7384f35a43773e4d17bcd5953b97fa2f933fe4b3,https://github.com/bytedance/scene/commit/7384f35a43773e4d17bcd5953b97fa2f933fe4b3,refactor: NavigationSceneOptions add setMergeNavigationSceneView method to improve startup performance
bytedance,scene,b84cde7b924f235a18e6e1d2d85e5b47326a92e8,https://github.com/bytedance/scene/commit/b84cde7b924f235a18e6e1d2d85e5b47326a92e8,refactor: NavigationSceneOptions add LazyLoadNavigationSceneUnnecessaryView option  NavigationScene will not add animation container at startup to improve performance
bytedance,scene,da7a4c142fd4f71c8df4c72751fa8b6e46a37456,https://github.com/bytedance/scene/commit/da7a4c142fd4f71c8df4c72751fa8b6e46a37456,refactor: NavigationSceneOptions add setUseActivityContextAndLayoutInflater method to improve startup performance
bytedance,scene,b6d4ea8c672981880cb6c68db97beecd733acbbf,https://github.com/bytedance/scene/commit/b6d4ea8c672981880cb6c68db97beecd733acbbf,refactor: improve save and restore performance  only restore visible scene
plutext,docx4j,fcef8cb577c462b307a8d3f612ced9e5a2e37f6d,https://github.com/plutext/docx4j/commit/fcef8cb577c462b307a8d3f612ced9e5a2e37f6d,GC is performed only if the temporary file is not successfully deleted
ta4j,ta4j,431e244d749779079a20aeff6b8d711ce3223ff4,https://github.com/ta4j/ta4j/commit/431e244d749779079a20aeff6b8d711ce3223ff4,move back to slightly more performant code
androidx,media,c872af4bc00608be70b74a2f61078e5d65ed8322.,https://github.com/androidx/media/commit/c872af4bc00608be70b74a2f61078e5d65ed8322.,Add missing call to `adjustUuid` in `FrameworkMediaDrm`  Before API 27  the platform DRM components incorrectly expected `C.COMMON_PSSH_UUID` instead of `C.CLEARKEY_UUID` in order to perform ClearKey decryption. `FrameworkMediaDrm` is responsible for doing this adjustment on these API levels  but this call was missed when refactoring some DRM code in
androidx,media,8b9e9203a19ff1cac359acdf4f3abb71de171e11,https://github.com/androidx/media/commit/8b9e9203a19ff1cac359acdf4f3abb71de171e11,
androidx,media,915130eb00c1fd6de370b2614e27862ab3531642,https://github.com/androidx/media/commit/915130eb00c1fd6de370b2614e27862ab3531642,Add `usePlatformDiagnostics` in Transformer.  Added a new `usePlatformDiagnostics` in Transformer. This parameter enables/disables forwarding editing events and performance data to the platform.  This is pre-work for metrics support in Transformer. In the following CLs  metrics collection and forwarding will be implemented.  PiperOrigin-RevId: 707930368
androidx,media,b5a1efdbce3220ef6a5733bd67ceb7dbaca3a5f3,https://github.com/androidx/media/commit/b5a1efdbce3220ef6a5733bd67ceb7dbaca3a5f3,`ForwardingTimeline`: Implement & `final`ize some methods  `equals`  `hashCode`  and `getPeriodByUid` are correctly implemented on `Timeline`. Overriding these in a way that maintains correctness is fiddly  so this CL prevents that for the 'simple' case of subclasses of `ForwardingTimeline`. Implementations of `Timeline` that need to override these methods for performance should extend `Timeline` or `AbstractConcatenatedTimeline` instead of `ForwardingTimeline`.  PiperOrigin-RevId: 703035721
androidx,media,b19b6ccc6093a115ffb992a813e6387b61b34e1c,https://github.com/androidx/media/commit/b19b6ccc6093a115ffb992a813e6387b61b34e1c,MP3 VBRI: Use `sampleCountToDurationUs()` and `samplesPerFrame`  PiperOrigin-RevId: 700340564
androidx,media,4b6e886ad2778e1163745c21b85fa3b52f912957,https://github.com/androidx/media/commit/4b6e886ad2778e1163745c21b85fa3b52f912957,Improve position estimate when transitioning to another checkpoint  When transitioning to the next media position parameter checkpoint we estimate the position because the audio processor chain no longer provides access to the actual playout duration.  The estimate using the declared speed and the last checkpoint may have drifted over time  so we currently estimate relative to the next checkpoint  which is closer and presumably provides a better estimate. However  this assumes that these checkpoint are perfectly aligned without any position jumps.  The current approach has two issues: - The next checkpoint may include a position jump by design  e.g. if it was set for a new item in the playlist and the duration of the current item wasn't perfectly accurate. - The sudden switch between two estimation methods may cause a jump in the output position  which is visible when we add new media position checkpoints to the queue  not when we actually reach the playback position of the checkpoint.  We can fix both issues by taking a slightly different approach: - Continuously monitor the estimate using the current checkpoint. If it starts drifting  we can adjust it directly. This way the estimate is always aligned with the actual position. - The change above means we can safely switch to using the estimate based on the previous checkpoint. This way we don't have to make assumptions about the next checkpoint and any position jumps will only happen when we actually reach this checkpoint (which is more what a user expects to see  e.g. at a playlist item transition).  Issue: androidx/media#1698 PiperOrigin-RevId: 690979859 (cherry picked from commit 7c0cffdca8e51223df96bda77b7b516eedb38a34)
androidx,media,7c0cffdca8e51223df96bda77b7b516eedb38a34,https://github.com/androidx/media/commit/7c0cffdca8e51223df96bda77b7b516eedb38a34,Improve position estimate when transitioning to another checkpoint  When transitioning to the next media position parameter checkpoint we estimate the position because the audio processor chain no longer provides access to the actual playout duration.  The estimate using the declared speed and the last checkpoint may have drifted over time  so we currently estimate relative to the next checkpoint  which is closer and presumably provides a better estimate. However  this assumes that these checkpoint are perfectly aligned without any position jumps.  The current approach has two issues: - The next checkpoint may include a position jump by design  e.g. if it was set for a new item in the playlist and the duration of the current item wasn't perfectly accurate. - The sudden switch between two estimation methods may cause a jump in the output position  which is visible when we add new media position checkpoints to the queue  not when we actually reach the playback position of the checkpoint.  We can fix both issues by taking a slightly different approach: - Continuously monitor the estimate using the current checkpoint. If it starts drifting  we can adjust it directly. This way the estimate is always aligned with the actual position. - The change above means we can safely switch to using the estimate based on the previous checkpoint. This way we don't have to make assumptions about the next checkpoint and any position jumps will only happen when we actually reach this checkpoint (which is more what a user expects to see  e.g. at a playlist item transition).  Issue: androidx/media#1698 PiperOrigin-RevId: 690979859
androidx,media,6147050b90aaaef91b921a832fbf88049f3be8d9, this fixes the,Together with the already submitted https://github.com/androidx/media/commit/6147050b90aaaef91b921a832fbf88049f3be8d9, this fixes the,Ramp up volume after AudioTrack flush to avoid pop sound  AudioTrack doesn't automatically ramp up the volume after a flush (only when resuming with play after a pause)  which causes audible pop sounds in most cases. The issue can be avoided by manually applying a short 20ms volume ramp  the same duration used by the platform for the automatic volume ramping where available. 
androidx,media,5e3dcea1bfffff0f641d34829d6965a89c791b68,https://github.com/androidx/media/commit/5e3dcea1bfffff0f641d34829d6965a89c791b68,If the media file itself contains a volume ramp at the beginning  we wouldn't need this additional ramping. Given the extremely short duration  this seems ignorable and we can treat it as a future feature request to mark the beginning of media in a special way that can then disable the volume ramping. - For seamless period transitions where we keep using the same AudioTrack  we may still get a pop sound at the transition. To solve this  we'd need a dedicated audio processor to either ramp the end of media down and the beginning of the next item up  or apply a very short cross-fade. Either way  we need new signalling to identify cases where the media originates from the same source and this effect should not be applied (e.g. when re-concatenating clipped audio snippets from the same file).  PiperOrigin-RevId: 676860234
androidx,media,fd3d8e1782bc74c23b545c3f24c25924d188f822,https://github.com/androidx/media/commit/fd3d8e1782bc74c23b545c3f24c25924d188f822,Add RATE_UNSET option to encoder performance setting  This is to allow not setting the MediaFormat OPERATING_RATE and PRIORITY altogether. The current behvaiour  if left the value `UNSET`  it'll apply the our optimizations  but apps might want to disable this optimization.  PiperOrigin-RevId: 675923909
androidx,media,f0fb3862245579ff3874d0bdc10a7481077984d8,https://github.com/androidx/media/commit/f0fb3862245579ff3874d0bdc10a7481077984d8,Add workaround for Galaxy Tab S7 FE device PerformancePoint issue  The Galaxy Tab S7 FE has a device issue that causes 60fps secure H264 streams to be marked as unsupported. This CL adds a workaround for this issue by checking the CDD required support for secure H264 in addition to the current check on standard H264. If the provided performance points do not cover the CDD requirement of support 720p H264 at 60fps  then it falls back to using legacy methods for checking frame rate and resolution support.  Issue: androidx/media#1619 PiperOrigin-RevId: 675920968
androidx,media,a1d23101704c025801d28b4cdd9afc41a302c935,https://github.com/androidx/media/commit/a1d23101704c025801d28b4cdd9afc41a302c935,Fix stuck player after seek  Seeking was causing the player to hang in the following scenario: 1. The surfaceTexture's onFrameAvailableListener is called in ExternalTextureManager to notify that a new frame is available. 2. This call submits a task on the GL thread. 3. A seek is performed and DefaultVideoFrameProcessor.flush() is called before the task submitted in 2 is executed. 4. DefaultVideoFrameProcessor.flush() flushes the task executor  so that the task submitted in 2 never gets executed. 5. Once the seek is over  the first frame is registered and rendered on the surface texture. 6. Playback hangs because the onFrameAvailableListener is never called for this new frame. This is because surfaceTexture.updateTexImage() was never called on the frame that became available in 1.  This fix is making sure that the task submitted in 2 always gets executed.  Issue: androidx/media#1535 PiperOrigin-RevId: 671389215
androidx,media,6e0e2d0ceeb9ee88434b79b6d2c4dfe74e93ec8f,https://github.com/androidx/media/commit/6e0e2d0ceeb9ee88434b79b6d2c4dfe74e93ec8f,Add QueuingGlShaderProgram for effects that run outside GL context  Implement a QueuingGlShaderProgram which queues up OpenGL frames and allows asynchronous execution of effects that operate on video frames without a performance penalty.  PiperOrigin-RevId: 666326611
androidx,media,931b0e25f1092674f94a43cdbcceb64c575199cc,https://github.com/androidx/media/commit/931b0e25f1092674f94a43cdbcceb64c575199cc,Add a DefaultDecoderFactory option to configure operating rate  This has the largest impact during operations with no encoder  such as frame extraction. Add a matching performance test.  PiperOrigin-RevId: 661220044
androidx,media,f238db8208af4575d7a151f94bf7b0281059f362,https://github.com/androidx/media/commit/f238db8208af4575d7a151f94bf7b0281059f362,Exit early if buffer becomes invalid  When the frame release control invalidates a buffer and returns that the buffer must be ignored  we need to exit early before performing additional checks that may result in method calls using the invalid buffer.  PiperOrigin-RevId: 640555688
JetBrains,intellij-plugins,20478259f3c846072931a05133bbc91b96e6b79d,https://github.com/JetBrains/intellij-plugins/commit/20478259f3c846072931a05133bbc91b96e6b79d,[perforce] get rid of references to package-private members of intellij.vcs.perforce.util from intellij.vcs.perforce (IJPL-149126)  A public OutputMessageParser.readOutputLines is extracted and reused.  GitOrigin-RevId: 506a2b60ec12539981f506f829dd70ac5456c88a
JetBrains,intellij-plugins,54e96393b59b94e490d1e55877cdc12c6ef25076,https://github.com/JetBrains/intellij-plugins/commit/54e96393b59b94e490d1e55877cdc12c6ef25076,[p4] use notification actions instead of plain HTML in server not available and not logged to Perforce notifications  GitOrigin-RevId: afd8858d55c18959bf9a2f8cc76c639dc0ca4efb
JetBrains,intellij-plugins,b64d75f3053076b51b8c91db106247874ccf2d4c,https://github.com/JetBrains/intellij-plugins/commit/b64d75f3053076b51b8c91db106247874ccf2d4c,[p4] prevent caching big content in VF attribute  Alternatively  GistStorage can be used for storing big files  but storing multiple big files can lead to performance problems which may not have been justified by "offline mode" functionality.  GitOrigin-RevId: 2acc4e5fd6b937743275c2567b4e59c1cbdcfed0
JetBrains,intellij-plugins,94033676f36c4fb76031f11409e3fe64667a3fd7,https://github.com/JetBrains/intellij-plugins/commit/94033676f36c4fb76031f11409e3fe64667a3fd7,[perforce] PerforceJob: remove unused myOtherFields  GitOrigin-RevId: 2ce82ca3ed991667e994ae47701d14719204c72e
PurpurMC,Purpur,db412df46619f2cd57bf8cb79bd6b49c444466ea,https://github.com/PurpurMC/Purpur/commit/db412df46619f2cd57bf8cb79bd6b49c444466ea,Add toggle for RNG manipulation  Paper patches RNG maniplulation by using a shared (and locked) random source. This comes with a performance gain  but technical players may prefer the ability to manipulate RNG.
Nekogram,Nekogram,ff812817953f7d5c52c12ecf7a50d02b69ba458c,https://github.com/Nekogram/Nekogram/commit/ff812817953f7d5c52c12ecf7a50d02b69ba458c,Fix photo viewer blur performance
Nekogram,Nekogram,15db3cec08bd6e3feec985ee9589f853de050489,https://github.com/Nekogram/Nekogram/commit/15db3cec08bd6e3feec985ee9589f853de050489,View large photos only on high performance devices
QuantumBadger,RedReader,652b523508287777a7ba2353c3bc95dfc556fa39,https://github.com/QuantumBadger/RedReader/commit/652b523508287777a7ba2353c3bc95dfc556fa39,Improve download performance
liferay,liferay-portal,07aa33707deb9775c196479574c0dd9ad74248c6,https://github.com/liferay/liferay-portal/commit/07aa33707deb9775c196479574c0dd9ad74248c6,LPD-53150 Verifying properties should be performed only once
liferay,liferay-portal,3553ace67b0b9e51a20ac5216a9e508fa52949e5,https://github.com/liferay/liferay-portal/commit/3553ace67b0b9e51a20ac5216a9e508fa52949e5,LPD-53150 Some verifications have to be performed only on one partition
liferay,liferay-portal,691b59f5bf05b06aa1415745a389f0c76afd0ce2,https://github.com/liferay/liferay-portal/commit/691b59f5bf05b06aa1415745a389f0c76afd0ce2,LPD-53150 Performing verification before upgrading. Stopping the server if a preupgrade verification process fails
liferay,liferay-portal,7ae57430d118516cc54ff6d2ab478be3bf167f50,https://github.com/liferay/liferay-portal/commit/7ae57430d118516cc54ff6d2ab478be3bf167f50,LPD-55062 gradle-plugins-jasper-jspc: removes code that is performed elsewhere
liferay,liferay-portal,9b544f0999ab853a9ed76b3ca63de39262a64eda,https://github.com/liferay/liferay-portal/commit/9b544f0999ab853a9ed76b3ca63de39262a64eda,LPD-54437 Avoid deadlock performing a sinle query. With four different queries performing a full scan since companyId isn't primary index the chances of a deadlock are bigger.
liferay,liferay-portal,bca74f3bb9870af80cd8eabf5fa632a68e45f882,https://github.com/liferay/liferay-portal/commit/bca74f3bb9870af80cd8eabf5fa632a68e45f882,LPD-52246 improve performance
liferay,liferay-portal,6c59a49108944f808adc48c1f8df1b18dd825941,https://github.com/liferay/liferay-portal/commit/6c59a49108944f808adc48c1f8df1b18dd825941,LPD-52727 If it's deleting the definition  perform a bulk deletion
liferay,liferay-portal,8e5c4a42251c061802f19dd77effe4c5e7f27430,https://github.com/liferay/liferay-portal/commit/8e5c4a42251c061802f19dd77effe4c5e7f27430,LPD-52727 If it's deleting the definition  perform a bulk deletion
liferay,liferay-portal,5efb379860c6b5c12830d1b2701d619375337f7a,https://github.com/liferay/liferay-portal/commit/5efb379860c6b5c12830d1b2701d619375337f7a,LPD-6627 Lets leave this outside for now. However it should be performed over the getConnection  not over the method invocation.
liferay,liferay-portal,374ef5689f11c5bcb1475b39474f3aa732e97922,https://github.com/liferay/liferay-portal/commit/374ef5689f11c5bcb1475b39474f3aa732e97922,LPD-53545 BigDecimal columns in PostgreSQL can be considered as Types.NUMERIC. Then there is no need to transform BigDecimal to String back then to BigDecimal. Perform the conversion directly.
liferay,liferay-portal,297c833c2e059e68f7f8e4fe806a3c720a1bdf9d,https://github.com/liferay/liferay-portal/commit/297c833c2e059e68f7f8e4fe806a3c720a1bdf9d,LPD-51094 Modify query to improve performance of hasSharingPermission and hasShareableSharingPermission
liferay,liferay-portal,cb1bda6fa064c546f5795671a1e4c2b403543d7c,https://github.com/liferay/liferay-portal/commit/cb1bda6fa064c546f5795671a1e4c2b403543d7c,LPD-51272 Make portlet perform role assignments in production mode
liferay,liferay-portal,a61a43625b765997790c6c914a5f0c37a038760d,https://github.com/liferay/liferay-portal/commit/a61a43625b765997790c6c914a5f0c37a038760d,LPD-50884 Improve performance. No need to count occurrences when creating or dropping schema.
liferay,liferay-portal,7f66949cf856dd4db380660476a4bd35a716483a,https://github.com/liferay/liferay-portal/commit/7f66949cf856dd4db380660476a4bd35a716483a,LPD-48867 Use NestedFields to avoid performance issues  Co-authored-by: Alicia García <alicia.garcia@liferay.com>
liferay,liferay-portal,5c13cd5fdf7c65d494256d0fb7bc4b55c09b31d4,https://github.com/liferay/liferay-portal/commit/5c13cd5fdf7c65d494256d0fb7bc4b55c09b31d4,LPD-49536 Use NestedFields to avoid performance issues
liferay,liferay-portal,df4ed4b2a94a02184e03bccc68500a0d61faf3c4,https://github.com/liferay/liferay-portal/commit/df4ed4b2a94a02184e03bccc68500a0d61faf3c4,LPD-51268 Fixed user creation related object action not being performed
liferay,liferay-portal,f21a424c1db94eeb77ad48a57153be86d99621d5,https://github.com/liferay/liferay-portal/commit/f21a424c1db94eeb77ad48a57153be86d99621d5,LPD-49226 Pending object entry should not have its status changed if a save draft action is performed  otherwise it will workflow engine
liferay,liferay-portal,ba446225eecb5166b32b1c2bb26a0fd0f9753490,https://github.com/liferay/liferay-portal/commit/ba446225eecb5166b32b1c2bb26a0fd0f9753490,LPD-49787 - Update the configuration directly since we already have it  in order to improve performance.
liferay,liferay-portal,5ac073349d6e75c06f682cafb6792f2fbcd62ce6,https://github.com/liferay/liferay-portal/commit/5ac073349d6e75c06f682cafb6792f2fbcd62ce6,LPD-49787 - Improve performance for the deleteRoleAccessToControlMenu method.
liferay,liferay-portal,c8b8e2b6ca9b8087e3a88de0995bbb8db300a4c6,https://github.com/liferay/liferay-portal/commit/c8b8e2b6ca9b8087e3a88de0995bbb8db300a4c6,LPD-48903 performance optimization
liferay,liferay-portal,3531e921749f441008bd13ce13390e62e5aca2e2,https://github.com/liferay/liferay-portal/commit/3531e921749f441008bd13ce13390e62e5aca2e2,LPD-45318 Improve performance
liferay,liferay-portal,0e6df9512eb3e8c24bde71d171e9b8bfb50d12b5,https://github.com/liferay/liferay-portal/commit/0e6df9512eb3e8c24bde71d171e9b8bfb50d12b5,LPD-49747 Do not execute one call per file to check if file exist in repo. Repo can be external and can cause performance issues.
liferay,liferay-portal,cb01e4647f8800c8804763f7cf1f2a652ba9ba26,https://github.com/liferay/liferay-portal/commit/cb01e4647f8800c8804763f7cf1f2a652ba9ba26,LPD-49747 Use parallel to improve performance.
liferay,liferay-portal,e89bce5ad708d0c2adbcb3d8e364c62b8511f578,https://github.com/liferay/liferay-portal/commit/e89bce5ad708d0c2adbcb3d8e364c62b8511f578,LPD-49010 [SCIM Groups] retrieve filter param and perform search by displayName
liferay,liferay-portal,7a9e4f965858fcd93b1c201a005f20b1f345cc90,https://github.com/liferay/liferay-portal/commit/7a9e4f965858fcd93b1c201a005f20b1f345cc90,LPD-49009 [SCIM Users] retrieve filter param and perform search by externalId or userName
liferay,liferay-portal,0067887f36a69ed173adb3e6d3c04f0487cf8b2d,https://github.com/liferay/liferay-portal/commit/0067887f36a69ed173adb3e6d3c04f0487cf8b2d,LPD-48242 Add "order by" to make the result consistent with the fetch method generated by persistence layer  this part of logic was originally added by LPD-28122(007246d4c619fbc4eebccb325fe106f119ba2b8c) to improve performance of groupPersistence.fetchByLiveGroupId(liveGroupId)
liferay,liferay-portal,dfe030d2acfba5716550a882d97828519307cf69,https://github.com/liferay/liferay-portal/commit/dfe030d2acfba5716550a882d97828519307cf69,LPD-48522 Associate operation in a one to many relationship is an update in the child object entry  but for many to many we do not have a reference because there is no parent or child  so both entries resource permission checks should be performed
liferay,liferay-portal,c798318fab35bf010ff5194f4878146d1457854e,https://github.com/liferay/liferay-portal/commit/c798318fab35bf010ff5194f4878146d1457854e,LPD-46955 Fix current behaviour  if no nullable information is passed assume null as with other databases. This was not discovered before because the column recreation was always performed.
liferay,liferay-portal,64d464c8042bd396253b8c9e27930193c128b109,https://github.com/liferay/liferay-portal/commit/64d464c8042bd396253b8c9e27930193c128b109,LPD-48419 Improve performance a tiny bit
liferay,liferay-portal,6b6ce5302d2c3482d4c8763c8dc894b5b6d041f7,https://github.com/liferay/liferay-portal/commit/6b6ce5302d2c3482d4c8763c8dc894b5b6d041f7,LPD-46011 Uses stepperFragmentEntryLinkId in the request to avoid unnecessary calls
liferay,liferay-portal,912c0f1e5c445102cd0aa491512f7887e8f39566,https://github.com/liferay/liferay-portal/commit/912c0f1e5c445102cd0aa491512f7887e8f39566,LPD-47730 Hide delete action when the user does not have the UPDATE permission because the delete action is performing a disassociation action that means an update
liferay,liferay-portal,cd93c5a8e650293481214da774f54921256ad5e9,https://github.com/liferay/liferay-portal/commit/cd93c5a8e650293481214da774f54921256ad5e9,LPD-41116 Perform changes requested in core infra review  As per https://github.com/liferay-core-infra/liferay-portal/pull/7629#issuecomment-2480451202
liferay,liferay-portal,7903764d8887843ee3912f320c126d56389e44de,https://github.com/liferay/liferay-portal/commit/7903764d8887843ee3912f320c126d56389e44de,LPD-45684 In general  it is better for performance to prepopulate the size  but this math is too complicated. So in this case  it's better to optimize for the human brain instead of runtime performance.
liferay,liferay-portal,5dd44741628de96aeede139f95f97ec4a48094f4,https://github.com/liferay/liferay-portal/commit/5dd44741628de96aeede139f95f97ec4a48094f4,LPD-47429 Revert f3855daf25a14f71063b259f8eddf7a5f558cf43. ExpireArticle does not perform the same logic  it expires all article versions  but the check article should only expire article versions equals or lower to the expired one to avoid expiring also newer versions.
liferay,liferay-portal,cd671b68b2e482e273bd101cbda142568f053e80,https://github.com/liferay/liferay-portal/commit/cd671b68b2e482e273bd101cbda142568f053e80,LPD-35071 - Improve performance of implementation by not looping through all frontendTokenDefinitions.
liferay,liferay-portal,8bbb0db5706862e8a357f3934e6e4abc70ad71e7,https://github.com/liferay/liferay-portal/commit/8bbb0db5706862e8a357f3934e6e4abc70ad71e7,LPD-42809 Clean up empty company caches  This makes sure we don't leave empty maps hanging around when all import maps for a company have been removed.  However it does so at the expense of cache write performance. Given how unlikely it is destroying companies and given that it is even less likely that so many are created and destroyed as to leave a noticeable trace in RAM usage due to empty maps maybe we prefer the empty maps than the synchronized blocks.  That's why I'm adding this commit as the last of the PR  in case we prefer removing it.
liferay,liferay-portal,39dfa38b7ef09db0cc8a8697ea087b2839fa8851,https://github.com/liferay/liferay-portal/commit/39dfa38b7ef09db0cc8a8697ea087b2839fa8851,LPD-46069 Improve performance by preventing computing the types of not input-type fragments
liferay,liferay-portal,274e5c9ee0f35190712c93c3cca51893114ab1a4,https://github.com/liferay/liferay-portal/commit/274e5c9ee0f35190712c93c3cca51893114ab1a4,LPD-45340 Improve performance by avoiding iterating all restricted methods every time getAsString is invoked
liferay,liferay-portal,61def0cc5c1105d1a8494318b837eb8de7a1ac7a,https://github.com/liferay/liferay-portal/commit/61def0cc5c1105d1a8494318b837eb8de7a1ac7a,LPD-43403 Perform this query via ES instead of relational database due to the complexity of retrieving the latest process definition versions  # breaking  ## What modules/apps/portal-workflow/portal-workflow-kaleo-api/src/main/java/com/liferay/portal/workflow/kaleo/service/KaleoDefinitionVersionLocalService.java  Adding parameters java.util.Locale locale to method addLayoutUtilityPageEntry  changing its signature from  public List<KaleoDefinitionVersion> getLatestKaleoDefinitionVersions( long companyId  String keywords  int status  int start  int end  OrderByComparator<KaleoDefinitionVersion> orderByComparator);  to  public List<KaleoDefinitionVersion> getLatestKaleoDefinitionVersions( long companyId  String keywords  Locale locale  int status  int start  int end  OrderByComparator<KaleoDefinitionVersion> orderByComparator);  Delete the unused method  public List<KaleoDefinitionVersion> getLatestKaleoDefinitionVersions( long companyId  int start  int end  OrderByComparator<KaleoDefinitionVersion> orderByComparator);  ## Why  The locale has to be used to sort the kaleo definition version by their title that is a localized column  ----
liferay,liferay-portal,3e7b83ee90154386d725215c4ad333072563825e,https://github.com/liferay/liferay-portal/commit/3e7b83ee90154386d725215c4ad333072563825e,LPD-44673 Improve performance by fetching the SEO entry layout only once to obtain the entire map
liferay,liferay-portal,b61612d15537f4be0b68a60c48865a09c51283bc,https://github.com/liferay/liferay-portal/commit/b61612d15537f4be0b68a60c48865a09c51283bc,LPD-40036 Using clay:navigation-bar instead of clay:tabs for better performance
liferay,liferay-portal,ee0ca959b217722df124b9ddda4f5990519e0d64,https://github.com/liferay/liferay-portal/commit/ee0ca959b217722df124b9ddda4f5990519e0d64,LPD-43466 Perform actions for each company
liferay,liferay-portal,459fbcdc4a60acf48e6ac645c15023169df98ebb,https://github.com/liferay/liferay-portal/commit/459fbcdc4a60acf48e6ac645c15023169df98ebb,LPD-30835 Perform the synchronization when the company is extracted.
liferay,liferay-portal,561f8020e0d9f5573d1963cf8a1fe18b07274742,https://github.com/liferay/liferay-portal/commit/561f8020e0d9f5573d1963cf8a1fe18b07274742,LPD-38670 if user has VIEW_ACCOUNTS permission  set the account instead of iterating to increase performance
liferay,liferay-portal,9b43ab494d3410833ecd8a1f94bb4a8b2fa8f273,https://github.com/liferay/liferay-portal/commit/9b43ab494d3410833ecd8a1f94bb4a8b2fa8f273,LPD-41899 Disable the cache for the current thread if any write operation which will invalidate the cache is performed on the thread  and only clean up the cache at transaction commit.
liferay,liferay-portal,096bcec934b9b5612f85eda13e122d3c17ce9de0,https://github.com/liferay/liferay-portal/commit/096bcec934b9b5612f85eda13e122d3c17ce9de0,LPD-29336 Use finder for better performance
liferay,liferay-portal,232862d7ad740199c8cf6ed1e926dd46da2a0a43,https://github.com/liferay/liferay-portal/commit/232862d7ad740199c8cf6ed1e926dd46da2a0a43,LPD-37354 If issue.key is set  only check perform upgrades that match the issue.key
liferay,liferay-portal,acef4cdebae0145e9287d98a940727ab66330126,https://github.com/liferay/liferay-portal/commit/acef4cdebae0145e9287d98a940727ab66330126,LPD-40843 No need to perform list transformation for each table.
liferay,liferay-portal,e5b51498d4f38f921c51834942f08cf859c33aae,https://github.com/liferay/liferay-portal/commit/e5b51498d4f38f921c51834942f08cf859c33aae,LPD-41913 Add methods to display context to get the right emptyResultsMessage in each case (2)  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/display/context/MBAdminListDisplayContext.java  modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/display/context/MBAdminListDisplayContext.java now has a new method to return a valid empty results message depending on the user interaction (if is performing a search or not)  ## Why  Currently  the MBCategories are not shown in the search results. With this changes we’ll avoid show a confusing message in a testable way (using unit tests).  ----
liferay,liferay-portal,519dbac648db167145f62a5db121aa47c5d8bfa1,https://github.com/liferay/liferay-portal/commit/519dbac648db167145f62a5db121aa47c5d8bfa1,LPD-41913 Add methods to display context to get the right emptyResultsMessage in each case (1)  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/display/context/MBListDisplayContext.java  modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/display/context/MBListDisplayContext.java now has a new method to return a valid empty results message depending on the user interaction (if is performing a search or not)  ## Why  Currently  the MBCategories are not shown in the search results. With this changes we’ll avoid show a confusing message in a testable way (using unit tests).  ----
liferay,liferay-portal,46a2e475f21d76d38dc128a3311e761f221df128,https://github.com/liferay/liferay-portal/commit/46a2e475f21d76d38dc128a3311e761f221df128,LPD-41227 Ensure performing a POST request via notification.rest.resource will work the same way it works for rest.client.resource
liferay,liferay-portal,19fec76f6a5ee217d2e43443efff5638ce32881d,https://github.com/liferay/liferay-portal/commit/19fec76f6a5ee217d2e43443efff5638ce32881d,LPD-41152 SF  this is superfluous  the base class already does it and invokes PUT
liferay,liferay-portal,60e54bfb5673febd4e20c6d0032f543482f8429a,https://github.com/liferay/liferay-portal/commit/60e54bfb5673febd4e20c6d0032f543482f8429a,LPD-39612 Prefer array because List#get performance is impl specific
liferay,liferay-portal,d7c166ce0666c685f002dd039fb41b878c5a431f,https://github.com/liferay/liferay-portal/commit/d7c166ce0666c685f002dd039fb41b878c5a431f,LPD-40120 SF - Match to SYZ's code for better readability and performance
liferay,liferay-portal,e2fe0c166783e1de4759b75b7096e0b7a519603e,https://github.com/liferay/liferay-portal/commit/e2fe0c166783e1de4759b75b7096e0b7a519603e,LPD-40120 Improve performance by only adding matching entries  rather than adding all then purging non-matches.
liferay,liferay-portal,b2a6f7e74120c2854e5c04ff400a855371babf2d,https://github.com/liferay/liferay-portal/commit/b2a6f7e74120c2854e5c04ff400a855371babf2d,LPD-40070 Use a non-recursive approach by default  In the first version we were extracting inline handlers from all nodes in the HTML but that may introduce issues that CSP is trying to prevent.  So now by default we will be only extracting handlers from the top level nodes of the HTML unless recursive=true is passed to the JSP tag or Util class.  This will also enhance performance.
liferay,liferay-portal,0d7fc3f8a8dbd03c64e86fb98f43a3e36a05b985,https://github.com/liferay/liferay-portal/commit/0d7fc3f8a8dbd03c64e86fb98f43a3e36a05b985,LPD-40070 Substitute <liferay-util:on> by <liferay-ui:csp>  Instead of creating a JSP tag that attaches an event handler to the previous DOM node  which is something that can fail due to SPA moving the nodes  refactoring the code  and so on...  we are creating a tag that rewrites all its body extracting inline handlers to their own <script> node using `ContentSecurityPolicyHTMLRewriterUtil`.  To enahnce performance  this transformation only takes place when a CSP nonce is active for the current request.
liferay,liferay-portal,713138203e1b9863993d033378dc708c202f15d5,https://github.com/liferay/liferay-portal/commit/713138203e1b9863993d033378dc708c202f15d5,LPD-31242 Since we are only upgrading method calls that use super  it is technically implied that we are extending BaseModelListener  so only check those files to improve performance
liferay,liferay-portal,c1d9c34a5a14993fe9472c8c80ae5957abad182b,https://github.com/liferay/liferay-portal/commit/c1d9c34a5a14993fe9472c8c80ae5957abad182b,LPD-37711 This is not needed since the validation is performed when the fragment entry link is added or updated
liferay,liferay-portal,5172e7ae5c3d1c13b68f9538c2874af266d37de4,https://github.com/liferay/liferay-portal/commit/5172e7ae5c3d1c13b68f9538c2874af266d37de4,LPD-36212 Improve performance a tiny bit  (cherry picked from commit 17c3577225b01dd18a0dfca1cd842231940e0d7e)
liferay,liferay-portal,ec26ee13fd3571305f62830f609e47840eea81f5,https://github.com/liferay/liferay-portal/commit/ec26ee13fd3571305f62830f609e47840eea81f5,LPD-37990 Improve performance
liferay,liferay-portal,5784d83b5d61a389610ef454e01a0297182b5b4c,https://github.com/liferay/liferay-portal/commit/5784d83b5d61a389610ef454e01a0297182b5b4c,LPD-35167 Do not perform cascade publish
liferay,liferay-portal,f88f1eb29c524d8098af96ae11d625d0fbf01bfa,https://github.com/liferay/liferay-portal/commit/f88f1eb29c524d8098af96ae11d625d0fbf01bfa,Revert "LPD-36212 Improve performance a tiny bit"  This reverts commit 17c3577225b01dd18a0dfca1cd842231940e0d7e.
liferay,liferay-portal,17c3577225b01dd18a0dfca1cd842231940e0d7e,https://github.com/liferay/liferay-portal/commit/17c3577225b01dd18a0dfca1cd842231940e0d7e,LPD-36212 Improve performance a tiny bit
liferay,liferay-portal,138733c55cc06a1fee71674db7f4b5faef9baca0,https://github.com/liferay/liferay-portal/commit/138733c55cc06a1fee71674db7f4b5faef9baca0,LPD-37702 Use a constant due to performance since it won't change and we will check it multiple times
liferay,liferay-portal,7dda41ffecce1639b389076bbb1ff49768a43a18,https://github.com/liferay/liferay-portal/commit/7dda41ffecce1639b389076bbb1ff49768a43a18,LPD-30986 Perform first table check to detect if table was removed and failed view creation.
liferay,liferay-portal,7efc98286c377be5e821c1c700b8243071ea5a2f,https://github.com/liferay/liferay-portal/commit/7efc98286c377be5e821c1c700b8243071ea5a2f,LPD-37234 Use a post request since we are performing an async request that depends on confirmation
liferay,liferay-portal,e19087bfeb3fe985398e78252629b60ed0676f7c,https://github.com/liferay/liferay-portal/commit/e19087bfeb3fe985398e78252629b60ed0676f7c,LPD-35166 A bind cannot be performed if one of the object definitions has object entries
liferay,liferay-portal,326260c12c7876426b6fb376a0e2a18ece1703fe,https://github.com/liferay/liferay-portal/commit/326260c12c7876426b6fb376a0e2a18ece1703fe,LPD-35625 Add internal class to save changes performed
liferay,liferay-portal,cb4cb4868c64a0c211c89df50cdec5d66d9deeba,https://github.com/liferay/liferay-portal/commit/cb4cb4868c64a0c211c89df50cdec5d66d9deeba,LPD-36475 Fix performance regression caused by 79da3895 . Delegating ResourceBundle.handleGetObject() to inner ResourceBundle.getObject() and ResourceBundle.handleKeySet() to inner ResourceBundle.keySet() is logcally "wrong" since it introduces a new layer of parent fallback lookup  but if the parent is absent the overall function is still fine. And it is very bad for performance  due to the extra lookup and ResourceBundle.keySet()'s copy protection  for large ResourceBundle  this copy protection generates a lot of garbage.
liferay,liferay-portal,6f9e8f97c4280914be960af7a93c5aa8637a8975,https://github.com/liferay/liferay-portal/commit/6f9e8f97c4280914be960af7a93c5aa8637a8975,LPD-32644 Only perform the copy between tables that are both on source and target DB.
liferay,liferay-portal,60026c4b1abf2fe5be8490b68d666a6f9442dc4c,https://github.com/liferay/liferay-portal/commit/60026c4b1abf2fe5be8490b68d666a6f9442dc4c,Revert "LPD-33730 show content performance icon navigation only when the feature flag is disabled"  This reverts commit 30f49d781c23d773e3cc3ddc075d883a02a8378f.
liferay,liferay-portal,709c5aa369a834cdfae9e2f41f47abd0d0de860f,https://github.com/liferay/liferay-portal/commit/709c5aa369a834cdfae9e2f41f47abd0d0de860f,LPD-33173 Provide MVC Resource for getting content performance tab
liferay,liferay-portal,d635da4e9fb82fe3d6801b19a49aa6ffe871a049,https://github.com/liferay/liferay-portal/commit/d635da4e9fb82fe3d6801b19a49aa6ffe871a049,LPD-33173 create GetContentPerformanceInfoMVCResourceCommand
liferay,liferay-portal,3becfbec9e7055e6d42b5ea5791f8c4f220b1a38,https://github.com/liferay/liferay-portal/commit/3becfbec9e7055e6d42b5ea5791f8c4f220b1a38,LPD-33730 show content performance icon navigation only when the feature flag is disabled
liferay,liferay-portal,0be2a67fa4fb8ef961e6a3e27fa6d830d1a4121a,https://github.com/liferay/liferay-portal/commit/0be2a67fa4fb8ef961e6a3e27fa6d830d1a4121a,LPD-32570 Improve performance a tiny bit
liferay,liferay-portal,61b4a424b479d5a2030ff5bd7715e680a43cbe7c,https://github.com/liferay/liferay-portal/commit/61b4a424b479d5a2030ff5bd7715e680a43cbe7c,LPD-25985 Add singleton into module commerce-shipping-engine-fixed-api  # breaking  ## What modules/apps/commerce/commerce-shipping-engine-fixed-api/src/main/java/com/liferay/commerce/shipping/engine/fixed/util/comparator/CommerceShippingFixedOptionPriorityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-shipping-engine-fixed-api/src/main/java/com/liferay/commerce/shipping/engine/fixed/util/comparator/CommerceShippingFixedOptionRelCountryIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,4ff28e4d29b8d39f9e6932a3d3787fe36ce287c8,https://github.com/liferay/liferay-portal/commit/4ff28e4d29b8d39f9e6932a3d3787fe36ce287c8,LPD-25985 Add singleton into module sharing-api  # breaking  ## What modules/apps/sharing/sharing-api/src/main/java/com/liferay/sharing/util/comparator/SharingEntryModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,edda3e86859f98cceda96fba09a07b53827869d2,https://github.com/liferay/liferay-portal/commit/edda3e86859f98cceda96fba09a07b53827869d2,LPD-25985 Add singleton into module saved-content-api  # breaking  ## What modules/apps/saved-content/saved-content-api/src/main/java/com/liferay/saved/content/util/comparator/SavedContentEntryClassNameIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,0754766015db663bd73c97291b1d027e149f2c66,https://github.com/liferay/liferay-portal/commit/0754766015db663bd73c97291b1d027e149f2c66,LPD-25955 Add singleton into module portal-reports-engine-console-api  # breaking  ## What modules/dxp/apps/portal-reports-engine-console/portal-reports-engine-console-api/src/main/java/com/liferay/portal/reports/engine/console/util/comparator/DefinitionCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/dxp/apps/portal-reports-engine-console/portal-reports-engine-console-api/src/main/java/com/liferay/portal/reports/engine/console/util/comparator/EntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/dxp/apps/portal-reports-engine-console/portal-reports-engine-console-api/src/main/java/com/liferay/portal/reports/engine/console/util/comparator/SourceCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,46d19d1c8f4d0c0f6d529c1027c73163eb4bda03,https://github.com/liferay/liferay-portal/commit/46d19d1c8f4d0c0f6d529c1027c73163eb4bda03,LPD-25955 Add singleton into module site-navigation-api  # breaking  ## What modules/apps/site-navigation/site-navigation-api/src/main/java/com/liferay/site/navigation/util/comparator/SiteNavigationMenuCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/site-navigation/site-navigation-api/src/main/java/com/liferay/site/navigation/util/comparator/SiteNavigationMenuModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/site-navigation/site-navigation-api/src/main/java/com/liferay/site/navigation/util/comparator/SiteNavigationMenuNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/site-navigation/site-navigation-api/src/main/java/com/liferay/site/navigation/util/comparator/SiteNavigationMenuItemOrderComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,baa550b2319768764350e528368e8e071250da4a,https://github.com/liferay/liferay-portal/commit/baa550b2319768764350e528368e8e071250da4a,LPD-25955 Add singleton into module calendar-api  # breaking  ## What modules/apps/calendar/calendar-api/src/main/java/com/liferay/calendar/util/comparator/CalendarBookingStartTimeComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/calendar/calendar-api/src/main/java/com/liferay/calendar/util/comparator/CalendarNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/calendar/calendar-api/src/main/java/com/liferay/calendar/util/comparator/CalendarResourceCodeComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/calendar/calendar-api/src/main/java/com/liferay/calendar/util/comparator/CalendarResourceNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,4ff0db43c7c3aa9cb507b5090d8170a3d9050015,https://github.com/liferay/liferay-portal/commit/4ff0db43c7c3aa9cb507b5090d8170a3d9050015,LPD-25955 Add singleton into module bookmarks-api  # breaking  ## What modules/apps/bookmarks/bookmarks-api/src/main/java/com/liferay/bookmarks/util/comparator/EntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/bookmarks/bookmarks-api/src/main/java/com/liferay/bookmarks/util/comparator/EntryNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/bookmarks/bookmarks-api/src/main/java/com/liferay/bookmarks/util/comparator/EntryPriorityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/bookmarks/bookmarks-api/src/main/java/com/liferay/bookmarks/util/comparator/EntryURLComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/bookmarks/bookmarks-api/src/main/java/com/liferay/bookmarks/util/comparator/FolderIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,b3a9cdd689c23353cd25bd88c6f67e86f497e0c3,https://github.com/liferay/liferay-portal/commit/b3a9cdd689c23353cd25bd88c6f67e86f497e0c3,LPD-25955 Add singleton into module style-book-api  # breaking  ## What modules/apps/style-book/style-book-api/src/main/java/com/liferay/style/book/util/comparator/StyleBookEntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,94d278ab980b10182758cf8a6c51fbd9837f5b85,https://github.com/liferay/liferay-portal/commit/94d278ab980b10182758cf8a6c51fbd9837f5b85,LPD-25955 Add singleton into module trash-api  # breaking  ## What modules/apps/trash/trash-api/src/main/java/com/liferay/trash/util/comparator/EntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/trash/trash-api/src/main/java/com/liferay/trash/util/comparator/EntryTypeComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/trash/trash-api/src/main/java/com/liferay/trash/util/comparator/EntryUserNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,739f94ca05cfabfc49d04a8e60abdd6d99fe3b2b,https://github.com/liferay/liferay-portal/commit/739f94ca05cfabfc49d04a8e60abdd6d99fe3b2b,LPD-25955 Add singleton into module wiki-api  # breaking  ## What modules/apps/wiki/wiki-api/src/main/java/com/liferay/wiki/util/comparator/NodeLastPostDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/wiki/wiki-api/src/main/java/com/liferay/wiki/util/comparator/NodeNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/wiki/wiki-api/src/main/java/com/liferay/wiki/util/comparator/PageCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/wiki/wiki-api/src/main/java/com/liferay/wiki/util/comparator/PageModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/wiki/wiki-api/src/main/java/com/liferay/wiki/util/comparator/PageTitleComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,821ccba5209638715a0e7e0f4618c742661b9c8e,https://github.com/liferay/liferay-portal/commit/821ccba5209638715a0e7e0f4618c742661b9c8e,LPD-26560 Add singleton into module blogs-api  # breaking  ## What modules/apps/blogs/blogs-api/src/main/java/com/liferay/blogs/util/comparator/EntryDisplayDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/blogs/blogs-api/src/main/java/com/liferay/blogs/util/comparator/EntryIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/blogs/blogs-api/src/main/java/com/liferay/blogs/util/comparator/EntryModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/blogs/blogs-api/src/main/java/com/liferay/blogs/util/comparator/EntryTitleComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,0c174ce05114b331380d9066ee8f65ced7be9eeb,https://github.com/liferay/liferay-portal/commit/0c174ce05114b331380d9066ee8f65ced7be9eeb,LPD-26560 Add singleton into module subscription-api  # breaking  ## What modules/apps/subscription/subscription-api/src/main/java/com/liferay/subscription/util/comparator/SubscriptionClassNameIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,bb39fd392663d1885c6263914507b26768b039b5,https://github.com/liferay/liferay-portal/commit/bb39fd392663d1885c6263914507b26768b039b5,LPD-26560 Add singleton into module object-api  # breaking  ## What modules/apps/object/object-api/src/main/java/com/liferay/object/util/comparator/ObjectFieldCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,1505de432a97bcd6233ff9e92d35873629348225,https://github.com/liferay/liferay-portal/commit/1505de432a97bcd6233ff9e92d35873629348225,LPD-26560 Add singleton into module push-notifications-api  # breaking  ## What modules/apps/push-notifications/push-notifications-api/src/main/java/com/liferay/push/notifications/util/comparator/PushNotificationsDevicePlatformComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,6a9576fb021af773b2bbd6d2084aa6c92abdfdd9,https://github.com/liferay/liferay-portal/commit/6a9576fb021af773b2bbd6d2084aa6c92abdfdd9,LPD-26560 Add singleton into module portal-security-service-access-policy-api  # breaking  ## What modules/apps/portal-security/portal-security-service-access-policy-api/src/main/java/com/liferay/portal/security/service/access/policy/util/comparator/SAPEntryNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,fc79693660f721f68d6ea31c415d4c942f778449,https://github.com/liferay/liferay-portal/commit/fc79693660f721f68d6ea31c415d4c942f778449,LPD-26560 Add singleton into module friendly-url-api  # breaking  ## What modules/apps/friendly-url/friendly-url-api/src/main/java/com/liferay/friendly/url/util/comparator/FriendlyURLEntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/friendly-url/friendly-url-api/src/main/java/com/liferay/friendly/url/util/comparator/FriendlyURLEntryLocalizationComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,14c321c27f4c50c031e3036f9432c0078523da29,https://github.com/liferay/liferay-portal/commit/14c321c27f4c50c031e3036f9432c0078523da29,LPD-26560 Add singleton into module document-library-content-api  # breaking  ## What modules/apps/document-library/document-library-content-api/src/main/java/com/liferay/document/library/content/util/comparator/DLContentVersionComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,35bd6e84854113b3928681a16ac2ffc5a90b90d6,https://github.com/liferay/liferay-portal/commit/35bd6e84854113b3928681a16ac2ffc5a90b90d6,LPD-23983 Create tool main class. No functionality is performed yet. Just a skeleton.
liferay,liferay-portal,2eb50b5ab1e36d8c4291d1f549c399a023e68347,https://github.com/liferay/liferay-portal/commit/2eb50b5ab1e36d8c4291d1f549c399a023e68347,LPD-33654 Add capability to use it without checking permissions for get a better performance when it is not needed to check it
liferay,liferay-portal,5b365fbcf625463c46bf603f4ad376494f5a56e1,https://github.com/liferay/liferay-portal/commit/5b365fbcf625463c46bf603f4ad376494f5a56e1,LPD-33361 No need to perform index operations on views.
liferay,liferay-portal,b7896873028dcc8001285e42e6d1b9d9407fca21,https://github.com/liferay/liferay-portal/commit/b7896873028dcc8001285e42e6d1b9d9407fca21,LPD-27027 Add singleton into module commerce-inventory-api  # breaking  ## What modules/apps/commerce/commerce-inventory-api/src/main/java/com/liferay/commerce/inventory/util/comparator/CommerceInventoryWarehouseCityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-inventory-api/src/main/java/com/liferay/commerce/inventory/util/comparator/CommerceInventoryWarehouseItemWarehouseNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-inventory-api/src/main/java/com/liferay/commerce/inventory/util/comparator/CommerceInventoryReplenishmentItemAvailabilityDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-inventory-api/src/main/java/com/liferay/commerce/inventory/util/comparator/CommerceInventoryWarehouseItemQuantityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-inventory-api/src/main/java/com/liferay/commerce/inventory/util/comparator/CommerceInventoryWarehouseNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,ebb5f1ce3a2816cadde557db2ae0700a63e35038,https://github.com/liferay/liferay-portal/commit/ebb5f1ce3a2816cadde557db2ae0700a63e35038,LPD-27027 Add singleton into module commerce-product-type-virtual-order-api  # breaking  ## What modules/apps/commerce/commerce-product-type-virtual-order-api/src/main/java/com/liferay/commerce/product/type/virtual/order/util/comparator/CommerceVirtualOrderItemCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,2442863bcdff03e8d91ad7b4b33b92e23ccdee3f,https://github.com/liferay/liferay-portal/commit/2442863bcdff03e8d91ad7b4b33b92e23ccdee3f,LPD-27027 Add singleton into module commerce-product-type-grouped-api  # breaking  ## What modules/apps/commerce/commerce-product-type-grouped-api/src/main/java/com/liferay/commerce/product/type/grouped/util/comparator/CPDefinitionGroupedEntryPriorityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-product-type-grouped-api/src/main/java/com/liferay/commerce/product/type/grouped/util/comparator/CPDefinitionGroupedEntryQuantityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,6616844165799681f918ac7b4ad262ad8df8750a,https://github.com/liferay/liferay-portal/commit/6616844165799681f918ac7b4ad262ad8df8750a,LPD-27027 Add singleton to module commerce-currency-api  # breaking  ## What modules/apps/commerce/commerce-currency-api/src/main/java/com/liferay/commerce/currency/util/comparator/CommerceCurrencyPriorityComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,cd4390a07225a534d728fa0ce9666585a09562fc,https://github.com/liferay/liferay-portal/commit/cd4390a07225a534d728fa0ce9666585a09562fc,LPD-27027 Add singleton into module commerce-wish-list-api  # breaking  ## What modules/apps/commerce/commerce-wish-list-api/src/main/java/com/liferay/commerce/wish/list/util/comparator/CommerceWishListCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-wish-list-api/src/main/java/com/liferay/commerce/wish/list/util/comparator/CommerceWishListItemCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-wish-list-api/src/main/java/com/liferay/commerce/wish/list/util/comparator/CommerceWishListNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,cdf5671992038a1fa1ddcff417b0a6507b8e0818,https://github.com/liferay/liferay-portal/commit/cdf5671992038a1fa1ddcff417b0a6507b8e0818,LPD-27027 Add singleton to module commerce-discount-api  # breaking  ## What modules/apps/commerce/commerce-discount-api/src/main/java/com/liferay/commerce/discount/util/comparator/CommerceDiscountRelCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-discount-api/src/main/java/com/liferay/commerce/discount/util/comparator/CommerceDiscountCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-discount-api/src/main/java/com/liferay/commerce/discount/util/comparator/CommerceDiscountRuleCreateDateComparator.java  the class is now a singleton  ## Why  this change was made since the class was unused  ----  # breaking  ## What modules/apps/commerce/commerce-discount-api/src/main/java/com/liferay/commerce/discount/util/comparator/CommerceDiscountCommerceAccountGroupRelCreateDateComparator.java  the class is now a singleton  ## Why  this change was made since the class was unused  ----
liferay,liferay-portal,44ee476c43d68a0e3b4d78ece3054a22f17a813a,https://github.com/liferay/liferay-portal/commit/44ee476c43d68a0e3b4d78ece3054a22f17a813a,LPD-27027 Add singleton to module commerce-tax-engine-fixed-api  # breaking  ## What modules/apps/commerce/commerce-tax-engine-fixed-api/src/main/java/com/liferay/commerce/tax/engine/fixed/util/comparator/CommerceTaxFixedRateAddressRelCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-tax-engine-fixed-api/src/main/java/com/liferay/commerce/tax/engine/fixed/util/comparator/CPTaxCategoryNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/commerce/commerce-tax-engine-fixed-api/src/main/java/com/liferay/commerce/tax/engine/fixed/util/comparator/CommerceTaxFixedRateCreateDateComparator.java  the class is now a singleton  ## Why  this change was made since the class was unused  ----
liferay,liferay-portal,9f83b40c25adcd9550e5f5aed1d357a3ce9941dd,https://github.com/liferay/liferay-portal/commit/9f83b40c25adcd9550e5f5aed1d357a3ce9941dd,LPD-26917 Add singleton to module dynamic-data-mapping-api  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/DataProviderInstanceModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/DataProviderInstanceNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/DDMFormInstanceNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/DDMFormInstanceRecordIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/StructureCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/StructureIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/StructureLayoutCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/StructureLayoutNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/StructureLinkStructureModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-mapping/dynamic-data-mapping-api/src/main/java/com/liferay/dynamic/data/mapping/util/comparator/TemplateIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,e1b76ca20fbfeaee8096f4185b11dccb9ad9d846,https://github.com/liferay/liferay-portal/commit/e1b76ca20fbfeaee8096f4185b11dccb9ad9d846,LPD-26917 Add singleton to module dynamic-data-lists-api  # breaking  ## What modules/apps/dynamic-data-lists/dynamic-data-lists-api/src/main/java/com/liferay/dynamic/data/lists/util/comparator/DDLRecordIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-lists/dynamic-data-lists-api/src/main/java/com/liferay/dynamic/data/lists/util/comparator/DDLRecordSetCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-lists/dynamic-data-lists-api/src/main/java/com/liferay/dynamic/data/lists/util/comparator/DDLRecordCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/dynamic-data-lists/dynamic-data-lists-api/src/main/java/com/liferay/dynamic/data/lists/util/comparator/DDLRecordSetNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,ccf4dc703cc3527c2b26d21b29c686f09054578d,https://github.com/liferay/liferay-portal/commit/ccf4dc703cc3527c2b26d21b29c686f09054578d,LPD-26917 Add singleton to module data-engine-api  # breaking  ## What modules/apps/data-engine/data-engine-api/src/main/java/com/liferay/data/engine/util/comparator/DEDataListViewCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/data-engine/data-engine-api/src/main/java/com/liferay/data/engine/util/comparator/DEDataListViewNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,1b766ab9f1cd2a3738af0100a3918794738d6431,https://github.com/liferay/liferay-portal/commit/1b766ab9f1cd2a3738af0100a3918794738d6431,LPD-26917 Add singleton to portal-workflow-metrics-api module  # breaking  ## What modules/dxp/apps/portal-workflow/portal-workflow-metrics-api/src/main/java/com/liferay/portal/workflow/metrics/util/comparator/WorkflowMetricsSLADefinitionVersionIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,3bf860c27150cf5ef3630c456383e14fe3c1b48e,https://github.com/liferay/liferay-portal/commit/3bf860c27150cf5ef3630c456383e14fe3c1b48e,LPD-26917 Add singleton to portal-workflow-kaleo-api module  # breaking  ## What modules/apps/portal-workflow/portal-workflow-kaleo-api/src/main/java/com/liferay/portal/workflow/kaleo/util/comparator/KaleoDefinitionVersionModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/portal-workflow/portal-workflow-kaleo-api/src/main/java/com/liferay/portal/workflow/kaleo/util/comparator/KaleoDefinitionVersionActiveComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/portal-workflow/portal-workflow-kaleo-api/src/main/java/com/liferay/portal/workflow/kaleo/util/comparator/KaleoDefinitionVersionIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/portal-workflow/portal-workflow-kaleo-api/src/main/java/com/liferay/portal/workflow/kaleo/util/comparator/KaleoDefinitionVersionNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----  # breaking  ## What modules/apps/portal-workflow/portal-workflow-kaleo-api/src/main/java/com/liferay/portal/workflow/kaleo/util/comparator/KaleoDefinitionVersionVersionComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the queries that uses this comparator  ----
liferay,liferay-portal,3da7367995a82314176b8705fd12d062cbcf62d9,https://github.com/liferay/liferay-portal/commit/3da7367995a82314176b8705fd12d062cbcf62d9,LPD-26703 Apply singleton into module layout-page-template-api  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionLayoutPageTemplateEntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionLayoutPageTemplateEntryModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionLayoutPageTemplateEntryNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateCollectionNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateEntryCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateEntryModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-page-template-api/src/main/java/com/liferay/layout/page/template/util/comparator/LayoutPageTemplateEntryNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,a209e133ce1979d483515097bd00f7c9b0cc5005,https://github.com/liferay/liferay-portal/commit/a209e133ce1979d483515097bd00f7c9b0cc5005,LPD-26703 Add singleton into module layout-api  # breaking  ## What modules/apps/layout/layout-api/src/main/java/com/liferay/layout/util/comparator/LayoutClassedModelUsageModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-api/src/main/java/com/liferay/layout/util/comparator/LayoutCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-api/src/main/java/com/liferay/layout/util/comparator/LayoutModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/layout/layout-api/src/main/java/com/liferay/layout/util/comparator/LayoutRelevanceComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,2458e70fe2d5b9e5d18d8a9c94234c55afe3a13d,https://github.com/liferay/liferay-portal/commit/2458e70fe2d5b9e5d18d8a9c94234c55afe3a13d,LPD-26703 Add singleton into module message-boards-api  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MessageCreateDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MessageModifiedDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MessageSubjectComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MessageURLSubjectComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/ThreadCreateDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/CategoryModifiedDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/CategoryTitleComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MBObjectsComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/ThreadModifiedDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/ThreadTitleComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MBObjectsModifiedDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/MBObjectsTitleComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----  # breaking  ## What modules/apps/message-boards/message-boards-api/src/main/java/com/liferay/message/boards/util/comparator/ThreadLastPostDateComparator.java  add singleton to class  ## Why  improve performance on comparator classes  ----
liferay,liferay-portal,098dbee4f21f1961cdb9a00d680820a3a55f5fb1,https://github.com/liferay/liferay-portal/commit/098dbee4f21f1961cdb9a00d680820a3a55f5fb1,LPD-26438 Add singleton to module journal-api  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleDisplayDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleIDComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleReviewDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleTitleComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleVersionComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/ArticleResourcePKComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FeedIDComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FeedNameComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderArticleArticleIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderArticleCreateDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderArticleDisplayDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderArticleModifiedDateComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderArticleTitleComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----  # breaking  ## What modules/apps/journal/journal-api/src/main/java/com/liferay/journal/util/comparator/FolderIdComparator.java  the class is now a singleton  ## Why  this change was made to improve the performance of the querys that uses this comparator  ----
liferay,liferay-portal,c9e093cee980690947fdc1fbbd47afbeb3537662,https://github.com/liferay/liferay-portal/commit/c9e093cee980690947fdc1fbbd47afbeb3537662,LPD-30941 Improve performance
liferay,liferay-portal,c7b603d6234c6416bef9e507bcf8d3f1e7771287,https://github.com/liferay/liferay-portal/commit/c7b603d6234c6416bef9e507bcf8d3f1e7771287,LPD-31137 Improve performance by avoiding the need to parse non java files
liferay,liferay-portal,cdb615ac7e9d636ccdb4979aae7d0028d8bb7e2f,https://github.com/liferay/liferay-portal/commit/cdb615ac7e9d636ccdb4979aae7d0028d8bb7e2f,LPD-31141 Add the groupId improves current performance with millions of files
liferay,liferay-portal,b74c4e909c1a4af89d605053d78d17085dda35c0,https://github.com/liferay/liferay-portal/commit/b74c4e909c1a4af89d605053d78d17085dda35c0,LPD-25552 Make use all the fetch for company type will be directed to fetchCompanyPortalPreferences. This is going to make sure all the fetches for company type will return consistent result when there are multiple results  meanwhile this can improve performance see LPS-196350(2cd9801d2a243ecbc5c1025b614c9300ce53627d)
liferay,liferay-portal,596b4b011f1b774998998ac15c4590ac114c7d87,https://github.com/liferay/liferay-portal/commit/596b4b011f1b774998998ac15c4590ac114c7d87,LPD-30519 Articles may have empty/null content  It is possible to create a journal article with empty content. The main use case is when defining default values for a structure; that is implemented by creating a hidden journal article containing those default values. In this case  it is perfectly sensible for the content field to be empty/null.
liferay,liferay-portal,315aa6bd1c09bdbafb6204eccf36c1229f65d488,https://github.com/liferay/liferay-portal/commit/315aa6bd1c09bdbafb6204eccf36c1229f65d488,LPD-2612 Users who have permission to edit Documents  Blogs and Web Contents can access Content Performance
liferay,liferay-portal,96b1d9e43e16f453f54da0114bf6731d822b4c3d,https://github.com/liferay/liferay-portal/commit/96b1d9e43e16f453f54da0114bf6731d822b4c3d,LPD-29508 Replace getLayoutsCount with fetchFirstLayout to improve performance while querying database
liferay,liferay-portal,a7c7d36941ace52af12a75349acd873120d96b47,https://github.com/liferay/liferay-portal/commit/a7c7d36941ace52af12a75349acd873120d96b47,LPD-29128 Move upgrade process to portal-impl  since otherwise other upgrade processes fail when trying to execute dynamic queries.  Example of failure trace:  [java] 2024-06-21 22:51:14.725 ERROR [main][SqlExceptionHelper:142] Unknown column 'this_.externalReferenceCode' in 'field list' [java] 2024-06-21 22:51:14.726 ERROR [main][BasePersistenceImpl:629] Caught unexpected exception [java] org.hibernate.exception.SQLGrammarException: could not extract ResultSet [java] 	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:63) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:37) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:113) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:99) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.engine.jdbc.internal.ResultSetReturnImpl.extract(ResultSetReturnImpl.java:67) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.loader.Loader.getResultSet(Loader.java:2322) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.loader.Loader.executeQueryStatement(Loader.java:2075) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.loader.Loader.executeQueryStatement(Loader.java:2037) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.loader.Loader.doQuery(Loader.java:956) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.loader.Loader.doQueryAndInitializeNonLazyCollections(Loader.java:357) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.loader.Loader.doList(Loader.java:2868) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.loader.Loader.doList(Loader.java:2850) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.loader.Loader.listIgnoreQueryCache(Loader.java:2682) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.loader.Loader.list(Loader.java:2677) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.loader.criteria.CriteriaLoader.list(CriteriaLoader.java:109) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.internal.SessionImpl.list(SessionImpl.java:1922) ~[hibernate-core.jar:5.6.7.Final] [java] 	at org.hibernate.internal.CriteriaImpl.list(CriteriaImpl.java:370) ~[hibernate-core.jar:5.6.7.Final] [java] 	at com.liferay.portal.dao.orm.hibernate.DynamicQueryImpl.list(DynamicQueryImpl.java:131) ~[portal-impl.jar:?] [java] 	at com.liferay.portal.dao.orm.hibernate.DynamicQueryImpl.list(DynamicQueryImpl.java:117) ~[portal-impl.jar:?] [java] 	at com.liferay.portal.kernel.service.persistence.impl.BasePersistenceImpl.findWithDynamicQuery(BasePersistenceImpl.java:516) ~[portal-kernel.jar:?] [java] 	at com.liferay.portal.service.base.LayoutLocalServiceBaseImpl.dynamicQuery(LayoutLocalServiceBaseImpl.java:175) ~[portal-impl.jar:?] [java] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_381] [java] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_381] [java] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_381] [java] 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_381] [java] 	at com.liferay.staging.internal.service.LayoutLocalServiceStagingAdvice$LayoutLocalServiceStagingInvocationHandler._invoke(LayoutLocalServiceStagingAdvice.java:870) ~[?:?] [java] 	at com.liferay.staging.internal.service.LayoutLocalServiceStagingAdvice$LayoutLocalServiceStagingInvocationHandler.invoke(LayoutLocalServiceStagingAdvice.java:709) ~[?:?] [java] 	at com.sun.proxy.$Proxy371.dynamicQuery(Unknown Source) ~[?:?] [java] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_381] [java] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_381] [java] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_381] [java] 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_381] [java] 	at com.liferay.portal.spring.aop.AopMethodInvocationImpl.proceed(AopMethodInvocationImpl.java:41) ~[portal-impl.jar:?] [java] 	at com.liferay.portal.spring.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:60) ~[portal-impl.jar:?] [java] 	at com.liferay.portal.spring.aop.AopMethodInvocationImpl.proceed(AopMethodInvocationImpl.java:48) ~[portal-impl.jar:?] [java] 	at com.liferay.portal.spring.aop.AopInvocationHandler.invoke(AopInvocationHandler.java:40) ~[portal-impl.jar:?] [java] 	at com.sun.proxy.$Proxy129.dynamicQuery(Unknown Source) ~[?:?] [java] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_381] [java] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_381] [java] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_381] [java] 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_381] [java] 	at com.liferay.portal.kernel.dao.orm.DefaultActionableDynamicQuery.executeDynamicQuery(DefaultActionableDynamicQuery.java:345) ~[portal-kernel.jar:?] [java] 	at com.liferay.portal.kernel.dao.orm.DefaultActionableDynamicQuery.lambda$doPerformActions$1(DefaultActionableDynamicQuery.java:262) ~[portal-kernel.jar:?] [java] 	at com.liferay.portal.kernel.dao.orm.DefaultActionableDynamicQuery.doPerformActions(DefaultActionableDynamicQuery.java:322) ~[portal-kernel.jar:?] [java] 	at com.liferay.portal.kernel.dao.orm.DefaultActionableDynamicQuery.performActions(DefaultActionableDynamicQuery.java:79) ~[portal-kernel.jar:?] [java] 	at com.liferay.layout.internal.upgrade.v1_2_1.LayoutAssetUpgradeProcess.doUpgrade(LayoutAssetUpgradeProcess.java:60) ~[?:?] [java] 	at com.liferay.portal.kernel.upgrade.UpgradeProcess.lambda$upgrade$0(UpgradeProcess.java:122) ~[portal-kernel.jar:?] [java] 	at com.liferay.portal.db.partition.util.DBPartitionUtil.forEachCompanyId(DBPartitionUtil.java:123) ~[portal-impl.jar:?] [java] 	at com.liferay.portal.dao.db.BaseDB.process(BaseDB.java:469) ~[portal-impl.jar:?] [java] 	at com.liferay.portal.kernel.dao.db.BaseDBProcess.process(BaseDBProcess.java:377) ~[portal-kernel.jar:?] [java] 	at com.liferay.portal.kernel.upgrade.UpgradeProcess.upgrade(UpgradeProcess.java:107) ~[portal-kernel.jar:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor._executeUpgradeInfos(UpgradeExecutor.java:202) ~[?:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor.executeUpgradeInfos(UpgradeExecutor.java:124) ~[?:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor.execute(UpgradeExecutor.java:85) ~[?:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor$UpgradeStepRegistratorServiceTrackerCustomizer.addingService(UpgradeExecutor.java:330) ~[?:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor$UpgradeStepRegistratorServiceTrackerCustomizer.addingService(UpgradeExecutor.java:290) ~[?:?] [java] 	at com.liferay.osgi.service.tracker.collections.internal.map.ServiceTrackerMapImpl$DefaultEmitter.emit(ServiceTrackerMapImpl.java:200) ~[com.liferay.osgi.service.tracker.collections.jar:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor.lambda$activate$0(UpgradeExecutor.java:179) ~[?:?] [java] 	at com.liferay.osgi.service.tracker.collections.internal.map.ServiceTrackerMapImpl$ServiceReferenceServiceTrackerCustomizer.addingService(ServiceTrackerMapImpl.java:256) ~[com.liferay.osgi.service.tracker.collections.jar:?] [java] 	at com.liferay.osgi.service.tracker.collections.internal.map.ServiceTrackerMapImpl$ServiceReferenceServiceTrackerCustomizer.addingService(ServiceTrackerMapImpl.java:244) ~[com.liferay.osgi.service.tracker.collections.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:943) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:1) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.trackAdding(AbstractTracked.java:256) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.track(AbstractTracked.java:229) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:903) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.doServiceChanged(FilteredServiceListener.java:114) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry._serviceChanged(ServiceRegistry.java:987) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:969) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:853) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:126) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:227) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:461) ~[org.eclipse.osgi.jar:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:906) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:892) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.RegistrationManager.changeRegistration(RegistrationManager.java:128) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.registerService(AbstractComponentManager.java:962) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.activateInternal(AbstractComponentManager.java:732) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1053) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1007) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1216) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1137) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.trackAdding(ServiceTracker.java:944) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.track(ServiceTracker.java:880) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1168) ~[?:?] [java] 	at org.apache.felix.scr.impl.BundleComponentActivator$ListenerInfo.serviceChanged(BundleComponentActivator.java:124) ~[?:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.doServiceChanged(FilteredServiceListener.java:114) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry._serviceChanged(ServiceRegistry.java:987) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:969) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:853) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:126) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:227) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:461) ~[org.eclipse.osgi.jar:?] [java] 	at com.liferay.portal.aop.internal.AopServiceRegistrar.register(AopServiceRegistrar.java:78) ~[?:?] [java] 	at com.liferay.portal.aop.internal.AopServiceManager$AopServiceServiceTrackerCustomizer.lambda$addingService$0(AopServiceManager.java:275) ~[?:?] [java] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_381] [java] 	at com.liferay.portal.aop.internal.AopServiceManager$AopServiceServiceTrackerCustomizer.addingService(AopServiceManager.java:293) ~[?:?] [java] 	at com.liferay.portal.aop.internal.AopServiceManager$AopServiceServiceTrackerCustomizer.addingService(AopServiceManager.java:234) ~[?:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:943) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:1) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.trackAdding(AbstractTracked.java:256) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.track(AbstractTracked.java:229) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:903) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.doServiceChanged(FilteredServiceListener.java:114) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry._serviceChanged(ServiceRegistry.java:987) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:969) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:853) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:126) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:227) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:461) ~[org.eclipse.osgi.jar:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:906) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:892) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.RegistrationManager.changeRegistration(RegistrationManager.java:128) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.registerService(AbstractComponentManager.java:962) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.activateInternal(AbstractComponentManager.java:732) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1053) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1007) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1216) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1137) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.trackAdding(ServiceTracker.java:944) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.track(ServiceTracker.java:880) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1168) ~[?:?] [java] 	at org.apache.felix.scr.impl.BundleComponentActivator$ListenerInfo.serviceChanged(BundleComponentActivator.java:124) ~[?:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.doServiceChanged(FilteredServiceListener.java:114) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry._serviceChanged(ServiceRegistry.java:987) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:969) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:853) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:126) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:227) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:461) ~[org.eclipse.osgi.jar:?] [java] 	at com.liferay.portal.aop.internal.AopServiceRegistrar.register(AopServiceRegistrar.java:78) ~[?:?] [java] 	at com.liferay.portal.aop.internal.AopServiceManager$AopServiceServiceTrackerCustomizer.lambda$addingService$0(AopServiceManager.java:275) ~[?:?] [java] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_381] [java] 	at com.liferay.portal.aop.internal.AopServiceManager$AopServiceServiceTrackerCustomizer.addingService(AopServiceManager.java:293) ~[?:?] [java] 	at com.liferay.portal.aop.internal.AopServiceManager$AopServiceServiceTrackerCustomizer.addingService(AopServiceManager.java:234) ~[?:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:943) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:1) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.trackAdding(AbstractTracked.java:256) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.track(AbstractTracked.java:229) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:903) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.doServiceChanged(FilteredServiceListener.java:114) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry._serviceChanged(ServiceRegistry.java:987) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:969) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:853) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:126) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:227) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:461) ~[org.eclipse.osgi.jar:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:906) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:892) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.RegistrationManager.changeRegistration(RegistrationManager.java:128) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.registerService(AbstractComponentManager.java:962) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.activateInternal(AbstractComponentManager.java:732) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1053) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1007) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1216) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1137) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.trackAdding(ServiceTracker.java:944) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.track(ServiceTracker.java:880) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1168) ~[?:?] [java] 	at org.apache.felix.scr.impl.BundleComponentActivator$ListenerInfo.serviceChanged(BundleComponentActivator.java:124) ~[?:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.doServiceChanged(FilteredServiceListener.java:114) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry._serviceChanged(ServiceRegistry.java:987) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:969) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:853) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:126) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:227) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:461) ~[org.eclipse.osgi.jar:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:906) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:892) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.RegistrationManager.changeRegistration(RegistrationManager.java:128) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.registerService(AbstractComponentManager.java:962) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.activateInternal(AbstractComponentManager.java:732) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1053) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1007) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1216) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1137) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.trackAdding(ServiceTracker.java:944) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.track(ServiceTracker.java:880) ~[?:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1168) ~[?:?] [java] 	at org.apache.felix.scr.impl.BundleComponentActivator$ListenerInfo.serviceChanged(BundleComponentActivator.java:124) ~[?:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.doServiceChanged(FilteredServiceListener.java:114) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry._serviceChanged(ServiceRegistry.java:987) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:969) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:853) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:126) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:227) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:461) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:479) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:986) ~[org.eclipse.osgi.jar:?] [java] 	at com.liferay.portal.spring.extender.internal.configuration.ServiceConfigurationInitializer.lambda$new$0(ServiceConfigurationInitializer.java:58) ~[?:?] [java] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_381] [java] 	at com.liferay.portal.spring.extender.internal.configuration.ServiceConfigurationInitializer.start(ServiceConfigurationInitializer.java:94) ~[?:?] [java] 	at com.liferay.portal.kernel.module.util.ServiceLatch$CapturingServiceTrackerCustomizer.addingService(ServiceLatch.java:114) ~[portal-kernel.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:943) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:1) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.trackAdding(AbstractTracked.java:256) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.track(AbstractTracked.java:229) ~[org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:903) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.doServiceChanged(FilteredServiceListener.java:114) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry._serviceChanged(ServiceRegistry.java:987) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:969) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:853) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:126) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:227) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:461) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:479) ~[org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:986) ~[org.eclipse.osgi.jar:?] [java] 	at com.liferay.portal.upgrade.internal.release.ReleasePublisher.publish(ReleasePublisher.java:66) ~[?:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor.executeUpgradeInfos(UpgradeExecutor.java:130) ~[?:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor.execute(UpgradeExecutor.java:85) ~[?:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor$UpgradeStepRegistratorServiceTrackerCustomizer.addingService(UpgradeExecutor.java:330) ~[?:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor$UpgradeStepRegistratorServiceTrackerCustomizer.addingService(UpgradeExecutor.java:290) ~[?:?] [java] 	at com.liferay.osgi.service.tracker.collections.internal.map.ServiceTrackerMapImpl$DefaultEmitter.emit(ServiceTrackerMapImpl.java:200) ~[com.liferay.osgi.service.tracker.collections.jar:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor.lambda$activate$0(UpgradeExecutor.java:179) ~[?:?] [java] 	at com.liferay.osgi.service.tracker.collections.internal.map.ServiceTrackerMapImpl$ServiceReferenceServiceTrackerCustomizer.addingService(ServiceTrackerMapImpl.java:256) [com.liferay.osgi.service.tracker.collections.jar:?] [java] 	at com.liferay.osgi.service.tracker.collections.internal.map.ServiceTrackerMapImpl$ServiceReferenceServiceTrackerCustomizer.addingService(ServiceTrackerMapImpl.java:244) [com.liferay.osgi.service.tracker.collections.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:943) [org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:1) [org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.trackAdding(AbstractTracked.java:256) [org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.trackInitial(AbstractTracked.java:183) [org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker.open(ServiceTracker.java:321) [org.eclipse.osgi.jar:?] [java] 	at com.liferay.osgi.service.tracker.collections.internal.ServiceTrackerManager.open(ServiceTrackerManager.java:58) [com.liferay.osgi.service.tracker.collections.jar:?] [java] 	at com.liferay.osgi.service.tracker.collections.internal.map.ServiceTrackerMapImpl.<init>(ServiceTrackerMapImpl.java:65) [com.liferay.osgi.service.tracker.collections.jar:?] [java] 	at com.liferay.osgi.service.tracker.collections.map.ServiceTrackerMapFactory.openSingleValueMap(ServiceTrackerMapFactory.java:177) [com.liferay.osgi.service.tracker.collections.jar:?] [java] 	at com.liferay.portal.upgrade.internal.executor.UpgradeExecutor.activate(UpgradeExecutor.java:174) [bundleFile:?] [java] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_381] [java] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_381] [java] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_381] [java] 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_381] [java] 	at org.apache.felix.scr.impl.inject.methods.BaseMethod.invokeMethod(BaseMethod.java:228) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.inject.methods.BaseMethod.access$500(BaseMethod.java:41) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.inject.methods.BaseMethod$Resolved.invoke(BaseMethod.java:664) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.inject.methods.BaseMethod.invoke(BaseMethod.java:510) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.inject.methods.ActivateMethod.invoke(ActivateMethod.java:310) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.inject.methods.ActivateMethod.invoke(ActivateMethod.java:300) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.SingleComponentManager.createImplementationObject(SingleComponentManager.java:341) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.SingleComponentManager.createComponent(SingleComponentManager.java:114) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.SingleComponentManager.getService(SingleComponentManager.java:983) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.SingleComponentManager.getServiceInternal(SingleComponentManager.java:956) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.SingleComponentManager.getService(SingleComponentManager.java:901) [bundleFile:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceFactoryUse.factoryGetService(ServiceFactoryUse.java:210) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceFactoryUse.getService(ServiceFactoryUse.java:111) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceConsumer$2.getService(ServiceConsumer.java:45) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.getService(ServiceRegistrationImpl.java:520) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.getService(ServiceRegistry.java:463) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.getService(BundleContextImpl.java:616) [org.eclipse.osgi.jar:?] [java] 	at org.apache.felix.scr.impl.manager.SingleRefPair.getServiceObject(SingleRefPair.java:73) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.inject.BindParameters.getServiceObject(BindParameters.java:47) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.inject.field.FieldHandler$ReferenceMethodImpl.getServiceObject(FieldHandler.java:519) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager.getServiceObject(DependencyManager.java:2308) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.prebind(DependencyManager.java:1162) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager.prebind(DependencyManager.java:1576) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.collectDependencies(AbstractComponentManager.java:1037) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.SingleComponentManager.getServiceInternal(SingleComponentManager.java:936) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.SingleComponentManager.getService(SingleComponentManager.java:901) [bundleFile:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceFactoryUse.factoryGetService(ServiceFactoryUse.java:210) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceFactoryUse.getService(ServiceFactoryUse.java:111) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceConsumer$2.getService(ServiceConsumer.java:45) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.getService(ServiceRegistrationImpl.java:520) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.getService(ServiceRegistry.java:463) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.getService(BundleContextImpl.java:616) [org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker.addingService(ServiceTracker.java:416) [org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:943) [org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.customizerAdding(ServiceTracker.java:1) [org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.trackAdding(AbstractTracked.java:256) [org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.AbstractTracked.track(AbstractTracked.java:229) [org.eclipse.osgi.jar:?] [java] 	at org.osgi.util.tracker.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:903) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.doServiceChanged(FilteredServiceListener.java:114) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry._serviceChanged(ServiceRegistry.java:987) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:969) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:853) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:126) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:227) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:461) [org.eclipse.osgi.jar:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:906) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:892) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.RegistrationManager.changeRegistration(RegistrationManager.java:128) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.registerService(AbstractComponentManager.java:962) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.activateInternal(AbstractComponentManager.java:732) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1053) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1007) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1216) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1137) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.trackAdding(ServiceTracker.java:944) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.track(ServiceTracker.java:880) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1168) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.BundleComponentActivator$ListenerInfo.serviceChanged(BundleComponentActivator.java:124) [bundleFile:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.doServiceChanged(FilteredServiceListener.java:114) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry._serviceChanged(ServiceRegistry.java:987) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:969) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:853) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:126) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:227) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:461) [org.eclipse.osgi.jar:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:906) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.register(AbstractComponentManager.java:892) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.RegistrationManager.changeRegistration(RegistrationManager.java:128) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.registerService(AbstractComponentManager.java:962) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.AbstractComponentManager.activateInternal(AbstractComponentManager.java:732) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1053) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:1007) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1216) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1137) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.trackAdding(ServiceTracker.java:944) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.track(ServiceTracker.java:880) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1168) [bundleFile:?] [java] 	at org.apache.felix.scr.impl.BundleComponentActivator$ListenerInfo.serviceChanged(BundleComponentActivator.java:124) [bundleFile:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.doServiceChanged(FilteredServiceListener.java:114) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry._serviceChanged(ServiceRegistry.java:987) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:969) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:853) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:126) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:227) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:461) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:479) [org.eclipse.osgi.jar:?] [java] 	at org.eclipse.osgi.internal.framework.BundleContextImpl.registerService(BundleContextImpl.java:986) [org.eclipse.osgi.jar:?] [java] 	at com.liferay.portal.tools.DBUpgrader._registerModuleServiceLifecycle(DBUpgrader.java:412) [portal-impl.jar:?] [java] 	at com.liferay.portal.tools.DBUpgrader.upgradeModules(DBUpgrader.java:234) [portal-impl.jar:?] [java] 	at com.liferay.portal.tools.DBUpgrader.main(DBUpgrader.java:170) [portal-impl.jar:?] [java] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_381] [java] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_381] [java] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_381] [java] 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_381] [java] 	at com.liferay.portal.tools.db.upgrade.client.DBUpgraderLauncher.main(DBUpgraderLauncher.java:45) [com.liferay.portal.tools.db.upgrade.client.jar:?] [java] Caused by: java.sql.SQLSyntaxErrorException: Unknown column 'this_.externalReferenceCode' in 'field list' [java] 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:121) ~[mysql.jar:8.3.0] [java] 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[mysql.jar:8.3.0] [java] 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:912) ~[mysql.jar:8.3.0] [java] 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:968) ~[mysql.jar:8.3.0] [java] 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeQuery(ProxyPreparedStatement.java:52) ~[hikaricp.jar:?] [java] 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeQuery(HikariProxyPreparedStatement.java) ~[hikaricp.jar:?] [java] 	at org.hibernate.engine.jdbc.internal.ResultSetReturnImpl.extract(ResultSetReturnImpl.java:57) ~[hibernate-core.jar:5.6.7.Final] [java] 	... 318 more [java] 2024-06-21 22:51:14.744 INFO  [main][UpgradeProcess:137] Failed upgrade process com.liferay.layout.internal.upgrade.v1_2_1.LayoutAssetUpgradeProcess in 42 ms [java] 2024-06-21 22:51:14.748 ERROR [main][UpgradeExecutor:333] Failed upgrade process for module com.liferay.layout.service [java] com.liferay.portal.kernel.upgrade.UpgradeException: com.liferay.portal.kernel.exception.SystemException: org.hibernate.exception.SQLGrammarException: could not extract ResultSet
liferay,liferay-portal,165291f6ab165ec77e2e6c1421154f99dff80ff1,https://github.com/liferay/liferay-portal/commit/165291f6ab165ec77e2e6c1421154f99dff80ff1,LPD-23097 No need to perform any calculation among disabled classes if view count is already disabled.
liferay,liferay-portal,021c8baa1e5061751f4d31b9f9bbb982c1c515c9,https://github.com/liferay/liferay-portal/commit/021c8baa1e5061751f4d31b9f9bbb982c1c515c9,LPD-27556 Catch the exception when publishing or expiring journal articles in order to perform other publications or expirations
liferay,liferay-portal,e5602bda1abbb17eae4930977e0a7eabb4b25280,https://github.com/liferay/liferay-portal/commit/e5602bda1abbb17eae4930977e0a7eabb4b25280,Revert "LPD-2612 Users who have permission to edit Documents  Blogs and Web Contents can access Content Performance"  This reverts commit b34197bee1f56858d2809b5601fb5293af8da6ac.
liferay,liferay-portal,b34197bee1f56858d2809b5601fb5293af8da6ac,https://github.com/liferay/liferay-portal/commit/b34197bee1f56858d2809b5601fb5293af8da6ac,LPD-2612 Users who have permission to edit Documents  Blogs and Web Contents can access Content Performance
hapifhir,hapi-fhir,c97567c1672d29cfcf5516f8785e0dfc5a82d317,https://github.com/hapifhir/hapi-fhir/commit/c97567c1672d29cfcf5516f8785e0dfc5a82d317,Transaction processing improvements (#6874)  * Add junk ine  * Add extension for placeholder ID  * Cleanup  * Header  * Cleanup  * Spotless  * Better logging  * Fix transaction conditional URL prefetch on non-token params  * Drop a check  * Adjust query counts  * Improve performance  * Single token optimized  * Test fixes  * About to try to ensure we prefetch resource bodies for conditional create  * Test fixes  * Some test fixes  * Test fixes  * Test fixes  * Test fixes  * Work on tests  * Test fix  * Spotless  * Add docs  * Add test  * Add grouping  * Spotless  * Add size option to CMD  * Adjust consumer  * Cleanup  * Work on bulk import  * Spotless  * More work  * Add logging  * More diagnostics  * add logs  * Cleanup  * Fix premature finishing of bulk import command  * Add changelogs  * Test cleanup  * Test fixes  * Fix tests  * Test fix  * Spotless  * Cleanup  * Resolve conflicts  * Clean up  * Add to transaction response parser  * TransactionUtil parsing improvements  * Work on tests  * Test fix  * Test fixes  * Spotless  * Bug fix  * Build tweak  * Test cleanup  * Add some test logging  * Bump to trigger CI  * Try to address intermittent  * Address review comments  * Version bump  * License header
hapifhir,hapi-fhir,0cf76779b7fe456fa2f13777374df9ee52fe79be,https://github.com/hapifhir/hapi-fhir/commit/0cf76779b7fe456fa2f13777374df9ee52fe79be,Issue when performing SQL insert with string of max set size and special characters (#6856)  * failing test.  * - more failing tests - migration task - solution  * - adding integration tests suite for oracle; - adding changelog  * fixing comments.  * pre-code review submission modifications.  * spotless hapiness  * fixing test failures  * making oracle IT execution conditional for Mac users.  * making oracle IT execution conditional for Mac users.  ---------  Co-authored-by: peartree <etienne.poirier@smilecdr.com>
hapifhir,hapi-fhir,38b67cdc00527c632635bc0801b975c55d815d7e,https://github.com/hapifhir/hapi-fhir/commit/38b67cdc00527c632635bc0801b975c55d815d7e,Performance tweak to Xml serialization (#6888)  * Performance tweak to Xml serialization  * Add changelog  * Address review comment  * Spotless  * Changelog fix
hapifhir,hapi-fhir,210bb82618f3db24e0b441684ea2e290d6c877d0,https://github.com/hapifhir/hapi-fhir/commit/210bb82618f3db24e0b441684ea2e290d6c877d0,Clean up Bulk Import (#6840)  * Fix transaction conditional URL prefetch on non-token params  * Drop a check  * Adjust query counts  * Improve performance  * Single token optimized  * Test fixes  * About to try to ensure we prefetch resource bodies for conditional create  * Test fixes  * Some test fixes  * Test fixes  * Test fixes  * Test fixes  * Work on tests  * Test fix  * Spotless  * Add docs  * Add test  * Add grouping  * Spotless  * Add size option to CMD  * Adjust consumer  * Cleanup  * Work on bulk import  * Spotless  * More work  * Add logging  * More diagnostics  * add logs  * Cleanup  * Fix premature finishing of bulk import command  * Add changelogs  * Test cleanup  * Test fixes  * Fix tests  * Test fix  * Spotless  * Account for review comments  * Address review comments  * Address review comments  * Add license headers  * Cleanup  * Restore CLI logging  * Test fix  * Add default method
hapifhir,hapi-fhir,eccedb53091cf3df82342fbe241407e4eb3201f6,https://github.com/hapifhir/hapi-fhir/commit/eccedb53091cf3df82342fbe241407e4eb3201f6,Fix transaction conditional URL prefetch on non-token params (#6818)  * Fix transaction conditional URL prefetch on non-token params  * Drop a check  * Adjust query counts  * Improve performance  * Single token optimized  * Test fixes  * About to try to ensure we prefetch resource bodies for conditional create  * Test fixes  * Some test fixes  * Test fixes  * Test fixes  * Test fixes  * Work on tests  * Test fix  * Spotless  * Add docs  * Address review comments
hapifhir,hapi-fhir,43ce89cda614a682200e5c5c48b39b7dbf717385,https://github.com/hapifhir/hapi-fhir/commit/43ce89cda614a682200e5c5c48b39b7dbf717385,Introduce hapi-fhir-client-apache-http5 module for Apache HttpClient 5 support (#6520)  * feat: Introduce hapi-fhir-client-apache-http5 module for Apache HttpClient 5 support  - Added a new module `hapi-fhir-client-apache-http5` to provide HAPI FHIR Client functionality using Apache HttpClient 5. - Supports gradual migration from HttpClient 4 to HttpClient 5. - Aligns with Spring Boot 3.0's adoption of HttpClient 5  enabling consistent HTTP client configuration for users of both libraries.  Key Changes: - Integrated Apache HttpClient 5 for modern  high-performance HTTP requests. - Ensured compatibility with existing `hapi-fhir-client` and `hapi-fhir-client-okhttp` modules. - Added basic tests to validate functionality and coexistence of HttpClient 4 and 5.  Impact: - Non-breaking change; the new module can be adopted independently. - Facilitates eventual migration of HAPI FHIR to HttpClient 5 across the codebase."  * Add new error codes to the apache-httpclient5 client module.
hapifhir,hapi-fhir,c9c8371a98663ab1f011bd454a214317856c598e,https://github.com/hapifhir/hapi-fhir/commit/c9c8371a98663ab1f011bd454a214317856c598e,6478 conditional updates should batch resourcesearchurl deletes (#6479)  * minor perf fix for transaction bundles  * spotless  * removing uneeded classes  * review fixes - adding jira to changelog  * code review points  * supporting single transaction processing  ---------  Co-authored-by: leif stawnyczy <leifstawnyczy@leifs-mbp.home>
hapifhir,hapi-fhir,e43c140a24eba4928117db2e2bde3907010cf42f,https://github.com/hapifhir/hapi-fhir/commit/e43c140a24eba4928117db2e2bde3907010cf42f,Refactor BaseHapiFhirDao.getOrCreateTag method to run in a separate thread for XA transaction compatibility (#6224)  * Fix typo in docs.  * Refactor the use of ConcurrentTaskScheduler to use the non-deprecated constructor.  * Refactor getOrCreateTag method to prevent transaction suspension for XA transaction compatibility  The getOrCreateTag method previously used a propagation behavior that caused issues with XA transactions when using the PostgreSQL JDBC driver. The PGXAConnection does not support transaction suspend/resume  which made it incompatible with the existing propagation strategy 'PROPAGATION_REQUIRES_NEW'.  This refactor changes the getOrCreateTag logic to perform a lookup/write in a new transaction as before  but running in a separate thread  such that the main transaction is not suspended. The result is retrieved through a future.  This change aims to improve compatibility and prevent transaction-related issues when using HAPI-FHIR with XA transactions and PostgreSQL.  Closes #3412  * Refactor tag creation logic and handle concurrent access:  - Simplified tag creation by removing unnecessary transaction complexity  since we allow duplicate tags in hfj_tag_def from #4813 - Removed redundant retry logic based on updated DB constraints
hapifhir,hapi-fhir,eadd8c6f0d3eafb46da58f7ce9ed8411b707434f,https://github.com/hapifhir/hapi-fhir/commit/eadd8c6f0d3eafb46da58f7ce9ed8411b707434f,improved performance of date searching (#6353)
hapifhir,hapi-fhir,9a73079c33133e50dc4fd4f77cd5207ab95bc2c1,https://github.com/hapifhir/hapi-fhir/commit/9a73079c33133e50dc4fd4f77cd5207ab95bc2c1,Improving performance  using caching when testing for primitives (#6252) (#6253)  * Improving performance  using caching when testing for primitives (#6252)  Caching primitive type names for faster lookup if a type is primitive.  * Credit for #6253  ---------  Co-authored-by: James Agnew <jamesagnew@gmail.com>
hapifhir,hapi-fhir,d2923da62bc7b414bcdcb36a882245994fafa6bb,https://github.com/hapifhir/hapi-fhir/commit/d2923da62bc7b414bcdcb36a882245994fafa6bb,Search Param Matcher Performance Improvement (#5999)  * filter function added  * spotless
hapifhir,hapi-fhir,024d84869046ac79e1c5c445b0c2973a95b35eb5,https://github.com/hapifhir/hapi-fhir/commit/024d84869046ac79e1c5c445b0c2973a95b35eb5,Skip MDM candidate search if parameters are missing (#5658)  * Issue #5657  Skip an MDM candidate search if any search parameter is missing (as per documentation  and to avoid non-performant searches)  rather than only when all of them are missing.  * Address the unit test failure.  * Add credit for #5658  ---------  Co-authored-by: James Agnew <jamesagnew@gmail.com>
Netflix,EVCache,74cceb812a1570f8c8648d90a1c5a7425c1cf200,https://github.com/Netflix/EVCache/commit/74cceb812a1570f8c8648d90a1c5a7425c1cf200,Merge pull request #162 from jasonk000/jkoch/get-decompress-async  perf: allow concurrent decompress away from network loop
Netflix,EVCache,60e2a773198e75e2605eeda61a0a9944386ec316,https://github.com/Netflix/EVCache/commit/60e2a773198e75e2605eeda61a0a9944386ec316,perf: allow concurrent decompress away from network loop  We want to ensure the transcode passes everything that is expected to be run asynchronously over to the decode loop. In general  memcached calls us back with gotData  then receivedStatus  then complete. We use gotData to launch the work onto the transcode threadpool and return control to memcached  which would immediately then call receivedStatus.  Previously  receivedStatus and complete were set up to interact and set a value on the underlying future but only by synchronously blocking for the transcode future. Given this callback is happening nearly immediately after the gotData callback  we were firing the transcode and nearly always performing a blocking get  which triggers a synchronous decompression on the network IO loop. This is of course very detrimental to evcache driver performance  since the driver cannot even accept new requests to issue to them to memcached backends  and must wait until decompression completes.  In this fix  we rearrange things a little to ensure that if the async decode is requested  that we push the completion status updates to happen only after the async decode completes. This is a little ugly because of the current arrangement of the memcached decoder. A future change might be to overhaul this integration and pull it out of the memcached transcode framework and use something a bit more friendly.  Also  add a property to control sync decode threading behavior  default to using the existing behaviors for now.
Netflix,EVCache,0cb4ea98c96011f3c646430807180150bfdf9045,https://github.com/Netflix/EVCache/commit/0cb4ea98c96011f3c646430807180150bfdf9045,Merge pull request #161 from jasonk000/jkoch/track-bulk-get-futures  perf: track bulk get operation completions explicitly
Netflix,EVCache,4cb82c272b3e557c5e4bb4bcc98cb6e66420cb4e,https://github.com/Netflix/EVCache/commit/4cb82c272b3e557c5e4bb4bcc98cb6e66420cb4e,Merge pull request #157 from jasonk000/jkoch/fast-key-validator  perf: Avoid charset lookup in key validation
Netflix,EVCache,962b76b867bde2e088ee758655d1a345f381ab29,https://github.com/Netflix/EVCache/commit/962b76b867bde2e088ee758655d1a345f381ab29,Merge pull request #158 from jasonk000/jkoch/add-operations-single-wakeup  perf: batch selector wakeup call when adding bulk requests
Netflix,EVCache,be626edd3167ed0d84d099d0e0053b7cf6b0ac5a,https://github.com/Netflix/EVCache/commit/be626edd3167ed0d84d099d0e0053b7cf6b0ac5a,Merge pull request #159 from jasonk000/jkoch/bulk-lookup-single-getprimary  perf: perform the getPrimary lookup only once during getBulk
Netflix,EVCache,5dd9ba42353b666ff85c5c02056b5a03b5822a57,https://github.com/Netflix/EVCache/commit/5dd9ba42353b666ff85c5c02056b5a03b5822a57,Merge pull request #156 from jasonk000/jkoch/less-locking-1  perf: remove unnecessary blocking in EVCacheOperationFuture
Netflix,EVCache,e4a2a9bdb37c59bc662c357bd6a323d2718e092c,https://github.com/Netflix/EVCache/commit/e4a2a9bdb37c59bc662c357bd6a323d2718e092c,perf: track bulk get operation completions explicitly  This commit makes it so that the operation callbacks are used to collect all the locked state that we might need to access  which makes the code after latch-release run without contending memcached to complete.  Before this commit  callbacks on the bulk get operation accumulate the state and when all operations have completed  we release the latch and then the calller calculates the final state. Unfortunately looking at the state  the caller must take operation locks  which are potentially still hold by memcache driver since it has only momentarily before released the latch.  The Memcache callbacks are made whilst holding the lock on the Operation objects  which means they are a great time to get locked state such as cancellation state. This patch makes the change to collect state during callbacks. The callback future interaction is a little clunky  but this avoids a major rewrite of the implementations.
Netflix,EVCache,6a63f2912d959452dab9c8114abd275f84c6843a,https://github.com/Netflix/EVCache/commit/6a63f2912d959452dab9c8114abd275f84c6843a,perf: Optimize the MD5 hashring algorithm to avoid Charset lookup
Netflix,EVCache,d29bac959ab946eaa998f1c161e0f290db67247d,https://github.com/Netflix/EVCache/commit/d29bac959ab946eaa998f1c161e0f290db67247d,perf: Add a BFS-arranged (aka Eytzinger) layout for node lookup
Netflix,EVCache,de9041dac0856ab89332f012b04737f017de743e,https://github.com/Netflix/EVCache/commit/de9041dac0856ab89332f012b04737f017de743e,perf: perform the getPrimary lookup only once during the bulk processing path  We can avoid doing the (potentially) expensive getPrimary call on the bulk get path with a little refactor. This retains the same functionality but pushes the validation logic and error emit into a callback  which allows metrics to work as-is.
Netflix,EVCache,fe3f5359f7d4012a4b9c1de8a08fdc9fa25e223b,https://github.com/Netflix/EVCache/commit/fe3f5359f7d4012a4b9c1de8a08fdc9fa25e223b,perf: batch selector wakeup call when adding bulk requests
Netflix,EVCache,168aa2b26b7d31f6990d45e22d99e364195722f0,https://github.com/Netflix/EVCache/commit/168aa2b26b7d31f6990d45e22d99e364195722f0,perf: Avoid charset lookup in key validation  The upstream implementation needs to do a "UTF-8" string lookup to Charset for every key validation. We can avoid this completely in our hot path by using a reference to StandardCharsets.
Netflix,EVCache,ca70c1d1458c8fed527830b7b13fbbfcd91c940a,https://github.com/Netflix/EVCache/commit/ca70c1d1458c8fed527830b7b13fbbfcd91c940a,perf: remove unnecessary blocking in EVCacheOperationFuture  Currently the EVCacheOperationFuture does an isCancelled check before emitting a metric  however this metric is now dead code. But  the isCancelled check is still a synchronized method which results in occasional blocking against memcache event loop.
Netflix,EVCache,9ddd6a9141689272f0ddb5ec181721c37008f99a,https://github.com/Netflix/EVCache/commit/9ddd6a9141689272f0ddb5ec181721c37008f99a,Merge pull request #151 from Netflix/jkoch/skip-cancel  perf: skip future cancellation when not needed
Netflix,EVCache,b200a5134ab546ce291edb9b96cc0df567e2c329,https://github.com/Netflix/EVCache/commit/b200a5134ab546ce291edb9b96cc0df567e2c329,perf: skip future cancellation when not needed  it implicitly throws an expensive exception
mybatis-flex,mybatis-flex,475c90c13b6edd8a777c41cefb1bcb176b06bd19,https://github.com/mybatis-flex/mybatis-flex/commit/475c90c13b6edd8a777c41cefb1bcb176b06bd19,refactor: optimize LambdaUtil performance
flyingsaucerproject,flyingsaucer,20e3f8757e5806474011d2fdf91d0476fd3f3045,https://github.com/flyingsaucerproject/flyingsaucer/commit/20e3f8757e5806474011d2fdf91d0476fd3f3045,#340 fix warning "single character strings being used as an argument in indexOf()calls"  A quick-fix is suggested to replace such string literals with equivalent character literals  gaining some performance enhancement.
flyingsaucerproject,flyingsaucer,2c4f0bd82dca0f2c1fb4a99a4f1bcba9a88f3dff,https://github.com/flyingsaucerproject/flyingsaucer/commit/2c4f0bd82dca0f2c1fb4a99a4f1bcba9a88f3dff,#340 cache attribute values of xml elements  My profiler shows that `nsh.getClass((Element) e)` consumes quite a remarkable percentage of total CPU time. So I hope caching these value could improve performance (though  my profiler doesn't show it :) ).
magefree,mage,99ca1e60296dcdbf44623c66dcad952939322db4,https://github.com/magefree/mage/commit/99ca1e60296dcdbf44623c66dcad952939322db4,AI: improved performance and server stability in games with "choose name" effects (related to #11285)
magefree,mage,f17cbbe72b7fff0f9ec3bb0a90698d7cda3d5e28,https://github.com/magefree/mage/commit/f17cbbe72b7fff0f9ec3bb0a90698d7cda3d5e28,AI: improved performance and fixed crashes on use cases with too much target options like "deals 5 damage divided as you choose" (related to #11285): * added DebugUtil.AI_ENABLE_DEBUG_MODE for better IDE's debugging AI code; * it's a target amount optimizations; * it's use a grouping of possible targets due same static and dynamic stats (name  abilities  rules  damage  etc); * instead of going through all possible combinations  AI uses only meaningful targets from particular groups;
magefree,mage,7d229e511cfcd76fb5f2938bc5e72ac28abd8c74,https://github.com/magefree/mage/commit/7d229e511cfcd76fb5f2938bc5e72ac28abd8c74,tests: added AI performance tests to reproduce bad use cases with too much possible targets
magefree,mage,b62ac065c1ed862cfcacf79568c04517c5458539,https://github.com/magefree/mage/commit/b62ac065c1ed862cfcacf79568c04517c5458539,AI: improved performance in tournament games (now computer will play AI vs AI games at the same time);
magefree,mage,faeca638de70d80ea46db2d9453910d2ba3dc0d3,https://github.com/magefree/mage/commit/faeca638de70d80ea46db2d9453910d2ba3dc0d3,refactor: clean up some superfluous null filters in optionals
magefree,mage,4a25c6e82661a4ee76014b4a99f5882870a91678,https://github.com/magefree/mage/commit/4a25c6e82661a4ee76014b4a99f5882870a91678,[FDN] Implement Perforating Artist
magefree,mage,587a68a837b1387c9c3bdc92eafb4b21c5db80bd,https://github.com/magefree/mage/commit/587a68a837b1387c9c3bdc92eafb4b21c5db80bd,refactor: simplify BeginningOfUpkeepTriggeredAbility (#13046)  * remove superfluous zone.battlefield  * remove redundant TargetController.YOU  * remove unneeded param  * simpler constructor with default optional false  * move to inheritance structure  remove setTargetPointer  * adjust  * align trigger phrase text  * add comments  fix param  * fix params and text  * fix params to fix text  * simplify  * package reorg
magefree,mage,aa7a610db2af3977578eaa6df270f8289981a752,https://github.com/magefree/mage/commit/aa7a610db2af3977578eaa6df270f8289981a752,refactor: clean up triggered abilities from graveyard (#13044)  * some improvements to counter removing triggers  * fix Zone.GRAVEYARD text/conditions for beginning of upkeep triggers  * remaining refactors  * add condition text  * text corrections  * remove remaining generateZoneString  * remove superfluous hardcoded rules text param
magefree,mage,83823acec78501e907dc2f5e0a38d9d18914a832,https://github.com/magefree/mage/commit/83823acec78501e907dc2f5e0a38d9d18914a832,GUI  performance: fixed memory/resources leaks on some components rendering
magefree,mage,1f3fad65944f76c81f31cd7dc21381313f35a623,https://github.com/magefree/mage/commit/1f3fad65944f76c81f31cd7dc21381313f35a623,GUI  preferences: reworked size settings: - added size settings for player's panel size (closes #12455  closes #12451  closes #5605); - size settings can be edit by slider or by text edit; - size settings for fonts has preview button with real text sample; - improved some tabs and hints for better UX; - improved GUI rendering performance;
magefree,mage,a5488228b85af67338e21a88f1b0e9afb22890a2,https://github.com/magefree/mage/commit/a5488228b85af67338e21a88f1b0e9afb22890a2,fix usable zone logic for abilities that function from other zones (#12446)  * remove superfluous constructor params  * fix Syrix  Carrier of the Flame  * standardize Zone = Battlefield  * rename class  * remove redundant class  * add docs  * adjustment
magefree,mage,7a76a3b005e9b4786d7869edb79d9b1f7f4a0886,https://github.com/magefree/mage/commit/7a76a3b005e9b4786d7869edb79d9b1f7f4a0886,remove superfluous usage of isPhasedOutIndirectly
locationtech,jts,ac7a165592ee7c31037e666e50b765f861163aff,https://github.com/locationtech/jts/commit/ac7a165592ee7c31037e666e50b765f861163aff,Improve `LineStringSnapper` performance by using squared distance (#1111)
naver,ngrinder,7418dc9b75057e4085bf52c349d2542702f75ee1,https://github.com/naver/ngrinder/commit/7418dc9b75057e4085bf52c349d2542702f75ee1,Fix Authentication object not found in perftest url (#1024)
MewX,light-novel-library_Wenku8_Android,bd8ec05efce2d5c132034416b52d6767e13a8d2f,https://github.com/MewX/light-novel-library_Wenku8_Android/commit/bd8ec05efce2d5c132034416b52d6767e13a8d2f,Adding default constructor for `NovelItemListFragment`.  Hopefully it'll fix the following issue:  ``` Fatal Exception: java.lang.RuntimeException: Unable to start activity ComponentInfo{org.mewx.wenku8/org.mewx.wenku8.activity.MainActivity}: androidx.fragment.app.e: Unable to instantiate fragment p2.e: could not find Fragment constructor at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:4093) at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:4243) at android.app.ActivityThread.handleRelaunchActivityInner(ActivityThread.java:6422) at android.app.ActivityThread.handleRelaunchActivity(ActivityThread.java:6281) at android.app.servertransaction.ActivityRelaunchItem.execute(ActivityRelaunchItem.java:76) at android.app.servertransaction.ActivityTransactionItem.execute(ActivityTransactionItem.java:45) at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:144) at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:101) at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2705) at android.os.Handler.dispatchMessage(Handler.java:106) at android.os.Looper.loopOnce(Looper.java:255) at android.os.Looper.loop(Looper.java:364) at android.app.ActivityThread.main(ActivityThread.java:8938) at java.lang.reflect.Method.invoke(Method.java) at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:572) at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1053) ```
MewX,light-novel-library_Wenku8_Android,872648254b666b396f60855c442b74c2d608e7b8,https://github.com/MewX/light-novel-library_Wenku8_Android/commit/872648254b666b396f60855c442b74c2d608e7b8,Fix several bugs  especially an IndexOutOfBoundsException issue.  The root cause was the length of `listNovelItemInfo` was not always matching the length of `listNovelItemAid`.  Full stack trace:  ``` Fatal Exception: java.lang.IndexOutOfBoundsException: Index 7 out of bounds for length 7 at jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64) at jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70) at jdk.internal.util.Preconditions.checkIndex(Preconditions.java:266) at java.util.Objects.checkIndex(Objects.java:359) at java.util.ArrayList.get(ArrayList.java:434) at org.mewx.wenku8.fragment.FavFragment$1$2.onPositive(FavFragment.java:50) at com.afollestad.materialdialogs.MaterialDialog.onClick(MaterialDialog.java:69) at android.view.View.performClick(View.java:8043) at android.widget.TextView.performClick(TextView.java:17816) at android.view.View.performClickInternal(View.java:8020) at android.view.View.-$$Nest$mperformClickInternal() at android.view.View$PerformClick.run(View.java:31850) at android.os.Handler.handleCallback(Handler.java:958) at android.os.Handler.dispatchMessage(Handler.java:99) at android.os.Looper.loopOnce(Looper.java:230) at android.os.Looper.loop(Looper.java:319) at android.app.ActivityThread.main(ActivityThread.java:8893) at java.lang.reflect.Method.invoke(Method.java) at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:608) at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1103) ```
MewX,light-novel-library_Wenku8_Android,00a102d8bcb9521ec3e153dc707f1e83da1cf7ed,https://github.com/MewX/light-novel-library_Wenku8_Android/commit/00a102d8bcb9521ec3e153dc707f1e83da1cf7ed,Tried fixing an IndexOutOfBoundsException issue in novel review list activity.  Full stack trace (although I haven't fully figured out why):  ``` Fatal Exception: java.lang.IndexOutOfBoundsException: Index -1 out of bounds for length 20 at jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64) at jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70) at jdk.internal.util.Preconditions.checkIndex(Preconditions.java:266) at java.util.Objects.checkIndex(Objects.java:359) at java.util.ArrayList.get(ArrayList.java:434) at org.mewx.wenku8.activity.NovelReviewListActivity.onItemClick(NovelReviewListActivity.java:12) at org.mewx.wenku8.adapter.ReviewItemAdapter$ViewHolder.onClick(ReviewItemAdapter.java:9) at android.view.View.performClick(View.java:8047) at android.view.View.performClickInternal(View.java:8024) at android.view.View.-$$Nest$mperformClickInternal() at android.view.View$PerformClick.run(View.java:31890) at android.os.Handler.handleCallback(Handler.java:958) at android.os.Handler.dispatchMessage(Handler.java:99) at android.os.Looper.loopOnce(Looper.java:230) at android.os.Looper.loop(Looper.java:319) at android.app.ActivityThread.main(ActivityThread.java:8919) at java.lang.reflect.Method.invoke(Method.java) at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:578) at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1103) ```
jdbi,jdbi,d3c6c4da2d9ddd803b14b6416097a52fa05496d8,https://github.com/jdbi/jdbi/commit/d3c6c4da2d9ddd803b14b6416097a52fa05496d8,Use a plain java Function instead of RowMapperFieldPostProcessor
apache,tinkerpop,134215a8ad4a64bba90dca7c148e9443694e30ba,https://github.com/apache/tinkerpop/commit/134215a8ad4a64bba90dca7c148e9443694e30ba,Preferred use of the GremlinBaseVisitor for the Translatevisitor  Deleted superflous method implementations. CTR
apache,tinkerpop,55f7e180c9284d29c2b136667aea7e0298f16ff1,https://github.com/apache/tinkerpop/commit/55f7e180c9284d29c2b136667aea7e0298f16ff1,Re-enable GraphSON message serializer for debugging. (#2836)  The GraphSONv4 MessageSerializer doesn't support chunking so the HttpObjectAggregator is added back and the HttpGremlinResponseStreamDecoder is replaced with the original  non-streaming version. This is mainly for debugging and testing purposes so lowered performance is expected.
apache,poi,719e7154a19c00e1c2464e5f93a1567d8ca5ed72,https://github.com/apache/poi/commit/719e7154a19c00e1c2464e5f93a1567d8ca5ed72,Optimize generating numbers for bullets in Word  Using char[] instead of String improves performance of this operation considerably  especially in JDK 11+ where StringBuilder was switched to work on bytes instead of chars.  This is likely only relevant for very large documents  it was visible in a synthetic test-file from fuzzing.  git-svn-id: https://svn.apache.org/repos/asf/poi/trunk@1919239 13f79535-47bb-0310-9956-ffa450edef68
winder,Universal-G-Code-Sender,e47ebaa46e079c9d78026d92f27c0c671c61ba3c,https://github.com/winder/Universal-G-Code-Sender/commit/e47ebaa46e079c9d78026d92f27c0c671c61ba3c,Visualizer performance (#2615)  * Moved the isEnabled to super class and cache it to improve performance * Made grid and plane use a vertex buffer object renderer with shaders to improve performance * Added actions for toggling visualizer features
winder,Universal-G-Code-Sender,8a6343209a2ca8b791f95d68b1a9bd3537b38d52,https://github.com/winder/Universal-G-Code-Sender/commit/8a6343209a2ca8b791f95d68b1a9bd3537b38d52,Added a new visualizer panel for rendering using NEWT for better performance (#2602)
loks666,get_jobs,f8e083c81d9e26de1e47eb8a0f21082f76d08316,https://github.com/loks666/get_jobs/commit/f8e083c81d9e26de1e47eb8a0f21082f76d08316,perf: 加入岗位详情的关键词匹配（关键词匹配实现较粗糙，需要进一步优化）
loks666,get_jobs,c8f2feddd9f8b46a99b20879d3af483ed5291dd7,https://github.com/loks666/get_jobs/commit/c8f2feddd9f8b46a99b20879d3af483ed5291dd7,perf: 更为激进的投递间隔 perf: 更改投递时间
loks666,get_jobs,f63f03dfe60465617e541f028047b58d9decc5da,https://github.com/loks666/get_jobs/commit/f63f03dfe60465617e541f028047b58d9decc5da,perf: 修改默认配置 perf: 缩短投递间隔
loks666,get_jobs,f88cd66ca847fb43790a15c72ae121b866d60ed7,https://github.com/loks666/get_jobs/commit/f88cd66ca847fb43790a15c72ae121b866d60ed7,feat: 支持多条打招呼语句 perf: 修改部分方法调用，让代码更加易读
flutter,flutter-intellij,da70cfa6a6bfeedef5ed24845f1664b3e82e0121,https://github.com/flutter/flutter-intellij/commit/da70cfa6a6bfeedef5ed24845f1664b3e82e0121,[PE] fix read context EDT juggling in `FlutterInitializer` (#8216)  The move to project activities has some nuance. In the long run we'll be able to do some real performance tuning but in the short-run some of our assumptions about being in a read context need to be rethunk. Here's one.  The rub is `autoCreateRunConfig` needs to be in a read context but the remaining initialization needs to be on the EDT.  Fixes #8215  See also: https://github.com/flutter/flutter-intellij/issues/8100   ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,6350cf8d1da5fc8d4739a2519ca22dfdc037bfcb),https://github.com/flutter/flutter-intellij/commit/6350cf8d1da5fc8d4739a2519ca22dfdc037bfcb),[CQ] remove inspector/preview deadcode (#8214)  Another deadcode round-up cleaning up unused bits:  * orphaned by removed preview support * unused (and stale) observatory integration * inspector cruft * commented out code (dating back 7 years 😬
flutter,flutter-intellij,d15559b4037fd58c3eb2eb6a0463f7451b53a380,https://github.com/flutter/flutter-intellij/commit/d15559b4037fd58c3eb2eb6a0463f7451b53a380, - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,0fdf15ce7def15cf0fe2efe310dbb31b9bd8757c,https://github.com/flutter/flutter-intellij/commit/0fdf15ce7def15cf0fe2efe310dbb31b9bd8757c,[CQ] remove (dead) `FlutterStudioInitializer` (#8212)  `FlutterStudioInitializer` exists to perform a check at startup to ensure AS is current enough for... something... I can't recall the motivation for this extra check (despite having reviewed it in https://github.com/flutter/flutter-intellij/pull/1346 😬) but it was **8 years ago**!  Needless to say  removing it is surely safe:  1. the `com.intellij.androidStudioInitializer` extension point seems to have gone away  and even if it hadn't  2. we're surely past the window where there are any version out there that are too early for the version check     ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,546022d8ced9619f3d2502ba84492b082d99c35e,https://github.com/flutter/flutter-intellij/commit/546022d8ced9619f3d2502ba84492b082d99c35e,[perf] optimize EDT `SdkRunConfig.getState` (#8149)  Speeds up `SdkRunConfig.getState` which necessarily runs on the Event Dispatch Thread.  The fix has two parts:  1. Move the expensive call to `ModuleUtilCore.findModuleForFile(..)` into an async read action (because we can) and 2. Speed up `inProject` because we can't background it and there's a speedy alternative to the old more expensive approach  Addresses stack traces like this:  ``` java.lang.Throwable: Slow operations are prohibited on EDT. See SlowOperations.assertSlowOperationsAreAllowed javadoc. at com.intellij.openapi.diagnostic.Logger.error(Logger.java:376) at com.intellij.util.SlowOperations.assertSlowOperationsAreAllowed(SlowOperations.java:114) at com.intellij.workspaceModel.core.fileIndex.impl.WorkspaceFileIndexDataImpl.ensureIsUpToDate(WorkspaceFileIndexDataImpl.kt:153) at com.intellij.workspaceModel.core.fileIndex.impl.WorkspaceFileIndexDataImpl.getFileInfo(WorkspaceFileIndexDataImpl.kt:98) at com.intellij.workspaceModel.core.fileIndex.impl.WorkspaceFileIndexImpl.getFileInfo(WorkspaceFileIndexImpl.kt:267) at com.intellij.workspaceModel.core.fileIndex.impl.WorkspaceFileIndexImpl.findFileSetWithCustomData(WorkspaceFileIndexImpl.kt:250) at com.intellij.openapi.roots.impl.ProjectFileIndexImpl.getModuleForFile(ProjectFileIndexImpl.java:102) at com.intellij.openapi.roots.impl.ProjectFileIndexImpl.getModuleForFile(ProjectFileIndexImpl.java:95) at com.intellij.openapi.module.ModuleUtilCore.lambda$findModuleForFile$0(ModuleUtilCore.java:84) at com.intellij.openapi.application.impl.AnyThreadWriteThreadingSupport.runReadAction(AnyThreadWriteThreadingSupport.kt:272) at com.intellij.openapi.application.impl.AnyThreadWriteThreadingSupport.runReadAction(AnyThreadWriteThreadingSupport.kt:262) at com.intellij.openapi.application.impl.ApplicationImpl.runReadAction(ApplicationImpl.java:863) at com.intellij.openapi.application.ReadAction.compute(ReadAction.java:66) at com.intellij.openapi.module.ModuleUtilCore.findModuleForFile(ModuleUtilCore.java:84) at io.flutter.run.SdkRunConfig.getState(SdkRunConfig.java:139) at io.flutter.run.SdkRunConfig.getState(SdkRunConfig.java:59) ```  See: https://github.com/flutter/flutter-intellij/issues/8089  ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,95592638ba5f96d8338f92fc8308eab2bf0e1d9e,https://github.com/flutter/flutter-intellij/commit/95592638ba5f96d8338f92fc8308eab2bf0e1d9e,[CQ] migrate `FlutterStudioStartupActivity` to a project activity (#8211)  The model for this change was set with https://github.com/flutter/flutter-intellij/pull/8200 and ties up our migration off of the deprecated `StartupActivity` in favor of project activities  leveraging IntelliJ's embrace of Kotlin coroutines for improved startup performance.  Fixes: https://github.com/flutter/flutter-intellij/issues/8100    ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,1169f5e8a3adeb1e83c2a6d83231cd87d3ed5d1c,https://github.com/flutter/flutter-intellij/commit/1169f5e8a3adeb1e83c2a6d83231cd87d3ed5d1c,[perf] create action presentations lazily (#8187)  Stops eagerly creating action presentations (allocating resources prematurely and possibly unnecessarily) in favor of the preferred registration in plugin metadata that will only get loaded as-needed.  From the inspection:  > Reports any actions that are registered in the plugin.xml file and instantiate the com.intellij.openapi.actionSystem.Presentation object in their constructors.  > Any of the constructors of AnAction with parameters instantiate the Presentation object. However  instantiating the Presentation object in constructor results in allocating resources  which may not be necessary. Instead of creating an instance of Presentation that stores text  description  or icon  it is more efficient to utilize no-argument constructors of AnAction and other base classes and follow the convention for setting the text  description  and icon in plugin.xml. The IDE will load text  description  and icon only when the action is actually displayed in the UI.  > The convention for setting the text  description  and icon is as follows: Set the id attribute for the action in the plugin.xml file. Optionally  set the icon attribute if an icon is needed.  Our actions:   ![image](https://github.com/user-attachments/assets/7eb77ea8-006b-44aa-afcf-e8c3784f52b3)  The only risk is in ensuring that the templated menu text is still getting properly filled in  and I confirmed that:   ![image](https://github.com/user-attachments/assets/00697fdf-9a32-4204-ac49-a73c32cdf0c2)      ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,0f8ffa6ab1f7634da74111d04d13868084064370,https://github.com/flutter/flutter-intellij/commit/0f8ffa6ab1f7634da74111d04d13868084064370,[CQ] dead code cleanup (icons  etc) (#8184)  🧹 Removes:  * a host of abandoned inspector  perf and profiler icons (+80) * unused `LOG` objects * unreferenced methods (from way back)   ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,f99f8cc60596f9dbf76d043367ce28d90b28520e,https://github.com/flutter/flutter-intellij/commit/f99f8cc60596f9dbf76d043367ce28d90b28520e,[PERF] migrate Project parentDisposables to Services (#8178)  Using a `Project` as a parent disposable can lead to plugins not being unloaded correctly leading to memory leaks. This follows the advice given in the Jetbrains [disposer docs](https://plugins.jetbrains.com/docs/intellij/disposers.html?from=IncorrectParentDisposable#choosing-a-disposable-parent) and is (I think) consistent with the Dart plugin (@alexander-doroshko?).  This gets us *most* of the way to fixing https://github.com/flutter/flutter-intellij/issues/8107 with an open question about `AndroidModuleLibraryManager`. (Question posed there 👍 .)  ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,9102f9c3d724701732e2affcbe0f37b812a659e6,https://github.com/flutter/flutter-intellij/commit/9102f9c3d724701732e2affcbe0f37b812a659e6,[CQ] `VMServiceManager` remove legacy heap/frame monitoring (#8174)  Performance monitoring is now in devtools so we can stop heap/frame monitoring (which has been unused since the legacy inspector view went away).  _There are more threads to pull on here (for example the `DisplayRefreshRateManager` may be able to go away too but we need to be sure it doesn't have side-effects that are depended on elsewhere) but this is a good place to start._  ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,d65fed7c721f14c4089138fc2b616ecafba5add3,https://github.com/flutter/flutter-intellij/commit/d65fed7c721f14c4089138fc2b616ecafba5add3,[CQ][perf] remove unused `LOG` instances (#8164)  Removes unused `LOG` instances.  ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,85424572d91a3da3b4ce356c9514e6989eca562a,https://github.com/flutter/flutter-intellij/commit/85424572d91a3da3b4ce356c9514e6989eca562a,[perf] ensure mock auto-closeable is closed (#8133)  `MockitoAnnotations.openMocks` returns an auto-closeable that a `try`-with-resources statement ensures will get closed.  Follow-up from https://github.com/flutter/flutter-intellij/pull/8111 and related to https://github.com/flutter/flutter-intellij/issues/8110.  (Also some opportunistic unused import cleanup.)  ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,894af538c32d04ef0e61bd56fff3c4a6c8406928,https://github.com/flutter/flutter-intellij/commit/894af538c32d04ef0e61bd56fff3c4a6c8406928,[perf] stop static initialization in line marker extension (#8114)  ![image](https://github.com/user-attachments/assets/bffce434-6527-4501-95d7-129541d8b4b7)  From the inspection description:  > Static initialization is performed once the class is loaded  which may cause excessive classloading or early initialization of heavy resources. Since extension point implementations are supposed to be cheap to create  they must not have static initializers.  ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,afb700d4ad3c9d9e3eae1a081a29391e72de213b,https://github.com/flutter/flutter-intellij/commit/afb700d4ad3c9d9e3eae1a081a29391e72de213b,[perf] use `try`-with-resources  for auto-closeables (#8111)  Fixes: https://github.com/flutter/flutter-intellij/issues/8110  ---  - [x] I’ve reviewed the contributor guide and applied the relevant portions to this PR.  <details> <summary>Contribution guidelines:</summary><br>  - See our [contributor guide]([https://github.com/dart-lang/sdk/blob/main/CONTRIBUTING.md](https://github.com/flutter/flutter/blob/main/docs/contributing/Tree-hygiene.md#overview) for general expectations for PRs. - Larger or significant changes should be discussed in an issue before creating a PR. - Dart contributions to our repos should follow the [Dart style guide](https://dart.dev/guides/language/effective-dart) and use `dart format`. - Java and Kotlin contributions should strive to follow Java and Kotlin best practices ([discussion](https://github.com/flutter/flutter-intellij/issues/8098)). </details>
flutter,flutter-intellij,f2c29c5c37e45778624d721b4c1dc94b8194b9bb,https://github.com/flutter/flutter-intellij/commit/f2c29c5c37e45778624d721b4c1dc94b8194b9bb,[perf] stop blocking on `WorkspaceCache.refresh` (#8097)  Fixes: https://github.com/flutter/flutter-intellij/issues/8083  See: https://github.com/flutter/flutter-intellij/issues/8089  Tested w/ https://github.com/scalio/bazel-flutter  ## Pre-launch Checklist  - [x] I read the [Contributor Guide] and followed the process outlined there for submitting PRs. - [x] I read the [Tree Hygiene] wiki page  which explains my responsibilities. - [x] I read the [Flutter Style Guide] _recently_  and have followed its advice. - [x] I signed the [CLA]. - [x] I listed at least one issue that this PR fixes in the description above. - [x] I updated/added relevant documentation (doc comments with `///`). - [x] I added new tests to check the change I am making  or this PR is [test-exempt]. - [x] All existing and new tests are passing.  If you need help  consider asking for advice on the #hackers-new channel on [Discord].  <!-- Links --> [Contributor Guide]: https://github.com/flutter/flutter/blob/master/docs/contributing/Tree-hygiene.md#overview [Tree Hygiene]: https://github.com/flutter/flutter/blob/master/docs/contributing/Tree-hygiene.md [test-exempt]: https://github.com/flutter/flutter/blob/master/docs/contributing/Tree-hygiene.md#tests [Flutter Style Guide]: https://github.com/flutter/flutter/blob/master/docs/contributing/Style-guide-for-Flutter-repo.md [CLA]: https://cla.developers.google.com/ [flutter/tests]: https://github.com/flutter/tests [breaking change policy]: https://github.com/flutter/flutter/blob/master/docs/contributing/Tree-hygiene.md#handling-breaking-changes [Discord]: https://github.com/flutter/flutter/blob/master/docs/contributing/Chat.md
flutter,flutter-intellij,d3490db3f5585c8e0ac568466e3221e94dca96b9,https://github.com/flutter/flutter-intellij/commit/d3490db3f5585c8e0ac568466e3221e94dca96b9,[perf] stop blocking on `SdkRunConfig.addConsoleFilters` (#8095)  The hunt for slow ops continues...  Addresses:  ``` java.lang.Throwable: Slow operations are prohibited on EDT. See SlowOperations.assertSlowOperationsAreAllowed javadoc. at com.intellij.openapi.diagnostic.Logger.error(Logger.java:376) at com.intellij.util.SlowOperations.assertSlowOperationsAreAllowed(SlowOperations.java:114) at com.intellij.workspaceModel.core.fileIndex.impl.WorkspaceFileIndexDataImpl.ensureIsUpToDate(WorkspaceFileIndexDataImpl.kt:153) at com.intellij.workspaceModel.core.fileIndex.impl.WorkspaceFileIndexDataImpl.getFileInfo(WorkspaceFileIndexDataImpl.kt:98) at com.intellij.workspaceModel.core.fileIndex.impl.WorkspaceFileIndexImpl.getFileInfo(WorkspaceFileIndexImpl.kt:267) at com.intellij.workspaceModel.core.fileIndex.impl.WorkspaceFileIndexImpl.findFileSet(WorkspaceFileIndexImpl.kt:220) at com.intellij.workspaceModel.core.fileIndex.impl.WorkspaceFileIndexImpl.isInContent(WorkspaceFileIndexImpl.kt:86) at com.intellij.openapi.roots.impl.ProjectFileIndexImpl.isInContent(ProjectFileIndexImpl.java:211) at com.jetbrains.lang.dart.util.PubspecYamlUtil.findPubspecYamlFile(PubspecYamlUtil.java:58) at com.jetbrains.lang.dart.util.DartUrlResolverImpl.<init>(DartUrlResolverImpl.java:47) at com.jetbrains.lang.dart.util.DartUrlResolver.getInstance(DartUrlResolver.java:36) at com.jetbrains.lang.dart.ide.runner.DartConsoleFilter.<init>(DartConsoleFilter.java:44) at io.flutter.run.SdkRunConfig.addConsoleFilters(SdkRunConfig.java:228) at io.flutter.run.SdkRunConfig.getState(SdkRunConfig.java:194) at io.flutter.run.SdkRunConfig.getState(SdkRunConfig.java:56) ```  See: https://github.com/flutter/flutter-intellij/issues/8089  ## Pre-launch Checklist  - [x] I read the [Contributor Guide] and followed the process outlined there for submitting PRs. - [x] I read the [Tree Hygiene] wiki page  which explains my responsibilities. - [x] I read the [Flutter Style Guide] _recently_  and have followed its advice. - [x] I signed the [CLA]. - [x] I listed at least one issue that this PR fixes in the description above. - [x] I updated/added relevant documentation (doc comments with `///`). - [x] I added new tests to check the change I am making  or this PR is [test-exempt]. - [x] All existing and new tests are passing.  If you need help  consider asking for advice on the #hackers-new channel on [Discord].  <!-- Links --> [Contributor Guide]: https://github.com/flutter/flutter/blob/master/docs/contributing/Tree-hygiene.md#overview [Tree Hygiene]: https://github.com/flutter/flutter/blob/master/docs/contributing/Tree-hygiene.md [test-exempt]: https://github.com/flutter/flutter/blob/master/docs/contributing/Tree-hygiene.md#tests [Flutter Style Guide]: https://github.com/flutter/flutter/blob/master/docs/contributing/Style-guide-for-Flutter-repo.md [CLA]: https://cla.developers.google.com/ [flutter/tests]: https://github.com/flutter/tests [breaking change policy]: https://github.com/flutter/flutter/blob/master/docs/contributing/Tree-hygiene.md#handling-breaking-changes [Discord]: https://github.com/flutter/flutter/blob/master/docs/contributing/Chat.md
flutter,flutter-intellij,88593ea5bbbb53a31ec5a1c57ba671ecff251d67,https://github.com/flutter/flutter-intellij/commit/88593ea5bbbb53a31ec5a1c57ba671ecff251d67,Suppress superfluous warnings of 'Boolean method is always inverted' (#7930)
flutter,flutter-intellij,db2f786a7e3a977c45c8eca3593873ed423765fd,https://github.com/flutter/flutter-intellij/commit/db2f786a7e3a977c45c8eca3593873ed423765fd,Remove the Flutter Performance window and associated functionality (#7856)  https://github.com/flutter/flutter-intellij/issues/7817
flutter,flutter-intellij,8367ffb1ad78f9da98b406798d437795abb80d41,https://github.com/flutter/flutter-intellij/commit/8367ffb1ad78f9da98b406798d437795abb80d41,Gracefully handle perf tool window not being created (#7713)  A workaround for #7691  not accessing the tool window if it wasn't created due to `isApplicableAsync` being false for the SDK version. Does it in this listener so if the SDK version changes  the event still can be handled properly.
flutter,flutter-intellij,8ea8af00ea8baa074368f55f667918866ad47b27,https://github.com/flutter/flutter-intellij/commit/8ea8af00ea8baa074368f55f667918866ad47b27,Use correct SDK version check for gating performance page (#7639)  Fixes error from https://github.com/flutter/flutter-intellij/pull/7637#pullrequestreview-2286867489  CC @kenzieschmoll
flutter,flutter-intellij,d9f3db3f468c944aed287e055eb269538eb11e31,https://github.com/flutter/flutter-intellij/commit/d9f3db3f468c944aed287e055eb269538eb11e31,Remove performance view for newer Flutter SDK versions (#7637)  Fixes https://github.com/flutter/flutter-intellij/issues/7624
flutter,flutter-intellij,193b5bc2c01cc30ca6b90b3314335d97a34b5e21,https://github.com/flutter/flutter-intellij/commit/193b5bc2c01cc30ca6b90b3314335d97a34b5e21,Make performance tab warning text red (#7581)  Follow up to https://github.com/flutter/flutter-intellij/pull/7577  <img width="581" alt="Screenshot 2024-07-29 at 10 45 47 AM" src="https://github.com/user-attachments/assets/c3316e53-c41a-4866-9592-56731d6bc1f8">
flutter,flutter-intellij,201e5087ec652f8db37c0abf3e07112b3d197313,https://github.com/flutter/flutter-intellij/commit/201e5087ec652f8db37c0abf3e07112b3d197313,Add warning message for removing performance panel (#7577)  <img width="511" alt="Screenshot 2024-07-26 at 1 17 17 PM" src="https://github.com/user-attachments/assets/73d6745b-0ada-4def-b5b0-d62c67d52875">
metasfresh,metasfresh,a86f7399408fb066e258f5b9488558d796e95129,https://github.com/metasfresh/metasfresh/commit/a86f7399408fb066e258f5b9488558d796e95129,Export both desadv-lines within packs and lines that have no packs with OK performance (#20100)  Add a dedicated view to achieve it  because we can't use "case" or "coalesce" to compute a view's key-column
metasfresh,metasfresh,a9131677b0e082b9d678a036534eeafbb6c0288b,https://github.com/metasfresh/metasfresh/commit/a9131677b0e082b9d678a036534eeafbb6c0288b,Fix severe performance problem with C_BPartner_UpdateStats (#19947)  * Fix severe performance problem with C_BPartner_UpdateStats refs: https://github.com/metasfresh/metasfresh/issues/19946  * use parameterized statements refs: https://github.com/metasfresh/metasfresh/issues/19946  * use parameterized statements refs: https://github.com/metasfresh/metasfresh/issues/19946  * use parameterized statements refs: https://github.com/metasfresh/metasfresh/issues/19946  * use parameterized statements refs: https://github.com/metasfresh/metasfresh/issues/19946
metasfresh,metasfresh,cb6510d4ccb5a0b2d8c16bf4e76b3fc783ac5071,https://github.com/metasfresh/metasfresh/commit/cb6510d4ccb5a0b2d8c16bf4e76b3fc783ac5071,Improve performance by using attribute-DB-function instead of view (#19898)  This prevents the DB from doing a sequential scan on M_AttributeInstance
metasfresh,metasfresh,883b1b3cfd38820f5a3a175fbfded857de6777b9,https://github.com/metasfresh/metasfresh/commit/883b1b3cfd38820f5a3a175fbfded857de6777b9,Add sysconfig to ignore MaterialEvents instead of processing them  to increase performance (#19047)
metasfresh,metasfresh,e00076f489dd6b4e309ca6a0c3bfe901f70253e1,https://github.com/metasfresh/metasfresh/commit/e00076f489dd6b4e309ca6a0c3bfe901f70253e1,Merge pull request #18702 from metasfresh/mergify/copy/yoyo_uat/pr-16127  Improve ESR-Performance and fix QuerySelectionToDeleteHelper (copy #16127)
metasfresh,metasfresh,591e985771277a932faba69a77c3f13e9b63381b,https://github.com/metasfresh/metasfresh/commit/591e985771277a932faba69a77c3f13e9b63381b,Improve ESR-Performance and fix QuerySelectionToDeleteHelper (#16127)  * attempt to avoid deadlock  * QuerySelectionToDeleteHelper.scheduleDeleteSelection schedules only UUIDs when needed  * findExistentPaymentId() - if there are no similar payments then don't search for ESR_ImportLine  * * ESR performance improvements (#12946)  * * Access database less  https://github.com/metasfresh/metasfresh/issues/12945  * * get rid of ilike  https://github.com/metasfresh/metasfresh/issues/12945  * * esr referenceNo matcher  https://github.com/metasfresh/metasfresh/issues/12945  * * Save not needed because it's already saved in de.metas.payment.esr.api.impl.ESRImportBL.evaluateLine  https://github.com/metasfresh/metasfresh/issues/12945  * * index and column length improvement  https://github.com/metasfresh/metasfresh/issues/12945  * * Make Amount numeric  https://github.com/metasfresh/metasfresh/issues/12945  * * Avoid NPE  https://github.com/metasfresh/metasfresh/issues/12945  * * That validation was leading to the possible recreation of the lines from an import file  which makes no sense. Get rid of it. https://github.com/metasfresh/metasfresh/issues/12945  * * format https://github.com/metasfresh/metasfresh/issues/12945  * * rolled back since it's out of scope https://github.com/metasfresh/metasfresh/issues/12945  * * avoid npe https://github.com/metasfresh/metasfresh/issues/12945  * * this index (mind the parameters order) improves performance more https://github.com/metasfresh/metasfresh/issues/12945  (cherry picked from commit eb7049697d9e3a86cd2ff6cb7fbc8f654819b464)  solved Conflicts: backend/de.metas.payment.esr/src/main/java/de/metas/payment/esr/api/impl/ESRImportBL.java backend/de.metas.payment.esr/src/main/java/de/metas/payment/esr/api/impl/ESRImportDAO.java  * recreate the index in a better configuration  * minor improvements after static code review  ---------  Co-authored-by: Ruxandra Craciunescu <ruxandra.craciunescu@metasfresh.com> (cherry picked from commit 8a18b475c045f390935e6caa0b56c03fb1662bc9)
metasfresh,metasfresh,8d08a4e5d26e3a09bbda4ad27951fba47d6b449b,https://github.com/metasfresh/metasfresh/commit/8d08a4e5d26e3a09bbda4ad27951fba47d6b449b,Material Cockpit performance improvements (#18257)  * Material Cockpit performance improvements  ---------  Co-authored-by: Teodor Sarca <teo.sarca@metasfresh.com> Co-authored-by: mergify[bot] <37929162+mergify[bot]@users.noreply.github.com>
metasfresh,metasfresh,c5bbeb6cdb4488b0c754ada0cf2f9c3097df6f81,https://github.com/metasfresh/metasfresh/commit/c5bbeb6cdb4488b0c754ada0cf2f9c3097df6f81,Merge pull request #18226 from metasfresh/inner_silence_uat_performance_imp  CP - Improve ESR-Performance and fix QuerySelectionToDeleteHelper
metasfresh,metasfresh,24e169464d90193d4ac409c58b6f5c972758f73d,https://github.com/metasfresh/metasfresh/commit/24e169464d90193d4ac409c58b6f5c972758f73d,Improve ESR-Performance and fix QuerySelectionToDeleteHelper (#16127)  * attempt to avoid deadlock  * QuerySelectionToDeleteHelper.scheduleDeleteSelection schedules only UUIDs when needed  * findExistentPaymentId() - if there are no similar payments then don't search for ESR_ImportLine  * * ESR performance improvements (#12946)  * * Access database less  https://github.com/metasfresh/metasfresh/issues/12945  * * get rid of ilike  https://github.com/metasfresh/metasfresh/issues/12945  * * esr referenceNo matcher  https://github.com/metasfresh/metasfresh/issues/12945  * * Save not needed because it's already saved in de.metas.payment.esr.api.impl.ESRImportBL.evaluateLine  https://github.com/metasfresh/metasfresh/issues/12945  * * index and column length improvement  https://github.com/metasfresh/metasfresh/issues/12945  * * Make Amount numeric  https://github.com/metasfresh/metasfresh/issues/12945  * * Avoid NPE  https://github.com/metasfresh/metasfresh/issues/12945  * * That validation was leading to the possible recreation of the lines from an import file  which makes no sense. Get rid of it. https://github.com/metasfresh/metasfresh/issues/12945  * * format https://github.com/metasfresh/metasfresh/issues/12945  * * rolled back since it's out of scope https://github.com/metasfresh/metasfresh/issues/12945  * * avoid npe https://github.com/metasfresh/metasfresh/issues/12945  * * this index (mind the parameters order) improves performance more https://github.com/metasfresh/metasfresh/issues/12945  (cherry picked from commit eb7049697d9e3a86cd2ff6cb7fbc8f654819b464)  solved Conflicts: backend/de.metas.payment.esr/src/main/java/de/metas/payment/esr/api/impl/ESRImportBL.java backend/de.metas.payment.esr/src/main/java/de/metas/payment/esr/api/impl/ESRImportDAO.java  * recreate the index in a better configuration  * minor improvements after static code review  ---------  Co-authored-by: Ruxandra Craciunescu <ruxandra.craciunescu@metasfresh.com>  (cherry picked from commit 8a18b475c045f390935e6caa0b56c03fb1662bc9)
smithy-lang,smithy,d2460476a16b0e8d94bde7907eb9ae0cc844c347,https://github.com/smithy-lang/smithy/commit/d2460476a16b0e8d94bde7907eb9ae0cc844c347,Improve isVirtualHostableBucket function  This commit removes the use of three regular expressions and string splitting when checking if a value is a virtual hostable bucket. This should help with both startup and runtime performance. It also exposes a static method for performing this check that I'll use in smithy-java.
smithy-lang,smithy,a20eb48c8041e1110e4efad58fd450f2e078852a,https://github.com/smithy-lang/smithy/commit/a20eb48c8041e1110e4efad58fd450f2e078852a,Add public static utilities to ParseUrl  First  I copied the IP address detection code from https://github.com/aws/aws-sdk-java-v2/blob/93f914840a071fb162693d1ebd753803281253cf/codegen/src/main/resources/software/amazon/awssdk/codegen/rules2/RuleUrl.java.resource#L58-L111 This should be much more performant than catching integer parsing errors.  I also exposed this method and the method for normalizing a path as a public utility method. This will allow me to use the method directly in smithy-java.
smithy-lang,smithy,697477fbad994c0ee79f8a8eabb90b223873e247,https://github.com/smithy-lang/smithy/commit/697477fbad994c0ee79f8a8eabb90b223873e247,Use ShapeId instead of name in OAS conversion  This path previously created a bunch of extra ShapeId garbage from converting strings into ShapeIds each time a test was performed. The ShapeId instances for these already exist  so we use those instead.
smithy-lang,smithy,d2139b7a0e339209afccb6cb333b4beecf10f9af,https://github.com/smithy-lang/smithy/commit/d2139b7a0e339209afccb6cb333b4beecf10f9af,Improve BottomUpIndex performance (#2367)  When doing some ad-hoc profiling on the language server using aws service models I noticed that BottomUpIndex was taking up a lot of CPU time. This was because it used selectors and PathFinder to determine the paths from resources/operations up to their enclosing service. Selectors are pretty slow in comparison to regular java code  and when the model is large with many services/operations  this index can take a while to compute. BottomUpIndex is used for validating `@cfnResource` (through CfnResourcePropertyValidator -> CfnResourceIndex)  so anyone using this trait is paying this performance cost (in particular  SDK code generators).  To get a rough idea of how much faster this commit makes BottomUpIndex  I ran model validation using the cli on all aws service models with and without this change a few times each: ``` smithy validate --quiet # 14 - 16 seconds  /path/to/smithy/smithy-cli/build/image/smithy-cli-darwin-aarch64/bin/smithy validate --quiet # 8 - 9 seconds ```
OpenLineage,OpenLineage,627a49255975a327d56bd3d0e52df1e4570d5e38,https://github.com/OpenLineage/OpenLineage/commit/627a49255975a327d56bd3d0e52df1e4570d5e38,address cycles in flattenrdds & perf improvement (#3371)  Signed-off-by: Sharanya Santhanam <s_santhanam@apple.com> Co-authored-by: Sharanya Santhanam <s_santhanam@apple.com>
apache,atlas,527ff208403715ad60cce5d1b6ea554c9c596867,https://github.com/apache/atlas/commit/527ff208403715ad60cce5d1b6ea554c9c596867,ATLAS-5025: entity-create perf improvement - skip setLabel call when empty (#339)
eclipse-jdtls,eclipse.jdt.ls,9b483dbce9985480edd6580654bb7d522415dc37,https://github.com/eclipse-jdtls/eclipse.jdt.ls/commit/9b483dbce9985480edd6580654bb7d522415dc37,Revert modifications to preference manager.  Reverts the following : - "Preference manager should merge new preferences instead of overwriting." - "Use AbstractMap.SimpleEntry instead of Map.entry to permit null values." - "Flatten incoming preference map so as to perform merging correctly." - "Preserve initialization options stored inside Preferences."
eclipse-jdtls,eclipse.jdt.ls,f68fbad4a205a1560cb4c1b17ba7e11d907ba2c0,https://github.com/eclipse-jdtls/eclipse.jdt.ls/commit/f68fbad4a205a1560cb4c1b17ba7e11d907ba2c0,Flatten incoming preference map so as to perform merging correctly.  - Add test case - Also revise JAXP limits based on pre JDK 24 values.  Signed-off-by: Roland Grunberg <rgrunber@redhat.com>
eclipse-jdtls,eclipse.jdt.ls,cc0946b51986ba32ea0408a9d1312f16527709d4,https://github.com/eclipse-jdtls/eclipse.jdt.ls/commit/cc0946b51986ba32ea0408a9d1312f16527709d4,Reuse quick fixes from LocalCorrectionsBaseSubProcessor.  - New quick fixes : - UnqualifiedFieldAccess -> addUnqualifiedFieldAccessProposal - UnnecessaryInstanceof -> addUnnecessaryInstanceofProposal - IllegalQualifiedEnumConstantLabel -> addIllegalQualifiedEnumConstantLabelProposal - UnnecessaryElse -> addUnnecessaryElseProposals - SuperclassMustBeAClass -> addInterfaceExtendsClassProposals - AssignmentHasNoEffect -> addAssignmentHasNoEffectProposals - FallthroughCase  IllegalFallthroughToPattern -> addFallThroughProposals - UnsafeTypeConversion  RawTypeReference  UnsafeRawMethodInvocation -> addDeprecatedFieldsToMethodsProposals - IllegalTotalPatternWithDefault -> addRemoveDefaultCaseProposal - SuperfluousSemicolon -> addSuperfluousSemicolonProposal - AbstractServiceImplementation  ProviderMethodOrConstructorRequiredForServiceImpl  ServiceImplDefaultConstructorNotPublic -> addServiceProviderConstructorProposals - NonStaticFieldFromStaticInvocation  NonStaticFieldFromStaticInvocation -> addObjectReferenceProposal  addVariableReferenceProposal  addNewObjectProposal - Fix the JDT-LS QuickFixProcessor to better match upstream (by preventing fall-throughs  with 'break' where appropriate) - Preserve non-upstreamed behaviour of surrounding all selected statements when "Surrounding with try/catch" - Based on https://eclip.se/545163 the "Surround with try-catch" quick-fix is not valid for a 'throw' statement so we should adjust those testcases to use methods with throw declarations instead - Ensure the current compilation unit primary type is never used when it is null  Signed-off-by: Roland Grunberg <rgrunber@redhat.com>
eclipse-jdtls,eclipse.jdt.ls,839831feefca6dbfdf75b6bea85145d76865cdf7,https://github.com/eclipse-jdtls/eclipse.jdt.ls/commit/839831feefca6dbfdf75b6bea85145d76865cdf7,Improvements to code action performance (#3322)  - React to cancellation requests while computing code actions - Avoid expensive calls for "Inline" & "Change Signature" refactorings - Ensure the active source file is updated when between multiple opened files  Signed-off-by: Snjezana Peco <snjezana.peco@redhat.com>
zalando,logbook,1392997d13d228fa3b207baca042c272bc2bfee5,https://github.com/zalando/logbook/commit/1392997d13d228fa3b207baca042c272bc2bfee5,Allow using precise floats in logs (#2005)  * Allow using precise floats in logs  * extract usePreciseFloats check into a separate method  * add a comment about performance penalty  * add strategy to handle different ways of handling floats  ...because no one likes boolean flags anymore ¯\_(ツ)_/¯  * leve only one level of wrappers (remove creators)  * update README
zalando,logbook,605db1ba988cf3836f9706eb4332c6c6973d4a4a,https://github.com/zalando/logbook/commit/605db1ba988cf3836f9706eb4332c6c6973d4a4a,Merge pull request #1839 from aukevanleeuwen/1838-fix-queryfilter-performance  Improve performance of query filters (especially on large bodies)
zalando,logbook,c2a2a2996a37e4a9a1a77992458e2a61cf3148f8,https://github.com/zalando/logbook/commit/c2a2a2996a37e4a9a1a77992458e2a61cf3148f8,Improve performance of query filters (especially on large bodies)  Fixes: #1838
apache,servicecomb-java-chassis,6e8b55a725b7715600a12a247092941228955644,https://github.com/apache/servicecomb-java-chassis/commit/6e8b55a725b7715600a12a247092941228955644,[#4532] Fixed when the servicecomb engine storage pool is faulty  the registration and configuration centers cannot perform HA switchover problem (#4533)
MarquezProject,marquez,7d0b290b2c7bbb706776449e85774522450b950a,https://github.com/MarquezProject/marquez/commit/7d0b290b2c7bbb706776449e85774522450b950a,Optimize Column Lineage Query Performance (#2821)  * Optimize Column Lineage Query Performance  Signed-off-by: Vinh Nguyen <phuvinh97ag@gmail.com>  * Optimize Column Lineage Query Performance - Format query - replace select * with uuid  namespace_name  name  Signed-off-by: Vinh Nguyen <phuvinh97ag@gmail.com>  ---------  Signed-off-by: Vinh Nguyen <phuvinh97ag@gmail.com> Co-authored-by: Peter Hicks <phixMe@users.noreply.github.com>
openjdk,loom,d9b6e4b13200684b69a161e288b9883ff0d96bec,https://github.com/openjdk/loom/commit/d9b6e4b13200684b69a161e288b9883ff0d96bec,8352642: Set zipinfo-time=false when constructing zipfs FileSystem in com.sun.tools.javac.file.JavacFileManager$ArchiveContainer for better performance  Reviewed-by: liach  jpai  jlahoda  lancea
openjdk,loom,84458ec18ce33295636f7b26b8e3ff25ecb349f2,https://github.com/openjdk/loom/commit/84458ec18ce33295636f7b26b8e3ff25ecb349f2,8353013: java.net.URI.create(String) may have low performance to scan the host/domain name from URI string when the hostname starts with number  Reviewed-by: michaelm  xpeng
openjdk,loom,d684867066edb886bc444c864ef9db3eff318c34,https://github.com/openjdk/loom/commit/d684867066edb886bc444c864ef9db3eff318c34,8346230: [perf] scalability issue for the specjvm2008::xml.transform workload  Reviewed-by: joehw  jbhateja
openjdk,loom,5481021ee64fd457279ea7083be0f977c7ce3e3c,https://github.com/openjdk/loom/commit/5481021ee64fd457279ea7083be0f977c7ce3e3c,8321591: (fs) Improve String -> Path conversion performance (win)  Reviewed-by: alanb
openjdk,loom,8b0602dbed2f7ced190ec81753defab8a4bc316d,https://github.com/openjdk/loom/commit/8b0602dbed2f7ced190ec81753defab8a4bc316d,8319447: Improve performance of delayed task handling  Reviewed-by: vklang  alanb
openjdk,loom,bbd5b174c50346152a624317b6bd76ec48f7e551,https://github.com/openjdk/loom/commit/bbd5b174c50346152a624317b6bd76ec48f7e551,8339280: jarsigner -verify performs cross-checking between CEN and LOC  Reviewed-by: mullan  weijun  lancea
openjdk,loom,b499c827a512fb209a806d95b97df0f5932a29c0,https://github.com/openjdk/loom/commit/b499c827a512fb209a806d95b97df0f5932a29c0,8349383: (fs) FileTreeWalker.next() superfluous null check of visit() return value  Reviewed-by: djelinski
openjdk,loom,250ff86dc86f73dbf7c944d9b5a792c4bdfeef0d,https://github.com/openjdk/loom/commit/250ff86dc86f73dbf7c944d9b5a792c4bdfeef0d,8349000: Performance improvement for Currency.isPastCutoverDate(String)  Reviewed-by: naoto  aturbanov
openjdk,loom,10d08dbc81aa14499410f0a7a64d0b3243b660f1,https://github.com/openjdk/loom/commit/10d08dbc81aa14499410f0a7a64d0b3243b660f1,8346142: [perf] scalability issue for the specjvm2008::xml.validation workload  Reviewed-by: joehw
openjdk,loom,9a60f4457bb56d0f5039a97e6b943e62a8a2c3ee,https://github.com/openjdk/loom/commit/9a60f4457bb56d0f5039a97e6b943e62a8a2c3ee,8345668: ZoneOffset.ofTotalSeconds performance regression  Reviewed-by: rriggs  aturbanov
openjdk,loom,06c44dd568d91e1bd68f60fd3e57abcbe97e5dca,https://github.com/openjdk/loom/commit/06c44dd568d91e1bd68f60fd3e57abcbe97e5dca,8345465: Fix performance regression on x64 after JDK-8345120  Reviewed-by: mcimadamore
openjdk,loom,5958463cadb04560ec85d9af972255bfe6dcc2f2,https://github.com/openjdk/loom/commit/5958463cadb04560ec85d9af972255bfe6dcc2f2,8343377: Performance regression in reflective invocation of native methods  Reviewed-by: mchung
openjdk,loom,d49f21043b84ebcc8b9176de3a84621ca7bca8fb,https://github.com/openjdk/loom/commit/d49f21043b84ebcc8b9176de3a84621ca7bca8fb,8342040: Further improve entry lookup performance for multi-release JARs  Co-authored-by: Claes Redestad <redestad@openjdk.org> Reviewed-by: redestad
openjdk,loom,83dcb02d776448aa04f3f41df489bd4355443a4d,https://github.com/openjdk/loom/commit/83dcb02d776448aa04f3f41df489bd4355443a4d,8340079: Modify rearrange/selectFrom Vector API methods to perform wrapIndexes instead of checkIndexes  Reviewed-by: jbhateja  psandoz
openjdk,loom,81ff91ef27a6a856ae2c453a9a9b8333b91da3ab,https://github.com/openjdk/loom/commit/81ff91ef27a6a856ae2c453a9a9b8333b91da3ab,8339531: Improve performance of MemorySegment::mismatch  Reviewed-by: mcimadamore
openjdk,loom,6be927260a84b1d7542167e526ff41f7dc26cab0,https://github.com/openjdk/loom/commit/6be927260a84b1d7542167e526ff41f7dc26cab0,8338591: Improve performance of MemorySegment::copy  Reviewed-by: mcimadamore
openjdk,loom,7a418fc07464fe359a0b45b6d797c65c573770cb,https://github.com/openjdk/loom/commit/7a418fc07464fe359a0b45b6d797c65c573770cb,8338967: Improve performance for MemorySegment::fill  Reviewed-by: mcimadamore  psandoz
openjdk,loom,ab8071d28027ecbf5e8984c30b35fa1c2d934de7,https://github.com/openjdk/loom/commit/ab8071d28027ecbf5e8984c30b35fa1c2d934de7,8338146: Improve Exchanger performance with VirtualThreads  Reviewed-by: alanb
openjdk,loom,75bea280b9adb6dac9fefafbb3f4b212f100fbb5,https://github.com/openjdk/loom/commit/75bea280b9adb6dac9fefafbb3f4b212f100fbb5,8333867: SHA3 performance can be improved  Reviewed-by: kvn  valeriep
openjdk,loom,a941397327972f130e683167a1b429f17603df46,https://github.com/openjdk/loom/commit/a941397327972f130e683167a1b429f17603df46,8329031: CPUID feature detection for Advanced Performance Extensions (Intel® APX)  Reviewed-by: sviswanathan  kvn
openjdk,loom,d826127970bd2ae8bf4cacc3c55634dc5af307c4,https://github.com/openjdk/loom/commit/d826127970bd2ae8bf4cacc3c55634dc5af307c4,8333462: Performance regression of new DecimalFormat() when compare to jdk11  Reviewed-by: liach  naoto  jlu
zfoo-project,zfoo,a245e63fc304706a286258cdbf8f8c2fe6d0e6a2,https://github.com/zfoo-project/zfoo/commit/a245e63fc304706a286258cdbf8f8c2fe6d0e6a2,perf[orm]: if create entity fail when load entity  and throw exception
zfoo-project,zfoo,290e8efcda83fd232256e42a744ae8003f990ffc,https://github.com/zfoo-project/zfoo/commit/290e8efcda83fd232256e42a744ae8003f990ffc,Merge pull request #137 from LucaLq/main  perf[net]: adjust access modifier
zfoo-project,zfoo,f9b334b3036c17cf7cde31fea8912e5f938c4786,https://github.com/zfoo-project/zfoo/commit/f9b334b3036c17cf7cde31fea8912e5f938c4786,perf[net]: adjust access modifier
zfoo-project,zfoo,0971bc8414a2cffbabcda6a43acb133bab589a28,https://github.com/zfoo-project/zfoo/commit/0971bc8414a2cffbabcda6a43acb133bab589a28,Merge pull request #135 from awakeyoyoyo/main  perf[orm]: code style
zfoo-project,zfoo,c05df578216314643838725e12c8e14446b2da9f,https://github.com/zfoo-project/zfoo/commit/c05df578216314643838725e12c8e14446b2da9f,perf[orm]: code style
zfoo-project,zfoo,12116091c46d6213ec46d3ccb0747979c3b237cf,https://github.com/zfoo-project/zfoo/commit/12116091c46d6213ec46d3ccb0747979c3b237cf,Merge pull request #134 from awakeyoyoyo/main  perf[orm]: code style
zfoo-project,zfoo,b3dc38dd2a3eec6ee5eb0a21ef1623c769041424,https://github.com/zfoo-project/zfoo/commit/b3dc38dd2a3eec6ee5eb0a21ef1623c769041424,perf[orm]: code style
zfoo-project,zfoo,3aaab2dc214a6c36a46648da24741580e8f749e1,https://github.com/zfoo-project/zfoo/commit/3aaab2dc214a6c36a46648da24741580e8f749e1,Merge pull request #133 from awakeyoyoyo/main  perf[orm]: persist finish  reset modifiedTime & writeToDbTime
zfoo-project,zfoo,6e76170f95ba0b4abb6d8d2fc0ae2f68cbabde8f,https://github.com/zfoo-project/zfoo/commit/6e76170f95ba0b4abb6d8d2fc0ae2f68cbabde8f,perf[orm]: persist finish  reset modifiedTime & writeToDbTime
zfoo-project,zfoo,628463dab56d4e31797f50f6ba5a14a3969a46eb,https://github.com/zfoo-project/zfoo/commit/628463dab56d4e31797f50f6ba5a14a3969a46eb,perf[map]: get put remove by primitive way
zfoo-project,zfoo,082c2fa5818d199570265bed4732a17fa65b3776,https://github.com/zfoo-project/zfoo/commit/082c2fa5818d199570265bed4732a17fa65b3776,perf[orm]: find thread by threadExecutorMap
zfoo-project,zfoo,6dd5b3ac655f3cd224ccd9fa08b9906400a3327e,https://github.com/zfoo-project/zfoo/commit/6dd5b3ac655f3cd224ccd9fa08b9906400a3327e,perf[id]: catch getIncrementIdFromMongo exception
zfoo-project,zfoo,a4fe83de9a1c01997dce40a3c6871290124184fd,https://github.com/zfoo-project/zfoo/commit/a4fe83de9a1c01997dce40a3c6871290124184fd,perf[cache]: reduce copy objects
zfoo-project,zfoo,1277ab03ea9a5903a6d4122e284806ed35e170c1,https://github.com/zfoo-project/zfoo/commit/1277ab03ea9a5903a6d4122e284806ed35e170c1,perf[cache]: reduce the duplication of objects
zfoo-project,zfoo,d676bc497264e6f6f7b2d91bc20ad95935e56ace,https://github.com/zfoo-project/zfoo/commit/d676bc497264e6f6f7b2d91bc20ad95935e56ace,perf[cache]: check expire cache in all access method
zfoo-project,zfoo,452723508ba993eeb6aae6b3c8f0f244a3527540,https://github.com/zfoo-project/zfoo/commit/452723508ba993eeb6aae6b3c8f0f244a3527540,perf[executor]: use & bits operation to improve performance
zfoo-project,zfoo,a844013eb035caa623c8cdfadf9571b58471fdc1,https://github.com/zfoo-project/zfoo/commit/a844013eb035caa623c8cdfadf9571b58471fdc1,perf[orm]: avoid resize
zfoo-project,zfoo,4bc8650e29e0c075aaf70fcf8303d9896f7f50d2,https://github.com/zfoo-project/zfoo/commit/4bc8650e29e0c075aaf70fcf8303d9896f7f50d2,perf[entity]: persist entity with different version strategy
zfoo-project,zfoo,ee43b08908e43b858ca98c16de3a702e00bc8911,https://github.com/zfoo-project/zfoo/commit/ee43b08908e43b858ca98c16de3a702e00bc8911,perf[entity]: persist entity with different version strategy
zfoo-project,zfoo,3217238c615bbab00f803fd7bdf4550ce3dfbf51,https://github.com/zfoo-project/zfoo/commit/3217238c615bbab00f803fd7bdf4550ce3dfbf51,perf[orm]: optimized cache landing
zfoo-project,zfoo,048eb48e78bf25e8e0622d4a0577989e76c8ee71,https://github.com/zfoo-project/zfoo/commit/048eb48e78bf25e8e0622d4a0577989e76c8ee71,perf[dart]: cut down dart generated code
zfoo-project,zfoo,ca21103bdc2332f8671ff9c87f5890656fa0ab1e,https://github.com/zfoo-project/zfoo/commit/ca21103bdc2332f8671ff9c87f5890656fa0ab1e,Merge pull request #125 from 1281956265/main  perf[orm]: 分布式id生成器优雅写法
zfoo-project,zfoo,25f6c06faf28bc3a878fc0b319d34ed032abe75b,https://github.com/zfoo-project/zfoo/commit/25f6c06faf28bc3a878fc0b319d34ed032abe75b,perf[orm]: 分布式id生成器优雅写法
zfoo-project,zfoo,9a336d8ab56305d48810142c7acf4c02b5ccccce,https://github.com/zfoo-project/zfoo/commit/9a336d8ab56305d48810142c7acf4c02b5ccccce,Merge pull request #124 from 1281956265/main  perf[orm]: for循环解决分布式id生成器创建时duplicate key问题
zfoo-project,zfoo,267cbbbff2a9495ecb28a269faf73d5e8415b633,https://github.com/zfoo-project/zfoo/commit/267cbbbff2a9495ecb28a269faf73d5e8415b633,perf[orm]: for循环解决分布式id生成器创建时duplicate key问题
zfoo-project,zfoo,fd92263427e6f08fc9ed3fdab34100aa3343f130,https://github.com/zfoo-project/zfoo/commit/fd92263427e6f08fc9ed3fdab34100aa3343f130,Merge pull request #123 from 1281956265/main  perf[orm]: 减少分布式id生成器创建时duplicate key问题
zfoo-project,zfoo,7724e83b5774faf2b3a3b7a30e7fb222474f3648,https://github.com/zfoo-project/zfoo/commit/7724e83b5774faf2b3a3b7a30e7fb222474f3648,perf[orm]: 减少分布式id生成器创建时duplicate key问题
zfoo-project,zfoo,beebb398429d7a6bf1bb08055b6fb9cb90ee5f63,https://github.com/zfoo-project/zfoo/commit/beebb398429d7a6bf1bb08055b6fb9cb90ee5f63,perf[protocol]: ruby code style
zfoo-project,zfoo,232b4a641e833b52cdc1b65a38c4f2a0f5640dcd,https://github.com/zfoo-project/zfoo/commit/232b4a641e833b52cdc1b65a38c4f2a0f5640dcd,perf[storage]: custom convert
zfoo-project,zfoo,06898096b0279851fec5b79aa9300c2d1db6a2b3,https://github.com/zfoo-project/zfoo/commit/06898096b0279851fec5b79aa9300c2d1db6a2b3,perf[storage]: catch convert exception
zfoo-project,zfoo,1f7b76dbbbe74e27e2347e2f2444bdef5c4312ac,https://github.com/zfoo-project/zfoo/commit/1f7b76dbbbe74e27e2347e2f2444bdef5c4312ac,perf[storage]: simplified method
zfoo-project,zfoo,921e8bd6a63cc8cbc3ee5ac3c7a1243198064f36,https://github.com/zfoo-project/zfoo/commit/921e8bd6a63cc8cbc3ee5ac3c7a1243198064f36,perf[storage]: array or collection is null safe
zfoo-project,zfoo,a0b6b8c57376719f9fa82c3fb1c8d53b2180088a,https://github.com/zfoo-project/zfoo/commit/a0b6b8c57376719f9fa82c3fb1c8d53b2180088a,perf[orm]: output stack trace with no exception word
zfoo-project,zfoo,57d704057939711080bf05aee88192564e4fbdb2,https://github.com/zfoo-project/zfoo/commit/57d704057939711080bf05aee88192564e4fbdb2,perf[java]: cut down java protocol generation
zfoo-project,zfoo,73aee5c7dec7d00fc325a17e94287fc15d16910a,https://github.com/zfoo-project/zfoo/commit/73aee5c7dec7d00fc325a17e94287fc15d16910a,perf[entity]: check getMethod return type of the id field
zfoo-project,zfoo,08a1e7a8ae2661205a1a1b44a0843022a8f2658a,https://github.com/zfoo-project/zfoo/commit/08a1e7a8ae2661205a1a1b44a0843022a8f2658a,perf[hashmap]: equals and hashcode
zfoo-project,zfoo,3cb3f304df163af4a46cb519dd204108341711f2,https://github.com/zfoo-project/zfoo/commit/3cb3f304df163af4a46cb519dd204108341711f2,perf[hashmap]: ConcurrentHashMapLongObject equals and hashcode
zfoo-project,zfoo,0c0dc62c2b0ff46d9f8121c6d23e57fa1810395f,https://github.com/zfoo-project/zfoo/commit/0c0dc62c2b0ff46d9f8121c6d23e57fa1810395f,perf[cow]: CopyOnWriteHashMap equals and hashcode
zfoo-project,zfoo,62290fb35f295b62c19a13a84b2d0e56c17b3fe5,https://github.com/zfoo-project/zfoo/commit/62290fb35f295b62c19a13a84b2d0e56c17b3fe5,perf[wrapper]: generic type interface
zfoo-project,zfoo,aa8c321ffe3805e515f4eed26ece8d4b918077b2,https://github.com/zfoo-project/zfoo/commit/aa8c321ffe3805e515f4eed26ece8d4b918077b2,perf[persist]: reduce the pressure of concurrent writes to the database
zfoo-project,zfoo,b591864c03ca337bd58d77d56f705157c4a00c26,https://github.com/zfoo-project/zfoo/commit/b591864c03ca337bd58d77d56f705157c4a00c26,perf[executor]: add method to get executor based on hash
zfoo-project,zfoo,5f14eeae831e26661381f99a32d99ecc087ec48d,https://github.com/zfoo-project/zfoo/commit/5f14eeae831e26661381f99a32d99ecc087ec48d,perf[orm]: thread safe entity
zfoo-project,zfoo,8705e28f60f767d4938223ec1b8b972374dad010,https://github.com/zfoo-project/zfoo/commit/8705e28f60f767d4938223ec1b8b972374dad010,perf[orm]: unsafe collection persist batch size
zfoo-project,zfoo,e82f3b3a1c878e1bf0e490ebce969059f94f6757,https://github.com/zfoo-project/zfoo/commit/e82f3b3a1c878e1bf0e490ebce969059f94f6757,perf[orm]: concurrent collection check
zfoo-project,zfoo,6d1fc56a859cdf56a9cc678c2cccc66513a1f1b2,https://github.com/zfoo-project/zfoo/commit/6d1fc56a859cdf56a9cc678c2cccc66513a1f1b2,perf[orm]: concurrent collection check
zfoo-project,zfoo,d4ccd796756742c816e8994f4a5171a474eb2c4c,https://github.com/zfoo-project/zfoo/commit/d4ccd796756742c816e8994f4a5171a474eb2c4c,Merge pull request #117 from awakeyoyoyo/main  perf[orm]: Rewrite collection declaration type check & Support deseri…
zfoo-project,zfoo,0ca1c9d78f85a3dbf8f179e325cccef1fa5c948b,https://github.com/zfoo-project/zfoo/commit/0ca1c9d78f85a3dbf8f179e325cccef1fa5c948b,perf[orm]: fix private id
zfoo-project,zfoo,c5b92d5171399a0c5035ad82c0f70a330d7b8718,https://github.com/zfoo-project/zfoo/commit/c5b92d5171399a0c5035ad82c0f70a330d7b8718,perf[orm]: Rewrite collection declaration type check & Support deserialization into thread safe classes
zfoo-project,zfoo,22d6ad4ee0728caf495c45c61ee6ea7aeaeb1998,https://github.com/zfoo-project/zfoo/commit/22d6ad4ee0728caf495c45c61ee6ea7aeaeb1998,perf[orm]: update entity cache time when call updateNow method
zfoo-project,zfoo,5652fdd3f99aae483d7241308b5c9ef0ce389795,https://github.com/zfoo-project/zfoo/commit/5652fdd3f99aae483d7241308b5c9ef0ce389795,perf[orm]: optimize codec registration
zfoo-project,zfoo,4c20ada20a7df510650d684c89be4e35208dc885,https://github.com/zfoo-project/zfoo/commit/4c20ada20a7df510650d684c89be4e35208dc885,perf[orm]: optimize codec registration
zfoo-project,zfoo,d964049ef735910d06118287851d1e85bf1911ff,https://github.com/zfoo-project/zfoo/commit/d964049ef735910d06118287851d1e85bf1911ff,perf[orm]: map key codec
zfoo-project,zfoo,6b6e653635d4d8ef86dfac5dd46da6a898e7f0d8,https://github.com/zfoo-project/zfoo/commit/6b6e653635d4d8ef86dfac5dd46da6a898e7f0d8,perf[orm]: map key codec
zfoo-project,zfoo,5446f8e4529aa7d709f0129459c1ceb9a1037959,https://github.com/zfoo-project/zfoo/commit/5446f8e4529aa7d709f0129459c1ceb9a1037959,perf[orm]: map type check
zfoo-project,zfoo,f7e6e51edd7d2c2728da3dfd1898da614aa7bfe4,https://github.com/zfoo-project/zfoo/commit/f7e6e51edd7d2c2728da3dfd1898da614aa7bfe4,perf[event]: post exception whatever if the exceptionFunction is override or not
zfoo-project,zfoo,837b6d8e9da35e934b58915c7e6559a9e7938e03,https://github.com/zfoo-project/zfoo/commit/837b6d8e9da35e934b58915c7e6559a9e7938e03,Merge pull request #112 from LucaLq/main  perf[event]: support the reimplementation of exception handling
zfoo-project,zfoo,6e4b4f4205417d52d9174114e5fd24a874d67656,https://github.com/zfoo-project/zfoo/commit/6e4b4f4205417d52d9174114e5fd24a874d67656,perf[event]: support the reimplementation of exception handling
zfoo-project,zfoo,3980af119c9651d4a910d2bfced1c52ad19758f0,https://github.com/zfoo-project/zfoo/commit/3980af119c9651d4a910d2bfced1c52ad19758f0,perf[event]: simple exception handle and remove unnecessary code
zfoo-project,zfoo,f2dfb1433af18eb5d205405cf12a37d65ddd5cae,https://github.com/zfoo-project/zfoo/commit/f2dfb1433af18eb5d205405cf12a37d65ddd5cae,perf[orm]: authSource judgement
zfoo-project,zfoo,cc27c41722f9691f9aa9d6ab723f86859166fc53,https://github.com/zfoo-project/zfoo/commit/cc27c41722f9691f9aa9d6ab723f86859166fc53,perf[orm]: load or create method
selenide,selenide,29487c8db51d7c7000e09a20f8a431aa5142e268,https://github.com/selenide/selenide/commit/29487c8db51d7c7000e09a20f8a431aa5142e268,Cached size to avoid repeated calls in loop condition — improves performance and code clarity.
