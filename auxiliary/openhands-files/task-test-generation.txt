You are at '/workspace', which is the root of the keycloak/keycloak repository. In this repository, the git HEAD is on commit 47288a9, which is the faster version, compared to commit 3cb9e0b, which is a slower version. The project uses the Maven wrapper ./mvnw. The faster version (commit 47288a9) has changed the 'services' module compared to the slower version (commit 3cb9e0b,). The tests of this module live under services/src/test/java.
You can run JUnit tests for this module with:
  ./mvnw -pl services -am test -Dtest=packagename.GeneratedTests -Dsurefire.runOrder=alphabetical -DfailIfNoTests=false -DskipITs
where packagename.GeneratedTests is the fully qualified name of the test class you want to execute.

Note that when running JUnit tests the time spent by a test class, for example with the name packagename.TestClass1, is given in the maven log in a line similar to what follows:
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.048 s -- in packagename.TestClass1

Your task is to generate tests whose execution time on the faster version (commit 47288a9) is lower than their execution time on the slower version (commit 3cb9e0b). Make sure you save your generated tests.

Note that running the test with the './mvnw -pl services -am test -Dsurefire.runOrder=alphabetical -DfailIfNoTests=false -DskipITs -DtrimStackTrace=false -q' command above should usually take less than 300 seconds, including compiling, building, and executing the tests.

Follow this loop:
1) Plan: make a plan of the steps you will take.
2) Implement: identify which classes and methods changed, and focus on those for testing. Maybe you can use 'git diff 3cb9e0b..47288a9 -- services' for this.
3) Design and implement JUnit performance tests that stress the changed methods in the services module. Prefer JUnit over JMH. If you decide to use JMH, also update the relevant pom.xml files so the project still builds and the benchmarks can be run via Maven. The execution time of the generated tests on the faster version should be lower compared to their execution time on the faster version.
4) Verify: after you generate new tests, run the generated tests. If the build or tests fail, fix the issues and re-run until they pass.
5) Summarize: create or update PERFORMANCE_REPORT.md in the repo root. Describe the optimization at a high level, the tests you added, sample timing results comparing the fast and slow versions, and any limitations or caveats.

If possible, also run the same tests on the slower commit 3cb9e0b (for example, by checking out that commit and reusing the new tests) so you can show the timing difference clearly in PERFORMANCE_REPORT.md.
